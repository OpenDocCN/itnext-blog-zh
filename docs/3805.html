<html>
<head>
<title>Understanding Real Time 3D Reconstruction and KinectFusion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解实时三维重建和动态融合</h1>
<blockquote>原文：<a href="https://itnext.io/understanding-real-time-3d-reconstruction-and-kinectfusion-33d61d1cd402?source=collection_archive---------0-----------------------#2020-03-01">https://itnext.io/understanding-real-time-3d-reconstruction-and-kinectfusion-33d61d1cd402?source=collection_archive---------0-----------------------#2020-03-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/45f997fcb60e6e9c8267f325cb7d593a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZmkkPiIYGYmF4TdYCksjw.png"/></div></div></figure><p id="3fe5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">去年，华为的P30 Pro智能手机凭借惊人的徕卡50倍变焦摄像头震惊了世界，然而这一成就掩盖了这款设备的另一个可能同样令人印象深刻的功能:飞行时间(TOF)背部摄像头。事实上，这款TOF相机最初由XBox Kinect machine采用，可用于开发实时深度扫描仪来实时重建模型！由于我很难理解提出这种方法的原始论文KinectFusion(最后的链接)的数学原理，我想分享一下这种3D重建算法的整个流程和方法。尽情享受吧！</p><p id="d5f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意:本文中没有代码片段。整个KinectFusion的数学实现可能是漫长而艰巨的，所以我相信引入数学概念和对数据结构的理解会更好。最后提供了Github上的实际代码。</p><h1 id="d1cd" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">概述</strong></h1><p id="7372" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">三维重建已经成为计算机视觉领域的一个热门研究课题。它的最终目标是基于多幅图像创建3D模型，这些图像可以是基于RGB或深度的。</p><p id="5443" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">KinectFusion是2011年开发的一种较老的传统方法，它试图使用深度图像——给出深度值而不是每个像素的RGB值的图像——作为生成整个3D模型的唯一输入。</p><h1 id="53fd" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">先决条件</strong></h1><p id="8f51" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">这将是我尝试用最简单明了的方式来解释KinectFusion的方法。尽管如此，矩阵代数的一点背景知识，如矩阵加法和乘法，将有助于更快更好地理解。下面是两个必要的基本线性代数概念:</p><p id="1dba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 1。矩阵变换</strong></p><p id="907e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">想象一个有<em class="kw"> x- </em>轴和<em class="kw"> y- </em>轴的2D平面，一个点可以用笛卡尔形式<em class="kw"> (x，y) </em>表示。任何平移都可以用一个向量<em class="kw"> (cx，cy) </em>来表示，一个点平移后会有一个新的坐标<em class="kw"> (x+cx，y+cy) </em>。因此，我们可以使用矩阵计算来表示翻译。同样，旋转也可以通过矩阵来表示，也可以通过旋转矩阵来表示。</p><p id="79f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">把旋转矩阵和平移矩阵结合起来，我们就能推导出一个矩阵方程，把任意一点变换到变换后的新坐标。例如，我们可以将通过<em class="kw"> (x，y) </em>的变换计算出的任意点<em class="kw"> (x₀，y₀) </em>表示如下:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/381ae93da1d251aae7d210ab722aa392.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/1*44kH9Ex0RTlpKevQsvC9Rg.gif"/></div></figure><p id="28bd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="kw"> θ </em>为逆时针旋转的度数，<em class="kw"> cx </em>、<em class="kw"> cy </em>分别为在<em class="kw"> x </em>和<em class="kw"> y- </em>轴上的平移。需要注意的一点是，这个等式先执行旋转，然后执行平移。</p><p id="530b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">同样的想法可以应用于三维空间；矩阵会更复杂，但概念非常相似。</p><p id="c97e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2。矩阵投影</strong></p><p id="1f53" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可以通过以下方式查看真实世界对象在相机图像上的投影:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mf"><img src="../Images/f306071f2e9df0fcfa53e8dad72468a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VpLVOc-j_gB67nNel0tpFQ.png"/></div></div></figure><p id="5f48" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">坐标为<em class="kw"> (u，v) </em>的图像平面上的像素对应于3D对象上的点<em class="kw"> (x，y，z) </em>，并且可以使用相似形状的概念通过这样的投影找到:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/b46d908712b189e444b1acf8f8368b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/1*cY20wWvhpobE8mSIsVY-2g.gif"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">三维坐标与2D坐标的关系方程</figcaption></figure><p id="6c04" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，我们也可以用矩阵来表示这种计算，也就是广为人知的摄像机固有矩阵<em class="kw"> K </em>:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/ea7cf4c91e1554bec375c8055770f6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/1*1PCOO61kOa2cx7UMpxXxkA.gif"/></div></figure><p id="9b95" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="kw"> tx </em>和<em class="kw"> ty </em>是将摄像机置于图像平面中心的偏移量。</p><p id="4e74" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">投影像素坐标<em class="kw"> (u，v) </em>可以通过下式计算:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/d59640696d4c18f9e16a21ff19b366d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/1*x2VOI52p58ESTsaIxBMI5A.gif"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">投影方程</figcaption></figure><p id="487d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="kw"> z </em>为点P <em class="kw"> (x，y，z) </em>的深度(<em class="kw"> z轴</em>长度)。</p><p id="58c8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">记住这两个概念，我们可以深入到实时三维重建的4个步骤。</p><h1 id="1a96" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">程序1 —表面测量</strong></h1><p id="51e8" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">假设我们已经获得了前方物体的深度图。第一件事是通过将深度图中的每个像素转换回其在三维世界中的对应点来计算顶点图和法线图。这可以通过应用于每个像素点的矩阵乘法的逆特性来实现:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/d2a42ab6b9b2cf09cb32708ca5d8ab8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/1*IgaTFp6s7MdwZkVIz-zv9Q.gif"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">重新排列投影方程</figcaption></figure><p id="423f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，通过计算两个相邻顶点的叉积，可以得到一个点的法向量，因为叉积给出了一个与两个顶点都正交的向量。</p><p id="0130" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">同样重要的是要知道，传感器检索的深度图通常容易受到噪声的影响，因此原始论文KinectFusion在每个像素点上应用了双边过滤器，并使用过滤后的深度图像来计算顶点和法线图。简而言之，这种滤波器消除了噪声，但也防止了其他滤波器经常出现的边缘模糊。</p><p id="419c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这一表面测量程序将适用于整个过程中检索到的每一张深度图。顶点和法线贴图是相机姿态估计和构建3D模型的基本元素。</p><p id="18d7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">实施</strong></p><p id="11a8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">获得深度图像后，将像素的位置视为<em class="kw"> (u，v) </em>，深度值视为<em class="kw">z。</em>根据TOF相机设备提供的规格获得<em class="kw"> K </em>，通过等式2计算三维点。对于XBox Kinect机器，<em class="kw"> f = 525.0 </em>，<em class="kw"> tx = 319.5 </em>，<em class="kw">，</em>，<em class="kw"> ty = 239.5。</em>我们可以创建一个名为<em class="kw">点</em>的类，并存储它们的一个向量来表示点云。</p><h1 id="f965" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">程序2——绘图和表面重建</h1><p id="28bc" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">为了解释这个过程，我们将首先介绍一个叫做TSDF的函数(Trucated Signed Distance Function)。为了解释这一功能，让我们来看看下图中的2D TSDF:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/073a94eb7631140a540ca124b43e5d52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*idC5z_qAkPlfaWEXHuqw6w.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">2D TSDF函数</figcaption></figure><p id="14cd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">中间的红色曲线代表物体的表面，在这种情况下，相机将位于上面网格/像素的右边。SDF是一个函数，其中每个像素的值由它从对象表面到相机的距离决定，越接近表面，值越接近0，像素“前面”(更接近相机)的所有值将为正，而“后面”的所有值将为负。TSDF是类似的，除了我们设置一个阈值，如果一个像素离表面太远，它将自动设置为1或-1。这将减少实时重建的实际计算时间，因为我们可以跳过1。</p><p id="91be" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在三维空间中，TSDF的概念保持不变，只是我们为每个体素(3D像素)分配一个值。</p><p id="2d6a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是我们为什么需要TSDF呢？因为TSDF很容易被熔断。如果我们现在在相同的体素中有两个TSDF函数，我们所要做的就是平均每个体素的值，我们将得到一个融合的TSDF。</p><p id="6134" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">实施</strong></p><p id="cebc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了实现TSDF，我们必须想象在摄像机前有假想的体素。例如，我们可以使用捕获的第一帧，并想象在它前面有<em class="kw"> 256x256x256 </em>体素，代表一个<em class="kw"> 2x2x2 </em> m的空间。在程序中，体素可以由具有TSDF值的类<em class="kw">体素</em>来表示。现在我们可以计算出一个体素对应于图像平面上的哪个像素，如果我们要投影它的话。然后，我们可以获得该像素的深度值，计算长度差，并获得TSDF函数。</p><p id="bbbc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将注意到的一件事是，相应的像素实际上可能不是一个整数。在这种情况下，我们必须对体素投影点周围像素的多个深度值进行平均，以获得更好的深度值。</p><p id="4e88" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，对于随后的每个相机帧，如果我们有相机的新姿态，我们可以对每个体素执行相机姿态变换的逆变换，以便体素仍然停留在我们第一次创建它们的相同空间中。简而言之，由于我们转动或移动了相机，相同的体素现在将被投影到不同的像素上。因此，我们可以继续将TSDFs融合在一起以重建模型！(当然，如果我们有相机姿势，这是KinectFusion中最重要的一步。)</p><h1 id="00ba" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">程序3——光线投射</strong></h1><p id="5981" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">这个过程允许我们在任何我们想要的角度可视化TSDF，通过发送假想的光线穿过TSDF来探测点云的表面。</p><p id="7bc7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这一过程中，我们将从每个像素生成一条假想的光线，该光线沿着朝向3D点的方向，通过等式2的反投影来计算。光线从像素的最小深度开始(因为任何具有更小深度的TSDF将具有值1)，以体素长度的步长行进(因此中间没有体素被跳过)，并且当存在过零点时停止(从TSDF的正值变为负值)。当出现从负到正的变化时，或者当它退出我们在计算TSDF时创建的体素空间的边界时，行进也停止。</p><p id="6957" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注意，通过光线投射，我们现在可以得到所有先前TSDFs融合的顶点图。我们还可以通过附近体素的TSDF值的值变化来计算零交叉表面上的梯度，从而计算法线贴图。我们将它们称为全局顶点图和全局法线图，因为它们将在KinectFusion的最后阶段使用——姿势估计——仅通过新的深度图和预先存在的TSDFs来估计每一帧的姿势。</p><p id="25fd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">实施</strong></p><p id="5ff7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以从该特定像素捕获的TSDFS方向上的每个像素计算单位向量。我们从深度0开始发出这条假想的光线，一次移动一个体素长度，并获取光线当前相交的体素的TSDF值。当我们遇到过零区域时(正TSDF值到负)，我们返回体素的3D坐标。</p><p id="efa6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">也可以应用插值，因为交点可能不是每次都在体素的中心，但是这种插值的时间复杂度非常高。另一种方法是只取最近的体素。</p><h1 id="f3b4" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">程序4——姿态估计</strong></h1><p id="fb30" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">姿态估计是KinectFusion算法的最后也是最重要的部分。如果对于拍摄的每个新的深度帧，相机的姿态是已知的，那么我们可以容易地将新的深度帧融合到TSDF中，并且随着更多的帧被捕捉，慢慢地生成3D模型。</p><p id="ed71" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">KinectFusion使用一种称为ICP的方法进行姿势估计。ICP代表迭代最近点，这是一种最小化两个点云之间差异的算法。ICP背后的数学方程可能很复杂，但想法很简单:慢慢地旋转和平移两个匹配点云中的一个，以便两个云最终根据每个点的向量和法线对齐。</p><p id="cc6f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，该ICP算法可以用于比较由TOF相机捕获的新点云与前述全局顶点图。由于捕捉深度图像的帧速率很高，两帧之间的移动将相当小，因此ICP将提供对相机如何从一帧移动到另一帧的相当准确的估计。</p><p id="8a09" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">实施</strong></p><p id="4043" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于新输入的深度图像，我们从<strong class="ka ir">过程1 </strong>中获得一组顶点和法线图。然后，我们通过光线投射获得全局顶点贴图，并使用TSDF内部的梯度获得全局法线贴图。我们通过ICP算法比较法线贴图和顶点贴图(这种比较的完整数学解释可以在KinectFusion论文的第3.5节中找到)，以最终获得相机在拍摄新的深度图像时的旋转和平移。</p><h1 id="b3ad" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">工作流程</h1><p id="37b6" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">因此，程序的总体流程如下:</p><ol class=""><li id="9acd" class="mp mq iq ka b kb kc kf kg kj mr kn ms kr mt kv mu mv mw mx bi translated">我们获得第一张深度图图像，执行<strong class="ka ir">过程1 </strong>和<strong class="ka ir">过程2 </strong>获得顶点和法线图，并将其存储到TSDF中。到目前为止，只获得了一幅图像，因此<strong class="ka ir">程序2 </strong>中的融合部分可以忽略。</li><li id="f58f" class="mp mq iq ka b kb my kf mz kj na kn nb kr nc kv mu mv mw mx bi translated">我们获得新的深度图像，并对该新图像执行<strong class="ka ir">过程1 </strong>，同时用<strong class="ka ir">过程3 </strong>处理TSDF，以获得TSDF和新深度图像的顶点和法线图。</li><li id="064d" class="mp mq iq ka b kb my kf mz kj na kn nb kr nc kv mu mv mw mx bi translated">我们执行<strong class="ka ir">过程4 </strong>以获得两个点云之间的姿态差异，并在转换新深度图像生成的点云的姿态后，通过<strong class="ka ir">过程2 </strong>融合TSDF。</li><li id="249e" class="mp mq iq ka b kb my kf mz kj na kn nb kr nc kv mu mv mw mx bi translated">我们重复步骤2。第三。直到所有深度图像融合在一起形成最终的融合点云。</li></ol><h1 id="aff0" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h1><p id="35e5" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">附件是我正在进行的代码。由于这个程序几乎是完全从头开始构建的，ICP算法中仍然存在一些小错误，希望有一天能够修复。</p><p id="0be1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">代码:</strong></p><p id="d6c2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae nd" href="https://github.com/ttchengab/KinectFusion" rel="noopener ugc nofollow" target="_blank">https://github.com/ttchengab/KinectFusion</a></p><p id="d0c7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您也可以使用TUM RGBD-SLAM数据集测试您自己的KinectFusion程序，该数据集可在以下位置找到:</p><p id="3668" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae nd" href="https://vision.in.tum.de/data/datasets/rgbd-dataset" rel="noopener ugc nofollow" target="_blank">https://vision.in.tum.de/data/datasets/rgbd-dataset</a></p><h1 id="0739" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结束注释</h1><p id="9bef" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">3D重建是一个计算机视觉主题，由于其复杂的数学计算，很少被讨论。希望这篇文章给你一个最有影响力的3D重建方法的基本概述。希望在不久的将来，我可以介绍一些其他的计算机视觉主题或3D重建的深度学习方法！</p><p id="0ee0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw"> KinectFusion论文:</em></p><p id="e9c9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae nd" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf" rel="noopener ugc nofollow" target="_blank">https://www . Microsoft . com/en-us/research/WP-content/uploads/2016/02/ismar 2011 . pdf</a></p><p id="6726" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw">如果你喜欢这篇文章，确保你</em>👏为了让更多的人也有机会阅读它！</p></div></div>    
</body>
</html>