<html>
<head>
<title>Hive on Spark in Kubernetes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kubernetes星火上的蜂巢</h1>
<blockquote>原文：<a href="https://itnext.io/hive-on-spark-in-kubernetes-115c8e9fa5c1?source=collection_archive---------0-----------------------#2020-09-27">https://itnext.io/hive-on-spark-in-kubernetes-115c8e9fa5c1?source=collection_archive---------0-----------------------#2020-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6020153eebe2ad69fecbab9d75fb5db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MIDFC_5UKB7V4WWp"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@williambout?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">威廉·布特</a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="086c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在Kubernetes上运行Hive并不容易。据我所知，Tez是一个hive执行引擎，可以运行在YARN上，而不是Kubernetes上。</p><p id="028e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在Kubernetes上运行Hive还有一个替代方法。Spark可以在Kubernetes上运行，兼容Hive Server2的Spark Thrift Server是一个很好的候选。也就是说，Spark将作为hive执行引擎运行。</p><p id="fc82" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将讨论如何在kubernetes集群上运行Hive on Spark。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="61f9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里提到的所有代码都可以从我的github repo中克隆:<a class="ae kc" href="https://github.com/mykidong/hive-on-spark-in-kubernetes" rel="noopener ugc nofollow" target="_blank">https://github.com/mykidong/hive-on-spark-in-kubernetes</a></p><h1 id="9df8" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">假设S3桶和NFS作为库伯内特存储可用</h1><p id="14f4" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">在kubernetes上运行Hive之前，您的S3存储桶和NFS存储应该可用于您的Kubernetes集群。</p><p id="e1a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您的S3桶将用于存储上传的spark依赖jar、hive表数据等。</p><p id="1c20" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">NFS存储将用于支持启动作业所需的PVC <code class="fe ml mm mn mo b">ReadWriteMany</code>访问模式。</p><p id="c523" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您没有这样的S3桶和NFS可用，您可以像我一样在您的kubernetes集群上手动安装它们:</p><ul class=""><li id="0a54" class="mp mq iq kf b kg kh kk kl ko mr ks ms kw mt la mu mv mw mx bi translated">MinIO Direct CSI:<a class="ae kc" href="https://github.com/minio/direct-csi" rel="noopener ugc nofollow" target="_blank">https://github.com/minio/direct-csi</a></li><li id="b625" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">https://github.com/minio/operator的迷你S3对象存储:<a class="ae kc" href="https://github.com/minio/operator" rel="noopener ugc nofollow" target="_blank"/></li><li id="97c5" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">NFS:<a class="ae kc" href="https://github.com/helm/charts/tree/master/stable/nfs-server-provisioner" rel="noopener ugc nofollow" target="_blank">https://github . com/helm/charts/tree/master/stable/NFS-server-provisioner</a></li></ul><h1 id="84f7" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">安装配置单元Metastore</h1><p id="9bec" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">Spark Thrift Server作为Hive Server2需要Hive metastore。为了在kubernetes上安装hive metastore，我引用了<a class="ae kc" href="https://github.com/joshuarobinson/presto-on-k8s/tree/master/hive_metastore" rel="noopener ugc nofollow" target="_blank">这个链接</a>。</p><p id="4e1d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Hive metastore需要mysql来存储元数据。<code class="fe ml mm mn mo b">hive-metastore/mysql.yaml</code>看起来是这样的:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="ed56" class="nl lj iq mo b gy nm nn l no np">---<br/>apiVersion: v1<br/>kind: Secret<br/>metadata:<br/>  name: mysql-secrets<br/>  namespace: my-namespace<br/>type: Opaque<br/>data:<br/>  ROOT_PASSWORD: aWNhcnVzMDMzNw==<br/>---<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: mysql-data-disk<br/>  namespace: my-namespace<br/>spec:<br/>  accessModes:<br/>    - ReadWriteOnce<br/>  resources:<br/>    requests:<br/>      storage: 2Gi<br/>  storageClassName: direct.csi.min.io<br/>---<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: mysql-deployment<br/>  namespace: my-namespace<br/>  labels:<br/>    app: mysql<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      app: mysql<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: mysql<br/>    spec:<br/>      containers:<br/>        - name: mysql<br/>          image: mysql:5.7<br/>          ports:<br/>            - containerPort: 3306<br/>          volumeMounts:<br/>            - mountPath: "/var/lib/mysql"<br/>              subPath: "mysql"<br/>              name: mysql-data<br/>          env:<br/>            - name: MYSQL_ROOT_PASSWORD<br/>              valueFrom:<br/>                secretKeyRef:<br/>                  name: mysql-secrets<br/>                  key: ROOT_PASSWORD<br/>      volumes:<br/>        - name: mysql-data<br/>          persistentVolumeClaim:<br/>            claimName: mysql-data-disk<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: mysql-service<br/>  namespace: my-namespace<br/>spec:<br/>  selector:<br/>    app: mysql<br/>  ports:<br/>    - protocol: TCP<br/>      port: 3306<br/>      targetPort: 3306</span></pre><p id="0535" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看一下PVC存储“Storage class name:direct . CSI . min . io ”,它应该被删除或更改以适合您的kubernetes集群。</p><p id="bcef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建mysql后，将运行hive metastore初始化作业，为Hive Metastore创建数据库和表。让我们看看<code class="fe ml mm mn mo b">hive-metastore/init-schema.yaml</code>:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="03fe" class="nl lj iq mo b gy nm nn l no np">apiVersion: batch/v1<br/>kind: Job<br/>metadata:<br/>  name: hive-initschema<br/>  namespace: my-namespace<br/>spec:<br/>  template:<br/>    spec:<br/>      affinity:<br/>        podAntiAffinity:<br/>          requiredDuringSchedulingIgnoredDuringExecution:<br/>            - labelSelector:<br/>                matchExpressions:<br/>                  - key: app<br/>                    operator: In<br/>                    values:<br/>                      - mysql<br/>              topologyKey: kubernetes.io/hostname<br/>      containers:<br/>        - name: hivemeta<br/>          image: mykidong/hivemetastore:v3.0.0<br/>          command: ["/opt/hive-metastore/bin/schematool"]<br/>          args: ["--verbose" ,"-initSchema" , "-dbType", "mysql" , "-userName", "root",<br/>                 "-passWord", "icarus0337" , "-url", "jdbc:mysql://mysql-service.my-namespace.svc.cluster.local:3306/metastore_db?createDatabaseIfNotExist=true&amp;connectTimeout=1000"]<br/>      restartPolicy: Never<br/>  backoffLimit: 4</span></pre><p id="eef3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，配置Hive metastore hadoop站点xml配置，参见<code class="fe ml mm mn mo b">hive-metastore/core-site.xml</code>和<code class="fe ml mm mn mo b">hive-metastore/metastore-site.xml</code>:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="1470" class="nl lj iq mo b gy nm nn l no np">&lt;configuration&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.s3a.connection.ssl.enabled&lt;/name&gt;<br/>       &lt;value&gt;true&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.defaultFS&lt;/name&gt;<br/>        &lt;value&gt;s3a://mykidong&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;<br/>        &lt;value&gt;true&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.s3a.access.key&lt;/name&gt;<br/>        &lt;value&gt;my-access-key&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;<br/>        &lt;value&gt;my-secret-key&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.s3a.impl&lt;/name&gt;<br/>        &lt;value&gt;org.apache.hadoop.fs.s3a.S3AFileSystem&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;<br/>        &lt;value&gt;https://s3-endpoint&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.s3a.fast.upload&lt;/name&gt;<br/>        &lt;value&gt;true&lt;/value&gt;<br/>    &lt;/property&gt;<br/>&lt;/configuration&gt;</span><span id="24a1" class="nl lj iq mo b gy nq nn l no np">..............</span><span id="ac9a" class="nl lj iq mo b gy nq nn l no np">&lt;configuration&gt;<br/>   &lt;property&gt;<br/>      &lt;name&gt;metastore.task.threads.always&lt;/name&gt;<br/>      &lt;value&gt;org.apache.hadoop.hive.metastore.events.EventCleanerTask&lt;/value&gt;<br/>   &lt;/property&gt;<br/>   &lt;property&gt;<br/>      &lt;name&gt;metastore.expression.proxy&lt;/name&gt;<br/>      &lt;value&gt;org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy&lt;/value&gt;<br/>   &lt;/property&gt;<br/>   &lt;property&gt;<br/>      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br/>      &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;<br/>   &lt;/property&gt;<br/>   &lt;property&gt;<br/>      &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br/>      &lt;value&gt;jdbc:mysql://mysql-service.my-namespace.svc.cluster.local:3306/metastore_db?useSSL=false&lt;/value&gt;<br/>   &lt;/property&gt;<br/>   &lt;property&gt;<br/>      &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br/>      &lt;value&gt;root&lt;/value&gt;<br/>   &lt;/property&gt;<br/>   &lt;property&gt;<br/>      &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br/>      &lt;value&gt;icarus0337&lt;/value&gt;<br/>   &lt;/property&gt;<br/>   &lt;property&gt;<br/>      &lt;name&gt;metastore.warehouse.dir&lt;/name&gt;<br/>      &lt;value&gt;s3a://mykidong/warehouse/&lt;/value&gt;<br/>   &lt;/property&gt;<br/>   &lt;property&gt;<br/>      &lt;name&gt;metastore.thrift.port&lt;/name&gt;<br/>      &lt;value&gt;9083&lt;/value&gt;<br/>   &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="0788" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您必须更改s3相关的属性以适应您的环境。</p><p id="87a0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">配置配置单元metastore站点xml后，可以使用清单运行配置单元metastore，<code class="fe ml mm mn mo b">hive-metastore/metastore.yaml</code>:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="9461" class="nl lj iq mo b gy nm nn l no np">apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: metastore<br/>  namespace: my-namespace<br/>spec:<br/>  ports:<br/>  - port: 9083<br/>  selector:<br/>    app: metastore<br/>---<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: metastore<br/>  namespace: my-namespace<br/>spec:<br/>  selector:<br/>    matchLabels:<br/>      app: metastore<br/>  strategy:<br/>    type: Recreate<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: metastore<br/>    spec:<br/>      containers:<br/>      - name: metastore<br/>        image: mykidong/hivemetastore:v3.0.0<br/>        env:<br/>        - name: AWS_ACCESS_KEY_ID<br/>          valueFrom:<br/>            secretKeyRef:<br/>              name: my-s3-keys<br/>              key: access-key<br/>        - name: AWS_SECRET_ACCESS_KEY<br/>          valueFrom:<br/>            secretKeyRef:<br/>              name: my-s3-keys<br/>              key: secret-key<br/>        ports:<br/>        - containerPort: 9083<br/>        volumeMounts:<br/>        - name: metastore-cfg-vol<br/>          mountPath: /opt/hive-metastore/conf/metastore-site.xml<br/>          subPath: metastore-site.xml<br/>        - name: metastore-cfg-vol<br/>          mountPath: /opt/hadoop/etc/hadoop/core-site.xml<br/>          subPath: core-site.xml<br/>        command: ["/opt/hive-metastore/bin/start-metastore"]<br/>        args: ["-p", "9083"]<br/>        resources:<br/>          requests:<br/>            memory: "2G"<br/>            cpu: 2<br/>        imagePullPolicy: Always<br/>      volumes:<br/>        - name: metastore-cfg-vol<br/>          configMap:<br/>            name: metastore-cfg</span></pre><p id="5390" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要运行installing hive metastore all in one，请参见shell脚本<code class="fe ml mm mn mo b">hive-metastore/create.sh</code>:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="fd3a" class="nl lj iq mo b gy nm nn l no np"><strong class="mo ir">#!/bin/sh<br/><br/></strong>export MY_NAMESPACE=my-namespace<br/><br/>kubectl create namespace ${MY_NAMESPACE};<br/><br/># create mysql server.<br/>kubectl apply -f mysql.yaml;<br/><br/># wait for mysql pod being ready.<br/>while [[ $(kubectl get pods -n ${MY_NAMESPACE} -l app=mysql -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True" ]]; do echo "waiting for mysql pod being ready" &amp;&amp; sleep 1; done<br/><br/># Update configmaps<br/>kubectl create configmap metastore-cfg --dry-run --from-file=metastore-site.xml --from-file=core-site.xml -o yaml -n ${MY_NAMESPACE} | kubectl apply -f -<br/><br/># create secret for aws keys.<br/>kubectl create secret generic my-s3-keys --from-literal=access-key='my-access-key' --from-literal=secret-key='my-secret-key' -n ${MY_NAMESPACE};<br/><br/># create db schemas.<br/>kubectl apply -f init-schema.yaml;<br/><br/># create metastore.<br/>kubectl apply -f metastore.yaml;<br/><br/># wait for finishing creating schemas.<br/>while [[ $(kubectl get pods -n ${MY_NAMESPACE} -l job-name=hive-initschema -o jsonpath={..status.phase}) != *"Succeeded"* ]]; do echo "waiting for finishing init schema job" &amp;&amp; sleep 2; done<br/><br/># restart hive metastore: not efficient, but...<br/>kubectl rollout restart deployment.apps/metastore -n ${MY_NAMESPACE};</span></pre><p id="c318" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，运行创建shell:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="9f8e" class="nl lj iq mo b gy nm nn l no np">cd hive-metastore;<br/>./create.sh;</span></pre><h1 id="d8cf" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">安装火花SA，RBAC，聚氯乙烯</h1><p id="30e6" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">因为spark thrift server是一个spark作业，它需要服务帐户、角色、角色绑定、<code class="fe ml mm mn mo b">ReadWriteMany</code>支持的PVC才能在kubernetes上运行，所以在将Spark Thrift Server作为hive server2运行之前，这样的服务帐户、RBAC和用于Spark作业的PVC应该是可用的。</p><p id="9849" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看<code class="fe ml mm mn mo b">spark-env/rbac-pvc.yaml</code>:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="1fb0" class="nl lj iq mo b gy nm nn l no np">---<br/>kind: ServiceAccount<br/>apiVersion: v1<br/>metadata:<br/>  namespace: my-namespace<br/>  name: spark<br/>---<br/>kind: Role<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  namespace: my-namespace<br/>  name: spark-role<br/>rules:<br/>  - apiGroups: [""]<br/>    resources: ["pods"]<br/>    verbs: ["get", "watch", "list", "create", "update", "patch", "delete"]<br/>  - apiGroups:<br/>      - ""<br/>    resources:<br/>      - secrets<br/>    verbs:<br/>      - get<br/>      - list<br/>  - apiGroups:<br/>      - ""<br/>    resources:<br/>      - events<br/>    verbs:<br/>      - list<br/>      - watch<br/>      - create<br/>      - update<br/>      - patch<br/>  - apiGroups:<br/>      - ""<br/>    resources:<br/>      - nodes<br/>    verbs:<br/>      - get<br/>      - list<br/>      - update<br/>      - watch<br/>  - apiGroups:<br/>      - ""<br/>    resources:<br/>      - namespaces<br/>    verbs:<br/>      - get<br/>      - list<br/>  - apiGroups:<br/>      - storage.k8s.io<br/>    resources:<br/>      - storageclasses<br/>    verbs:<br/>      - get<br/>      - list<br/>      - watch<br/>  - apiGroups:<br/>      - ""<br/>    resources:<br/>      - persistentvolumeclaims<br/>    verbs:<br/>      - get<br/>      - list<br/>      - watch<br/>      - update<br/>  - apiGroups:<br/>      - ""<br/>    resources:<br/>      - persistentvolumes<br/>    verbs:<br/>      - get<br/>      - list<br/>      - watch<br/>      - update<br/>      - create<br/>  - apiGroups:<br/>      - storage.k8s.io<br/>    resources:<br/>      - volumeattachments<br/>    verbs:<br/>      - get<br/>      - list<br/>      - watch<br/>      - update<br/>---<br/>kind: RoleBinding<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  name: spark-rolebinding<br/>  namespace: my-namespace<br/>subjects:<br/>  - kind: ServiceAccount<br/>    name: spark<br/>roleRef:<br/>  kind: Role<br/>  name: spark-role<br/>  apiGroup: rbac.authorization.k8s.io<br/>---<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: spark-driver-pvc<br/>  namespace: my-namespace<br/>  labels: {}<br/>  annotations: {}<br/>spec:<br/>  accessModes:<br/>    - ReadWriteMany<br/>  resources:<br/>    requests:<br/>      storage: 2Gi<br/>  storageClassName: nfs<br/>---<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: spark-exec-pvc<br/>  namespace: my-namespace<br/>  labels: {}<br/>  annotations: {}<br/>spec:<br/>  accessModes:<br/>    - ReadWriteMany<br/>  resources:<br/>    requests:<br/>      storage: 50Gi<br/>  storageClassName: nfs<br/>---<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: spark-driver-localdir-pvc<br/>  namespace: my-namespace<br/>  labels: {}<br/>  annotations: {}<br/>spec:<br/>  accessModes:<br/>    - ReadWriteMany<br/>  resources:<br/>    requests:<br/>      storage: 2Gi<br/>  storageClassName: nfs<br/>---<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: spark-exec-localdir-pvc<br/>  namespace: my-namespace<br/>  labels: {}<br/>  annotations: {}<br/>spec:<br/>  accessModes:<br/>    - ReadWriteMany<br/>  resources:<br/>    requests:<br/>      storage: 50Gi<br/>  storageClassName: nfs</span></pre><p id="38a0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PVC接入模式必须是NFS支持的<code class="fe ml mm mn mo b">ReadWriteMany</code>。</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="534f" class="nl lj iq mo b gy nm nn l no np">spec:<br/>  accessModes:<br/>    - ReadWriteMany</span></pre><p id="69ff" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看一下存储类“storageClassName: nfs ”,应该对其进行更改以适应您的kubernetes集群。</p><p id="d750" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，使用以下命令创建spark sa、pvc和role，rolebinding:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="8a36" class="nl lj iq mo b gy nm nn l no np">cd spark-env;</span><span id="24c4" class="nl lj iq mo b gy nq nn l no np">kubectl apply -f . ;</span></pre><h1 id="73f7" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">安装Spark节俭服务器</h1><p id="3e79" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">您可以下载预构建的spark版本来使用，但是我不打算使用预构建的spark包。因为想和<code class="fe ml mm mn mo b">3.2.0</code>的版本有hadoop依赖，所以要从源代码重新构建spark。让我们按照下面的步骤来重建spark:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="e344" class="nl lj iq mo b gy nm nn l no np"># build spark with source.<br/>export SPARK_VERSION=3.0.0;<br/>cd spark;<br/>git clone <a class="ae kc" href="https://github.com/apache/spark.git" rel="noopener ugc nofollow" target="_blank">https://github.com/apache/spark.git</a>;<br/>cd spark;<br/>git checkout v$SPARK_VERSION;</span><span id="51c0" class="nl lj iq mo b gy nq nn l no np">export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=1g";</span><span id="2d39" class="nl lj iq mo b gy nq nn l no np"># build.<br/>./dev/make-distribution.sh --name custom-spark --pip --tgz -DskipTests=true -Dhadoop.version=3.2.0 -Phive -Phive-thriftserver -Pkubernetes;</span></pre><p id="0ee5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是建立火花真的需要很长时间。好在我已经建好了，带hadoop 3.2.0的spark包可以从我的google drive下载。</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="ca7b" class="nl lj iq mo b gy nm nn l no np"># download spark tar file from google drive.<br/># https://drive.google.com/file/d/1_hpk6p_mgQ3gCA3ZV_cSUuEdt_yZlAaX/view?usp=sharing<br/>SPARK_FILE_NAME=spark-3.0.0-bin-custom-spark<br/>fileId=1_hpk6p_mgQ3gCA3ZV_cSUuEdt_yZlAaX<br/>fileName=${SPARK_FILE_NAME}.tgz<br/><br/>curl -sc /tmp/cookie "https://drive.google.com/uc?export=download&amp;id=${fileId}" &gt; /dev/null<br/>code="$(awk '/_warning_/ {print $NF}' /tmp/cookie)"<br/>curl -Lb /tmp/cookie "https://drive.google.com/uc?export=download&amp;confirm=${code}&amp;id=${fileId}" -o ${fileName}</span></pre><p id="7b0c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有一件事对我们来说是必要的，那就是建立spark的docker映像，让我们建立spark docker映像，它将用于运行spark thrift server和稍后的另一个spark作业:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="ea7e" class="nl lj iq mo b gy nm nn l no np"># create spark docker image.<br/>export SPARK_VERSION=3.0.0;<br/>cd $SPARK_HOME;</span><span id="39bd" class="nl lj iq mo b gy nq nn l no np">bin/docker-image-tool.sh -r your-repo -t v$SPARK_VERSION build;<br/>bin/docker-image-tool.sh -r your-repo -t v$SPARK_VERSION push;</span></pre><p id="1f90" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，几乎可以安装spark thrift server了，让我们创建jdbc客户端可以连接的spark thrift server服务:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="766c" class="nl lj iq mo b gy nm nn l no np">kind: Service<br/>apiVersion: v1<br/>metadata:<br/>  name: spark-thrift-server-service<br/>  namespace: my-namespace<br/>spec:<br/>  type: LoadBalancer<br/>  selector:<br/>    spark-role: driver<br/>  ports:<br/>    - name: jdbc-port<br/>      port: 10016<br/>      protocol: TCP<br/>      targetPort: 10016</span></pre><p id="9725" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Spark submit不允许默认的spark thrift服务器在kubernetes上以集群模式运行。这里有一个技巧来避免这种情况，我写了一个简单的包装类，其中将调用spark thrift server，让我们看看包装类“SparkThriftServerRunner ”:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="35f3" class="nl lj iq mo b gy nm nn l no np">package io.mykidong.hive;<br/><br/>public class SparkThriftServerRunner {<br/><br/>    public static void main(String[] args) {<br/>     org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.<em class="nr">main</em>(args);<br/><br/> while (true) {<br/>    try {<br/>        Thread.<em class="nr">sleep</em>(Long.<em class="nr">MAX_VALUE</em>);<br/>    } catch (Exception e) {<br/>        e.printStackTrace();<br/>    }<br/>}</span><span id="7c51" class="nl lj iq mo b gy nq nn l no np"><br/>    }<br/>}</span></pre><p id="272f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个类将被调用来在spark submit中运行spark thrift server，如下所示:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="04ac" class="nl lj iq mo b gy nm nn l no np">spark-submit \<br/>--master $MASTER \<br/>--deploy-mode cluster \<br/>--name spark-thrift-server \<br/>--class io.mykidong.hive.SparkThriftServerRunner \<br/>...</span></pre><p id="3403" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当你用maven构建为这个博客提供的源代码时,<code class="fe ml mm mn mo b">org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde</code>存在依赖性问题。下面的maven的服务器配置要加在<code class="fe ml mm mn mo b">settings.xml</code>里。</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="84ee" class="nl lj iq mo b gy nm nn l no np">&lt;servers&gt;<br/>   &lt;server&gt;<br/>      &lt;id&gt;pentaho-public&lt;/id&gt;<br/>      &lt;username&gt;devreaduser&lt;/username&gt;<br/>      &lt;password&gt;{zIMyJWfHKfoHiBJAVsAgW4E5BcJzR+nhTtgPy0J+/rs=}&lt;/password&gt;<br/>   &lt;/server&gt;<br/>&lt;/servers&gt;</span></pre><p id="3e3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要构建spark thrift server uber jar，请在<code class="fe ml mm mn mo b">examples/spark-thrift-server</code>中键入以下命令:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="4272" class="nl lj iq mo b gy nm nn l no np">mvn -e -DskipTests=true clean install shade:shade;</span></pre><p id="64ee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，spark thrift server只是运行在kubernetes上的一个spark作业，让我们在kubernetes上的<code class="fe ml mm mn mo b">cluster mode</code>中看到spark提交来运行spark thrift server。</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="64ab" class="nl lj iq mo b gy nm nn l no np"># submit spark thrift server job.<br/>MASTER=k8s://your-k8s-master-url;<br/>NAMESPACE=${MY_NAMESPACE};<br/>ENDPOINT=https://s3-endpoint;<br/>BUCKET=mykidong;<br/>HIVE_METASTORE=metastore:9083;<br/><br/>spark-submit \<br/>--master $MASTER \<br/>--deploy-mode cluster \<br/>--name spark-thrift-server \<br/>--class io.mykidong.hive.SparkThriftServerRunner \<br/>--packages com.amazonaws:aws-java-sdk-s3:1.11.375,org.apache.hadoop:hadoop-aws:3.2.0 \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-driver-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-exec-pvc \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-driver-localdir-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-exec-localdir-pvc \<br/>--conf spark.kubernetes.file.upload.path=s3a://${BUCKET}/spark-thrift-server \<br/>--conf spark.kubernetes.container.image.pullPolicy=Always \<br/>--conf spark.kubernetes.namespace=$NAMESPACE \<br/>--conf spark.kubernetes.container.image=mykidong/spark:v3.0.0 \<br/>--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \<br/>--conf spark.hadoop.hive.metastore.client.connect.retry.delay=5 \<br/>--conf spark.hadoop.hive.metastore.client.socket.timeout=1800 \<br/>--conf spark.hadoop.hive.metastore.uris=thrift://${HIVE_METASTORE} \<br/>--conf spark.hadoop.hive.server2.enable.doAs=false \<br/>--conf spark.hadoop.hive.server2.thrift.http.port=10002 \<br/>--conf spark.hadoop.hive.server2.thrift.port=10016 \<br/>--conf spark.hadoop.hive.server2.transport.mode=binary \<br/>--conf spark.hadoop.metastore.catalog.default=spark \<br/>--conf spark.hadoop.hive.execution.engine=spark \<br/>--conf spark.hadoop.hive.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.hadoop.hive.tez.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.sql.warehouse.dir=s3a:/${BUCKET}/apps/spark/warehouse \<br/>--conf spark.hadoop.fs.defaultFS=s3a://${BUCKET} \<br/>--conf spark.hadoop.fs.s3a.access.key=my-access-key \<br/>--conf spark.hadoop.fs.s3a.secret.key=my-secret-key \<br/>--conf spark.hadoop.fs.s3a.connection.ssl.enabled=true \<br/>--conf spark.hadoop.fs.s3a.endpoint=$ENDPOINT \<br/>--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \<br/>--conf spark.hadoop.fs.s3a.fast.upload=true \<br/>--conf spark.hadoop.fs.s3a.path.style.access=true \<br/>--conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \<br/>--conf spark.executor.instances=2 \<br/>--conf spark.executor.memory=2G \<br/>--conf spark.executor.cores=1 \<br/>--conf spark.driver.memory=1G \<br/>--conf spark.jars=${tempDirectory}/${DELTA_CORE_FILE_NAME}.jar,${tempDirectory}/${HIVE_DELTA_FILE_NAME}.jar \<br/>file://&lt;src&gt;/examples/spark-thrift-server/target/spark-thrift-server-1.0.0-SNAPSHOT-spark-job.jar \<br/>&gt; /dev/null 2&gt;&amp;1 &amp;</span></pre><p id="739f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您必须用源目录的完整路径替换<src>。</src></p><p id="31aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看看S3相关属性、Kubernetes主URL、Hive Metastore端点的配置，应该对其进行更改以满足您的需求。</p><p id="61c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以找到PVC的几种火花配置，这是火花驱动器和执行器保存临时数据所必需的:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="790b" class="nl lj iq mo b gy nm nn l no np">...<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-driver-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-exec-pvc \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-driver-localdir-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-exec-localdir-pvc \<br/>...</span></pre><p id="c114" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果spark作业被提交，首先，依赖jar文件将被上传到上面配置的s3桶，然后，spark驱动程序和执行器将从S3桶下载上传的依赖jar，并动态地将它们添加到自己的类加载器中。</p><p id="c30d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看运行spark thrift server的完整shell脚本。</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="123b" class="nl lj iq mo b gy nm nn l no np"><strong class="mo ir">#!/bin/sh<br/><br/></strong>export MY_NAMESPACE=my-namespace<br/>export tempDirectory=~/temp/spark;<br/>mkdir -p ${tempDirectory};<br/><br/>cd ${tempDirectory};</span><span id="f826" class="nl lj iq mo b gy nq nn l no np"># download spark tar file from google drive.<br/># https://drive.google.com/file/d/1_hpk6p_mgQ3gCA3ZV_cSUuEdt_yZlAaX/view?usp=sharing<br/>SPARK_FILE_NAME=spark-3.0.0-bin-custom-spark<br/>fileId=1_hpk6p_mgQ3gCA3ZV_cSUuEdt_yZlAaX<br/>fileName=${SPARK_FILE_NAME}.tgz<br/><br/>curl -sc /tmp/cookie "https://drive.google.com/uc?export=download&amp;id=${fileId}" &gt; /dev/null<br/>code="$(awk '/_warning_/ {print $NF}' /tmp/cookie)"<br/>curl -Lb /tmp/cookie "https://drive.google.com/uc?export=download&amp;confirm=${code}&amp;id=${fileId}" -o ${fileName}<br/><br/><br/># download delta core jar file from google drive.<br/># https://drive.google.com/file/d/1WCzSnwXEYc3Q8VkvvJ5nuidq9yGYDIsa/view?usp=sharing<br/>DELTA_CORE_FILE_NAME=delta-core-shaded-assembly_2.12-0.1.0<br/>fileId=1WCzSnwXEYc3Q8VkvvJ5nuidq9yGYDIsa<br/>fileName=${DELTA_CORE_FILE_NAME}.jar<br/><br/>curl -sc /tmp/cookie "https://drive.google.com/uc?export=download&amp;id=${fileId}" &gt; /dev/null<br/>code="$(awk '/_warning_/ {print $NF}' /tmp/cookie)"<br/>curl -Lb /tmp/cookie "https://drive.google.com/uc?export=download&amp;confirm=${code}&amp;id=${fileId}" -o ${fileName}<br/><br/><br/><br/># download hive delta file from google drive.<br/># https://drive.google.com/file/d/1PcSraIo9Fc5sKIuDmDfFytcE8i5DcgN_/view?usp=sharing<br/>HIVE_DELTA_FILE_NAME=hive-delta_2.12-0.1.0<br/>fileId=1PcSraIo9Fc5sKIuDmDfFytcE8i5DcgN_<br/>fileName=${HIVE_DELTA_FILE_NAME}.jar<br/><br/>curl -sc /tmp/cookie "https://drive.google.com/uc?export=download&amp;id=${fileId}" &gt; /dev/null<br/>code="$(awk '/_warning_/ {print $NF}' /tmp/cookie)"<br/>curl -Lb /tmp/cookie "https://drive.google.com/uc?export=download&amp;confirm=${code}&amp;id=${fileId}" -o ${fileName}<br/><br/># install spark.<br/>mkdir -p spark;<br/>tar zxvf ${SPARK_FILE_NAME}.tgz -C spark/;<br/>cd spark/;<br/>cp -R ${SPARK_FILE_NAME}/* .;<br/>rm -rf ${SPARK_FILE_NAME};<br/><br/># set spark home.<br/>SPARK_HOME=${tempDirectory}/spark;<br/>PATH=$PATH:$SPARK_HOME/bin;<br/><br/><br/>cd ${tempDirectory};<br/><br/># submit spark thrift server job.<br/>MASTER=k8s://your-k8s-master-url;<br/>NAMESPACE=${MY_NAMESPACE};<br/>ENDPOINT=https://s3-endpoint;<br/>BUCKET=mykidong;<br/>HIVE_METASTORE=metastore:9083;<br/><br/>spark-submit \<br/>--master $MASTER \<br/>--deploy-mode cluster \<br/>--name spark-thrift-server \<br/>--class io.mykidong.hive.SparkThriftServerRunner \<br/>--packages com.amazonaws:aws-java-sdk-s3:1.11.375,org.apache.hadoop:hadoop-aws:3.2.0 \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-driver-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-exec-pvc \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-driver-localdir-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-exec-localdir-pvc \<br/>--conf spark.kubernetes.file.upload.path=s3a://${BUCKET}/spark-thrift-server \<br/>--conf spark.kubernetes.container.image.pullPolicy=Always \<br/>--conf spark.kubernetes.namespace=$NAMESPACE \<br/>--conf spark.kubernetes.container.image=mykidong/spark:v3.0.0 \<br/>--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \<br/>--conf spark.hadoop.hive.metastore.client.connect.retry.delay=5 \<br/>--conf spark.hadoop.hive.metastore.client.socket.timeout=1800 \<br/>--conf spark.hadoop.hive.metastore.uris=thrift://${HIVE_METASTORE} \<br/>--conf spark.hadoop.hive.server2.enable.doAs=false \<br/>--conf spark.hadoop.hive.server2.thrift.http.port=10002 \<br/>--conf spark.hadoop.hive.server2.thrift.port=10016 \<br/>--conf spark.hadoop.hive.server2.transport.mode=binary \<br/>--conf spark.hadoop.metastore.catalog.default=spark \<br/>--conf spark.hadoop.hive.execution.engine=spark \<br/>--conf spark.hadoop.hive.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.hadoop.hive.tez.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.sql.warehouse.dir=s3a:/${BUCKET}/apps/spark/warehouse \<br/>--conf spark.hadoop.fs.defaultFS=s3a://${BUCKET} \<br/>--conf spark.hadoop.fs.s3a.access.key=my-access-key \<br/>--conf spark.hadoop.fs.s3a.secret.key=my-secret-key \<br/>--conf spark.hadoop.fs.s3a.connection.ssl.enabled=true \<br/>--conf spark.hadoop.fs.s3a.endpoint=$ENDPOINT \<br/>--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \<br/>--conf spark.hadoop.fs.s3a.fast.upload=true \<br/>--conf spark.hadoop.fs.s3a.path.style.access=true \<br/>--conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \<br/>--conf spark.executor.instances=2 \<br/>--conf spark.executor.memory=2G \<br/>--conf spark.executor.cores=1 \<br/>--conf spark.driver.memory=1G \<br/>--conf spark.jars=${tempDirectory}/${DELTA_CORE_FILE_NAME}.jar,${tempDirectory}/${HIVE_DELTA_FILE_NAME}.jar \<br/>file://&lt;src&gt;/examples/spark-thrift-server/target/spark-thrift-server-1.0.0-SNAPSHOT-spark-job.jar \<br/>&gt; /dev/null 2&gt;&amp;1 &amp;<br/><br/>PID=$!<br/>echo "$PID" &gt; pid<br/><br/># check if spark thrift server pod is running.<br/><br/>SPARK_THRIFT_SERVER_IS_RUNNING="False";<br/>check_spark_thrift_server_is_running() {<br/>    POD_STATUS=$(kubectl get pods -n ${MY_NAMESPACE} -l spark-role=driver -o jsonpath={..status.phase});<br/>    POD_NAME=$(kubectl get pods -n ${MY_NAMESPACE} -l spark-role=driver -o jsonpath={..metadata.name});<br/>    <br/>    if [[ ${POD_STATUS} != "" ]]<br/>    then<br/>      # Set space as the delimiter<br/>      IFS=' ';<br/>  <br/>      #Read the split words into an array based on space delimiter<br/>      read -a POD_STATUS_ARRAY &lt;&lt;&lt; "$POD_STATUS";<br/>      read -a POD_NAME_ARRAY &lt;&lt;&lt; "$POD_NAME";<br/>  <br/>      for ((i = 0; i &lt; ${#POD_STATUS_ARRAY[@]}; ++i)); do<br/>          pod_status=${POD_STATUS_ARRAY[i]};<br/>          pod_name=${POD_NAME_ARRAY[i]};<br/>          printf "status: %s, name: %s\n" "${pod_status}" "${pod_name}";<br/>  <br/>          if [[ $pod_status == "Running" ]]<br/>          then<br/>              if [[ $pod_name =~ "spark-thrift-server" ]]<br/>              then<br/>                  printf "selected pod - status: %s, name: %s\n" "${pod_status}" "${pod_name}";<br/>                  SPARK_THRIFT_SERVER_IS_RUNNING="True";<br/>              fi<br/>          fi<br/>      done<br/>    fi<br/>}<br/><br/>while [[ $SPARK_THRIFT_SERVER_IS_RUNNING != "True" ]];<br/>do<br/>    echo "waiting for spark thrift server being run...";<br/>    sleep 2;<br/>    check_spark_thrift_server_is_running;<br/>done<br/><br/># kill current spark submit process.<br/>kill $(cat pid);<br/><br/># create service.<br/>kubectl apply -f spark-thrift-server-service.yaml;<br/><br/>unset SPARK_HOME;</span></pre><p id="3051" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这看起来有点复杂，但事实并非如此。</p><p id="dea6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第一部分，从google drive下载必要的jars和用hadoop 3.2.0重建的spark包:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="0536" class="nl lj iq mo b gy nm nn l no np"># download spark tar file from google drive.<br/># https://drive.google.com/file/d/1_hpk6p_mgQ3gCA3ZV_cSUuEdt_yZlAaX/view?usp=sharing<br/>SPARK_FILE_NAME=spark-3.0.0-bin-custom-spark<br/>fileId=1_hpk6p_mgQ3gCA3ZV_cSUuEdt_yZlAaX<br/>fileName=${SPARK_FILE_NAME}.tgz<br/><br/>curl -sc /tmp/cookie "https://drive.google.com/uc?export=download&amp;id=${fileId}" &gt; /dev/null<br/>code="$(awk '/_warning_/ {print $NF}' /tmp/cookie)"<br/>curl -Lb /tmp/cookie "https://drive.google.com/uc?export=download&amp;confirm=${code}&amp;id=${fileId}" -o ${fileName}<br/>...</span></pre><p id="9ecb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">spark提交在后台执行:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="9e10" class="nl lj iq mo b gy nm nn l no np"># submit spark thrift server job.<br/>MASTER=k8s://your-k8s-master-url;<br/>NAMESPACE=${MY_NAMESPACE};<br/>ENDPOINT=https://s3-endpoint;<br/>BUCKET=mykidong;<br/>HIVE_METASTORE=metastore:9083;<br/><br/>spark-submit \<br/>--master $MASTER \<br/>--deploy-mode cluster \<br/>--name spark-thrift-server \<br/>--class io.mykidong.hive.SparkThriftServerRunner \<br/>--packages com.amazonaws:aws-java-sdk-s3:1.11.375,org.apache.hadoop:hadoop-aws:3.2.0 \<br/>...</span></pre><p id="666d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并检查spark thrift server pod是否正在运行:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="d3c8" class="nl lj iq mo b gy nm nn l no np"># check if spark thrift server pod is running.<br/><br/>SPARK_THRIFT_SERVER_IS_RUNNING="False";<br/>check_spark_thrift_server_is_running() {<br/>    POD_STATUS=$(kubectl get pods -n ${MY_NAMESPACE} -l spark-role=driver -o jsonpath={..status.phase});<br/>    POD_NAME=$(kubectl get pods -n ${MY_NAMESPACE} -l spark-role=driver -o jsonpath={..metadata.name});<br/>    <br/>    if [[ ${POD_STATUS} != "" ]]<br/>    then<br/>      # Set space as the delimiter<br/>      IFS=' ';<br/>  <br/>      #Read the split words into an array based on space delimiter<br/>      read -a POD_STATUS_ARRAY &lt;&lt;&lt; "$POD_STATUS";<br/>      read -a POD_NAME_ARRAY &lt;&lt;&lt; "$POD_NAME";<br/>  <br/>      for ((i = 0; i &lt; ${#POD_STATUS_ARRAY[@]}; ++i)); do<br/>          pod_status=${POD_STATUS_ARRAY[i]};<br/>          pod_name=${POD_NAME_ARRAY[i]};<br/>          printf "status: %s, name: %s\n" "${pod_status}" "${pod_name}";<br/>  <br/>          if [[ $pod_status == "Running" ]]<br/>          then<br/>              if [[ $pod_name =~ "spark-thrift-server" ]]<br/>              then<br/>                  printf "selected pod - status: %s, name: %s\n" "${pod_status}" "${pod_name}";<br/>                  SPARK_THRIFT_SERVER_IS_RUNNING="True";<br/>              fi<br/>          fi<br/>      done<br/>    fi<br/>}<br/><br/>while [[ $SPARK_THRIFT_SERVER_IS_RUNNING != "True" ]];<br/>do<br/>    echo "waiting for spark thrift server being run...";<br/>    sleep 2;<br/>    check_spark_thrift_server_is_running;<br/>done<br/>...</span></pre><p id="63b8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，在终止spark提交进程后，创建spark thrift服务器服务。</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="4517" class="nl lj iq mo b gy nm nn l no np"># kill current spark submit process.<br/>kill $(cat pid);<br/><br/># create service.<br/>kubectl apply -f spark-thrift-server-service.yaml;</span></pre><p id="fb82" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，您可以看到kubectl的名称空间中的pod，如下所示:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="30a3" class="nl lj iq mo b gy nm nn l no np">kubectl get po -n my-namespace;<br/>NAME                                          READY   STATUS      RESTARTS   AGE<br/>metastore-5544f95b6b-cqmkx                    1/1     Running     0          3d10h<br/>mysql-deployment-5b68bb45bc-hs7g5             1/1     Running     0          3d10h<br/>spark-thrift-server-b35bcc74c46273c3-driver   1/1     Running     0          3d3h<br/>spark-thrift-server-ce356974c4636711-exec-1   1/1     Running     0          3d3h<br/>spark-thrift-server-ce356974c4636711-exec-2   1/1     Running     0          3d3h<br/>spark-thrift-server-ce356974c4636711-exec-3   1/1     Running     0          3d3h</span></pre><p id="e4e1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看spark thrift服务器的日志:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="4eb5" class="nl lj iq mo b gy nm nn l no np">kubectl logs -f spark-thrift-server-b35bcc74c46273c3-driver -n my-namespace;</span></pre><h1 id="72cc" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">运行火花三角洲湖示例作业</h1><p id="dbc9" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">您可以运行spark delta lake示例作业来测试通过JDBC对spark thrift server的查询。</p><p id="b530" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们先看看代码:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="081c" class="nl lj iq mo b gy nm nn l no np">public class DeltaLakeExample {<br/><br/>    public static void main(String[] args) throws Exception {<br/><br/>        OptionParser parser = new OptionParser();<br/>        parser.accepts("master").withRequiredArg().ofType(String.class);<br/><br/>        OptionSet options = parser.parse(args);<br/><br/>        String master = (String) options.valueOf("master");<br/><br/>        SparkConf sparkConf = new SparkConf().setAppName(DeltaLakeExample.class.getName());<br/>        sparkConf.setMaster(master);<br/><br/>        // delta lake log store for s3.<br/>        sparkConf.set("spark.delta.logStore.class", "org.apache.spark.sql.delta.storage.S3SingleDriverLogStore");<br/><br/>        SparkSession spark = SparkSession<br/>                .<em class="nr">builder</em>()<br/>                .config(sparkConf)<br/>                .enableHiveSupport()<br/>                .getOrCreate();<br/><br/><br/>        // read json.<br/>        String json = StringUtils.<em class="nr">fileToString</em>("data/test.json");<br/>        String lines[] = json.split("\\r?\\n");<br/>        Dataset&lt;Row&gt; df = spark.read().json(new JavaSparkContext(spark.sparkContext()).parallelize(Arrays.<em class="nr">asList</em>(lines)));<br/><br/>        df.show(10);<br/><br/>        // write delta to ceph.<br/>        df.write().format("delta")<br/>                .option("path", "s3a://mykidong/test-delta")<br/>                .mode(SaveMode.<em class="nr">Overwrite</em>)<br/>                .save();<br/><br/>        // create delta table.<br/>        spark.sql("CREATE TABLE IF NOT EXISTS test_delta USING DELTA LOCATION 's3a://mykidong/test-delta'");<br/><br/>        // read delta from ceph.<br/>        Dataset&lt;Row&gt; delta = spark.sql("select * from test_delta");<br/><br/>        delta.show(10);<br/><br/>        // create persistent parquet table with external path.<br/>        delta.write().format("parquet")<br/>                .option("path", "s3a://mykidong/test-parquet")<br/>                .mode(SaveMode.<em class="nr">Overwrite</em>)<br/>                .saveAsTable("test_parquet");<br/><br/>        // read parquet from table.<br/>        Dataset&lt;Row&gt; parquet = spark.sql("select * from test_parquet");<br/><br/>        parquet.show(10);<br/>    }<br/>}</span></pre><p id="9877" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在S3上创建拼花地板数据和三角洲湖数据，并在hive metastore中创建hive表是一项简单的spark工作。</p><p id="f277" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，运行示例作业。</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="1b1b" class="nl lj iq mo b gy nm nn l no np">cd examples/spark;</span><span id="7271" class="nl lj iq mo b gy nq nn l no np"># build spark uber jar.<br/>mvn -e -DskipTests=true clean install shade:shade;</span><span id="453c" class="nl lj iq mo b gy nq nn l no np"># submit spark job onto kubernetes.<br/>export MASTER=k8s://<a class="ae kc" href="https://52.231.203.153:6443" rel="noopener ugc nofollow" target="_blank">y</a>our-k8-master-url;<br/>export NAMESPACE=my-namespace;<br/>export ENDPOINT=<a class="ae kc" href="https://mykidong-tenant.minio.cloudchef-labs.com" rel="noopener ugc nofollow" target="_blank">s</a>3-endpoint;<br/>export HIVE_METASTORE=metastore:9083;</span><span id="cc2a" class="nl lj iq mo b gy nq nn l no np">spark-submit \<br/>--master ${MASTER} \<br/>--deploy-mode cluster \<br/>--name spark-delta-example \<br/>--class io.mykidong.spark.examples.DeltaLakeExample \<br/>--packages com.amazonaws:aws-java-sdk-s3:1.11.375,org.apache.hadoop:hadoop-aws:3.2.0 \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-driver-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-exec-pvc \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-driver-localdir-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-exec-localdir-pvc \<br/>--conf spark.kubernetes.file.upload.path=s3a://mykidong/spark-thrift-server \<br/>--conf spark.kubernetes.container.image.pullPolicy=Always \<br/>--conf spark.kubernetes.namespace=$NAMESPACE \<br/>--conf spark.kubernetes.container.image=mykidong/spark:v3.0.0 \<br/>--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \<br/>--conf spark.hadoop.hive.metastore.client.connect.retry.delay=5 \<br/>--conf spark.hadoop.hive.metastore.client.socket.timeout=1800 \<br/>--conf spark.hadoop.hive.metastore.uris=thrift://${HIVE_METASTORE} \<br/>--conf spark.hadoop.hive.server2.enable.doAs=false \<br/>--conf spark.hadoop.hive.server2.thrift.http.port=10002 \<br/>--conf spark.hadoop.hive.server2.thrift.port=10016 \<br/>--conf spark.hadoop.hive.server2.transport.mode=binary \<br/>--conf spark.hadoop.metastore.catalog.default=spark \<br/>--conf spark.hadoop.hive.execution.engine=spark \<br/>--conf spark.hadoop.hive.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.hadoop.hive.tez.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.sql.warehouse.dir=s3a:/mykidong/apps/spark/warehouse \<br/>--conf spark.hadoop.fs.defaultFS=s3a://mykidong \<br/>--conf spark.hadoop.fs.s3a.access.key=my-access-key \<br/>--conf spark.hadoop.fs.s3a.secret.key=my-secret-key \<br/>--conf spark.hadoop.fs.s3a.connection.ssl.enabled=true \<br/>--conf spark.hadoop.fs.s3a.endpoint=$ENDPOINT \<br/>--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \<br/>--conf spark.hadoop.fs.s3a.fast.upload=true \<br/>--conf spark.hadoop.fs.s3a.path.style.access=true \<br/>--conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \<br/>--conf spark.executor.instances=3 \<br/>--conf spark.executor.memory=2G \<br/>--conf spark.executor.cores=1 \<br/>--conf spark.driver.memory=1G \<br/>file:///&lt;src&gt;/examples/spark/target/spark-example-1.0.0-SNAPSHOT-spark-job.jar \<br/>--master ${MASTER};</span></pre><p id="7825" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完成这项工作后，一些数据将被保存在S3桶，并在配置单元中创建拼花表和三角洲湖表进行查询。</p><h1 id="9142" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">通过JDBC连接到Spark节俭服务器</h1><p id="aba9" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">我们可以通过JDBC用直线连接到Spark Thrift服务器。</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="494e" class="nl lj iq mo b gy nm nn l no np">cd &lt;spark-home&gt;;</span><span id="0b73" class="nl lj iq mo b gy nq nn l no np">bin/beeline -u jdbc:hive2://$(kubectl get svc spark-thrift-server-service -n my-namespace -o jsonpath={.status.loadBalancer.ingress[0].ip}):10016;</span></pre><p id="0f20" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以在其中键入一些查询:</p><pre class="nd ne nf ng gt nh mo ni nj aw nk bi"><span id="0b59" class="nl lj iq mo b gy nm nn l no np">show tables;<br/>select * from test_parquet;<br/>select * from test_delta;<br/>select count(*) from test_delta;<br/>...</span></pre></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="3687" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！</p><h1 id="3c7f" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">参考</h1><ul class=""><li id="6395" class="mp mq iq kf b kg mg kk mh ko ns ks nt kw nu la mu mv mw mx bi translated">参见<a class="ae kc" rel="noopener ugc nofollow" target="_blank" href="/hive-on-spark-with-spark-operator-9a43ea7ebe06">Spark Thrift Server with data roaster Spark Operator</a></li><li id="79a1" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated">【https://github.com/cloudcheflabs/dataroaster T4】</li><li id="a7e9" class="mp mq iq kf b kg my kk mz ko na ks nb kw nc la mu mv mw mx bi translated"><a class="ae kc" href="https://youtu.be/AeqkkQDwPqY" rel="noopener ugc nofollow" target="_blank"> demo </a>展示了如何使用<a class="ae kc" href="https://github.com/cloudcheflabs/dataroaster" rel="noopener ugc nofollow" target="_blank"> DataRoaster </a>轻松创建一个由运行在Kubernetes上的hive metastore、spark thrift server、trino、redash和jupyterhub等组成的数据平台。</li></ul></div></div>    
</body>
</html>