<html>
<head>
<title>Wrong Spark configuration that cost us $3k/month</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每月花费3000美元的错误火花配置</h1>
<blockquote>原文：<a href="https://itnext.io/wrong-spark-configuration-that-cost-us-3k-month-dc21f2a0a113?source=collection_archive---------2-----------------------#2022-06-17">https://itnext.io/wrong-spark-configuration-that-cost-us-3k-month-dc21f2a0a113?source=collection_archive---------2-----------------------#2022-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/941dd21ca91d436e7ad1da852312895a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*H3nOrnC9P9oL7RR_"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">由<a class="ae jd" href="https://unsplash.com/@joshappel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔希·阿佩尔</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><div class=""/><p id="23f4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Spark版本:3.2.0</p><p id="d1c6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里的<a class="ae jd" href="https://www.dynamicyield.com/" rel="noopener ugc nofollow" target="_blank"> Dynamic Yield </a>，我们每天运行数千个<a class="ae jd" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Spark </a>应用。在过去的两年里，我们努力升级我们的基础设施，以便在Kubernetes上运行这些工作，并且我们不断改进我们的系统。</p><p id="d92d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">几个月前我们添加的一个简单配置是:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="2809" class="lk ll jg lg b gy lm ln l lo lp">spark.eventLog.rolling.enabled: true<br/>spark.eventLog.rolling.maxFileSize: 16m</span></pre><p id="e3af" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样。两条线路，每月3000多美元:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lq"><img src="../Images/6021bb1464e064a4b9d0f2fee430e6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAIk9YSWYJTqUPGzpkd5iw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">滚动标记前后的AWS成本浏览器(图片由作者提供)</figcaption></figure><p id="4629" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设您正在使用Spark历史服务器来监控您的Spark应用程序，请确保它不会花费您过多的成本。</p><h2 id="8c36" class="lk ll jg bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated">火花历史配置</h2><p id="0fcc" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">Spark history server是监控应用程序的一个非常有用的工具。通过在<code class="fe mn mo mp lg b">spark-submit</code>命令中添加一些配置行，应用程序日志保存在您的首选存储中(在我们的例子中是AWS S3 ),并可用于以后的探索。最基本的配置是:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="a504" class="lk ll jg lg b gy lm ln l lo lp">spark.eventLog.enabled: true<br/>spark.eventLog.dir: <!-- -->s3a://a-team-bucket/history-logs</span></pre><p id="add3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将确保应用程序可用于未来的探索。</p><p id="264c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在阅读文档时，<a class="ae jd" href="https://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options" rel="noopener ugc nofollow" target="_blank">对滚动事件日志文件</a>应用压缩看起来很有希望，尤其是在过去监控极其缓慢的应用程序时遭受了UI无响应的痛苦之后。没有暗示这个<a class="ae jd" href="https://docs.google.com/document/d/12bdCC4nA58uveRxpeo8k7kGOI2NRTXmXyBOweSi4YcY/edit#heading=h.7bmfccqq7ozy" rel="noopener ugc nofollow" target="_blank">新功能</a>的价格。</p><h2 id="5d99" class="lk ll jg bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated">实施细节</h2><p id="bfb2" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">历史服务器的刷新间隔默认为10秒，可以通过<code class="fe mn mo mp lg b">spark.history.fs.update.interval</code>进行调整。现在，假设我们将最后50k个应用程序存储在一个S3存储桶中，每个应用程序由一个文件表示。在这种情况下，每10秒钟就有50个ListBucket后台操作(每个操作返回多达1，000个对象):</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="a907" class="lk ll jg lg b gy lm ln l lo lp">.<br/>├── spark-00079d419d924b4d900a0a27cd6a9ae0<br/>├── spark-000af08856194e6e82046bc65237bc78<br/>├── ...<br/>├── ...<br/>└── spark-z01051c66d93409583b17001c34fc21c</span></pre><p id="77e9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，使用滚动日志将为每个应用程序创建一个文件夹，如下所示:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="1def" class="lk ll jg lg b gy lm ln l lo lp">.<br/>├── eventlog_v2_spark-00079d419d924b4d900a0a27cd6a9ae0<br/>|   ├── appstatus_spark-00079d419d924b4d900a0a27cd6a9ae0<br/>│   └── events_1_spark-00079d419d924b4d900a0a27cd6a9ae0<br/>├── eventlog_v2_spark-000af08856194e6e82046bc65237bc78<br/>|   ├── appstatus_spark-000af08856194e6e82046bc65237bc78<br/>│   └── events_1_spark-000af08856194e6e82046bc65237bc78<br/>├── ...<br/>├── ...<br/>└── eventlog_v2_spark-z01051c66d93409583b17001c34fc21c<br/>    ├── appstatus_spark-z01051c66d93409583b17001c34fc21c<br/>    └── events_1_spark-z01051c66d93409583b17001c34fc21c</span></pre><p id="056f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个时间历史服务器执行相同的50个ListBucket操作来获取所有文件夹，然后执行另外的<strong class="kf jh"> 50，000个ListBucket操作来获取每个文件夹的内容！</strong>换句话说，每10s进行50050次ListBucket操作。</p><p id="52e9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">调试完源代码后，我发现对于文件系统抽象，日志既可以存储在本地文件系统中，也可以存储在像S3这样的hdfs中——使用了树遍历(列出所有文件夹，然后列出每个文件夹的文件),尽管深度树扫描是可用的。</p><h1 id="6145" class="mq ll jg bd lr mr ms mt lu mu mv mw lx mx my mz ma na nb nc md nd ne nf mg ng bi translated">结论</h1><ul class=""><li id="8a20" class="nh ni jg kf b kg mi kk mj ko nj ks nk kw nl la nm nn no np bi translated">调整适合您的更新间隔。如果您每天监控一次应用程序，1分钟、1小时或每天更新一次可能就足够了。</li><li id="ad80" class="nh ni jg kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">没有问题就不要急着加配置。Rotate可能有助于监控大型/重型应用程序。如果你在使用你的历史服务器时没有感到痛苦，你可能不需要它。</li><li id="1ef5" class="nh ni jg kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">你可以把它们混合在一起。同一历史服务器路径支持文件夹(循环日志)和文件的混合。如果您使用循环日志配置，请仅将它们添加到您最大的应用程序中。</li><li id="8b05" class="nh ni jg kf b kg nq kk nr ko ns ks nt kw nu la nm nn no np bi translated">别忘了监控你的储物价格。虽然与计算资源相比，它被认为是廉价的，但您不希望将预算浪费在多余的操作上。</li></ul><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lq"><img src="../Images/ca9a757121f597dcfc5dab8519c79c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rv-0Fb9WN5iLJDkxl-bQMA.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Spark历史服务器(图片由作者提供)</figcaption></figure><h1 id="b4d9" class="mq ll jg bd lr mr ms mt lu mu mv mw lx mx my mz ma na nb nc md nd ne nf mg ng bi translated">附录</h1><p id="4d29" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">如果您有兴趣在本地运行Spark历史服务器并了解发生了什么，您可能会发现本指南很有用。</p><h2 id="c639" class="lk ll jg bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated">在IntelliJ中运行spark历史服务器</h2><ol class=""><li id="11ab" class="nh ni jg kf b kg mi kk mj ko nj ks nk kw nl la nv nn no np bi translated">从Github克隆spark库。</li><li id="00b7" class="nh ni jg kf b kg nq kk nr ko ns ks nt kw nu la nv nn no np bi translated">打开IntelliJ。</li><li id="1866" class="nh ni jg kf b kg nq kk nr ko ns ks nt kw nu la nv nn no np bi translated">搜索<code class="fe mn mo mp lg b">org.apache.spark.deploy.history.HistoryServer</code>并运行(很可能会失败)。</li><li id="fef6" class="nh ni jg kf b kg nq kk nr ko ns ks nt kw nu la nv nn no np bi translated">编辑运行配置:</li></ol><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/5271af60626039fa219aa2b77a037793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXMOZu21qUMhbq3i96euPQ.png"/></div></div></figure><p id="5ab1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">选择模块。我选择了<code class="fe mn mo mp lg b">spark-core_2.12</code>模块，我们将立即扩展它。</p><p id="f239" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">提示:如果您尝试从脚本(<code class="fe mn mo mp lg b">/bin/start-history-server.sh</code>)运行，您会看到运行命令打印在日志的第一行。</p><p id="7066" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">5.进入项目结构→项目设置→模块，选择spark-core_2.12。点击Dependencies选项卡，添加assembly/target/Scala-2.12/jars中的所有jar。如果您正在测试AWS/Hadoop jar，也要添加它们。</p><p id="3458" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">6.在<code class="fe mn mo mp lg b">conf</code>文件夹中，添加<code class="fe mn mo mp lg b">spark-defaults.conf</code>文件并添加您的自定义配置，例如:</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="f6f0" class="lk ll jg lg b gy lm ln l lo lp">spark.history.fs.logDirectory   s3a://logs-bucket/history<br/>spark.hadoop.fs.s3a.aws.credentials.provider    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider<br/>spark.hadoop.fs.s3a.endpoint    http://localhost:9000<br/>spark.hadoop.fs.s3a.connection.ssl.enabled  false<br/>spark.hadoop.fs.s3a.path.style.access   true<br/>spark.history.fs.update.interval    30s</span></pre><p id="de73" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您正在使用<a class="ae jd" href="https://min.io/" rel="noopener ugc nofollow" target="_blank"> minio </a>(带有docker)来模拟S3，您可以通过环境变量在运行配置(步骤4)中添加凭证。</p><p id="8671" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">7.如果您想查看调试日志，将<code class="fe mn mo mp lg b">log4j2.properties.template</code>文件从<code class="fe mn mo mp lg b">conf</code>目录复制到<code class="fe mn mo mp lg b">log4j2.properties</code>，并更改<code class="fe mn mo mp lg b">rootLogger.level</code>严重性。</p></div></div>    
</body>
</html>