<html>
<head>
<title>Hydrating a Data Lake using Query-based CDC with Apache Kafka Connect and Kubernetes on AWS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在AWS上使用基于查询的CDC和Apache Kafka Connect和Kubernetes来补充数据湖</h1>
<blockquote>原文：<a href="https://itnext.io/hydrating-a-data-lake-using-query-based-cdc-with-apache-kafka-connect-and-kubernetes-on-aws-cd4725b58c2e?source=collection_archive---------2-----------------------#2021-08-11">https://itnext.io/hydrating-a-data-lake-using-query-based-cdc-with-apache-kafka-connect-and-kubernetes-on-aws-cd4725b58c2e?source=collection_archive---------2-----------------------#2021-08-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c987" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用亚马逊EKS、亚马逊MSK和Apache Kafka Connect将数据从亚马逊RDS数据库导入到基于亚马逊S3的数据湖中</h2></div><p id="4d85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据AWS的说法，数据湖是一个集中的存储库，允许你存储任何规模的结构化和非结构化数据。数据从多个来源收集，并转移到数据湖中。一旦进入数据湖，数据就会被组织、编目、转换、丰富并转换为常见的文件格式，为分析和机器学习进行优化。</p><p id="365d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">组织在构建数据湖时首先面临的挑战之一是如何从不同的数据源持续导入数据，如关系和非关系数据库引擎、企业ERP、SCM、CRM和SIEM软件、平面文件、消息传递平台、物联网设备以及日志和指标收集系统。每个数据源都有自己独特的连接方法、安全性、数据存储格式和数据导出功能。有许多闭源和开源工具可以帮助从不同的数据源提取数据。</p><p id="22c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个流行的开源工具是<a class="ae le" href="https://kafka.apache.org/documentation/#connect" rel="noopener ugc nofollow" target="_blank"> Kafka Connect </a>，它是<a class="ae le" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Kafka </a>生态系统的一部分。Apache Kafka是一个开源的分布式事件流平台，被数千家公司用于高性能数据管道、流分析、数据集成和任务关键型应用程序。Kafka Connect是一个工具，用于在Apache Kafka和其他系统之间可扩展和可靠地传输数据。Kafka Connect使得快速定义将大量数据移入和移出Kafka的连接器<em class="lf">变得简单。</em></p><p id="7b2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在接下来的文章中，我们将学习如何使用Kafka Connect将数据从我们的数据源(一个用于PostgreSQL的关系数据库<a class="ae le" href="https://aws.amazon.com/rds/postgresql/" rel="noopener ugc nofollow" target="_blank"> Amazon RDS)导出到Kafka。然后，我们会将Kafka中的数据导出到我们的数据接收器中——一个建立在亚马逊简单存储服务(亚马逊S3)上的数据湖。导入S3的数据将被转换为</a><a class="ae le" href="https://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Parquet </a>列存储文件格式，进行压缩和分区，以实现最佳分析性能，所有这些都使用Kafka Connect。</p><p id="7456" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最重要的是，为了保持数据湖的数据新鲜度，当在PostgreSQL中添加或更新数据时，Kafka Connect会自动检测这些变化，并将这些变化传输到数据湖中。这个过程通常被称为<a class="ae le" href="https://en.wikipedia.org/wiki/Change_data_capture" rel="noopener ugc nofollow" target="_blank">变更数据捕获</a> (CDC)。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/844106b55f7a2e370ba449606e167173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ksj7hgW6oleJKfScNkvASw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">这篇文章演示的高级架构</figcaption></figure><h1 id="d7c1" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">变更数据捕获</h1><p id="e7e7" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">据参与Debezium和Hibernate项目的Red Hat首席软件工程师Gunnar Morling和著名的行业发言人说，有两种类型的<a class="ae le" href="https://en.wikipedia.org/wiki/Change_data_capture" rel="noopener ugc nofollow" target="_blank">变更数据捕获</a>——基于查询和基于日志的CDC。Gunnar在2021年2月的<a class="ae le" href="https://jokerconf.com/en/" rel="noopener ugc nofollow" target="_blank"> Joker </a>国际Java大会上的演讲中详细介绍了这两种CDC的区别，<a class="ae le" href="https://youtu.be/-PugtDeJQFs?t=1320" rel="noopener ugc nofollow" target="_blank">用Debezium和Kafka Streams </a>改变数据捕获管道。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mt"><img src="../Images/8564b4d6ee5583bf12745a95cfaf2424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fYRCy6LcscSadTcMn4JxIQ.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">小丑2021:用Debezium和Kafka流改变数据捕获管道(图片:<a class="ae le" href="https://www.youtube.com/watch?t=1320&amp;v=-PugtDeJQFs&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank"> YouTube </a>)</figcaption></figure><p id="ba1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在Rockset的Lewis Gavin最近的帖子中找到对CDC的另一个很好的解释，<a class="ae le" href="https://rockset.com/blog/change-data-capture-what-it-is-and-how-to-use-it/" rel="noopener ugc nofollow" target="_blank">更改数据捕获:它是什么以及如何使用它</a>。</p><h2 id="3a1d" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">基于查询与基于日志的CDC</h2><p id="0fb3" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">为了有效地展示基于查询的CDC和基于日志的CDC之间的区别，请检查用这两种方法捕获的SQL UPDATE语句的结果。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="32e5" class="mu lx it nh b gy nl nm l nn no">UPDATE public.address<br/><strong class="nh iu">SET address2 = 'Apartment #1234'<br/></strong>WHERE address_id = 105;</span></pre><p id="e9cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是如何使用本文中描述的基于查询的CDC方法将更改表示为JSON消息有效负载。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="fc8a" class="mu lx it nh b gy nl nm l nn no">{<br/>  "address_id": 105,<br/>  "address": "733 Mandaluyong Place",<br/><strong class="nh iu">  "address2": "Apartment #1234",<br/></strong>  "district": "Asir",<br/>  "city_id": 2,<br/>  "postal_code": "77459",<br/>  "phone": "196568435814",<br/>  "last_update": "2021-08-13T00:43:38.508Z"<br/>}</span></pre><p id="452c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是如何使用基于日志的CDC和Debezium将相同的变化表示为JSON消息负载。注意与基于查询的消息相比，基于日志的CDC消息的元数据丰富的结构。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="cf62" class="mu lx it nh b gy nl nm l nn no">{<br/>  "after": {<br/>    "address": "733 Mandaluyong Place",<br/><strong class="nh iu">    "address2": "Apartment #1234",<br/></strong>    "phone": "196568435814",<br/>    "district": "Asir",<br/>    "last_update": "2021-08-13T00:43:38.508453Z",<br/>    "address_id": 105,<br/>    "postal_code": "77459",<br/>    "city_id": 2<br/>  },<br/>  "source": {<br/>    "schema": "public",<br/>    "sequence": "[\"1090317720392\",\"1090317720392\"]",<br/>    "xmin": null,<br/>    "connector": "postgresql",<br/>    "lsn": 1090317720624,<br/>    "name": "pagila",<br/>    "txId": 16973,<br/>    "version": "1.6.1.Final",<br/>    "ts_ms": 1628815418508,<br/>    "snapshot": "false",<br/>    "db": "pagila",<br/>    "table": "address"<br/>  },<br/>  "op": "u",<br/>  "ts_ms": 1628815418815<br/>}</span></pre><p id="586c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/hydrating-a-data-lake-using-log-based-change-data-capture-cdc-with-debezium-apicurio-and-kafka-799671e0012f">即将发布的</a>中，我们将探索<a class="ae le" href="https://debezium.io/" rel="noopener ugc nofollow" target="_blank"> Debezium </a>以及<a class="ae le" href="https://avro.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arvo </a>和<a class="ae le" href="https://docs.confluent.io/platform/current/schema-registry/index.html" rel="noopener ugc nofollow" target="_blank">模式注册表</a>，使用PostgreSQL的<a class="ae le" href="https://debezium.io/documentation/reference/connectors/postgresql.html#postgresql-overview" rel="noopener ugc nofollow" target="_blank">预写日志</a> (WAL)构建一个基于日志的CDC解决方案。在本帖中，我们将使用“更新时间戳”技术来研究基于查询的CDC。</p><h1 id="a067" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">Kafka连接连接器</h1><p id="b769" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在这篇文章中，我们将使用来自<a class="ae le" href="https://www.confluent.io/" rel="noopener ugc nofollow" target="_blank">汇合</a>的源和宿连接器。融合是通过他们的<a class="ae le" href="https://www.confluent.io/confluent-cloud" rel="noopener ugc nofollow" target="_blank">融合云</a>和<a class="ae le" href="https://www.confluent.io/product/confluent-platform" rel="noopener ugc nofollow" target="_blank">融合平台</a>产品提供企业级托管Kafka的无可争议的领导者。Confluent提供了几十个源和汇连接器，涵盖了最流行的数据源和汇。本文中使用的连接器包括:</p><ul class=""><li id="dfc8" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated">Confluent的Kafka Connect <a class="ae le" href="https://docs.confluent.io/kafka-connect-jdbc/current/source-connector/index.html" rel="noopener ugc nofollow" target="_blank"> JDBC源连接器</a>使用JDBC驱动程序将数据从任何关系数据库导入Apache Kafka主题。Kafka Connect JDBC接收器连接器将数据从Kafka主题导出到任何具有JDBC驱动程序的关系数据库。</li><li id="c370" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">Confluent的Kafka Connect <a class="ae le" href="https://docs.confluent.io/kafka-connect-s3-sink/current/overview.html" rel="noopener ugc nofollow" target="_blank">亚马逊S3接收器连接器</a>以Avro、Parquet、JSON或Raw字节的形式将数据从Apache Kafka主题导出到S3对象。</li></ul><h1 id="55de" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">先决条件</h1><p id="9e55" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这篇文章将关注Kafka Connect的数据移动，而不是如何部署所需的AWS资源。要跟进这篇文章，您需要在AWS上已经部署和配置了以下资源:</p><ol class=""><li id="1246" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld od nv nw nx bi translated">Amazon RDS for PostgreSQL实例(数据<em class="lf">来源</em>)；</li><li id="a356" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld od nv nw nx bi translated">亚马逊S3水桶(数据<em class="lf">水槽</em>)；</li><li id="8156" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld od nv nw nx bi translated">亚马逊MSK集群；</li><li id="ec5d" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld od nv nw nx bi translated">亚马逊EKS集群；</li><li id="463a" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld od nv nw nx bi translated">亚马逊RDS实例和亚马逊MSK集群之间的连接；</li><li id="8294" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld od nv nw nx bi translated">亚马逊EKS集群和亚马逊MSK集群之间的连通性；</li><li id="a491" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld od nv nw nx bi translated">确保亚马逊MSK配置有<code class="fe oe of og nh b">auto.create.topics.enable=true</code>。该设置默认为<code class="fe oe of og nh b">false</code>；</li><li id="c6be" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld od nv nw nx bi translated">与Kubernetes服务帐户(称为<a class="ae le" href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html" rel="noopener ugc nofollow" target="_blank"> IRSA </a>)相关联的IAM角色，将允许从EKS访问MSK和S3 ( <em class="lf">见下面的详细信息</em>)；</li></ol><p id="4eb9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上面的架构图所示，我在同一个AWS账户和AWS区域<code class="fe oe of og nh b">us-east-1</code>内使用了三个独立的VPC，分别用于亚马逊RDS、亚马逊EKS和亚马逊MSK。使用<a class="ae le" href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html" rel="noopener ugc nofollow" target="_blank"> VPC对等</a>连接三个VPC。确保您在亚马逊RDS、亚马逊EKS和亚马逊MSK安全组上公开了正确的入口端口和相应的CIDR范围。为了增加安全性和节约成本，使用一个<a class="ae le" href="https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html" rel="noopener ugc nofollow" target="_blank"> VPC端点</a>来确保亚马逊EKS和亚马逊S3之间的私人通信。</p><h1 id="d5d0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">源代码</h1><p id="7711" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这篇文章的所有源代码，包括Kafka Connect配置文件和Helm图表，都是开源的，位于GitHub上。</p><div class="oh oi gp gr oj ok"><a href="https://github.com/garystafford/kafka-connect-msk-demo" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">GitHub-garystafter/Kafka-connect-MSK-demo:对于这篇文章，使用变更数据为数据湖补水…</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">对于这篇文章，在AWS - GitHub上使用变更数据捕获(CDC)、Apache Kafka和Kubernetes来补充数据湖…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">github.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy lq ok"/></div></div></a></div><h1 id="870d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">认证和授权</h1><p id="0318" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">亚马逊MSK提供多种<a class="ae le" href="https://docs.aws.amazon.com/msk/latest/developerguide/kafka_apis_iam.html" rel="noopener ugc nofollow" target="_blank">认证和授权方法</a>来与Apache Kafka APIs交互。例如，您可以使用IAM对客户端进行身份验证，并允许或拒绝Apache Kafka操作。或者，您可以使用TLS或SASL/SCRAM来验证客户端，并使用Apache Kafka ACLs来允许或拒绝操作。在我的上一篇文章中，我演示了SASL/SCRAM和Kafka ACLs在亚马逊MSK上的使用:</p><div class="oh oi gp gr oj ok"><a rel="noopener  ugc nofollow" target="_blank" href="/securely-decoupling-applications-on-amazon-eks-using-kafka-with-sasl-scram-48c340e1ffe9"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">使用带有SASL/SCRAM的Kafka安全地解耦亚马逊EKS上的应用</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">使用具有IRSA、SASL/SCRAM和数据加密的亚马逊MSK，安全地分离亚马逊EKS上基于Go的微服务</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">itnext.io</p></div></div><div class="ot l"><div class="oz l ov ow ox ot oy lq ok"/></div></div></a></div><p id="f408" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设您正确配置了亚马逊MSK、亚马逊EKS和Kafka Connect，任何MSK身份验证和授权都应该与Kafka Connect一起工作。在这篇文章中，我们使用<a class="ae le" href="https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html#kafka-actions" rel="noopener ugc nofollow" target="_blank"> IAM访问控制</a>。与Kubernetes服务帐户(<a class="ae le" href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html" rel="noopener ugc nofollow" target="_blank"> IRSA </a>)相关联的IAM角色允许EKS使用IAM访问MSK和S3(<em class="lf">参见下面的更多细节</em>)。</p><h1 id="8fda" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">示例PostgreSQL数据库</h1><p id="12d4" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">我们可以使用许多示例PostgreSQL数据库来探索Kafka Connect。我最喜欢的，虽然有点过时，是PostgreSQL的Pagila数据库。该数据库包含模拟电影租赁数据。数据集相当小，这使得它不太适合“大数据”用例，但也足够小，可以快速安装并最大限度地降低数据存储和分析成本。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pa"><img src="../Images/2fd0ad9a4d03bfdd3bb4d5fa615e821e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQjTzEyf6hV_hAWPucJ0Vg.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">Pagila数据库模式图</figcaption></figure><p id="2771" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在继续之前，在Amazon RDS PostgreSQL实例上创建一个新数据库，并用Pagila示例数据填充它。一些人发布了这个数据库的更新版本，带有易于安装的SQL脚本。查看Devrim Gündüz在<a class="ae le" href="https://github.com/devrimgunduz/pagila" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上提供的Pagila脚本，以及Robert Treat在<a class="ae le" href="https://github.com/xzilla" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上提供的pagi la脚本。</p><h2 id="d017" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">上次更新的触发器</h2><p id="d828" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Pagila数据库中的每个表都有一个<code class="fe oe of og nh b">last_update</code>字段。检测Pagila数据库中的更改并确保这些更改从RDS传到S3的一种简便方法是让Kafka Connect使用<code class="fe oe of og nh b">last_update</code>字段。这是使用基于查询的CDC来确定是否以及何时对数据进行了更改的常用技术。</p><p id="3ac8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当这些表中的记录发生变化时，现有的数据库函数和每个表的触发器将确保<code class="fe oe of og nh b">last_update</code>字段自动更新到当前日期和时间。你可以在Dominick Lombardo的文章<a class="ae le" href="https://lombardo-chcg.github.io/tools/2017/11/25/database-update-event-stream.html" rel="noopener ugc nofollow" target="_blank"> kafka connect in action，part 3 </a>中找到关于数据库函数和触发器如何与Kafka Connect一起工作的更多信息。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="99d4" class="mu lx it nh b gy nl nm l nn no">CREATE OR REPLACE FUNCTION <em class="lf">update_last_update_column</em>()<br/>    RETURNS TRIGGER AS<br/>$$<br/>BEGIN<br/>    NEW.last_update = now();<br/>    RETURN NEW;<br/>END;<br/>$$ language 'plpgsql';<br/><br/>CREATE TRIGGER update_last_update_column_address<br/>    BEFORE UPDATE<br/>    ON address<br/>    FOR EACH ROW<br/>EXECUTE PROCEDURE <em class="lf">update_last_update_column</em>();</span></pre><h1 id="79b4" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">基于Kubernetes的卡夫卡连接</h1><p id="7410" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在亚马逊EKS上的Kubernetes上部署和管理Kafka Connect和其他必需的Kafka管理工具有几种选择。流行的解决方案包括Kubernetes  (CFK)的<a class="ae le" href="https://strimzi.io/" rel="noopener ugc nofollow" target="_blank"> Strimzi </a>和<a class="ae le" href="https://docs.confluent.io/operator/current/overview.html" rel="noopener ugc nofollow" target="_blank"> Confluent，或者使用官方的</a><a class="ae le" href="https://kafka.apache.org/downloads" rel="noopener ugc nofollow" target="_blank"> Apache Kafka </a>二进制文件构建你自己的Docker映像。在这篇文章中，我选择使用最新的Kafka二进制文件构建自己的Kafka Connect Docker映像。然后，我将Confluent的连接器及其依赖项安装到Kafka安装中。虽然不如使用现成的OSS容器高效，但在我看来，构建自己的映像确实可以教会你Kafka和Kafka Connect是如何工作的。</p><p id="bd22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你选择使用这篇文章中使用的相同的Kafka Connect图像，这篇文章的GitHub资源库中会包含一个Helm图表。Helm chart将在亚马逊EKS上的<code class="fe oe of og nh b">kafka</code>名称空间部署一个Kubernetes pod。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="e9c4" class="mu lx it nh b gy nl nm l nn no">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: kafka-connect-msk<br/>  labels:<br/>    app: kafka-connect-msk<br/>    component: service<br/>spec:<br/>  replicas: 1<br/>  strategy:<br/>    type: Recreate<br/>  selector:<br/>    matchLabels:<br/>      app: kafka-connect-msk<br/>      component: service<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: kafka-connect-msk<br/>        component: service<br/>    spec:<br/><strong class="nh iu">      serviceAccountName: kafka-connect-msk-iam-serviceaccount<br/></strong>      containers:<br/>        - image: garystafford/kafka-connect-msk:1.0.0<br/>          name: kafka-connect-msk<br/>          imagePullPolicy: IfNotPresent</span></pre><p id="a3a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在部署图表之前，用与Kafka Connect pod ( <code class="fe oe of og nh b">serviceAccountName</code>)关联的<a class="ae le" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" rel="noopener ugc nofollow" target="_blank"> Kubernetes服务帐户</a>的名称更新<code class="fe oe of og nh b">value.yaml</code>文件。附加到与pod的服务帐户相关联的IAM角色的IAM策略应该提供从EKS对运行在亚马逊MSK集群上的Kafka的足够访问。该策略还应该提供对您的S3存储桶的访问，如Confluent在此处所详述的<a class="ae le" href="https://docs.confluent.io/kafka-connect-s3-sink/current/overview.html#iam-policy-for-s3" rel="noopener ugc nofollow" target="_blank">。下面是一个(<em class="lf">过于宽泛</em> ) IAM策略的示例，该策略允许完全访问MSK上运行的任何Kafka集群，并允许从EKS上运行的Kafka Connect访问S3。</a></p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="8b51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更新服务帐户变量后，使用以下命令部署Helm图表:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="9d6c" class="mu lx it nh b gy nl nm l nn no">helm install kafka-connect-msk ./kafka-connect-msk \<br/>  --namespace $NAMESPACE --create-namespace</span></pre><p id="5e1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要获得正在运行的Kafka Connect容器的shell，请使用下面的<code class="fe oe of og nh b">kubectl exec</code>命令:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="5acd" class="mu lx it nh b gy nl nm l nn no">export KAFKA_CONTAINER=$(<br/>  kubectl get pods -n kafka -l app=kafka-connect-msk | \<br/>    awk 'FNR == 2 {print $1}')</span><span id="d1ac" class="mu lx it nh b gy pd nm l nn no">kubectl exec -it $KAFKA_CONTAINER -n kafka -- bash</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/f142ba31e696394bac7ed121ff3b0bf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djYSwUe4DfYfRvBXaGuN9g.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">与EKS上运行的Kafka Connect容器交互</figcaption></figure><h2 id="30d2" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">配置引导代理</h2><p id="baeb" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在启动Kafka Connect之前，您需要修改Kafka Connect的配置文件。Kafka Connect能够以独立和分布式模式运行工作线程。由于我们将使用Kafka Connect的分布式模式，修改<code class="fe oe of og nh b">config/connect-distributed.properties</code>文件。我在这篇文章中使用的配置文件的完整示例如下所示。</p><p id="5c68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Kafka Connect将在pod的容器内运行，而Kafka和<a class="ae le" href="https://zookeeper.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache ZooKeeper </a>则在亚马逊MSK上运行。更新<code class="fe oe of og nh b">bootstrap.servers</code>属性以反映您自己的逗号分隔的亚马逊MSK Kafka Bootstrap Brokers列表。要获得Amazon MSK集群的引导代理列表，请使用AWS管理控制台或以下AWS CLI命令:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="e3ea" class="mu lx it nh b gy nl nm l nn no"># get the msk cluster's arn<br/>aws kafka list-clusters --query 'ClusterInfoList[*].ClusterArn'</span><span id="0a53" class="mu lx it nh b gy pd nm l nn no"># use msk arn to get the brokers<br/>aws kafka get-bootstrap-brokers --cluster-arn <strong class="nh iu">your-msk-cluster-arn</strong></span><span id="ff3a" class="mu lx it nh b gy pd nm l nn no"># alternately, if you only have one cluster, then<br/>aws kafka get-bootstrap-brokers --cluster-arn $(<br/>  aws kafka list-clusters | jq -r '.ClusterInfoList[0].ClusterArn')</span></pre><p id="be04" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更新<code class="fe oe of og nh b">config/connect-distributed.properties</code>文件。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="c0ce" class="mu lx it nh b gy nl nm l nn no"><strong class="nh iu"># ***** CHANGE ME! *****<br/>bootstrap.servers=b-1.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098,b-2.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098, b-3.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098</strong></span><span id="fc71" class="mu lx it nh b gy pd nm l nn no">group.id=connect-cluster</span><span id="66e7" class="mu lx it nh b gy pd nm l nn no">key.converter.schemas.enable=true<br/>value.converter.schemas.enable=true</span><span id="9d66" class="mu lx it nh b gy pd nm l nn no">offset.storage.topic=connect-offsets<br/>offset.storage.replication.factor=2<br/>#offset.storage.partitions=25</span><span id="6532" class="mu lx it nh b gy pd nm l nn no">config.storage.topic=connect-configs<br/>config.storage.replication.factor=2</span><span id="7d28" class="mu lx it nh b gy pd nm l nn no">status.storage.topic=connect-status<br/>status.storage.replication.factor=2<br/>#status.storage.partitions=5</span><span id="9fba" class="mu lx it nh b gy pd nm l nn no">offset.flush.interval.ms=10000</span><span id="b22b" class="mu lx it nh b gy pd nm l nn no">plugin.path=/usr/local/share/kafka/plugins</span><span id="751a" class="mu lx it nh b gy pd nm l nn no"># kafka connect auth using iam<br/>ssl.truststore.location=/tmp/kafka.client.truststore.jks<br/>security.protocol=SASL_SSL<br/>sasl.mechanism=AWS_MSK_IAM<br/>sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;<br/>sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler</span><span id="fbd7" class="mu lx it nh b gy pd nm l nn no"># kafka connect producer auth using iam<br/>producer.ssl.truststore.location=/tmp/kafka.client.truststore.jks<br/>producer.security.protocol=SASL_SSL<br/>producer.sasl.mechanism=AWS_MSK_IAM<br/>producer.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;<br/>producer.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler</span><span id="4f8a" class="mu lx it nh b gy pd nm l nn no"># kafka connect consumer auth using iam<br/>consumer.ssl.truststore.location=/tmp/kafka.client.truststore.jks<br/>consumer.security.protocol=SASL_SSL<br/>consumer.sasl.mechanism=AWS_MSK_IAM<br/>consumer.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;<br/>consumer.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler</span></pre><p id="cef1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了方便执行Kafka命令，请将<code class="fe oe of og nh b">BBROKERS</code>环境变量设置为相同的Kafka引导程序代理的逗号分隔列表，例如:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="3438" class="mu lx it nh b gy nl nm l nn no">export BBROKERS="b-1.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098,b-2.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098, b-3.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098"</span></pre><h2 id="12a9" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">确认从Kafka Connect访问亚马逊MSK</h2><p id="d0e7" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">要确认您可以从运行在亚马逊EKS上的Kafka Connect容器访问运行在亚马逊MSK上的Kafka，请尝试列出现有的Kafka主题:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="7b3b" class="mu lx it nh b gy nl nm l nn no">bin/kafka-topics.sh --list \<br/>  --bootstrap-server $BBROKERS \<br/>  --command-config config/client-iam.properties</span></pre><p id="e977" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你也可以尝试列出现有的卡夫卡消费群体:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="bbe6" class="mu lx it nh b gy nl nm l nn no">bin/kafka-consumer-groups.sh --list \<br/>  --bootstrap-server $BBROKERS \<br/>  --command-config config/client-iam.properties</span></pre><p id="d77a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果其中任何一个失败，你可能有网络或安全问题阻止从亚马逊EKS到亚马逊MSK的访问。检查您的VPC对等、路由表、IAM/IRSA和安全组入口设置。这些项目中的任何一个都可能导致容器和亚马逊MSK上运行的Kafka之间的通信问题。</p><h2 id="0195" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">卡夫卡连接</h2><p id="1320" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">我建议使用下面显示的任一种方法作为后台进程启动Kafka Connect。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="bff8" class="mu lx it nh b gy nl nm l nn no">bin/connect-distributed.sh \<br/>  config/connect-distributed.properties &gt; /dev/null 2&gt;&amp;1 &amp;</span><span id="2abf" class="mu lx it nh b gy pd nm l nn no"># alternately use nohup<br/>nohup bin/connect-distributed.sh \<br/>  config/connect-distributed.properties &amp;</span></pre><p id="dd79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要确认Kafka Connect正确启动，请立即跟踪<code class="fe oe of og nh b">connect.log</code>文件。该日志将捕获任何启动错误，以便进行故障排除。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="e534" class="mu lx it nh b gy nl nm l nn no">tail -f logs/connect.log</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/33bccd8dfe3e3923a97ce2b703b42c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_NnPHDPCZtajPAwqN51dg.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">显示Kafka Connect作为后台进程启动的Kafka Connect日志</figcaption></figure><p id="e843" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您还可以使用<code class="fe oe of og nh b">ps</code>命令检查后台进程，以确认Kafka Connect正在运行。注意下面PID 4915的过程。如有必要，使用<code class="fe oe of og nh b">kill</code>命令和PID停止Kafka连接。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/74dbf61cb1a11e0301ad3cb723a21e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m0o4kmHYZOmaxPuIQtF1Ng.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">Kafka Connect作为后台进程运行</figcaption></figure><p id="f60a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果配置正确，Kafka Connect将在第一次启动时创建三个新主题，称为Kafka Connect内部主题，如<code class="fe oe of og nh b">config/connect-distributed.properties</code>文件中定义的:<code class="fe oe of og nh b">connect-configs</code>、<code class="fe oe of og nh b">connect-offsets</code>和<code class="fe oe of og nh b">connect-status</code>。根据<a class="ae le" href="https://docs.confluent.io/home/connect/userguide.html#kconnect-internal-topics" rel="noopener ugc nofollow" target="_blank">汇合</a>，Connect存储这些主题中的连接器和任务配置、偏移和状态。内部主题必须具有高复制系数、压缩清理策略和适当数量的分区。使用以下命令可以确认这些新主题。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="0a9e" class="mu lx it nh b gy nl nm l nn no">bin/kafka-topics.sh --list \<br/>  --bootstrap-server $BBROKERS \<br/>  --command-config config/client-iam.properties \<br/>  | grep connect-</span></pre><h1 id="d8de" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">Kafka连接连接器</h1><p id="ae08" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这篇文章展示了三个越来越复杂的Kafka连接源和接收器连接器。每个人都将展示不同的连接器功能，以便在Amazon RDS for PostgreSQL和Amazon S3之间导入/导出和转换数据。</p><h2 id="4163" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">连接器源#1</h2><p id="90bd" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">创建一个名为<code class="fe oe of og nh b">config/jdbc_source_connector_postgresql_00.json</code>的新文件(或者修改现有文件，如果使用我的Kafka Connect容器的话)。修改第3–5行，如下所示，以反映您的RDS实例的JDBC连接细节。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="862c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个Kafka Connect源连接器使用Confluent的Kafka Connect <a class="ae le" href="https://docs.confluent.io/kafka-connect-jdbc/current/source-connector/index.html" rel="noopener ugc nofollow" target="_blank"> JDBC源连接器</a> ( <code class="fe oe of og nh b">io.confluent.connect.jdbc.JdbcSourceConnector</code>)通过JDBC驱动程序从RDS导出数据，并将数据导入一系列Kafka主题。我们将从Pagila的<code class="fe oe of og nh b">public</code>模式中的三个表中导出数据:<code class="fe oe of og nh b">address</code>、<code class="fe oe of og nh b">city</code>和<code class="fe oe of og nh b">country</code>。我们将把这些数据写到一系列主题中，任意加上数据库名称和模式前缀<code class="fe oe of og nh b">pagila.public.</code>。源连接器将自动创建三个新主题:<code class="fe oe of og nh b">pagila.public.address</code>、<code class="fe oe of og nh b">pagila.public.city</code>和<code class="fe oe of og nh b">pagila.public.country</code>。</p><p id="3894" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意连接器的<code class="fe oe of og nh b">mode</code>属性值被设置为<code class="fe oe of og nh b">timestamp</code>，并且<code class="fe oe of og nh b">last_update</code>字段在<code class="fe oe of og nh b">timestamp.column.name</code>属性中被引用。回想一下，我们在前面的文章中向这三个表添加了数据库函数和触发器，每当在Pagila数据库中创建或更新记录时，它们都会更新<code class="fe oe of og nh b">last_update</code>字段。除了初始导出整个表之外，源连接器将每5秒钟轮询一次数据库(<code class="fe oe of og nh b">poll.interval.ms</code>属性)，查找比最近导出的<code class="fe oe of og nh b">last_modified</code>日期更新的更改。这是由源连接器使用参数化查询完成的，例如:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="2768" class="mu lx it nh b gy nl nm l nn no">SELECT *<br/>FROM "public"."address"<br/><strong class="nh iu">WHERE "public"."address"."last_update" &gt; ?<br/>  AND "public"."address"."last_update" &lt; ?<br/>ORDER BY "public"."address"."last_update" ASC</strong></span></pre><h2 id="ef20" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">连接器接收器#1</h2><p id="49aa" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">接下来，创建并配置第一个Kafka Connect sink连接器。创建新文件或修改<code class="fe oe of og nh b">config/s3_sink_connector_00.json</code>。修改第7行，如下所示，以反映您的亚马逊S3桶的名称。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="2526" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个Kafka Connect sink连接器使用Confluent的Kafka Connect <a class="ae le" href="https://docs.confluent.io/kafka-connect-s3-sink/current/overview.html" rel="noopener ugc nofollow" target="_blank">亚马逊S3 Sink连接器</a> ( <code class="fe oe of og nh b">io.confluent.connect.s3.S3SinkConnector</code>)以JSON格式将数据从Kafka主题导出到亚马逊S3对象。</p><h2 id="6c6a" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">部署连接器#1</h2><p id="a73d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">使用<a class="ae le" href="https://docs.confluent.io/platform/current/connect/references/restapi.html" rel="noopener ugc nofollow" target="_blank"> Kafka Connect REST接口</a>部署源和接收器连接器。许多教程演示了针对<code class="fe oe of og nh b">/connectors</code>端点的<code class="fe oe of og nh b">POST</code>方法。然而，这需要一个<code class="fe oe of og nh b">DELETE</code>和一个额外的<code class="fe oe of og nh b">POST</code>来更新连接器。针对<code class="fe oe of og nh b">/config</code>端点使用<code class="fe oe of og nh b">PUT</code>，您可以更新连接器，而无需首先发出<code class="fe oe of og nh b">DELETE</code>。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="14f1" class="mu lx it nh b gy nl nm l nn no">curl -s -d @"config/jdbc_source_connector_postgresql_00.json" \<br/>    -H "Content-Type: application/json" \<br/>    -X PUT <a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/config" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/config</a> | jq</span><span id="575d" class="mu lx it nh b gy pd nm l nn no">curl -s -d @"config/s3_sink_connector_00.json" \<br/>    -H "Content-Type: application/json" \<br/>    -X PUT <a class="ae le" href="http://localhost:8083/connectors/s3_sink_connector_00/config" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/s3_sink_connector_00/config</a> | jq</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/0c07016ad50bc2b5ebb52350add2f179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oOJXgLhOygxU27pMr7qS5A.png"/></div></div></figure><p id="7a39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以使用以下命令确认源连接器和接收器连接器已部署并正在运行:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="3534" class="mu lx it nh b gy nl nm l nn no">curl -s -X GET <a class="ae le" href="http://localhost:8083/connectors" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors</a> | \<br/>  jq '. | sort_by(.)'</span><span id="b6cb" class="mu lx it nh b gy pd nm l nn no">curl -s -H "Content-Type: application/json" \<br/>    -X GET <a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status</a> | jq</span><span id="a38c" class="mu lx it nh b gy pd nm l nn no">curl -s -H "Content-Type: application/json" \<br/>    -X GET <a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/</a>s3_sink_connector_00<a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status" rel="noopener ugc nofollow" target="_blank">/status</a> | jq</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/51abf3b0f931ac20c41e1447e94f19cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMpQxwvi74dGbyXfT127Gg.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">Kafka Connect源连接器运行成功</figcaption></figure><p id="1ec3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">阻止连接器正确启动的错误将使用<code class="fe oe of og nh b">/status</code>端点显示，如下例所示。在这种情况下，与pod相关联的Kubernetes服务帐户缺少对亚马逊S3目标存储桶的适当IAM权限。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/f3ea866b771cae891492c48f8a97b341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Huio1eztp6iMAMgGi3wUDQ.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">Kafka Connect接收器连接器运行失败，出现错误</figcaption></figure><h2 id="605d" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">确认连接器#1成功</h2><p id="41b0" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这三个表的全部内容将通过source连接器从RDS导出到Kafka，然后通过sink连接器从Kafka导出到S3。为了确认源连接器工作正常，验证应该已经创建的三个新Kafka主题是否存在:<code class="fe oe of og nh b">pagila.public.address</code>、<code class="fe oe of og nh b">pagila.public.city</code>和<code class="fe oe of og nh b">pagila.public.country</code>。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="76de" class="mu lx it nh b gy nl nm l nn no">bin/kafka-topics.sh --list \<br/>  --bootstrap-server $BBROKERS \<br/>  --command-config config/client-iam.properties \<br/>  | grep pagila.public.</span></pre><p id="5190" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了确认接收器连接器工作正常，验证新的S3对象已经在数据湖的S3桶中创建。如果您使用AWS CLI v2的<code class="fe oe of og nh b">s3</code> API，我们可以查看我们的目标S3存储桶的内容:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="1881" class="mu lx it nh b gy nl nm l nn no">aws s3api list-objects \<br/>  --bucket your-s3-bucket \<br/>  --query 'Contents[].{Key: Key}' \<br/>  --output text</span></pre><p id="a00e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您应该在S3桶中看到大约15个新的S3对象(JSON文件),它们的键是按照它们的主题名组织的。接收器连接器每100条记录或60秒向S3刷新一次新数据。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="d0cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您还可以使用AWS管理控制台来查看S3桶的内容。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pf"><img src="../Images/bab3bf830b4aa4b12c84e7b9c680e859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pca2d-StsHMSNMhgs8-W3w.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">亚马逊S3桶显示卡夫卡连接S3接收器连接器的结果，按主题名称组织</figcaption></figure><p id="cf65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用亚马逊S3控制台的“使用S3选择查询”来查看JSON格式文件中包含的数据。或者，您可以使用s3 API:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="36b6" class="mu lx it nh b gy nl nm l nn no">export SINK_BUCKET="your-s3-bucket"</span><span id="d80f" class="mu lx it nh b gy pd nm l nn no">export KEY="topics/pagila.public.address/partition=0/pagila.public.address+0+0000000100.json"</span><span id="971a" class="mu lx it nh b gy pd nm l nn no">aws s3api select-object-content \<br/>    --bucket $SINK_BUCKET \<br/>    --key $KEY \<br/>    --expression "select * from s3object limit 5" \<br/>    --expression-type "SQL" \<br/>    --input-serialization '{"JSON": {"Type": "DOCUMENT"}, "CompressionType": "NONE"}' \<br/>    --output-serialization '{"JSON": {}}' "output.json" \<br/>  &amp;&amp; cat output.json | jq \<br/>  &amp;&amp; rm output.json</span></pre><p id="a34f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，通过控制台或API使用“使用S3选择进行查询”功能，<code class="fe oe of og nh b">address</code>表的数据将类似于以下内容:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="984f" class="mu lx it nh b gy nl nm l nn no">{<br/>  "address_id": 100,<br/>  "address": "1308 Arecibo Way",<br/>  "address2": "",<br/>  "district": "Georgia",<br/>  "city_id": 41,<br/>  "postal_code": "30695",<br/>  "phone": "6171054059",<br/>  "last_update": 1487151930000<br/>}<br/>{<br/>  "address_id": 101,<br/>  "address": "1599 Plock Drive",<br/>  "address2": "",<br/>  "district": "Tete",<br/>  "city_id": 534,<br/>  "postal_code": "71986",<br/>  "phone": "817248913162",<br/>  "last_update": 1487151930000<br/>}<br/>{<br/>  "address_id": 102,<br/>  "address": "669 Firozabad Loop",<br/>  "address2": "",<br/>  "district": "Abu Dhabi",<br/>  "city_id": 12,<br/>  "postal_code": "92265",<br/>  "phone": "412903167998",<br/>  "last_update": 1487151930000<br/>}</span></pre><p id="3178" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">恭喜您，您已经使用Kafka Connect成功地将关系数据库中的数据导入到您的数据湖中！</p><h2 id="271c" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">连接器源#2</h2><p id="6f30" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">创建新文件或修改<code class="fe oe of og nh b">config/jdbc_source_connector_postgresql_01.json</code>。修改第3–5行，如下所示，以反映您的RDS实例连接细节。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="1744" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二个Kafka Connect源连接器也使用Confluent的Kafka Connect JDBC源连接器，通过JDBC驱动程序从just <code class="fe oe of og nh b">address</code>表中导出数据，并将该数据导入到新的Kafka主题<code class="fe oe of og nh b">pagila.public.alt.address</code>。这个源连接器的不同之处在于<a class="ae le" href="https://docs.confluent.io/platform/current/connect/transforms/overview.html" rel="noopener ugc nofollow" target="_blank">转换</a>，称为单消息转换(SMTs)。当消息通过Connect从RDS流到Kafka时，SMT被应用于消息。</p><p id="83ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此连接器中，有四个转换，它们执行以下常见功能:</p><ol class=""><li id="60d1" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld od nv nw nx bi translated">提取<code class="fe oe of og nh b">address_id</code>整数字段作为Kafka消息键，详见本<a class="ae le" href="https://www.confluent.io/blog/kafka-connect-deep-dive-jdbc-source-connector/" rel="noopener ugc nofollow" target="_blank">博文</a>由Confluence ( <em class="lf">见</em>“设置Kafka消息键”)。</li><li id="4f46" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld od nv nw nx bi translated">将Kafka主题名称作为新的静态字段添加到消息中；</li><li id="4601" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld od nv nw nx bi translated">将数据库名称作为新的静态字段追加到消息中；</li></ol><h2 id="9ca3" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">连接器接收器#2</h2><p id="1348" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">创建新文件或修改<code class="fe oe of og nh b">config/s3_sink_connector_01.json</code>。修改第6行，如下所示，以反映您的亚马逊S3桶的名称。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="ec35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二个接收器连接器与第一个接收器连接器几乎相同，只是它只将数据从一个Kafka主题<code class="fe oe of og nh b">pagila.public.alt.address</code>导出到S3。</p><h2 id="1402" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">部署连接器#2</h2><p id="e627" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">使用<a class="ae le" href="https://docs.confluent.io/platform/current/connect/references/restapi.html" rel="noopener ugc nofollow" target="_blank"> Kafka Connect REST接口</a>部署第二组源和接收器连接器，就像第一对连接器一样。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="33e0" class="mu lx it nh b gy nl nm l nn no">curl -s -d @"config/jdbc_source_connector_postgresql_01.json" \<br/>    -H "Content-Type: application/json" \<br/>    -X PUT <a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/config" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/jdbc_source_connector_postgresql_01/config</a> | jq</span><span id="bd2e" class="mu lx it nh b gy pd nm l nn no">curl -s -d @"config/s3_sink_connector_01.json" \<br/>    -H "Content-Type: application/json" \<br/>    -X PUT <a class="ae le" href="http://localhost:8083/connectors/s3_sink_connector_00/config" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/s3_sink_connector_01/config</a> | jq</span></pre><h2 id="1c33" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">确认连接器#2成功</h2><p id="9a1b" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">使用与前面相同的命令，确认新的一组连接器已经部署并正在运行，与第一组连接器一起，总共有四个连接器。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="fad5" class="mu lx it nh b gy nl nm l nn no">curl -s -X GET <a class="ae le" href="http://localhost:8083/connectors" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors</a> | \<br/>  jq '. | sort_by(.)'</span><span id="1c65" class="mu lx it nh b gy pd nm l nn no">curl -s -H "Content-Type: application/json" \<br/>    -X GET <a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/jdbc_source_connector_postgresql_01/status</a> | jq</span><span id="883c" class="mu lx it nh b gy pd nm l nn no">curl -s -H "Content-Type: application/json" \<br/>    -X GET <a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/</a>s3_sink_connector_01<a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status" rel="noopener ugc nofollow" target="_blank">/status</a> | jq</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/64e30afcaff13f9ca77c854d0c83990f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-IfeFBvaHNutVUYBuYYgMQ.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">Kafka连接源和接收器连接器成功运行</figcaption></figure><p id="0772" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要查看第一次转换的结果，提取<code class="fe oe of og nh b">address_id</code>整数字段作为Kafka消息密钥，我们可以使用Kafka命令行消费者:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="0430" class="mu lx it nh b gy nl nm l nn no">bin/kafka-console-consumer.sh \<br/>  --topic pagila.public.alt.address \<br/>  --offset 102 --partition 0 --max-messages 5 \<br/>  --property print.key=true --property print.value=true \<br/>  --property print.offset=true --property print.partition=true \<br/>  --property print.headers=false --property print.timestamp=false \<br/>  --bootstrap-server $BBROKERS \<br/>  --consumer.config config/client-iam.properties</span></pre><p id="bc55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的输出中，注意每条消息的开头，它显示Kafka消息密钥，与<code class="fe oe of og nh b">address_id</code>相同。比如<code class="fe oe of og nh b">{"type":"int32","optional":false},"payload":<strong class="kk iu">100</strong>}</code>。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/cd29f852592f620336a8bca6c71f2733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m41O5TSe9GRR3Plko2i0Yg.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">显示Kafka pagi la . public . alt . address<code class="fe oe of og nh b"> </code>主题中消息的输出</figcaption></figure><p id="6c85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用AWS管理控制台或CLI检查Amazon S3 bucket，您应该注意到在<code class="fe oe of og nh b">/topics/pagila.public.alt.address/</code>对象关键字前缀中的第四组S3对象。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pf"><img src="../Images/dff6919e79a5868c77e6250257700fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPfpN3rcdLuy9GgyCCJ5zA.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">显示包含地址数据的JSON格式文件的亚马逊S3桶</figcaption></figure><p id="6b3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用亚马逊S3控制台的“使用S3选择查询”来查看JSON格式文件中包含的数据。或者，您可以使用s3 API:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="c648" class="mu lx it nh b gy nl nm l nn no">export SINK_BUCKET="your-s3-bucket"</span><span id="b600" class="mu lx it nh b gy pd nm l nn no">export KEY="topics/pagila.public.alt.address/partition=0/pagila.public.address+0+0000000100.json"</span><span id="e21b" class="mu lx it nh b gy pd nm l nn no">aws s3api select-object-content \<br/>    --bucket $SINK_BUCKET \<br/>    --key $KEY \<br/>    --expression "select * from s3object limit 5" \<br/>    --expression-type "SQL" \<br/>    --input-serialization '{"JSON": {"Type": "DOCUMENT"}, "CompressionType": "NONE"}' \<br/>    --output-serialization '{"JSON": {}}' "output.json" \<br/>  &amp;&amp; cat output.json | jq \<br/>  &amp;&amp; rm output.json</span></pre><p id="5059" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的示例数据中，请注意添加到每个记录中的两个新字段，这是Kafka连接器转换的结果:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="bc1c" class="mu lx it nh b gy nl nm l nn no">{<br/>  "address_id": 100,<br/>  "address": "1308 Arecibo Way",<br/>  "address2": "",<br/>  "district": "Georgia",<br/>  "city_id": 41,<br/>  "postal_code": "30695",<br/>  "phone": "6171054059",<br/>  "last_update": 1487151930000,<br/><strong class="nh iu">  "message_topic": "pagila.public.alt.address",<br/>  "message_source": "pagila"<br/></strong>}<br/>{<br/>  "address_id": 101,<br/>  "address": "1599 Plock Drive",<br/>  "address2": "",<br/>  "district": "Tete",<br/>  "city_id": 534,<br/>  "postal_code": "71986",<br/>  "phone": "817248913162",<br/>  "last_update": 1487151930000,<br/><strong class="nh iu">  "message_topic": "pagila.public.alt.address",<br/>  "message_source": "pagila"<br/></strong>}<br/>{<br/>  "address_id": 102,<br/>  "address": "669 Firozabad Loop",<br/>  "address2": "",<br/>  "district": "Abu Dhabi",<br/>  "city_id": 12,<br/>  "postal_code": "92265",<br/>  "phone": "412903167998",<br/>  "last_update": 1487151930000,<br/><strong class="nh iu">  "message_topic": "pagila.public.alt.address",<br/>  "message_source": "pagila"<br/></strong>}</span></pre><p id="71ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">恭喜您，您已经成功地将更多的数据从关系数据库导入到您的数据湖中，包括使用Kafka Connect执行一系列简单的转换！</p><h2 id="8d9d" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">连接器源#3</h2><p id="a9ca" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">创建或修改<code class="fe oe of og nh b">config/jdbc_source_connector_postgresql_02.json</code>。修改第3–5行，如下所示，以反映您的RDS实例连接细节。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="3447" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与前两个从表中导出数据的源连接器不同，这个连接器使用SELECT查询从Pagila数据库的<code class="fe oe of og nh b">address</code>、<code class="fe oe of og nh b">city</code>和<code class="fe oe of og nh b">country</code>表中导出数据，并将该SQL查询数据的结果导入到新的Kafka主题<code class="fe oe of og nh b">pagila.public.alt.address</code>中。源连接器配置中的SQL查询如下:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="a142" class="mu lx it nh b gy nl nm l nn no">SELECT a.address_id,<br/>             a.address,<br/>             a.address2,<br/>             city.city,<br/>             a.district,<br/>             a.postal_code,<br/>             country.country,<br/>             a.phone,<br/>             a.last_update<br/>      FROM address AS a<br/>               INNER JOIN city ON a.city_id = city.city_id<br/>               INNER JOIN country ON country.country_id = city.country_id<br/>      ORDER BY address_id) AS addresses</span></pre><p id="4980" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由源连接器执行的最终参数化查询允许它基于<code class="fe oe of og nh b">last_update</code>字段检测更改，如下所示:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="b249" class="mu lx it nh b gy nl nm l nn no">SELECT *<br/>FROM (SELECT a.address_id,<br/>             a.address,<br/>             a.address2,<br/>             city.city,<br/>             a.district,<br/>             a.postal_code,<br/>             country.country,<br/>             a.phone,<br/>             a.last_update<br/>      FROM address AS a<br/>               INNER JOIN city ON a.city_id = city.city_id<br/>               INNER JOIN country ON country.country_id = city.country_id<br/>      ORDER BY address_id) AS addresses<br/><strong class="nh iu">WHERE "last_update" &gt; ?<br/>  AND "last_update" &lt; ?<br/>ORDER BY "last_update" ASC</strong></span></pre><h2 id="3f65" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">连接器接收器#3</h2><p id="0f4a" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">创建或修改<code class="fe oe of og nh b">config/s3_sink_connector_02.json</code>。修改第6行，如下所示，以反映您的亚马逊S3桶的名称。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="df67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个接收器连接器与前两个接收器连接器有很大的不同。除了在相应的源连接器中利用SMT之外，我们还在这个接收连接器中使用它们。当记录被写入亚马逊S3时，sink connect使用<a class="ae le" href="https://docs.confluent.io/platform/current/connect/transforms/insertfield.html#insertfield" rel="noopener ugc nofollow" target="_blank"> InsertField </a>转换向每个记录追加三个任意静态字段— <code class="fe oe of og nh b">message_source</code>、<code class="fe oe of og nh b">message_source_engine</code>和<code class="fe oe of og nh b">environment</code>。接收器连接器还使用<a class="ae le" href="https://docs.confluent.io/platform/current/connect/transforms/replacefield.html#replacefield" rel="noopener ugc nofollow" target="_blank"> ReplaceField </a>转换将<code class="fe oe of og nh b">district</code>字段重命名为<code class="fe oe of og nh b">state_province</code>。</p><p id="6352" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">前两个sink连接器将未压缩的JSON格式文件写入亚马逊S3。第三个接收器连接器优化了导入S3的数据，以便进行下游数据分析。接收器连接器将GZIP压缩的Apache Parquet文件写到亚马逊S3。此外，压缩的拼花文件由<code class="fe oe of og nh b">country</code>字段<a class="ae le" href="https://docs.aws.amazon.com/athena/latest/ug/partitions.html" rel="noopener ugc nofollow" target="_blank">分区</a>。使用列文件格式、压缩和分区，对数据的查询应该更快、更有效。</p><h2 id="596b" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">部署连接器#3</h2><p id="5f1a" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">使用Kafka Connect REST接口部署最终的源和接收器连接器，就像前两对连接器一样。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="574b" class="mu lx it nh b gy nl nm l nn no">curl -s -d @"config/jdbc_source_connector_postgresql_02.json" \<br/>    -H "Content-Type: application/json" \<br/>    -X PUT <a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/config" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/jdbc_source_connector_postgresql_02/config</a> | jq</span><span id="6a19" class="mu lx it nh b gy pd nm l nn no">curl -s -d @"config/s3_sink_connector_02.json" \<br/>    -H "Content-Type: application/json" \<br/>    -X PUT <a class="ae le" href="http://localhost:8083/connectors/s3_sink_connector_00/config" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/s3_sink_connector_02/config</a> | jq</span></pre><h2 id="fc15" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">确认连接器#3成功</h2><p id="b41d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">使用与前面相同的命令，确认新的连接器集与前两组连接器一起部署并运行，总共有六个连接器。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="fcee" class="mu lx it nh b gy nl nm l nn no">curl -s -X GET <a class="ae le" href="http://localhost:8083/connectors" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors</a> | \<br/>  jq '. | sort_by(.)'</span><span id="eb0d" class="mu lx it nh b gy pd nm l nn no">curl -s -H "Content-Type: application/json" \<br/>    -X GET <a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/jdbc_source_connector_postgresql_02/status</a> | jq</span><span id="3692" class="mu lx it nh b gy pd nm l nn no">curl -s -H "Content-Type: application/json" \<br/>    -X GET <a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors/</a>s3_sink_connector_02<a class="ae le" href="http://localhost:8083/connectors/jdbc_source_connector_postgresql_00/status" rel="noopener ugc nofollow" target="_blank">/status</a> | jq</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/c4d12a4c3c62217d72586db6e8c02ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KoZWEgVrHiF1xbGE73ogRA.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">Kafka连接源和接收器连接器成功运行</figcaption></figure><p id="dd9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看新的<code class="fe oe of og nh b">pagila.query</code>主题中的消息，注意源连接器已经将<code class="fe oe of og nh b">message_topic</code>字段附加到消息中，但没有将<code class="fe oe of og nh b">message_source</code>、<code class="fe oe of og nh b">message_source_engine</code>和<code class="fe oe of og nh b">environment</code>字段附加到消息中。接收器连接器在将邮件写入S3时会附加这些字段。另外，注意<code class="fe oe of og nh b">district</code>字段还没有被接收器连接器重命名为<code class="fe oe of og nh b">state_province</code>。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/4ff2c79fe62752960cc43f04ff39d91c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIepj8AZnmuDRard2gdHLw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">显示Kafka <code class="fe oe of og nh b">pagila.query </code>主题中消息的输出</figcaption></figure><p id="ffd4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次检查亚马逊S3存储桶，您应该注意到在<code class="fe oe of og nh b">/topics/pagila.query/</code>对象关键字前缀中的第五组S3对象。其中的拼花格式文件由<code class="fe oe of og nh b">country</code>进行分区。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pf"><img src="../Images/44bade49f2f94e2649457cfe40968d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUqYvvplcwzvps0x6ZywUQ.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">显示按国家划分的数据的亚马逊S3存储桶</figcaption></figure><p id="a1f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在每个<code class="fe oe of og nh b">country</code>分区中，都有拼花文件，其记录包含这些国家的地址。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pf"><img src="../Images/88106ccc47d2a8162bb3df028724cd9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eO4XbnPsA8uY3vUdJhPg6w.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">显示GZIP压缩的Apache拼花格式文件的亚马逊S3桶</figcaption></figure><p id="b695" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次使用亚马逊S3控制台的“用S3选择查询”来查看拼花格式文件中包含的数据。或者，您可以使用<code class="fe oe of og nh b">s3</code> API:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="d2b0" class="mu lx it nh b gy nl nm l nn no">export SINK_BUCKET="your-s3-bucket"</span><span id="13f5" class="mu lx it nh b gy pd nm l nn no">export KEY="topics/pagila.query/country=United States/pagila.query+0+0000000003.gz.parquet"</span><span id="175f" class="mu lx it nh b gy pd nm l nn no">aws s3api select-object-content \<br/>    --bucket $SINK_BUCKET \<br/>    --key $KEY \<br/>    --expression "select * from s3object limit 5" \<br/>    --expression-type "SQL" \<br/>    --input-serialization '{"Parquet": {}}' \<br/>    --output-serialization '{"JSON": {}}' "output.json" \<br/>  &amp;&amp; cat output.json | jq \<br/>  &amp;&amp; rm output.json</span></pre><p id="9666" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的示例数据中，注意添加到每个记录中的四个新字段，这是源和接收器连接器SMT的结果。另外，请注意重命名的<code class="fe oe of og nh b">district</code>字段:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="ee73" class="mu lx it nh b gy nl nm l nn no">{<br/>  "address_id": 599,<br/>  "address": "1895 Zhezqazghan Drive",<br/>  "address2": "",<br/>  "city": "Garden Grove",<br/><strong class="nh iu">  "state_province": "California",<br/></strong>  "postal_code": "36693",<br/>  "country": "United States",<br/>  "phone": "137809746111",<br/>  "last_update": "2017-02-15T09:45:30.000Z",<br/><strong class="nh iu">  "message_topic": "pagila.query",<br/>  "message_source": "pagila",<br/>  "message_source_engine": "postgresql",<br/>  "environment": "development"<br/></strong>}<br/>{<br/>  "address_id": 6,<br/>  "address": "1121 Loja Avenue",<br/>  "address2": "",<br/>  "city": "San Bernardino",<br/><strong class="nh iu">  "state_province": "California",<br/></strong>  "postal_code": "17886",<strong class="nh iu"><br/></strong>  "country": "United States",<br/>  "phone": "838635286649",<br/>  "last_update": "2017-02-15T09:45:30.000Z",<br/><strong class="nh iu">  "message_topic": "pagila.query",<br/>  "message_source": "pagila",<br/>  "message_source_engine": "postgresql",<br/>  "environment": "development"<br/></strong>}<br/>{<br/>  "address_id": 18,<br/>  "address": "770 Bydgoszcz Avenue",<br/>  "address2": "",<br/>  "city": "Citrus Heights",<br/><strong class="nh iu">  "state_province": "California",<br/></strong>  "postal_code": "16266",<br/>  "country": "United States",<br/>  "phone": "517338314235",<br/>  "last_update": "2017-02-15T09:45:30.000Z",<br/><strong class="nh iu">  "message_topic": "pagila.query",<br/>  "message_source": "pagila",<br/>  "message_source_engine": "postgresql",<br/>  "environment": "development"<br/></strong>}</span></pre><h1 id="dc3b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">记录更新和基于查询的CDC</h1><p id="8912" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">当我们更改Kafka Connect每5秒轮询一次的表中的数据时会发生什么？要回答这个问题，让我们做一些<a class="ae le" href="https://www.geeksforgeeks.org/sql-ddl-dql-dml-dcl-tcl-commands/" rel="noopener ugc nofollow" target="_blank"> DML </a>更改:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="c066" class="mu lx it nh b gy nl nm l nn no">-- update address field<br/>UPDATE public.address<br/>SET address = '123 CDC Test Lane'<br/>WHERE address_id = 100;<br/></span><span id="f8fd" class="mu lx it nh b gy pd nm l nn no">-- update address2 field<br/>UPDATE public.address<br/>SET address2 = 'Apartment #2201'<br/>WHERE address_id = 101;</span><span id="1a99" class="mu lx it nh b gy pd nm l nn no">-- second update to same record<br/>UPDATE public.address<br/>SET address2 = 'Apartment #2202'<br/>WHERE address_id = 101;</span><span id="4918" class="mu lx it nh b gy pd nm l nn no"><br/>-- insert new country<br/>INSERT INTO public.country (country)<br/>values ('Wakanda');</span><span id="d996" class="mu lx it nh b gy pd nm l nn no">-- should be 110<br/>SELECT country_id FROM country WHERE country='Wakanda';</span><span id="95f2" class="mu lx it nh b gy pd nm l nn no">-- insert new city<br/>INSERT INTO public.city (city, country_id)<br/>VALUES ('Birnin Zana', 110);</span><span id="6836" class="mu lx it nh b gy pd nm l nn no">-- should be 601<br/>SELECT city_id FROM public.city WHERE country_id=110;</span><span id="89a0" class="mu lx it nh b gy pd nm l nn no">-- update city_id to new city_id<br/>UPDATE public.address<br/>SET phone = city_id = 601<br/>WHERE address_id = 102;</span><span id="bde6" class="mu lx it nh b gy pd nm l nn no">-- second update to same record<br/>UPDATE public.address<br/>SET district = 'Lake Turkana'<br/>WHERE address_id = 102;</span><span id="9104" class="mu lx it nh b gy pd nm l nn no">-- delete an address record<br/>UPDATE public.customer<br/>SET address_id = 200<br/>WHERE customer_id IN (<br/>    SELECT customer_id FROM customer WHERE address_id = 104);</span><span id="f86a" class="mu lx it nh b gy pd nm l nn no">DELETE<br/>FROM public.address<br/>WHERE address_id = 104;</span></pre><p id="d2de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要了解这些变化是如何传播的，首先，检查Kafka Connect日志。下面，我们将看到与上面显示的一些数据库更改相对应的示例日志事件。三个Kafka Connect源连接器检测更改，这些更改从PostgreSQL导出到Kafka。然后，三个接收器连接器将这些对新JSON和Parquet文件的更改写入目标S3存储桶。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pe"><img src="../Images/61a802715146a9b06bc91e7d939ee8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5VVVN441G8RSUuENDQT0nA.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">Kafka连接日志显示正在导出/导入的Pagila数据库的更改</figcaption></figure><h2 id="5771" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">查看数据湖中的数据</h2><p id="33b5" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在我们的数据湖中检查现有数据和正在发生的数据变化的一种便捷方式是<a class="ae le" href="https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html" rel="noopener ugc nofollow" target="_blank">抓取</a>并用<a class="ae le" href="https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html" rel="noopener ugc nofollow" target="_blank"> AWS Glue </a>对S3桶的内容进行编目，然后用<a class="ae le" href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html" rel="noopener ugc nofollow" target="_blank"> Amazon Athena </a>查询结果。AWS Glue的数据目录是一个与Apache Hive兼容的、完全管理的、持久的元数据存储。AWS Glue可以在S3存储我们数据的模式、元数据和位置。Amazon Athena是一个基于Presto(PrestoDB)的无服务器特别分析引擎，它可以查询AWS Glue数据目录表和基于S3的底层数据。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pf"><img src="../Images/9ff824e758a33d8b4a782d8dabb29ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xLZ0fHd6FbkwtYHiK7F6Xw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">AWS Glue数据目录显示了五个新表，这是AWS Glue Crawler的结果</figcaption></figure><p id="dd58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当将拼花写到分区中时，Kafka Connect S3接收器连接器的一个缺点是在AWS胶中重复列名。因此，任何用作分区的列都会在粘合数据目录的数据库表模式中复制。该问题将导致在执行查询时出现类似于<code class="fe oe of og nh b">HIVE_INVALID_METADATA: Hive metadata for table pagila_query is invalid: Table descriptor contains duplicate columns</code>的错误。要解决这个问题，请预定义表和表的模式。或者，在搜索后编辑粘合数据目录表的模式，并删除重复的非分区列。下面，这意味着删除重复的<code class="fe oe of og nh b">country</code>列7。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pf"><img src="../Images/4f740ca73d3f3e8e2411511219e06921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vZOG_wmFWfJUkss1XLT4gw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">显示重复列的AWS粘附数据目录表架构</figcaption></figure><p id="cadd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Athena中执行一个典型的SQL SELECT查询将返回所有原始记录以及我们之前所做的更改，作为重复记录(相同的<code class="fe oe of og nh b">address_id</code>主键)。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pf"><img src="../Images/e158c5a7846fe220c539667a5ce5dc0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MwU3P_-FdIy1iccUsx1RQ.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">Amazon Athena展示了SQL查询和结果集</figcaption></figure><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="615f" class="mu lx it nh b gy nl nm l nn no">SELECT address_id, address, address2, city, state_province,<br/>         postal_code, country, last_update<br/>FROM "pagila_kafka_connect"."pagila_query"<br/>WHERE address_id BETWEEN 100 AND 105<br/>ORDER BY address_id;</span></pre><p id="3a21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意<code class="fe oe of og nh b">address_id</code>100–103的原始记录以及我们之前所做的每个更改。<code class="fe oe of og nh b">last_update</code>字段反映了记录创建或更新的日期和时间。另外，请注意查询结果中带有<code class="fe oe of og nh b">address_id</code> 104的记录。这是我们从帕吉拉数据库中删除的记录。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="8bb3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要仅查看<em class="lf">的</em>最新数据，我们可以使用Athena的<code class="fe oe of og nh b">ROW_NUMBER()</code> <a class="ae le" href="https://prestodb.io/docs/current/release/release-0.75.html#row-number-optimizations" rel="noopener ugc nofollow" target="_blank">函数</a>:</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="e94e" class="mu lx it nh b gy nl nm l nn no">SELECT address_id, address, address2, city, state_province,<br/>         postal_code, country, last_update<br/>FROM (SELECT *, ROW_NUMBER() OVER (<br/>                  PARTITION BY address_id<br/>                  ORDER BY last_UPDATE DESC) AS row_num<br/>      FROM "pagila_kafka_connect"."pagila_query") AS x<br/>WHERE x.row_num = 1<br/>  AND address_id BETWEEN 100 AND 105<br/>ORDER BY address_id;</span></pre><p id="783f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们只看到最新的记录。不幸的是，我们用<code class="fe oe of og nh b">address_id</code> 104删除的记录仍然存在于查询结果中。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="1d94" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Debezium中使用基于日志的CDC，而不是基于查询的CDC，我们会在S3收到一条指示删除的记录。如下所示，空值消息在Kafka中被称为<a class="ae le" href="https://rmoff.net/2020/11/03/kafka-connect-ksqldb-and-kafka-tombstone-messages/" rel="noopener ugc nofollow" target="_blank">墓碑消息</a>。注意delete记录的“before”语法，这与我们前面观察到的update记录的“after”语法相反。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="f815" class="mu lx it nh b gy nl nm l nn no">{<br/><strong class="nh iu">  "before": {<br/>    "address": "",<br/>    "address2": null,<br/>    "phone": "",<br/>    "district": "",<br/>    "last_update": "1970-01-01T00:00:00Z",<br/>    "address_id": 104,<br/>    "postal_code": null,<br/>    "city_id": 0<br/>  },</strong><br/>  "source": {<br/>    "schema": "public",<br/>    "sequence": "[\"1101256482032\",\"1101256482032\"]",<br/>    "xmin": null,<br/>    "connector": "postgresql",<br/>    "lsn": 1101256483936,<br/>    "name": "pagila",<br/>    "txId": 17137,<br/>    "version": "1.6.1.Final",<br/>    "ts_ms": 1628864251512,<br/>    "snapshot": "false",<br/>    "db": "pagila",<br/>    "table": "address"<br/>  },<br/>  "op": "d",<br/>  "ts_ms": 1628864251671<br/>}</span></pre><p id="68b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用基于查询的CDC进行复制和删除的一个<em class="lf">低效的</em>解决方案是每次从Pagila数据库中批量接收整个查询结果集，而不仅仅是基于<code class="fe oe of og nh b">last_update</code>字段的更改。对大型数据集重复执行无限制的查询会对数据库性能产生负面影响。尽管如此，除非在重新导入新的查询结果之前先清除S3中的数据，否则数据湖中仍然会有重复的数据。</p><h2 id="e1d1" class="mu lx it bd ly mv mw dn mc mx my dp mg kr mz na mi kv nb nc mk kz nd ne mm nf bi translated">数据传送</h2><p id="3667" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">使用Amazon Athena，我们可以轻松地将<code class="fe oe of og nh b">ROW_NUMBER()</code>查询的结果写回到数据湖中，以便进一步丰富或分析。Athena的<code class="fe oe of og nh b">CREATE TABLE AS SELECT </code> ( <a class="ae le" href="https://docs.aws.amazon.com/athena/latest/ug/ctas.html" rel="noopener ugc nofollow" target="_blank"> CTAS </a> ) SQL语句根据子查询中<code class="fe oe of og nh b">SELECT</code>语句的结果在Athena(AWS粘合数据目录中的外部表)中创建新表。Athena将CTAS语句创建的数据文件存储在Amazon S3的指定位置，并创建了一个新的AWS Glue数据目录表来存储结果集的模式和元数据信息。CTAS支持多种文件格式和存储选项。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/844106b55f7a2e370ba449606e167173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ksj7hgW6oleJKfScNkvASw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">这篇文章演示的高级架构</figcaption></figure><p id="2b50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将最后一个查询封装在Athena的CTAS语句中，如下所示，我们可以将查询结果写成快速压缩的Parquet格式文件，由<code class="fe oe of og nh b">country</code>进行分区，写入亚马逊S3桶中的一个新位置。使用<a class="ae le" href="https://www.jamesserra.com/archive/2019/10/databricks-delta-lake/" rel="noopener ugc nofollow" target="_blank">公共数据湖术语</a>，我将把过滤和清理后的数据集称为<em class="lf">精炼的</em>或<em class="lf">银的</em>，而不是通过Kafka从我们的数据源PostgreSQL获得的<em class="lf">原始摄取的</em>或<em class="lf">青铜的</em>数据。</p><pre class="lh li lj lk gt ng nh ni nj aw nk bi"><span id="6048" class="mu lx it nh b gy nl nm l nn no">CREATE TABLE pagila_kafka_connect.pagila_query_processed<br/>WITH (<br/>  format='PARQUET',<br/>  parquet_compression='SNAPPY',<br/>  partitioned_by=ARRAY['country'],<br/>  external_location='s3://your-s3-bucket/processed/pagila_query'<br/>) AS<br/>SELECT address_id, last_update, address, address2, city,<br/>         state_province, postal_code, country<br/>FROM (SELECT *, ROW_NUMBER() OVER (<br/>                  PARTITION BY address_id<br/>                  ORDER BY last_update DESC) AS row_num<br/>      FROM "pagila_kafka_connect"."pagila_query") AS x<br/>WHERE x.row_num = 1 AND address_id BETWEEN 0 and 100<br/>ORDER BY address_id;</span></pre><p id="6b2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">检查亚马逊S3桶，最后一次，你应该在<code class="fe oe of og nh b">/processed/pagila_query/</code>键路径中新建一组S3对象。按国家划分的拼花格式文件是CTAS查询的结果。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pf"><img src="../Images/ba1ee181d14c31421bed6c7b24e78cac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v3MQtCUud8or9BI8cL8Wzw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">亚马逊S3桶显示包含CTAS查询结果的快速压缩拼花格式文件</figcaption></figure><p id="8f2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们应该在同一个AWS Glue数据目录中看到一个新表，其中包含我们使用CTAS查询写入S3的数据的元数据、位置和模式信息。我们可以对处理后的数据执行额外的查询。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi pf"><img src="../Images/f0fd419b59e3998e1487a41f3ce7e6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x5Pulttfr0asFehxhOU13w.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">Amazon Athena显示来自AWS Glue数据目录中已处理数据表的查询结果</figcaption></figure><h1 id="686d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">具有数据湖的ACID事务</h1><p id="a4e4" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">为了充分利用CDC并最大化数据湖中数据的新鲜度，我们还需要采用现代数据湖文件格式，如<a class="ae le" href="https://hudi.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache胡迪</a>、<a class="ae le" href="https://iceberg.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache冰山</a>或<a class="ae le" href="https://delta.io/" rel="noopener ugc nofollow" target="_blank"> Delta湖</a>，以及分析引擎，如<a class="ae le" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>和<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark结构化流</a>来处理数据变化。使用这些技术，可以在像亚马逊S3这样的对象存储中执行记录级的数据更新和删除。胡迪、Iceberg和Delta Lake提供的功能包括ACID事务、模式演化、增插、删除、时间旅行和数据湖中的增量数据消耗。像Spark这样的ELT引擎可以从Kafka读取流Debezium生成的CDC消息，并使用胡迪、冰山或三角洲湖处理这些变化。</p><h1 id="515b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="0838" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这篇文章探讨了CDC如何帮助我们将来自亚马逊RDS数据库的数据合并到基于亚马逊S3的数据湖中。我们利用了亚马逊EKS、亚马逊MSK和Apache Kafka Connect的功能。我们学习了基于查询的CDC，用于捕获对源数据的持续更改。在随后的<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/hydrating-a-data-lake-using-log-based-change-data-capture-cdc-with-debezium-apicurio-and-kafka-799671e0012f">文章</a>中，我们将使用Debezium探索基于日志的CDC，并了解数据湖文件格式，如Apache Avro、Apache胡迪、Apache Iceberg和Delta Lake，如何帮助我们管理数据湖中的数据。</p></div><div class="ab cl pg ph hx pi" role="separator"><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl"/></div><div class="im in io ip iq"><p id="bfcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本博客代表我自己的观点，不代表我的雇主亚马逊网络服务公司(AWS)的观点。所有产品名称、徽标和品牌都是其各自所有者的财产。</p></div></div>    
</body>
</html>