<html>
<head>
<title>Shrink your Tensorflow.js Web Model Size with Weight Quantization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过权重量化缩小Tensorflow.js Web模型的大小</h1>
<blockquote>原文：<a href="https://itnext.io/shrink-your-tensorflow-js-web-model-size-with-weight-quantization-6ddb4fcb6d0d?source=collection_archive---------0-----------------------#2018-09-24">https://itnext.io/shrink-your-tensorflow-js-web-model-size-with-weight-quantization-6ddb4fcb6d0d?source=collection_archive---------0-----------------------#2018-09-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8580" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Tensorflow.js权重量化快速简单指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/030b919c1bce75b2857c431d7e7bd21a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*0RGGk9v5XQRVYeBDuDOQgQ.jpeg"/></div></figure><p id="2431" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">由于<a class="ae lj" href="https://github.com/tensorflow/tfjs" rel="noopener ugc nofollow" target="_blank"> <strong class="kp ir"> tensorflow.js </strong> </a>团队的出色工作，浏览器中的机器学习已经成为我们javascript和web开发人员中非常热门的话题。Tensorflow.js允许我们将预训练的模型下载到客户端浏览器，并直接在客户端运行推理。</p><p id="ac82" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">幸运的是，从技术上讲，客户只需下载一次模型，因为通常这样的网络模型会被分成<strong class="kp ir"> 4MB </strong>的碎片，这样浏览器就会<strong class="kp ir">缓存</strong>它们。然而，我们希望初始加载时间尽可能短，并减少客户端为我们的任何模型存储的字节数。</p><h1 id="6338" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">为什么你一定要量化你的模型权重！</h1><p id="a44f" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated"><strong class="kp ir">简单回答:</strong>我们宁愿下载一个<strong class="kp ir"> 15MB </strong>的型号，也不愿下载一个<strong class="kp ir"> 60MB </strong>的型号，对吧？这是显而易见的！是的，我们可以将模型的尺寸缩小<strong class="kp ir"> 4 </strong>并且基本上是<strong class="kp ir">免费</strong>！我对<a class="ae lj" href="https://github.com/justadudewhohacks/face-api.js" rel="noopener ugc nofollow" target="_blank"> <strong class="kp ir"> face-api.js </strong> </a>曝光的所有模特都使用这种技术。</p><p id="7d66" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">简而言之，利用权重量化，我们可以通过将由<strong class="kp ir"> 255⁴ </strong>位表示的范围【最小值，最大值】中的每个张量的值映射到由<strong class="kp ir"> 255 </strong>位表示的范围【0，255】来将我们的模型参数从<strong class="kp ir">float 32</strong>s<strong class="kp ir">s</strong>(4字节)压缩到<strong class="kp ir">uint 8</strong>s<strong class="kp ir">t29】(单字节)。因此，我们减去张量值的最小值，并对其应用一个比例因子。我们存储每个张量的最小值和比例以及我们的模型元数据，一旦我们再次加载模型权重，我们就应用逆运算(<strong class="kp ir">去量化</strong>)。</strong></p><h1 id="8fdd" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">模型的精度呢？</h1><p id="98a9" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">现在你可能想知道:将张量值从一个<strong class="kp ir"> 255⁴ </strong>位表示映射到<strong class="kp ir"> 255 </strong>位？我的模型在准确性上肯定有严重的损失，对吗？嗯，不一定。更准确地说，在这个过程中，张量值将被剪裁，这样它们最终会具有较低的浮点精度，但根据我的经验，在大多数情况下，整体模型精度根本不会受到权重量化的影响。</p><p id="1faa" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">平心而论，也有一些例外，当一些张量出现不幸的值分布时，我用从dlib移植到tfjs的人脸识别模型到目前为止只遇到过一次。但在这些情况下，这并不意味着，根本不可能减少我们的模型权重的大小。事实上，你可以识别这些张量，并简单地保持它们的权重不变，同时我们仍然可以减少剩余张量的大小(如果你遇到这个问题，参见上一节)。</p><h1 id="ee29" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">好吧，我相信了！但是怎么做呢？</h1><p id="5f3f" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">好问题！这可能非常简单，也可能非常简单。首先，我们将讨论非常简单的情况，即你已经有一个tensorflow或keras模型。当您将模型转换为web模型时，您可以简单地使用<a class="ae lj" href="https://github.com/tensorflow/tfjs-converter" rel="noopener ugc nofollow" target="_blank"> <strong class="kp ir"> tfjs-converter </strong> </a>工具并将量化标志传递给CLI:</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="717b" class="mm ll iq mi b gy mn mo l mp mq">tensorflowjs_converter --quantization_bytes 1 --input_format=tf_frozen_model --output_node_names=logits/BiasAdd --saved_model_tags=serve ./model/input_graph.pb ./web_model</span></pre><p id="6d4d" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">显然，这是量化模型权重的最简单也是最好的方法。所以如果可以的话，我会推荐使用这个工具。</p><h2 id="7f6c" class="mm ll iq bd lm mr ms dn lq mt mu dp lu kw mv mw lw la mx my ly le mz na ma nb bi translated"><strong class="ak">但是如果:</strong>怎么办</h2><ol class=""><li id="a415" class="nc nd iq kp b kq mc kt md kw ne la nf le ng li nh ni nj nk bi translated">正如上一节指出的，我们的一个或多个张量是令人讨厌的那种，它扰乱了我们模型的准确性？</li><li id="9e91" class="nc nd iq kp b kq nl kt nm kw nn la no le np li nh ni nj nk bi translated">我们把一些现有的模型架构(caffee，torch，darknet，随便什么…)直接移植到tfjs？</li><li id="a127" class="nc nd iq kp b kq nl kt nm kw nn la no le np li nh ni nj nk bi translated">我们在浏览器中用tfjs训练了我们的模型？</li></ol><h2 id="d726" class="mm ll iq bd lm mr ms dn lq mt mu dp lu kw mv mw lw la mx my ly le mz na ma nb bi translated"><strong class="ak">简答:</strong></h2><p id="b656" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">我们简单地通过tfjs-converter <a class="ae lj" href="https://github.com/tensorflow/tfjs-converter/blob/master/python/tensorflowjs/quantization.py" rel="noopener ugc nofollow" target="_blank"> <strong class="kp ir">量子化脚本</strong> </a>运行我们的张量。</p><h2 id="5be8" class="mm ll iq bd lm mr ms dn lq mt mu dp lu kw mv mw lw la mx my ly le mz na ma nb bi translated">长回答:</h2><p id="ef28" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">显然，这是一个python脚本，隐藏在tfjs-converter源代码的深处。我们必须存储所有张量数据，通过脚本运行它们，并构建<strong class="kp ir"> weights_manifest.json </strong>文件。当然，你当然可以这么做。或者，你可以用javascript做任何事情:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="0e06" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">使用javascript量化实现，我们可以将以下内容复制并粘贴到一个html页面中，添加用于加载我们想要收缩的模型的权重张量的逻辑，在浏览器中打开它，并从控制台调用<strong class="kp ir"><em class="ns">quantizandsave()</em></strong>，这将下载量化的模型碎片和<strong class="kp ir"> weights_manifest.json </strong>文件:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="3122" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">注意，<strong class="kp ir"><em class="ns">getNamedTensors</em></strong>的空函数体。这里你要实现自己的逻辑，它返回<strong class="kp ir"> { name: string，tensor: tf。</strong>张量}对。如果我们查看一个<strong class="kp ir"> weights_manifest.json </strong>文件，我们可以看到每个张量都被命名为:</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="8a02" class="mm ll iq mi b gy mn mo l mp mq">{<br/>  "name":"conv0/filter",<br/>  "shape":[3,3,3,32],<br/>  "dtype":"float32",<br/>  "quantization":{<br/>    "dtype":"uint8",<br/>    "scale":0.004699238725737029,<br/>    "min":-0.7471789573921876<br/>  }<br/>}</span></pre><p id="cdbf" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">此外，我们使用<strong class="kp ir"> FileSaver.js </strong>来下载文件。简单地从<a class="ae lj" href="https://github.com/eligrey/FileSaver.js" rel="noopener ugc nofollow" target="_blank"> <strong class="kp ir">这个回购</strong> </a>或者通过运行<strong class="kp ir"> npm i文件保护</strong>获得这个脚本。</p><p id="a6a4" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">之后，我们可以简单地使用<strong class="kp ir"><em class="ns">TF . io . load weights(manifest，modelBaseUri) </em> </strong>加载我们的压缩模型，如下所示:</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="58c2" class="mm ll iq mi b gy mn mo l mp mq">const manifest = await (await fetch('/models/my-model-weights_manifest.json')).json()</span><span id="f208" class="mm ll iq mi b gy nt mo l mp mq">const tensorMap = tf.io.loadWeights(manifest, '/models')</span></pre><p id="0355" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这将对每个张量应用反量化，在其清单条目中有一个“<strong class="kp ir">量化</strong>”对象。最后，它返回命名的张量图，就这样:</p><pre class="kg kh ki kj gt mh mi mj mk aw ml bi"><span id="13f0" class="mm ll iq mi b gy mn mo l mp mq">{<br/>  "conv0/filter": tf.Tensor4D,<br/>  ...<br/>}</span></pre><h1 id="8dfc" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">那么，如果我的模型的精度确实下降了呢？</h1><p id="c77c" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">如果您没有注意到权重量化后模型精度的任何损失，一切都很好。但是，如果准确性确实受到量子化的影响，如何识别罪魁祸首呢？</p><p id="52ee" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">不幸的是，这是一个有点棘手的情况。在这种情况下，您应该迭代地尝试从量化中排除张量(或尝试迭代地包括张量)，以确定张量，这将导致模型精度在其值被量化后下降。</p><p id="38e3" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">要跳过张量的量化，只需确保在<strong class="kp ir"><em class="ns">getNamedTensors</em></strong>中为相应的张量设置了“<strong class="kp ir"><em class="ns">isSkipQuantization</em></strong>”标志。</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="fce1" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="ns">如果你喜欢这篇文章，欢迎留下一些掌声，并在medium和/或</em><a class="ae lj" href="https://twitter.com/justadudewhohax" rel="noopener ugc nofollow" target="_blank"><em class="ns">Twitter</em></a><em class="ns">:)上关注我。也请继续关注进一步的文章，如果你有兴趣，请查看我的</em> <a class="ae lj" href="https://github.com/justadudewhohacks" rel="noopener ugc nofollow" target="_blank"> <em class="ns">开源作品</em> </a> <em class="ns">！</em></p></div></div>    
</body>
</html>