<html>
<head>
<title>Let’s Build a Web Scraper with Python &amp; BeautifulSoup4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们用Python和BeautifulSoup4构建一个Web Scraper</h1>
<blockquote>原文：<a href="https://itnext.io/lets-build-a-web-scraper-with-python-beautifulsoup4-2b550d10438?source=collection_archive---------1-----------------------#2020-11-23">https://itnext.io/lets-build-a-web-scraper-with-python-beautifulsoup4-2b550d10438?source=collection_archive---------1-----------------------#2020-11-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6b9176f6127254b20351ff8f447a4e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oHuSrEZgsGxNIcYc-9SDAw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">亚历克斯·伊比在<a class="ae kc" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="7e97" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章最初发表在我的博客上——https://thecodingpie.com<a class="ae kc" href="https://thecodingpie.com" rel="noopener ugc nofollow" target="_blank"/></p><p id="342e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想知道如何自动化抓取网站、收集数据并将其导出为CSV等有用格式的过程吗？如果你在做数据科学/机器学习，那么你可能已经经历过几次这种情况。</p><p id="1d50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是我写这篇教程的原因，在这篇教程中，你将通过构建一个Python脚本来学习所有关于Web抓取的知识，该脚本将抓取一个电影网站并获取有用的信息，最后，它将把收集到的数据导出到一个CSV(逗号分隔值)文件中。</p><p id="032c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而且好的是再也不用手动做网页抓取了！</p><p id="1f84" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">听起来很有趣？那我们就直入主题吧。</p><p id="93af" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以从我的Github repo — <a class="ae kc" href="https://github.com/the-coding-pie/web_scraper" rel="noopener ugc nofollow" target="_blank"> Web Scraper </a>下载完成的代码</p><h1 id="9baf" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">什么是网络抓取</h1><p id="c9b3" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">网络抓取是从互联网上的任何网站收集有用/需要的信息的过程。像任何其他过程一样，有两种方法可以做到:一种是从网站手动复制粘贴所需的数据。另一种方式，传说的方式，是聪明地自动化它！</p><p id="e906" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你想成为第二类人。但是这样做有一些挑战…</p><p id="5939" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个挑战是，并不是所有的网站所有者都喜欢抓取他们网站的过程。<strong class="kf ir">因此，如果你打算在网站上进行网络抓取，那么请确保他们允许你这样做</strong>。</p><p id="2dc4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二个挑战是，并非所有的网站都是一样的。我的意思是你为一个网站写的剧本不能用于其他网站。因为两个网站的结构完全不同。也许几天后你甚至不能在同一个网站上使用相同的脚本，因为网站开发者一直在改变他们网站的布局，以应对网页抓取工具。</p><h1 id="f5d5" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">网页抓取替代方案</h1><p id="d2bb" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">如果有这么多挑战，有没有替代方案？是的，有一个替代方案叫做<strong class="kf ir"> API </strong>。<strong class="kf ir">应用编程接口</strong>是从任何网站获取数据的唯一<strong class="kf ir">合法</strong>和<strong class="kf ir">稳定</strong>的方式。</p><p id="343b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大多数网站都提供了一个API，通过它你可以得到你想要的更好的格式的数据，比如JSON或者XML。但是有一个问题，你可能要付钱。当然，可能会有<strong class="kf ir">免费</strong>的计划，但从长远来看，为了使用他们<strong class="kf ir">珍贵的</strong>数据，你必须向他们付费。</p><p id="5f01" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是网络抓取概念派上用场的地方！</p><h1 id="83af" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">我们将要建造的东西</h1><p id="932b" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们将通过构建一个真实的项目来学习使用Python和BeautifulSoup4进行Web抓取的所有知识。</p><p id="972d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我不想教你如何刮一个千变万化的动态网站而让你头疼。所以，我建立了一个静态电影网站，名为<a class="ae kc" href="https://the-coding-pie.github.io/top_movies/" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> TopMovies </strong> </a>，其中包含了一份IMDb电影<strong class="kf ir">前25名的列表</strong>。这是我们要刮的网站。因此，在继续之前，请先检查一下— <a class="ae kc" href="https://the-coding-pie.github.io/top_movies/" rel="noopener ugc nofollow" target="_blank">热门电影</a>。</p><p id="58aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请看<a class="ae kc" href="https://the-coding-pie.github.io/top_movies/" rel="noopener ugc nofollow" target="_blank">热门电影</a>网站上有一份<strong class="kf ir">25部IMDb电影</strong>的名单。每部电影都包含以下细节:</p><ul class=""><li id="08f1" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">title</code></li><li id="fc02" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">genre</code></li><li id="49ef" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">rating</code></li><li id="88b6" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">length</code> —电影运行时间</li><li id="5be7" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">year</code></li><li id="633c" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">budget</code></li><li id="919c" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">gross</code></li><li id="92be" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">img</code></li></ul><p id="8049" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将从热门电影网站上搜集这些细节。然后，在获得所有这些详细信息后，我们将把它导出为CSV等有用的格式，以便您以后可以将其导入到您的数据科学项目中，并可以毫无顾虑地进行一些预测！</p><p id="9d60" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，我们要刮下面这个网站:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/c5f1009cc70e31abb06bacd10269d30c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8zt_wFLyhTAPgT0LwLw7w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">我们将抓取这个网站并…</figcaption></figure><p id="1d1c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将抓取的数据导出为CSV文件，如下所示:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/cd15136b85e762957e3e45b30b416ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XSwnb6o5m18_LoB7-hDm1A.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">…生成这样的CSV文件</figcaption></figure><p id="afe1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，如果你愿意，你可以在Jupyter笔记本里把它作为一个熊猫数据框来阅读，你可以很容易地做所有的分析和预测！：</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/d8038d3488625dd2395fcf0f0f73bc79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LwDRD4D9lIfu3TvKydeQzg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">生成的CSV文件在jupyter笔记本中读取为熊猫数据帧。</figcaption></figure><p id="c449" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你不是数据科学/机器学习的人，那就不要管这最后一张图片，只是忘了它！</p><p id="7e56" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过做这个简单的项目，你将学会建立任何一种能够抓取任何你想删除的网站的网络抓取工具的技巧。您还将学习如何使用Python生成CSV文件。</p><h1 id="a91d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">我们要怎么做呢？</h1><p id="a96c" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">非常简单明了:</p><ul class=""><li id="3530" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">首先，我们将使用<code class="fe mn mo mp mq b">requests</code>库获取我们想要的网页。</li><li id="6172" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">然后，我们将在合适的解析器(如<code class="fe mn mo mp mq b">lxml</code>)的帮助下，将该页面转换成一个<code class="fe mn mo mp mq b">BeautifulSoup</code>对象。这将使刮削过程容易得多。</li><li id="ac7e" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">然后我们将从soup对象中获取所有需要的数据。</li><li id="b22f" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">最后，我们将在<code class="fe mn mo mp mq b">csv</code>模块的帮助下，将所有抓取的数据导出到一个名为<code class="fe mn mo mp mq b">top25.csv</code>的文件中。</li></ul><p id="ac10" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！</p><h1 id="7e70" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">先决条件</h1><ul class=""><li id="ba51" class="me mf iq kf b kg lz kk ma ko nd ks ne kw nf la mj mk ml mm bi translated">你应该擅长python3。</li><li id="8ae1" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">你应该对HTML和CSS有一点点的了解。</li><li id="da44" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">你的电脑上应该安装了<strong class="kf ir"> python3.4 </strong>或更高版本。你可以阅读这篇文章来学习如何在任何操作系统上安装python 3—<a class="ae kc" href="https://realpython.com/installing-python/" rel="noopener ugc nofollow" target="_blank">https://realpython.com/installing-python/</a></li><li id="31e7" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">你应该已经安装了<strong class="kf ir"> venv </strong>。</li><li id="7561" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">最后，你需要一个现代的代码编辑器，比如visual studio code<strong class="kf ir">。您可以根据您的操作系统从这里下载visual studio代码—<a class="ae kc" href="https://code.visualstudio.com/download" rel="noopener ugc nofollow" target="_blank">https://code.visualstudio.com/download</a>。</strong></li></ul><p id="8d19" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了这些东西，现在让我们开始吧。</p><h1 id="cfd3" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">初始设置</h1><ul class=""><li id="dec7" class="me mf iq kf b kg lz kk ma ko nd ks ne kw nf la mj mk ml mm bi translated">首先，在你的电脑上创建一个名为<code class="fe mn mo mp mq b">web_scraper</code>的文件夹。</li><li id="70cc" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">然后在visual studio代码中打开它。</li><li id="b385" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">现在让我们<strong class="kf ir">使用<strong class="kf ir"> venv </strong>创建</strong>一个新的虚拟环境，然后<strong class="kf ir">激活</strong>它。为此:</li><li id="1213" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">在文本编辑器中，打开<strong class="kf ir">终端&gt;新建终端</strong>。</li><li id="f72c" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">然后键入:</li></ul><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="b3f8" class="nk lc iq mq b gy nl nm l nn no">python3 -m venv venv</span></pre><p id="4470" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个命令将为我们创建一个名为<strong class="kf ir"> venv </strong>的虚拟环境。</p><ul class=""><li id="67ba" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">要激活它，如果您在<strong class="kf ir">窗口</strong>上，请键入以下内容:</li></ul><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="2f04" class="nk lc iq mq b gy nl nm l nn no">venv\Scripts\activate.bat</span></pre><ul class=""><li id="d833" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">如果您使用的是Linux/Mac系统，请键入以下内容:</li></ul><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="fde7" class="nk lc iq mq b gy nl nm l nn no">source venv/bin/activate</span></pre><p id="4d4e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，您应该会看到类似这样的内容:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e8c0469e4bf8ac0ad6ac23f9011bbc07.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*QGs8yqmd0uLH77YppxRWyQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">这个前缀意味着您已经成功激活了虚拟环境</figcaption></figure><ul class=""><li id="a97f" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">最后，直接在<code class="fe mn mo mp mq b">web_scraper</code>文件夹中创建一个名为<code class="fe mn mo mp mq b">scraper.py</code>的新文件:</li></ul><p id="93e7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，您应该有一个类似如下的文件结构:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/064a96494d2e2e64b5e8c891c749286c.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*u_cYrJgcGLvKGw1iIL5YAg.png"/></div></figure><p id="0418" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">注意</strong>:如果你还对如何设置虚拟环境感到困惑，那么就来看看这篇<a class="ae kc" href="https://thecodingpie.com/post/a-quick-guide-on-how-to-setup-a-python-virtual-environment-windows-linux-mac/" rel="noopener ugc nofollow" target="_blank">快速指南</a>。</p><p id="41bd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，你已经完成了初始设置，现在是时候开始有趣的事情了！</p><h1 id="cc61" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">获取网页</h1><p id="e7d0" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">让我问你一个问题。为了手动抓取一个网站，我的意思是从一个网站复制粘贴数据，你首先会做什么？</p><p id="fd04" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，你需要打开网络浏览器并输入网址，对吗？因为为了从网页中获取数据，<strong class="kf ir">你必须先加载它</strong>。这正是我们在这里要做的。</p><p id="fe55" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">首先，我们需要从网站</strong>加载网页。但是我们根本不打算使用网络浏览器。相反，我们将使用一个名为<code class="fe mn mo mp mq b"><strong class="kf ir">requests</strong></code>的Python模块。</p><p id="f71f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在<strong class="kf ir">终端</strong>中键入以下命令，然后<strong class="kf ir">安装</strong>模块<code class="fe mn mo mp mq b"><strong class="kf ir">requests</strong></code>:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="d805" class="nk lc iq mq b gy nl nm l nn no">pip install requests</span></pre><p id="2324" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，在<code class="fe mn mo mp mq b">scraper.py</code>文件中键入:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="500a" class="nk lc iq mq b gy nl nm l nn no">import requests</span><span id="6b39" class="nk lc iq mq b gy nr nm l nn no"># fetch the web page<br/>page = requests.get('https://the-coding-pie.github.io/top_movies/')</span></pre><ul class=""><li id="36a7" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">这段代码将<code class="fe mn mo mp mq b"><strong class="kf ir">get</strong></code>中的<code class="fe mn mo mp mq b"><strong class="kf ir">Response</strong></code>对象从<code class="fe mn mo mp mq b"><a class="ae kc" href="https://the-coding-pie.github.io/top_movies/." rel="noopener ugc nofollow" target="_blank">https://the-coding-pie.github.io/top_movies/</a></code>URL中整体取出<a class="ae kc" href="https://the-coding-pie.github.io/top_movies/." rel="noopener ugc nofollow" target="_blank">。但是我们想要网页本身或者网页本身，对吗？</a></li></ul><p id="f638" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了获得web页面<code class="fe mn mo mp mq b"><strong class="kf ir">content</strong></code>，您必须像这样从<code class="fe mn mo mp mq b"><strong class="kf ir">page</strong></code>变量中访问<code class="fe mn mo mp mq b"><strong class="kf ir">content</strong></code>:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/3a041ad094f874f17e92d826c4647921.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*C61vEso0VaSp3zF7tXn0HQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">这将获得页面的内容，即。整个HTML内容。</figcaption></figure><p id="1abe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你<code class="fe mn mo mp mq b">print(page.content)</code>:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/f1b9807d5e5740b69c96d6445ab029ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G2CzZPHBzbvXJd0bFFc8rg.png"/></div></div></figure><p id="18e6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后你会看到网页的<strong class="kf ir"> HTML </strong>是这样的:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/e17f3ff763dfefdb9dfeaba7d457a8dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JRzm6FE6nj7NCBwvLFnylQ.png"/></div></div></figure><p id="e0e4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是这里有一个问题。如果你看一下<code class="fe mn mo mp mq b">type(page.content)</code>:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/1e4bfe7afb2f126c83615f716d57f5c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*minikGTxMKdDQwM22ENxAQ.png"/></div></div></figure><p id="e24d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后你可以看到它的类型是<code class="fe mn mo mp mq b">bytes</code>:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/9e02c0e88ddbf306d643a2f85f605774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*QKwkSCwePsV5o_1COxWjJQ.png"/></div></figure><p id="497b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们无法解析那些字节类型！字节类型是没有用的，除非你把它们转换成其他有用的格式/类型。</p><p id="83a9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在应该做什么？</p><h1 id="7742" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">美丽的救援队！</h1><p id="e3bb" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><a class="ae kc" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank">美汤</a>是一个Python库，用于像上面那样从HTML和XML格式中提取数据。</p><p id="5386" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe mn mo mp mq b"><strong class="kf ir">BeautfulSoup</strong></code>借助<code class="fe mn mo mp mq b"><strong class="kf ir">parser</strong></code> <strong class="kf ir"> </strong>将复杂的HTML文档转换成复杂的Python对象树。</p><p id="4541" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">注:</strong>关于BeautifulSoup是如何工作的，本教程我不想深入探讨。如果你很好奇想知道，那么请使用这个链接——<a class="ae kc" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank">官方美汤文档</a>。</p><p id="dbe0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">简而言之，在</strong> <code class="fe mn mo mp mq b"><strong class="kf ir">BeautfulSoup</strong></code> <strong class="kf ir">和一个</strong> <code class="fe mn mo mp mq b"><strong class="kf ir">parser</strong></code> <strong class="kf ir">的帮助下，我们可以像上面(bytes type)一样，通过将解析后的HTML/XML内容中的一切都视为Python对象，轻松地对其进行导航、搜索、抓取和修改！</strong></p><p id="a7e5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，让我们安装<code class="fe mn mo mp mq b">BeautifulSoup4</code>和一个类似<code class="fe mn mo mp mq b">lxml</code>的解析器。在终端窗口中键入以下命令:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="3faf" class="nk lc iq mq b gy nl nm l nn no">pip install beautifulsoup4</span><span id="799a" class="nk lc iq mq b gy nr nm l nn no">pip install lxml</span></pre><ul class=""><li id="3588" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">lxml</code>是BeautifulSoup社区推荐的解析器。还有一个类似<code class="fe mn mo mp mq b">html5lib</code>的备选。但是我们将坚持使用<code class="fe mn mo mp mq b">lxml</code>解析器。</li></ul><p id="0715" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在在最顶端的<code class="fe mn mo mp mq b">scraper.py</code>文件中键入以下代码:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="ee67" class="nk lc iq mq b gy nl nm l nn no">from bs4 import BeautifulSoup</span></pre><ul class=""><li id="eef7" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">这里我们从<code class="fe mn mo mp mq b">BeautifulSoup</code>库导入<code class="fe mn mo mp mq b">bs4</code>。</li></ul><p id="d274" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后在此行的下方— <code class="fe mn mo mp mq b">page = requests.get(‘https://the-coding-pie.github.io/top_movies/')</code>，键入以下内容:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="3221" class="nk lc iq mq b gy nl nm l nn no"># turn page into a BeautifulSoup object<br/>soup = BeautifulSoup(page.content, 'lxml')</span></pre><ul class=""><li id="6b9a" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">这里，我们将类型为<code class="fe mn mo mp mq b">bytes</code>的<code class="fe mn mo mp mq b">page.content</code>转换成一个<code class="fe mn mo mp mq b">BeautifulSoup</code>对象。</li></ul><h1 id="6cb0" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">让我们刮掉这一页</h1><p id="0565" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">现在我们已经有了整个网页(以一种有用的格式)。剩下的两个工作之一就是刮。让我们开始吧。我们需要从网页中抓取以下内容:</p><ul class=""><li id="4284" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">titles</code> —所有电影名称</li><li id="b8df" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">genres</code> —所有流派</li><li id="48b4" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">ratings</code> —所有电影评分</li><li id="de8c" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">lengths</code> —所有电影运行时间</li><li id="0f1f" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">years</code> —电影上映的所有年份</li><li id="859c" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">budgets</code> —所有预算</li><li id="8515" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">grosses</code> —所有总信息</li><li id="c7e6" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">img_urls</code> —所有图像的src URLs。</li></ul><p id="d772" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我们一个一个来。</p><p id="9509" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们刮掉所有的<code class="fe mn mo mp mq b">titles</code>:</p><p id="adbb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们正在寻找的<code class="fe mn mo mp mq b">title</code>在一个名为<code class="fe mn mo mp mq b">&lt;h3&gt;</code>的HTML元素中。等等，我怎么知道的？</p><p id="f8f6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很简单:</p><ul class=""><li id="0bf1" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">在浏览器中打开你想要搜索的网址。</li></ul><p id="6e02" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的情况下，打开这个<a class="ae kc" href="https://the-coding-pie.github.io/top_movies/" rel="noopener ugc nofollow" target="_blank">热门电影</a>网站。然后:</p><ul class=""><li id="4db8" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated"><strong class="kf ir">借助浏览器上的<strong class="kf ir">开发工具</strong>检查</strong>数据。就我而言，我使用的是Chrome，所以</li><li id="c896" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated"><strong class="kf ir">右击</strong>想要刮除的元素，</li><li id="0740" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">并点击<strong class="kf ir">检查</strong></li></ul><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/00289be1bc49c8664c06543314973f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bfY7XCjmQMsHUZv3lPD_lQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">右键单击元素，然后单击检查</figcaption></figure><ul class=""><li id="b956" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">现在会弹出一个新框，如下所示:</li></ul><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/c53f734091a6b691c94343682ca129bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CgMSzv9vE7kxoGhlNmzVsA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">这是Chrome开发者工具</figcaption></figure><ul class=""><li id="8a2e" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">看，我告诉过你我们正在寻找的<code class="fe mn mo mp mq b">title</code>在一个叫做<code class="fe mn mo mp mq b">&lt;h3&gt;</code>的HTML元素中:</li></ul><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/a5b35e624038efb1a80dfdc2d2c50912.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XhgnZs03Dop4EMpQgQHdWA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">标题数据位于<h3>元素内</h3></figcaption></figure><p id="3fe2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们知道我们的数据在哪里，让我们刮它。在您键入的最后一行下面键入以下内容:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="041e" class="nk lc iq mq b gy nl nm l nn no">""" first, scraping using find_all() method """<br/># scrape all the titles<br/>titles = [] <br/>for h3 in soup.find_all('h3'):<br/>  titles.append(h3.string.strip())</span></pre><p id="42ef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是对上述代码的逐行解释:</p><ul class=""><li id="ddc8" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">我们将在一个名为<code class="fe mn mo mp mq b">titles</code>的数组中存储我们所有的游戏，这就是我们在第一行中所做的，我们正在创建这个<code class="fe mn mo mp mq b">titles</code>数组。</li><li id="5217" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">然后我们在之前创建的<code class="fe mn mo mp mq b">soup</code>对象中使用<code class="fe mn mo mp mq b">find_all()</code>方法，找到我们需要的所有<code class="fe mn mo mp mq b">h3</code>元素。这个<code class="fe mn mo mp mq b">find_all()</code>方法返回一个可迭代列表。所以我们遍历所有找到的<code class="fe mn mo mp mq b">h3</code>元素…</li><li id="912b" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">在最后一行，在<code class="fe mn mo mp mq b">for loop</code>中，我们取<code class="fe mn mo mp mq b">h3.string</code>值。为什么是<code class="fe mn mo mp mq b">string</code>值？因为整体上每个<code class="fe mn mo mp mq b">h3</code>都会这样<code class="fe mn mo mp mq b">&lt;h3&gt; Title inside &lt;/h3&gt;</code>。但是我们只需要最里面的<code class="fe mn mo mp mq b">string</code>在里面，对吗？所以我们用<code class="fe mn mo mp mq b">h3.string</code>。在获取之后，我们<code class="fe mn mo mp mq b">.strip()</code>移除所有尾随的空白。然后我们把它放到数组中。</li></ul><p id="db2d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">喔，里面发生了很多事。所以请花点时间了解一下。这正是我们要从这里开始重复的步骤，以收集所有其他数据。</p><p id="bb9d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们最初使用HTML <code class="fe mn mo mp mq b">bytes</code>类型数据创建的<code class="fe mn mo mp mq b">soup</code>对象为我们提供了如此多的内置方法来轻松导航和抓取HTML树。<code class="fe mn mo mp mq b">find_all()</code>只是其中之一。我们将继续探索其中的一些。</p><p id="3457" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将抓取的数据存储在Python列表中的原因是，将这些列表转换成CSV文件会容易得多，这就是我们这么做的原因。</p><p id="2174" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经收集了所有我们需要的东西。现在让我们继续刮所有的<code class="fe mn mo mp mq b">genres</code>。键入以下代码:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="7960" class="nk lc iq mq b gy nl nm l nn no"># genres<br/>genres = []<br/>for genre in soup.find_all('p', class_='genre'):<br/>  genres.append(genre.string.strip())</span></pre><ul class=""><li id="c977" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">非常相似，但是这次我们用<code class="fe mn mo mp mq b">class_=’genre’</code>找到了所有的<code class="fe mn mo mp mq b">&lt;p&gt;</code>元素。<code class="fe mn mo mp mq b">class</code>是python中的保留关键字，所以我们不能使用它，这就是为什么我们在<code class="fe mn mo mp mq b">class</code>后面加了下划线(<code class="fe mn mo mp mq b">_</code>)。</li></ul><p id="49b9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">剩下的就不言自明了。</p><p id="aff0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们刮掉所有的<code class="fe mn mo mp mq b">ratings</code>，但是使用一个不同的方法叫做<code class="fe mn mo mp mq b">select()</code>:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="fb24" class="nk lc iq mq b gy nl nm l nn no">""" scraping using css_selector eg: select('span.class_name') """<br/># ratings, selecting all span with class="rating"<br/>ratings = []<br/>for rating in soup.select('span.rating'):<br/>  ratings.append(rating.string.strip())</span></pre><ul class=""><li id="489b" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated"><code class="fe mn mo mp mq b">select()</code>方法用于使用CSS选择器之类的语法查找所有元素。在这里，我们选择了所有等级为<code class="fe mn mo mp mq b"> span.rating</code>的<code class="fe mn mo mp mq b">span</code>。然后我们将它们存储在一个名为<code class="fe mn mo mp mq b">ratings</code>的Python列表中。</li></ul><p id="4dd9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在该做一个小练习了。使用<code class="fe mn mo mp mq b">select()</code>方法，你必须清除所有的<code class="fe mn mo mp mq b">lengths</code>(电影运行时间)，和<code class="fe mn mo mp mq b">years</code>(电影上映的年份)。我可以给你两个提示:</p><ul class=""><li id="b0c1" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">每个电影<code class="fe mn mo mp mq b">length</code>都在一个<code class="fe mn mo mp mq b">span</code>里面，带有<code class="fe mn mo mp mq b">length</code> ( <code class="fe mn mo mp mq b">span.length</code>)。</li><li id="e38d" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">并且每个<code class="fe mn mo mp mq b">year</code>都在一个带有<code class="fe mn mo mp mq b">year</code> ( <code class="fe mn mo mp mq b">span.year</code>)类的<code class="fe mn mo mp mq b">span</code>内。</li></ul><p id="a46d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代码将与上面的代码非常相似。你只需要改变相应的部分。</p><p id="530c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你做到了，那么恭喜你！确保用下面的解决方案交叉检查你的代码。如果您无法做到这一点，那么没有关系，只需输入以下解决方案。</p><p id="a33d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解决方案是:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="3d2d" class="nk lc iq mq b gy nl nm l nn no"># lengths, selecting all span with class="length"<br/>lengths = []<br/>for length in soup.select('span.length'):<br/>  lengths.append(length.string.strip())</span><span id="74a4" class="nk lc iq mq b gy nr nm l nn no"># years, selecting all span with class="year"<br/>years = []<br/>for year in soup.select('span.year'):<br/>  years.append(year.string.strip())</span></pre><ul class=""><li id="1847" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">我认为这里不需要解释。</li></ul><p id="4132" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">剩下要刮的是<code class="fe mn mo mp mq b">budgets</code>、<code class="fe mn mo mp mq b">grosses</code>和<code class="fe mn mo mp mq b">img_urls</code>。这里我们将使用古老的好方法:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="5cef" class="nk lc iq mq b gy nl nm l nn no">""" scraping by navigating through elements eg: div.span.string """<br/># budget<br/>budgets = []<br/>for budget in soup.find_all('div', class_='budget'):<br/>  # from &lt;div class="budget"&gt;&lt;/div&gt;, get the span.string<br/>  budgets.append(budget.span.string.strip())</span><span id="85dc" class="nk lc iq mq b gy nr nm l nn no"># gross<br/>grosses = []<br/>for gross in soup.find_all('div', class_='gross'):<br/>  grosses.append(gross.span.string.strip())<br/></span><span id="9487" class="nk lc iq mq b gy nr nm l nn no">""" parsing all the "src" attribute's value of &lt;img /&gt; tag """<br/>img_urls = []<br/>for img in soup.find_all('img', class_='poster'):<br/>  img_urls.append(img.get('src').strip())</span></pre><ul class=""><li id="6911" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">这里需要注意的一点是，在最后几行中，我们试图获取img的<code class="fe mn mo mp mq b">src</code>属性。因为img的网址就在那里。要访问元素的任何一个<code class="fe mn mo mp mq b">attributes</code>，我们可以在找到特定元素后使用<code class="fe mn mo mp mq b">.get()</code>方法。<code class="fe mn mo mp mq b">Beautiful Soup</code>在将bytes类型转换为<code class="fe mn mo mp mq b">BeautifulSoup</code>类型时，将所有元素的<code class="fe mn mo mp mq b">attributes</code>存储为一个Python字典。这就是为什么我们使用<code class="fe mn mo mp mq b">.get()</code>方法来访问字典中的值。</li></ul><p id="e7dc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就这样，我们成功地搜集到了所有需要的数据。</p><p id="8e5f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们将这些数据导出到一个CSV文件中。</p><h1 id="fe95" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">创建CSV文件</h1><p id="1b48" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了使用Python生成CSV文件，我们需要一个名为<code class="fe mn mo mp mq b">csv</code>的模块。是内置模块，不用安装。你只需要在<code class="fe mn mo mp mq b">scraper.py</code>文件的最顶端导入它。</p><p id="f050" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以在最上面输入这个:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="bc5b" class="nk lc iq mq b gy nl nm l nn no">import csv</span></pre><p id="4e0e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，在文件的最底部，键入以下代码:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="6df5" class="nk lc iq mq b gy nl nm l nn no">""" writing data to CSV """</span><span id="f104" class="nk lc iq mq b gy nr nm l nn no"># open top25.csv file in "write" mode<br/>with open('top25.csv', 'w') as file:<br/>  # create a "writer" object<br/>  writer = csv.writer(file, delimiter=',')</span><span id="a027" class="nk lc iq mq b gy nr nm l nn no">  # use "writer" obj to write <br/>  # you should give a "list"<br/>  writer.writerow(["title", "genre", "ratings", "length", "year", "budget", "gross", "img_url"])</span><span id="4a67" class="nk lc iq mq b gy nr nm l nn no">  for i in range(25):<br/>    writer.writerow([<br/>      titles[i], <br/>      genres[i], <br/>      ratings[i], <br/>      lengths[i], <br/>      years[i], <br/>      budgets[i], <br/>      grosses[i], <br/>      img_urls[i]<br/>    ])</span></pre><ul class=""><li id="1b3a" class="me mf iq kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">首先，我们以<code class="fe mn mo mp mq b">‘w’</code>模式打开文件。<code class="fe mn mo mp mq b">‘w’</code>为写模式。如果没有给定文件名的文件存在，它将创建一个。如果这样的文件存在，那么它将覆盖该文件。这里我们打开/创建一个名为<code class="fe mn mo mp mq b">top25.csv</code>的新文件。</li><li id="adef" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">然后我们通过给<code class="fe mn mo mp mq b">file</code>和逗号<code class="fe mn mo mp mq b">‘,’</code>作为<code class="fe mn mo mp mq b">delimiter</code>字符来创建一个<code class="fe mn mo mp mq b">csv.writer()</code>对象。</li><li id="8fc3" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">然后使用那个<code class="fe mn mo mp mq b">writer</code>对象，我们<code class="fe mn mo mp mq b">write.row()</code>。我们写的第一行是用于<strong class="kf ir">标题</strong>，你可以把它们想象成表格标题。</li><li id="8a26" class="me mf iq kf b kg mr kk ms ko mt ks mu kw mv la mj mk ml mm bi translated">最后，我们循环25次，在每次迭代中，我们<strong class="kf ir">写一行</strong>，这将是一个<strong class="kf ir">单个电影</strong>。每一行都是关于一部电影。</li></ul><p id="9cfe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就这样，让我们试着运行我们的脚本。我希望您的终端(在您的代码编辑器中)已经打开，并且您的<code class="fe mn mo mp mq b">venv</code>是活动的。现在输入这个:</p><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="b17e" class="nk lc iq mq b gy nl nm l nn no">python scraper.py</span></pre><p id="97cb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果一切顺利，那么您应该在同一个目录中创建了一个名为<code class="fe mn mo mp mq b">top25.csv</code>的新文件，它将包含如下数据:</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/cd15136b85e762957e3e45b30b416ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XSwnb6o5m18_LoB7-hDm1A.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">top25.csv</figcaption></figure><p id="6497" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您有任何错误，那么请确保您到目前为止在<code class="fe mn mo mp mq b">scraper.py</code>文件中键入的代码与下面的最终代码完全一样…</p><h1 id="1c41" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">最终代码</h1><pre class="mx my mz na gt ng mq nh ni aw nj bi"><span id="fb73" class="nk lc iq mq b gy nl nm l nn no">import requests<br/>from bs4 import BeautifulSoup<br/>import csv</span><span id="ab86" class="nk lc iq mq b gy nr nm l nn no"># fetch the web page<br/>page = requests.get('https://the-coding-pie.github.io/top_movies/')</span><span id="1211" class="nk lc iq mq b gy nr nm l nn no"># turn page into a BeautifulSoup object<br/>soup = BeautifulSoup(page.content, 'lxml')<br/></span><span id="3ba3" class="nk lc iq mq b gy nr nm l nn no">""" first, scraping using find_all() method """<br/># scrape all the titles<br/>titles = [] <br/>for h3 in soup.find_all('h3'):<br/>  titles.append(h3.string.strip())</span><span id="cc5d" class="nk lc iq mq b gy nr nm l nn no"># genres<br/>genres = []<br/>for genre in soup.find_all('p', class_='genre'):<br/>  genres.append(genre.string.strip())<br/></span><span id="09ad" class="nk lc iq mq b gy nr nm l nn no">""" scraping using css_selector eg: select('span.class_name') """<br/># ratings, selecting all span with class="rating"<br/>ratings = []<br/>for rating in soup.select('span.rating'):<br/>  ratings.append(rating.string.strip())</span><span id="b6e4" class="nk lc iq mq b gy nr nm l nn no"># lengths, selecting all span with class="length"<br/>lengths = []<br/>for length in soup.select('span.length'):<br/>  lengths.append(length.string.strip())</span><span id="74cc" class="nk lc iq mq b gy nr nm l nn no"># years, selecting all span with class="year"<br/>years = []<br/>for year in soup.select('span.year'):<br/>  years.append(year.string.strip())<br/></span><span id="4778" class="nk lc iq mq b gy nr nm l nn no">""" scraping by navigating through elements eg: div.span.string """<br/># budget<br/>budgets = []<br/>for budget in soup.find_all('div', class_='budget'):<br/>  # from &lt;div class="budget"&gt;&lt;/div&gt;, get the span.string<br/>  budgets.append(budget.span.string.strip())</span><span id="562f" class="nk lc iq mq b gy nr nm l nn no"># gross<br/>grosses = []<br/>for gross in soup.find_all('div', class_='gross'):<br/>  grosses.append(gross.span.string.strip())<br/></span><span id="8052" class="nk lc iq mq b gy nr nm l nn no">""" parsing all the "src" attribute's value of &lt;img /&gt; tag """<br/>img_urls = []<br/>for img in soup.find_all('img', class_='poster'):<br/>  img_urls.append(img.get('src').strip())<br/></span><span id="2236" class="nk lc iq mq b gy nr nm l nn no">""" writing data to CSV """</span><span id="5833" class="nk lc iq mq b gy nr nm l nn no"># open top25.csv file in "write" mode<br/>with open('top25.csv', 'w') as file:<br/>  # create a "writer" object<br/>  writer = csv.writer(file, delimiter=',')</span><span id="44f2" class="nk lc iq mq b gy nr nm l nn no">  # use "writer" obj to write <br/>  # you should give a "list"<br/>  writer.writerow(["title", "genre", "ratings", "length", "year", "budget", "gross", "img_url"])</span><span id="90e2" class="nk lc iq mq b gy nr nm l nn no">  for i in range(25):<br/>    writer.writerow([<br/>      titles[i], <br/>      genres[i], <br/>      ratings[i], <br/>      lengths[i], <br/>      years[i], <br/>      budgets[i], <br/>      grosses[i], <br/>      img_urls[i]<br/>    ])</span></pre><h1 id="50b3" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">包扎</h1><p id="9ccc" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我希望你喜欢这个教程。有些地方，我有意跳过了解释部分。因为这些代码简单明了。所以我让你自己去解码。</p><p id="247d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">真正的学习发生在你自己尝试的时候。仅仅跟随教程不会让你成为更好的程序员。你得用你自己的大脑。</p><p id="dcc7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你仍然有任何错误，首先尝试通过谷歌搜索自己解码。</p><p id="0bf3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你没有找到任何解决方案，那就在下面评论一下吧。因为你应该知道如何自己找到并解决一个bug，这是每个程序员都应该具备的技能！</p><p id="6557" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">仅此而已，谢谢；)​</p></div></div>    
</body>
</html>