<html>
<head>
<title>Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自耦变压器:用于长期序列预测的具有自相关的分解变压器</h1>
<blockquote>原文：<a href="https://itnext.io/autoformer-decomposition-transformers-with-auto-correlation-for-long-term-series-forecasting-8f5a8b115430?source=collection_archive---------0-----------------------#2022-03-20">https://itnext.io/autoformer-decomposition-transformers-with-auto-correlation-for-long-term-series-forecasting-8f5a8b115430?source=collection_archive---------0-----------------------#2022-03-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ad60" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">不可否认的是，当涉及到时间序列预测时，我们需要预测长相关性，以便在未来更好地决策，以应对无论哪个行业的挑战。虽然变形金刚在深度学习时代是革命性的，但它们在捕捉长依赖关系方面存在一些困难。正如我在上一篇文章“<a class="ae kf" href="https://rezayazdanfar.medium.com/informer-beyond-efficient-transformer-for-long-sequence-time-series-forecasting-4eeabb669eb" rel="noopener">Informer:Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</a>”中所讨论的那样，为了预测长达480的序列长度，我们需要超越Transformer的算法。这篇文章与前一篇相同，但是对于工业中高度要求的更长的序列长度。本文提供了关于Autoformer(具有自相关的分解变压器)的信息，以便以出色的性能捕获更长的依赖关系。</h2></div><blockquote class="kg kh ki"><p id="fed3" class="kj kk kl km b kn ko jr kp kq kr ju ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">老实说，当我第一次看到这项研究时，它让我大吃一惊，因为我对时间序列很感兴趣，并且正在研究，这是一个重大的突破。所以，对你来说我也一样。好好享受吧。😉我试图增加乐趣或避免过多关注数学，以防止无聊。</p></blockquote><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">[ <a class="ae kf" href="https://giphy.com/gifs/looneytunesworldofmayhem-2tMy2K0QLJIMUhzGUB" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><blockquote class="lr"><p id="af68" class="ls lt iq bd lu lv lw lx ly lz ma lf dk translated"><em class="mb">在仔细阅读之前，如果你觉得这篇文章有趣，或者我写的主题在你的作品中很实用，请不要犹豫，在medium上关注我，从我这里获取更多的文章。</em></p></blockquote><h1 id="203c" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">概观</h1><p id="db5a" class="pw-post-body-paragraph kj kk iq km b kn mu jr kp kq mv ju ks mw mx kv kw my mz kz la na nb ld le lf ij bi translated">为了使<strong class="km ir">变形金刚</strong> <em class="kl">在<em class="kl">长依赖</em>上高效</em>，我们不得不对它们进行一些修改(<em class="kl">添加了稀疏版本的逐点自关注机制</em>)，这导致了<strong class="km ir"> <em class="kl">的信息利用瓶颈</em> </strong>。为了超越简单的变压器，研究人员将<em class="kl">自耦变压器</em>设计成现代的<strong class="km ir"> <em class="kl">分解</em> </strong> <em class="kl">架构</em>与<strong class="km ir"> <em class="kl">自相关</em> </strong>机制。这种设计使得自耦变压器具有增强的分解能力。他们在<strong class="km ir"><em class="kl"/></strong>系列周期性<em class="kl">随机过程理论</em>的启发下提出了基于的模型<strong class="km ir">(你可以找到很多关于<a class="ae kf" href="https://medium.com/tag/stochastic-process" rel="noopener"> <em class="kl">随机过程</em> </a>的内容)。</strong></p><p id="2f3f" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">结果提供了极好的准确性，意味着在5个应用中的<em class="kl">六个基准</em>上<strong class="km ir"> <em class="kl">相对提高了38%</em></strong>:<em class="kl">能源</em>，<em class="kl">交通</em>，<em class="kl">经济</em>，<em class="kl">天气</em>和<em class="kl">疾病</em>。</p><p id="33e3" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">此外，<strong class="km ir">自形成器</strong>是基于<em class="kl">变换器</em>的模型，遵循<strong class="km ir"> <em class="kl">残差和编解码器架构</em> </strong>，但是<strong class="km ir">将<em class="kl">变换器</em>重构</strong>为<strong class="km ir"> <em class="kl">分解预测架构</em> </strong>。</p><blockquote class="kg kh ki"><p id="1a77" class="kj kk kl km b kn ko jr kp kq kr ju ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">有趣，多么巨大的作品。🤯你同意吗？！我会让它变得可以理解。🙂所以，不用担心。🨊🤞😉</p></blockquote><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c89b466cdea860954a0a111093560aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/1*gP5NvecOsNy32bPmV4bNXQ.gif"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">[ <a class="ae kf" href="https://giphy.com/gifs/SWR-Kindernetz-kindernetz-ausschau-halten-alles-im-blick-gfT2U41sXH1ONqPAud" rel="noopener ugc nofollow" target="_blank">来源</a> ]</figcaption></figure><p id="f834" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">在深入了解自耦变压器之前，我先给大家简单介绍一下基本原理:</p><blockquote class="kg kh ki"><p id="c529" class="kj kk kl km b kn ko jr kp kq kr ju ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated"><strong class="km ir"> <em class="iq">基于分解的方法</em> </strong></p></blockquote><p id="34d5" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">对于<strong class="km ir"> <em class="kl">【建模】</em></strong><strong class="km ir"><em class="kl">【时间序列预测】</em> </strong>，它是一种典型的<strong class="km ir">简单</strong>但<strong class="km ir">稳健</strong>的方法。主要的想法是，他们将数据建模为<em class="kl">趋势</em>、<em class="kl">季节</em>和<em class="kl">剩余成分</em>，而不是像其他传统模型那样仅仅试图捕捉数据中的时间依赖性和自相关性(<em class="kl"> ARIMA </em>、<em class="kl">图表</em>、<em class="kl">等</em>)。).</p><p id="8fcb" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">在预测之前，我们经常使用时间序列分解作为<strong class="km ir"> <em class="kl">分析步骤</em> </strong>。此外，我们可以使用它来预测本身，以防您知道数据的前景结构。这种方法包括将它分成3或4个部分，每个部分代表一个更可预测的类别。</p><p id="beb7" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">这里的自形成器将分解限制为深度模型的内部块，该深度模型可以在整个预测过程中分解秘密序列(包含过去的序列和预测的中间结果)。</p><blockquote class="kg kh ki"><p id="fc2f" class="kj kk kl km b kn ko jr kp kq kr ju ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated"><strong class="km ir"> <em class="iq">自相关机制</em> </strong></p></blockquote><p id="e2c8" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">在这个机制中，我们<strong class="km ir">计算</strong><strong class="km ir"><em class="kl">之间的<strong class="km ir">关系</strong>变量</em> </strong>的当前值和它的<strong class="km ir"> <em class="kl">过去值</em> </strong>。我们使用自相关机制，而不是我们过去使用的注意。自相关的架构如图1所示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nf"><img src="../Images/b7d1a1d3533cec7c182ae3b0641a8085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JOWfe1j6PLB--iQmnOoFmg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">图一。自相关(左)和时间延迟累计(右)。我们利用快速傅立叶变换来计算自相关R(τ),它反映了时间延迟的相似性。然后，基于选定的延迟T将相似的子过程滚动到相同的索引，并通过R(T)进行聚合。[ <a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nk"><img src="../Images/2af5e3766581ef54a11db4d31e31adab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HccWKOov50K8ILiFW9qFOw.png"/></div></div></figure><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nl lm l"/></div></figure><p id="edbd" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">自相关通过<strong class="km ir">计数</strong> 1来探究<strong class="km ir"> <em class="kl">基于周期的依赖性</em> </strong>。<em class="kl">数列自相关</em>与，2。<em class="kl">聚合相似子序列</em>通过<strong class="km ir"> <em class="kl">延时聚合</em> </strong>。</p><p id="f35b" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated"><strong class="km ir">基于期间的依赖关系</strong>:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/5d91570c4c0ab48d5de0d1f5b8c7a121.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*OsdrPTlmpqwWYzZYwTvFtA.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">Eq 1。</figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nn"><img src="../Images/a538c5de4ed711e9082643cf7a9bd5a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GhnPwJC9wbk8fy9LQW81eA.png"/></div></div></figure><p id="6e4a" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated"><strong class="km ir">延时累计</strong>:</p><p id="0723" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">在图1中可以很容易地看到该块的位置。这种操作不同于自我关注中的点-点-积聚合(为了更好地理解自我关注，请参见:“<a class="ae kf" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"><em class="kl"/></a>”)。</p><p id="3626" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">对于一个个体头部情况和长度为-L的时间序列X，在投影后，我们得到查询Q，key K，value V，所以，我们可以获得它而不是自我关注。此外，公式可以在等式2中看到。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fe74b8be89ab0964ffda3d19077f9309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*qAXQDySB_H1knpVuI056Qg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">等式2–4。</figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi np"><img src="../Images/820dd91b4ef993974201def74e078c7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c2urC6sdudDB6KmZpXMjJQ.png"/></div></div></figure><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nl lm l"/></div></figure><blockquote class="kg kh ki"><p id="0008" class="kj kk kl km b kn ko jr kp kq kr ju ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">请注意，我没有给出任何关于注意力机制的说明，所以如果你不知道它，我强烈推荐阅读“<a class="ae kf" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="iq">注意力是你所需要的全部</em> </a>”。</p></blockquote></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><p id="c048" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated"><em class="kl">让我们大致了解一下自耦变压器架构:</em></p><h1 id="ac92" class="mc md iq bd me mf mg mh mi mj mk ml mm jw nx jx mo jz ny ka mq kc nz kd ms mt bi translated">自耦变压器概述</h1><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e835e7927f0be155671ae198d73b25da.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*IYJwAJ5iHWhyzBjEem6Ieg.gif"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">[ <a class="ae kf" href="https://giphy.com/gifs/wjcollege-flying-campus-pHYESyIDdjrV0Shdbq" rel="noopener ugc nofollow" target="_blank">来源</a> ]</figcaption></figure><p id="1871" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">预测时间序列中的长期相关性有两个挑战:</p><ol class=""><li id="6da9" class="ob oc iq km b kn ko kq kr mw od my oe na of lf og oh oi oj bi translated">管理基于时间的复杂模式</li><li id="bec4" class="ob oc iq km b kn ok kq ol mw om my on na oo lf og oh oi oj bi translated">突破计算效率和信息使用的瓶颈</li></ol><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi op"><img src="../Images/a4d5aa8e5b75086496128b90a0112088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BEitiRFcFQvrN-qJEh-rBw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">图二。自耦变压器架构。编码器通过序列分解块(<strong class="bd me">蓝色</strong>块)消除长期趋势循环部分，并专注于季节模式建模。编码器-解码器自相关(解码器中的中间<strong class="bd me">绿色</strong>块)[ <a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">源</a> ]利用来自编码器的过去季节信息</figcaption></figure><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nl lm l"/></div></figure><p id="e870" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">仔细检查总体架构，我们可以发现，我们可以将模型拆分为两个<strong class="km ir"><em class="kl"/></strong><em class="kl">部分</em>；所以，它有一个<strong class="km ir"> <em class="kl">的编解码结构</em> </strong>。让我们深入了解这两者的更多细节:</p><h2 id="f798" class="oq md iq bd me or os dn mi ot ou dp mm mw ov ow mo my ox oy mq na oz pa ms pb bi translated">编码器:</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pc"><img src="../Images/e8ac2a6f1616889e81be41f9e843da7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOIXdwApYvDNeY7JGSyL9g.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">图3。自耦变压器编码器结构[ <a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="d84a" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">这里我们可以理解一些要点:<em class="kl">编码器</em>的<strong class="km ir">焦点</strong>是<strong class="km ir"> <em class="kl">季节性零件造型</em></strong>；表示<em class="kl">编码器</em> <strong class="km ir"> <em class="kl"> </em> </strong>的<strong class="km ir">输出</strong>包含<strong class="km ir"> <em class="kl">过去的季节信息</em> </strong>。然后，我们使用<strong class="km ir">编码器的输出</strong>(作为<strong class="km ir"> <em class="kl">交叉信息</em> </strong>)来帮助<strong class="km ir">解码器更好地预测。图3右上方的<strong class="km ir"> N x </strong>。传达<strong class="km ir"> <em class="kl">编码器层</em> </strong>的<strong class="km ir">号。当然，数学(<em class="kl">我不想讨论它们；所以，我只是用论文的插图</em>提到了他们:</strong></strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/6380e33b00fd4c658c2ce2e55fe1b4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*wRWvLd-Qh-wvTl_ur-73Pg.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">情商。5</figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d613ec72b34dede5d789eeb924c14b2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*NxVf4LthGmMxDzMRJcTPmw.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">等式6和7</figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pf"><img src="../Images/f4894758e94465cbf077ce46b77773d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L5A0y8-gePsPcnPqYeqVBA.png"/></div></div></figure><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nl lm l"/></div></figure><h2 id="ee9b" class="oq md iq bd me or os dn mi ot ou dp mm mw ov ow mo my ox oy mq na oz pa ms pb bi translated">解码器</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pg"><img src="../Images/c93f2f52442cc54ccdc2dd826afbb4f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8F9znrawkSOMpyzGAaxaSw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">图4。自耦变压器解码器结构[ <a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="9b69" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">图4。显示了解码器的架构，包括两部分:</p><ol class=""><li id="d7bd" class="ob oc iq km b kn ko kq kr mw od my oe na of lf og oh oi oj bi translated"><strong class="km ir"> <em class="kl">累加</em> </strong>结构(<em class="kl">为趋势-周期成分</em>)</li><li id="306a" class="ob oc iq km b kn ok kq ol mw om my on na oo lf og oh oi oj bi translated"><strong class="km ir"> <em class="kl">叠加自相关</em> </strong>机制(季节性成分<em class="kl"/>)</li></ol><p id="8bc5" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">我们可以看到，解码器包括一个<strong class="km ir"> <em class="kl">自相关(</em> </strong> <em class="kl">细化预测</em> <strong class="km ir"> <em class="kl"> ) </em> </strong>和<strong class="km ir"> <em class="kl">编码器-解码器自相关(</em> </strong> <em class="kl">使用过去的季节信息</em> <strong class="km ir"> <em class="kl">)。</em> </strong>图4右下方的<strong class="km ir"> M x </strong>。传达<strong class="km ir">解码器层的<strong class="km ir">号</strong>。 </strong>还有公式:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ph"><img src="../Images/90e4765805bfcc31949c42fc12ba703e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZxmsUj6rQvrzb49KZUZB4A.jpeg"/></div></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pi"><img src="../Images/b9b3552829590f26df25bcd04089c652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LkdQpswaKu_1UT3gRbd7lw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">情商。8–11</figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pj"><img src="../Images/2dc77aa735c66421a3f87dbbe9793c2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6TtreIoD9BEHC34lO-RH8g.png"/></div></div></figure><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nl lm l"/></div></figure></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><h1 id="4131" class="mc md iq bd me mf pk mh mi mj pl ml mm jw pm jx mo jz pn ka mq kc po kd ms mt bi translated">自相关与自我关注家庭</h1><p id="e282" class="pw-post-body-paragraph kj kk iq km b kn mu jr kp kq mv ju ks mw mx kv kw my mz kz la na nb ld le lf ij bi translated">我说过，当在最先进的算法中广泛使用自我关注时，我们使用自相关而不是自我关注。但不是在这项研究中。顺便说一句，研究人员在<em class="kl">自相关</em>和<strong class="km ir"> <em class="kl">各种</em> </strong> <em class="kl">自我关注结构</em>之间进行了<strong class="km ir">比较</strong>。(<em class="kl">我不打算说明自我注意的结构，只是把它们与我们的自相关性进行比较</em>)</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pp"><img src="../Images/2fccfdd848adad8ca74a3c1a161de504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lnlxh0aQ2AQN2TthsgmsSQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">图5。自相关与自我关注家庭。<strong class="bd me">充分注意</strong> ( <strong class="bd me"> a </strong>)适应所有时间点之间的充分联系。<strong class="bd me">稀疏注意力</strong> ( <strong class="bd me"> b </strong>)基于提议的相似性度量选择点。<strong class="bd me">对数稀疏注意力</strong> ( <strong class="bd me"> c </strong>)选择遵循指数递增间隔的点。<strong class="bd me">自相关</strong> ( <strong class="bd me"> d </strong>)关注的是基础期间子序列之间的联系。[ <a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="0d3a" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">尽管一些自我关注(<em class="kl"> LogSparse </em>和<em class="kl"> Sparse Attention </em>)考虑<strong class="km ir"> <em class="kl">局部信息</em> </strong>，它们使用它来<strong class="km ir">辅助<em class="kl">发现点状依赖</em> </strong>。在<em class="kl">信息聚合</em>方面，研究人员采用<strong class="km ir"> <em class="kl">延时块</em> </strong>从<strong class="km ir"> <em class="kl">潜在周期中收集<em class="kl">相似子序列</em>。</em> </strong>在对面，<strong class="km ir"><em class="kl"/></strong>通过<strong class="km ir"><em class="kl"/></strong>点积执行此操作。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pq"><img src="../Images/04e6762ee2c5a1a24d433bca8b2ee0f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kYy-PCBCE3TtKHPJ3lQ0RA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">图6。学习依赖的可视化。为了清除，我们选择自相关的前6个时间延迟大小T1，…，T6，并在原始序列中标记它们(<strong class="bd me">红色</strong>线)。出于自我关注，关于上一时间步的前5个相似点(<strong class="bd me">红色</strong>星)也用<strong class="bd me">橙色</strong>点标记。[ <a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">来源</a> ]</figcaption></figure><p id="256e" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">此外，为了分析使用注意力或自相关的效率，我们可以参见图7。如下图:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pr"><img src="../Images/efec10bba39f6b56104e5da008a2db49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8mhH8WS8yPEOxTo_o-YjIQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">图7。效率分析。对于记忆，我们在Autoformer中用自关注族代替自相关，并用输入96记录记忆。对于运行时间，我们运行自相关或自我关注1000次，以获得每步的执行时间。输出长度呈指数增长。[ <a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">来源</a> ]</figcaption></figure></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><h1 id="6754" class="mc md iq bd me mf pk mh mi mj pl ml mm jw pm jx mo jz pn ka mq kc po kd ms mt bi translated">模型评估</h1><p id="ffd3" class="pw-post-body-paragraph kj kk iq km b kn mu jr kp kq mv ju ks mw mx kv kw my mz kz la na nb ld le lf ij bi translated">研究人员用单变量和多变量时间序列数据的其他最新模型评估了该模型。评估部分使用了六个真实数据集，包括:</p><ol class=""><li id="6678" class="ob oc iq km b kn ko kq kr mw od my oe na of lf og oh oi oj bi translated">ETT(电力变压器温度)| 2。<a class="ae kf" href="https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014" rel="noopener ugc nofollow" target="_blank">电</a> | 3。Exchange | 4。<a class="ae kf" href="http://pems.dot.ca.gov/" rel="noopener ugc nofollow" target="_blank">交通</a> |5。<a class="ae kf" href="https://www.bgc-jena.mpg.de/wetter/" rel="noopener ugc nofollow" target="_blank">天气</a> | 6。<a class="ae kf" href="https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html" rel="noopener ugc nofollow" target="_blank"> ILI </a></li></ol><p id="fb7b" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated"><code class="fe ps pt pu pv b">please note that I didn't provide any information about the datasets. If your want to explore this you just need to google it.😉</code></p><p id="a0ee" class="pw-post-body-paragraph kj kk iq km b kn ko jr kp kq kr ju ks mw ku kv kw my ky kz la na lc ld le lf ij bi translated">两个指标(MSE和MAE)用于显示评估。比较结果总结在两个表中(表1和表2)。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pw"><img src="../Images/8a4097afe9fe23d10d8cf4e0a57a3ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RCBlzaUtRcyc8V4M0jVv_A.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">表1。<strong class="bd me">预测长度<strong class="bd me">不同的多元结果</strong></strong>O = {<strong class="bd me">96</strong>，<strong class="bd me"> 192 </strong>，<strong class="bd me"> 336 </strong>，<strong class="bd me"> 720 </strong> }。我们设定<strong class="bd me">输入长度I </strong>为<strong class="bd me"> ILI </strong>的<strong class="bd me"> 36 </strong>和<strong class="bd me"> 96 </strong>的<strong class="bd me">其余</strong>。<strong class="bd me">较低的</strong> MSE或MAW表示<strong class="bd me">更好的预测。</strong> [ <a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">来源</a> ]</figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi px"><img src="../Images/546ddc26a20ed0c938ee1347c8242baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8dV_PVa68kFFVWkvv9wRw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">表二。<strong class="bd me">典型数据集上<strong class="bd me">不同预测长度</strong> O={ <strong class="bd me"> 96 </strong>，<strong class="bd me"> 192 </strong>，<strong class="bd me"> 336 </strong>，<strong class="bd me"> 720 </strong> }的单变量结果。我们设定<strong class="bd me">输入长度I </strong>为<strong class="bd me"> 96 </strong>。一个<strong class="bd me">较低的</strong> <strong class="bd me"> MSE </strong>和<strong class="bd me"> MAE </strong>表示一个<strong class="bd me">较好的预测</strong>。[ <a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">来源</a> ]</strong></figcaption></figure></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><h1 id="b00a" class="mc md iq bd me mf pk mh mi mj pl ml mm jw pm jx mo jz pn ka mq kc po kd ms mt bi translated">主要参考:</h1><ol class=""><li id="7ad9" class="ob oc iq km b kn mu kq mv mw py my pz na qa lf og oh oi oj bi translated"><a class="ae kf" href="https://arxiv.org/abs/2106.13008" rel="noopener ugc nofollow" target="_blank">吴，h，等，<em class="kl">自形成器:用于长期序列预测的具有自相关的分解变压器。</em>神经信息处理系统进展，2021。<strong class="km ir"> 34 </strong>。</a></li></ol></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><blockquote class="kg kh ki"><p id="c8dc" class="kj kk kl km b kn ko jr kp kq kr ju ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">此外，这篇文章是为了在我未来的时间序列工作中使用而写的，所以我试图彻底了解它的各个方面；如果您发现任何错误或差距，请让我知道立即修复它。最后，如果你有什么问题，尽管问；我会尽快回复。</p><p id="a5f3" class="kj kk kl km b kn ko jr kp kq kr ju ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">请注意，这篇文章是为了我将来的研究回顾和复习这个主题的材料。如果你发现任何错误，请让我知道。同时，你可以直接在Twitter <a class="ae kf" href="https://twitter.com/reza__yazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a>或者LinkedIn <a class="ae kf" href="http://www.linkedin.com/in/rezayazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a>联系我。</p></blockquote></div></div>    
</body>
</html>