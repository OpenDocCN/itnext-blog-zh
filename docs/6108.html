<html>
<head>
<title>Hydrating a Data Lake using Log-based Change Data Capture (CDC) with Debezium, Apicurio, and Kafka Connect on AWS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在AWS上使用基于日志的变更数据捕获(CDC)、Debezium、Apicurio和Kafka Connect来修复数据湖</h1>
<blockquote>原文：<a href="https://itnext.io/hydrating-a-data-lake-using-log-based-change-data-capture-cdc-with-debezium-apicurio-and-kafka-799671e0012f?source=collection_archive---------0-----------------------#2021-08-21">https://itnext.io/hydrating-a-data-lake-using-log-based-change-data-capture-cdc-with-debezium-apicurio-and-kafka-799671e0012f?source=collection_archive---------0-----------------------#2021-08-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a47b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用亚马逊MSK、Apache Kafka Connect、Debezium、Apicurio Registry和亚马逊EKS将数据从亚马逊RDS导入亚马逊S3</h2></div><p id="5dce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上一篇文章<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/hydrating-a-data-lake-using-query-based-cdc-with-apache-kafka-connect-and-kubernetes-on-aws-cd4725b58c2e">中，我们利用</a><a class="ae le" href="https://kafka.apache.org/documentation/#connect" rel="noopener ugc nofollow" target="_blank"> Kafka Connect </a>从亚马逊RDS for PostgreSQL关系数据库中导出数据，并将数据导入到构建在亚马逊简单存储服务(亚马逊S3)上的数据湖中。导入到S3的数据被转换为Apache Parquet列存储文件格式，进行压缩和分区，以实现最佳分析性能，所有这些都使用Kafka Connect。为了提高数据的新鲜度，当在PostgreSQL数据库中添加或更新数据时，Kafka Connect会自动检测这些更改，并使用基于查询的更改数据捕获(CDC)将它们传输到数据湖中。</p><div class="lf lg gp gr lh li"><a rel="noopener  ugc nofollow" target="_blank" href="/hydrating-a-data-lake-using-query-based-cdc-with-apache-kafka-connect-and-kubernetes-on-aws-cd4725b58c2e"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">在AWS上使用基于查询的CDC和Apache Kafka Connect和Kubernetes来补充数据湖</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">使用亚马逊EKS、亚马逊MSK和Apache Kafka Connect将数据从亚马逊RDS数据库导入到基于亚马逊S3的数据湖中</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">itnext.io</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw lx li"/></div></div></a></div><p id="023f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇后续文章将研究基于日志的CDC，这是对基于查询的CDC的一个显著改进，可以持续地将变化从PostgreSQL数据库传输到数据湖。我们将使用<a class="ae le" href="https://debezium.io/documentation/reference/connectors/postgresql.html" rel="noopener ugc nofollow" target="_blank"> Debezium的Kafka Connect Source Connector for PostgreSQL</a>来执行基于日志的CDC，而不是在之前的文章中用于基于查询的CDC的Confluent的Kafka Connect JDBC源连接器。我们将把消息作为<a class="ae le" href="https://avro.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Avro </a>存储在Kafka中，Kafka运行在亚马逊管理的Apache Kafka(亚马逊MSK)流上。Avro消息模式将存储在<a class="ae le" href="https://www.apicur.io/registry/" rel="noopener ugc nofollow" target="_blank"> Apicurio注册表</a>中。模式注册中心将与Kafka Connect一起运行在Amazon Elastic Kubernetes服务(Amazon EKS)上。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/cf544287e7ab303360c79e6caf27be9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TpXpTlb8Y_Z6PIhdkl52Pw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">这篇文章演示的高级架构</figcaption></figure><h1 id="7e38" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">变更数据捕获</h1><p id="e29a" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">据参与Debezium和Hibernate项目的Red Hat首席软件工程师Gunnar Morling和著名的行业发言人说，有两种类型的<a class="ae le" href="https://en.wikipedia.org/wiki/Change_data_capture" rel="noopener ugc nofollow" target="_blank">变更数据捕获</a>——基于查询和基于日志的CDC。Gunnar在2021年2月的<a class="ae le" href="https://jokerconf.com/en/" rel="noopener ugc nofollow" target="_blank"> Joker </a>国际Java大会上的演讲中详细介绍了这两种CDC的区别，<a class="ae le" href="https://youtu.be/-PugtDeJQFs?t=1320" rel="noopener ugc nofollow" target="_blank">用Debezium和Kafka Streams </a>改变数据捕获管道。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nk"><img src="../Images/b66efbe4e850daaa641ab150c568efbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qfoMvFD4Wx2pq5y6.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">小丑2021:用Debezium和Kafka流改变数据捕获管道(图片:<a class="ae le" href="https://www.youtube.com/watch?t=1320&amp;v=-PugtDeJQFs&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank"> YouTube </a>)</figcaption></figure><p id="d8a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在Rockset的Lewis Gavin最近的帖子中找到对CDC的另一个精彩解释，<a class="ae le" href="https://rockset.com/blog/change-data-capture-what-it-is-and-how-to-use-it/" rel="noopener ugc nofollow" target="_blank">更改数据捕获:它是什么以及如何使用它</a>。</p><h2 id="acf8" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">基于查询与基于日志的CDC</h2><p id="0fb3" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">为了展示基于查询的CDC和基于日志的CDC之间的高级差异，让我们检查用这两种CDC方法捕获的简单SQL UPDATE语句的结果。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="ff60" class="nl mo it ny b gy oc od l oe of">UPDATE public.address<br/><strong class="ny iu">SET address2 = 'Apartment #1234'<br/></strong>WHERE address_id = 105;</span></pre><p id="e6ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是如何使用在<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/hydrating-a-data-lake-using-query-based-cdc-with-apache-kafka-connect-and-kubernetes-on-aws-cd4725b58c2e">上一篇文章</a>中描述的基于查询的CDC方法将这种变化表示为JSON消息负载。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="a638" class="nl mo it ny b gy oc od l oe of">{<br/>  "address_id": 105,<br/>  "address": "733 Mandaluyong Place",<br/><strong class="ny iu">  "address2": "Apartment #1234",<br/></strong>  "district": "Asir",<br/>  "city_id": 2,<br/>  "postal_code": "77459",<br/>  "phone": "196568435814",<br/>  "last_update": "2021-08-13T00:43:38.508Z"<br/>}</span></pre><p id="4f7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是如何使用基于日志的CDC和Debezium将相同的变化表示为JSON消息负载。注意与基于查询的消息相比，基于日志的CDC消息的元数据丰富的结构。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="6fb4" class="nl mo it ny b gy oc od l oe of">{<br/>  "after": {<br/>    "address": "733 Mandaluyong Place",<br/><strong class="ny iu">    "address2": "Apartment #1234",<br/></strong>    "phone": "196568435814",<br/>    "district": "Asir",<br/>    "last_update": "2021-08-13T00:43:38.508453Z",<br/>    "address_id": 105,<br/>    "postal_code": "77459",<br/>    "city_id": 2<br/>  },<br/>  "source": {<br/>    "schema": "public",<br/>    "sequence": "[\"1090317720392\",\"1090317720392\"]",<br/>    "xmin": null,<br/>    "connector": "postgresql",<br/>    "lsn": 1090317720624,<br/>    "name": "pagila",<br/>    "txId": 16973,<br/>    "version": "1.6.1.Final",<br/>    "ts_ms": 1628815418508,<br/>    "snapshot": "false",<br/>    "db": "pagila",<br/>    "table": "address"<br/>  },<br/><strong class="ny iu">  "op": "u",<br/></strong>  "ts_ms": 1628815418815<br/>}</span></pre><h1 id="6d5d" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">Avro和模式注册表</h1><p id="1a7e" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">根据<a class="ae le" href="https://avro.apache.org/docs/current/" rel="noopener ugc nofollow" target="_blank">文档</a>，Apache Avro是一种紧凑、快速的二进制数据格式。Avro依赖于模式。读取Avro数据时，写入数据时使用的模式始终存在。这允许写入每个数据时没有每个值的开销，使得序列化既快又小。这也有助于使用动态脚本语言，因为数据及其模式是完全自描述的。</p><p id="bb0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过使用像<a class="ae le" href="https://docs.confluent.io/platform/current/schema-registry/index.html" rel="noopener ugc nofollow" target="_blank">汇合模式注册中心</a>或<a class="ae le" href="https://www.apicur.io/registry/" rel="noopener ugc nofollow" target="_blank"> Apicurio注册中心</a>这样的模式注册中心将数据从模式中分离出来。根据Apicurio的说法，在消息和事件流架构中，发布到主题和队列的数据通常必须使用模式进行序列化或验证(例如，<a class="ae le" href="https://avro.apache.org/docs/current/" rel="noopener ugc nofollow" target="_blank"> Apache Avro </a>、<a class="ae le" href="https://json-schema.org/" rel="noopener ugc nofollow" target="_blank"> JSON模式</a>或<a class="ae le" href="https://developers.google.com/protocol-buffers" rel="noopener ugc nofollow" target="_blank"> Google协议缓冲区</a>)。当然，模式可以打包在每个应用程序中。尽管如此，在外部系统[schema registry]中注册模式，然后从每个应用程序中引用它们通常是更好的架构模式。</p><blockquote class="og oh oi"><p id="58d9" class="ki kj oj kk b kl km ju kn ko kp jx kq ok ks kt ku ol kw kx ky om la lb lc ld im bi translated">在外部系统中注册模式，然后从每个应用程序中引用它们，这通常是更好的架构模式。</p></blockquote><p id="b09e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用Debezium的PostgreSQL source连接器，我们将把PostgreSQL数据库的<a class="ae le" href="https://debezium.io/documentation/reference/connectors/postgresql.html#postgresql-streaming-changes" rel="noopener ugc nofollow" target="_blank">预写日志</a> (WAL)中的更改存储为Kafka中的Avro，运行在亚马逊MSK上。消息的模式将单独存储在Apicurio注册表中，而不是与消息一起存储，从而减少Kafka中消息的大小，并允许<a class="ae le" href="https://www.apicur.io/registry/docs/apicurio-registry/2.0.1.Final/getting-started/assembly-intro-to-the-registry.html#client-serde" rel="noopener ugc nofollow" target="_blank">模式验证</a>和<a class="ae le" href="https://docs.confluent.io/platform/current/schema-registry/avro.html" rel="noopener ugc nofollow" target="_blank">模式演化</a>。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/3a75d9f4d8f55a1c795f21ff596db8f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gqaaFOVnQlQ-yYxssmv9dQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">显示<code class="fe oo op oq ny b">pagila.public.film</code>模式版本的Apicurio注册表</figcaption></figure><h1 id="52b6" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">Debezium</h1><p id="b9cc" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">据Debezium的网站称，它会持续监控你的数据库，并让你的任何应用程序按照提交给数据库的顺序来处理每一个行级的变化。事件流可用于清除缓存、更新搜索索引、生成派生视图和数据，以及保持其他数据源同步。Debezium是一组分布式服务，可以捕获数据库中行级别的变化。Debezium在一个事务日志中记录提交给每个数据库表的所有行级更改。然后，每个应用程序读取它们感兴趣的事务日志，并且它们以事件发生的相同顺序看到所有事件。Debezium构建在Apache Kafka之上，并与Kafka Connect集成。</p><p id="e5d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Debezium的最新版本包括对监控<a class="ae le" href="https://debezium.io/documentation/reference/1.0/connectors/mysql/" rel="noopener ugc nofollow" target="_blank"> MySQL数据库服务器</a>、<a class="ae le" href="https://debezium.io/documentation/reference/1.0/connectors/mongodb/" rel="noopener ugc nofollow" target="_blank"> MongoDB副本集或分片集群</a>、<a class="ae le" href="https://debezium.io/documentation/reference/1.0/connectors/postgresql/" rel="noopener ugc nofollow" target="_blank"> PostgreSQL服务器</a>和<a class="ae le" href="https://debezium.io/documentation/reference/1.0/connectors/sqlserver/" rel="noopener ugc nofollow" target="_blank"> SQL服务器数据库</a>的支持。我们将使用Debezium的PostgreSQL连接器来捕获Pagila PostgreSQL数据库中的行级变化。根据Debezium的<a class="ae le" href="https://debezium.io/documentation/reference/1.6/connectors/postgresql.html" rel="noopener ugc nofollow" target="_blank">文档</a>，第一次连接到PostgreSQL服务器或集群时，连接器会对所有模式进行一致的快照。快照完成后，连接器会连续捕获行级更改，这些更改会插入、更新和删除提交到数据库的数据库内容。连接器生成数据变更事件记录，并将它们传输到Kafka主题。对于每个表，默认行为是连接器将所有生成的事件流式传输到该表的一个单独的Kafka主题。应用程序和服务使用该主题的数据更改事件记录。</p><h1 id="2ec1" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">先决条件</h1><p id="99e1" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">与<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/hydrating-a-data-lake-using-query-based-cdc-with-apache-kafka-connect-and-kubernetes-on-aws-cd4725b58c2e">上一篇文章</a>类似，这篇文章将关注数据移动，而不是如何部署所需的AWS资源。要跟进这篇文章，您需要在AWS上已经部署和配置了以下资源:</p><ol class=""><li id="1246" class="or os it kk b kl km ko kp kr ot kv ou kz ov ld ow ox oy oz bi translated">Amazon RDS for PostgreSQL实例(数据<em class="oj">来源</em>)；</li><li id="a356" class="or os it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated">亚马逊S3桶(数据<em class="oj">汇</em>)；</li><li id="8156" class="or os it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated">亚马逊MSK集群；</li><li id="ec5d" class="or os it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated">亚马逊EKS集群；</li><li id="463a" class="or os it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated">亚马逊RDS实例和亚马逊MSK集群之间的连接；</li><li id="8294" class="or os it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated">亚马逊EKS集群和亚马逊MSK集群之间的连通性；</li><li id="a491" class="or os it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated">确保亚马逊MSK配置有<code class="fe oo op oq ny b">auto.create.topics.enable=true</code>。该设置默认为<code class="fe oo op oq ny b">false</code>；</li><li id="c6be" class="or os it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated">与Kubernetes服务帐户(称为<a class="ae le" href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html" rel="noopener ugc nofollow" target="_blank"> IRSA </a>)相关联的IAM角色，将允许从EKS访问MSK和S3 ( <em class="oj">见下面的详细信息</em>)；</li></ol><p id="4eb9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上面的架构图所示，我在同一个AWS账户和AWS区域<code class="fe oo op oq ny b">us-east-1</code>内使用了三个独立的VPC，分别用于亚马逊RDS、亚马逊EKS和亚马逊MSK。使用<a class="ae le" href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html" rel="noopener ugc nofollow" target="_blank"> VPC对等</a>连接三个VPC。确保您在亚马逊RDS、亚马逊EKS和亚马逊MSK安全组上公开了正确的入口端口和相应的CIDR范围。为了增加安全性和节约成本，使用一个<a class="ae le" href="https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html" rel="noopener ugc nofollow" target="_blank"> VPC端点</a>来确保亚马逊EKS和亚马逊S3之间的私人通信。</p><h1 id="d5d0" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">源代码</h1><p id="7711" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">这篇文章和上一篇文章的所有源代码，包括Kafka Connect和connector配置文件以及Helm图表，都是开源的，位于GitHub上。</p><div class="lf lg gp gr lh li"><a href="https://github.com/garystafford/kafka-connect-msk-demo" rel="noopener  ugc nofollow" target="_blank"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">GitHub——garystaf得起/kafka-connect-msk-demo:对于帖子，使用变更数据为数据湖补水…</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">在这篇文章中，使用变更数据捕获(CDC)、Apache Kafka和Kubernetes on AWS-GitHub来补充数据湖…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">github.com</p></div></div><div class="lr l"><div class="pf l lt lu lv lr lw lx li"/></div></div></a></div><h1 id="870d" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">认证和授权</h1><p id="0318" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">亚马逊MSK提供多种<a class="ae le" href="https://docs.aws.amazon.com/msk/latest/developerguide/kafka_apis_iam.html" rel="noopener ugc nofollow" target="_blank">认证和授权方法</a>来与Apache Kafka APIs交互。例如，您可以使用IAM对客户端进行身份验证，并允许或拒绝Apache Kafka操作。或者，您可以使用TLS或SASL/SCRAM来验证客户端，并使用Apache Kafka ACLs来允许或拒绝操作。在我的上一篇文章中，我演示了SASL/SCRAM和Kafka ACLs在亚马逊MSK上的使用:</p><div class="lf lg gp gr lh li"><a rel="noopener  ugc nofollow" target="_blank" href="/securely-decoupling-applications-on-amazon-eks-using-kafka-with-sasl-scram-48c340e1ffe9"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">使用带有SASL/SCRAM的Kafka安全地解耦亚马逊EKS上的应用</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">使用具有IRSA、SASL/SCRAM和数据加密的亚马逊MSK，安全地分离亚马逊EKS上基于Go的微服务</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">itnext.io</p></div></div><div class="lr l"><div class="pg l lt lu lv lr lw lx li"/></div></div></a></div><p id="f408" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设您正确配置了亚马逊MSK、亚马逊EKS和Kafka Connect，任何MSK身份验证和授权都应该与Kafka Connect一起工作。在这篇文章中，我们使用<a class="ae le" href="https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html#kafka-actions" rel="noopener ugc nofollow" target="_blank"> IAM访问控制</a>。与Kubernetes服务帐户(<a class="ae le" href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html" rel="noopener ugc nofollow" target="_blank"> IRSA </a>)相关联的IAM角色允许EKS使用IAM访问MSK和S3(<em class="oj">参见下面的更多细节</em>)。</p><h1 id="8fda" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">示例PostgreSQL数据库</h1><p id="12d4" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">对于这篇文章，我们将继续使用PostgreSQL的<a class="ae le" href="https://www.postgresql.org/ftp/projects/pgFoundry/dbsamples/pagila/pagila/" rel="noopener ugc nofollow" target="_blank"> Pagila数据库</a>。该数据库包含模拟电影租赁数据。数据集相对较小，这使得它不太适合“大数据”用例，但也足够小，可以快速安装并最大限度地降低数据存储和分析查询成本。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ph"><img src="../Images/2fd0ad9a4d03bfdd3bb4d5fa615e821e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQjTzEyf6hV_hAWPucJ0Vg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">Pagila数据库模式图</figcaption></figure><p id="2771" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在继续之前，在Amazon RDS PostgreSQL实例上创建一个新数据库，并用Pagila示例数据填充它。一些人发布了这个数据库的更新版本，带有易于安装的SQL脚本。查看Devrim Gündüz在<a class="ae le" href="https://github.com/devrimgunduz/pagila" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上提供的Pagila脚本，以及Robert Treat在<a class="ae le" href="https://github.com/xzilla" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上提供的pagi la脚本。</p><h2 id="d017" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">上次更新的触发器</h2><p id="d828" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">Pagila数据库中的每个表都有一个<code class="fe oo op oq ny b">last_update</code>字段。检测Pagila数据库变化的一个简单方法是使用<code class="fe oo op oq ny b">last_update</code>字段。这是使用基于查询的CDC来确定是否以及何时对数据进行了更改的常用技术，如前一篇文章中所演示的。当这些表中的记录发生变化时，现有的数据库函数和每个表的触发器将确保<code class="fe oo op oq ny b">last_update</code>字段自动更新到当前日期和时间。你可以在Dominick Lombardo的文章<a class="ae le" href="https://lombardo-chcg.github.io/tools/2017/11/25/database-update-event-stream.html" rel="noopener ugc nofollow" target="_blank"> kafka connect in action，part 3 </a>中找到关于数据库函数和触发器如何与Kafka Connect一起工作的更多信息。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="99d4" class="nl mo it ny b gy oc od l oe of">CREATE OR REPLACE FUNCTION <em class="oj">update_last_update_column</em>()<br/>    RETURNS TRIGGER AS<br/>$$<br/>BEGIN<br/>    NEW.last_update = now();<br/>    RETURN NEW;<br/>END;<br/>$$ language 'plpgsql';</span><span id="4fdc" class="nl mo it ny b gy pi od l oe of">CREATE TRIGGER update_last_update_column_address<br/>    BEFORE UPDATE<br/>    ON address<br/>    FOR EACH ROW<br/>EXECUTE PROCEDURE <em class="oj">update_last_update_column</em>();</span></pre><h1 id="79b4" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">Kafka连接和模式注册表</h1><p id="7410" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">部署和管理Kafka Connect、Kafka管理API和命令行工具以及Apicurio注册表有几个选项。我更喜欢在亚马逊EKS的Kubernetes上部署一个容器化的解决方案。一些受欢迎的集装箱化卡夫卡选项包括<a class="ae le" href="https://strimzi.io/" rel="noopener ugc nofollow" target="_blank"> Strimzi </a>、<a class="ae le" href="https://docs.confluent.io/operator/current/overview.html" rel="noopener ugc nofollow" target="_blank">Kubernetes</a>(CFK)和<a class="ae le" href="https://hub.docker.com/r/debezium/server" rel="noopener ugc nofollow" target="_blank"> Debezium </a>。另一个选择是使用官方的Apache Kafka二进制文件构建你自己的Docker映像。在这篇文章中，我选择使用最新的Kafka二进制文件来构建自己的Kafka Connect Docker映像。然后，我将必要的Confluent和Debezium连接器及其相关的Java依赖项安装到Kafka安装中。虽然没有使用现成的容器有效，但在我看来，构建自己的映像会教你如何使用Kafka、Kafka Connect和Debezium。</p><p id="6071" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于模式注册中心，<a class="ae le" href="https://hub.docker.com/r/confluentinc/cp-schema-registry" rel="noopener ugc nofollow" target="_blank">汇合</a>和<a class="ae le" href="https://hub.docker.com/r/apicurio/apicurio-registry-mem" rel="noopener ugc nofollow" target="_blank"> Apicurio </a>都提供了容器化的解决方案。Apicurio有三个版本的注册表，每个版本都有不同的存储机制:内存、SQL和Kafka。因为我们已经有一个现有的Amazon RDS PostgreSQL实例作为演示的一部分，所以我为这篇文章选择了基于Apicurio SQL的registry Docker映像，<code class="fe oo op oq ny b">apicurio/apicurio-registry-sql:2.1.0.Final</code>。</p><p id="ca1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你选择使用我在这篇文章中使用的相同的Kafka Connect和Apicurio解决方案，这篇文章的GitHub资源库中有一个Helm Chart，<code class="fe oo op oq ny b">kafka-connect-msk-v2</code>。Helm chart将把一个Kubernetes pod部署到亚马逊EKS上的<code class="fe oo op oq ny b">kafka</code>名称空间。pod由Kafka Connect和Apicurio注册容器组成。该部署旨在进行演示，而不是设计用于生产。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="f45b" class="nl mo it ny b gy oc od l oe of">apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: kafka-connect-msk<br/>spec:<br/>  type: NodePort<br/>  selector:<br/>    app: kafka-connect-msk<br/>  ports:<br/>    - port: 8080<br/>---<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: kafka-connect-msk<br/>  labels:<br/>    app: kafka-connect-msk<br/>    component: service<br/>spec:<br/>  replicas: 1<br/>  strategy:<br/>    type: Recreate<br/>  selector:<br/>    matchLabels:<br/>      app: kafka-connect-msk<br/>      component: service<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: kafka-connect-msk<br/>        component: service<br/>    spec:<br/><strong class="ny iu">      serviceAccountName: kafka-connect-msk-iam-serviceaccount<br/></strong>      containers:<br/>        - image: garystafford/kafka-connect-msk:1.1.0<br/>          name: kafka-connect-msk<br/>          imagePullPolicy: IfNotPresent<br/>        - image: apicurio/apicurio-registry-sql:2.1.0.Final<br/>          name: apicurio-registry-mem<br/>          imagePullPolicy: IfNotPresent<br/>          env:<br/>            - name: REGISTRY_DATASOURCE_URL<br/><strong class="ny iu">              value: jdbc:postgresql://your-pagila-database-url.us-east-1.rds.amazonaws.com:5432/apicurio-registry<br/></strong>            - name: REGISTRY_DATASOURCE_USERNAME<br/>              value: apicurio_registry<br/>            - name: REGISTRY_DATASOURCE_PASSWORD<br/>              value: 1L0v3Kafka!</span></pre><p id="9a57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在部署图表之前，在RDS实例上创建新的PostgreSQL数据库、用户和授权，以便Apicurio注册表用于存储:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="702d" class="nl mo it ny b gy oc od l oe of">CREATE DATABASE "apicurio-registry";</span><span id="463c" class="nl mo it ny b gy pi od l oe of">CREATE USER apicurio_registry WITH PASSWORD '1L0v3KafKa!';<br/><br/>GRANT CONNECT, CREATE ON DATABASE "apicurio-registry" to apicurio_registry;</span></pre><p id="bbc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用您与Kafka Connect pod ( <code class="fe oo op oq ny b">serviceAccountName</code>)关联的<a class="ae le" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" rel="noopener ugc nofollow" target="_blank"> Kubernetes服务帐户</a>的名称和您的RDS URL ( <code class="fe oo op oq ny b">registryDatasourceUrl</code>)更新Helm chart的<code class="fe oo op oq ny b">value.yaml</code>文件。附加到与pod的服务帐户相关联的IAM角色的IAM策略应该提供从EKS对运行在亚马逊MSK集群上的Kafka的足够访问。该策略还应该提供对您的S3存储桶的访问，如Confluent在此处<a class="ae le" href="https://docs.confluent.io/kafka-connect-s3-sink/current/overview.html#iam-policy-for-s3" rel="noopener ugc nofollow" target="_blank">详述的那样。下面是一个(<em class="oj">过于宽泛</em> ) IAM策略的示例，该策略允许完全访问亚马逊MSK上运行的任何Kafka集群，以及亚马逊EKS上运行的Kafka Connect中的S3 bucket。</a></p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="8b51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变量更新后，使用以下命令部署舵图:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="9d6c" class="nl mo it ny b gy oc od l oe of">helm install kafka-connect-msk-v2 ./kafka-connect-msk-v2 \<br/>  --namespace $NAMESPACE --create-namespace</span></pre><p id="e620" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过检查pod的状态确认图表安装成功:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="614e" class="nl mo it ny b gy oc od l oe of">kubectl get pods -n kafka -l app=kafka-connect-msk</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pl"><img src="../Images/404358c580e581120d81977242507f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7X8u2yTLaurbntk7zb2Dag.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">成功运行两个容器且没有错误的pod视图</figcaption></figure><p id="ef09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您在部署时对任何一个容器有任何问题，请查看单个容器的日志:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="5524" class="nl mo it ny b gy oc od l oe of">export KAFKA_CONTAINER=$(<br/>  kubectl get pods -n kafka -l app=kafka-connect-msk | \<br/>    awk 'FNR == 2 {print $1}')</span><span id="7ead" class="nl mo it ny b gy pi od l oe of">kubectl logs $KAFKA_CONTAINER -n kafka kafka-connect-msk</span><span id="4422" class="nl mo it ny b gy pi od l oe of">kubectl logs $KAFKA_CONTAINER -n kafka apicurio-registry-mem</span></pre><h2 id="6050" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">卡夫卡连接</h2><p id="f6f7" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">使用<code class="fe oo op oq ny b">kubectl exec</code>命令获取正在运行的Kafka Connect容器的shell:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="5acd" class="nl mo it ny b gy oc od l oe of">export KAFKA_CONTAINER=$(<br/>  kubectl get pods -n kafka -l app=kafka-connect-msk | \<br/>    awk 'FNR == 2 {print $1}')</span><span id="d1ac" class="nl mo it ny b gy pi od l oe of">kubectl exec -it $KAFKA_CONTAINER -n kafka -c kafka-connect-msk -- bash</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pl"><img src="../Images/0b04d0a0abdef6b1f6fe5e0a6950baa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrvfzY4Tp89c1_VszH8YgA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">与EKS上运行的Kafka Connect容器交互</figcaption></figure><h2 id="48f6" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">确认从Kafka Connect访问注册表</h2><p id="c9bf" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">如果Helm图表部署成功，您现在应该在新的<code class="fe oo op oq ny b">apicurio-registry</code>数据库的<code class="fe oo op oq ny b">public</code>模式中观察到11个新表。下面，我们看到了新的数据库和表格，如<a class="ae le" href="https://www.postgresql.org/ftp/pgadmin/pgadmin4/v5.6/macos/" rel="noopener ugc nofollow" target="_blank"> pgAdmin </a>所示。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pm"><img src="../Images/b14cd201851f26d4d0f02045bd406589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5lzz5EWROp6iMKgp9PFqg.png"/></div></div></figure><p id="a3a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过调用注册表的<code class="fe oo op oq ny b">system/info</code> REST API端点，确认注册表正在运行并且可以从Kafka Connect容器访问:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="37af" class="nl mo it ny b gy oc od l oe of">curl -s <a class="ae le" href="http://localhost:8080/apis/registry/v2/system/info" rel="noopener ugc nofollow" target="_blank">http://localhost:8080/apis/registry/v2/system/info</a> | jq</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pl"><img src="../Images/9e96c79fd5c87a43daf3eda4f1d026c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0TpwxyThIBYCt0lg-uu96Q.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">从Kafka连接容器调用Apicurio注册表的REST API</figcaption></figure><p id="6707" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Apicurio注册表的服务目标是TCP端口8080。该服务在Kubernetes worker节点的外部IP地址的一个静态端口<code class="fe oo op oq ny b">NodePort</code>上公开。要获得服务的<code class="fe oo op oq ny b">NodePort</code>，使用以下命令:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="a17e" class="nl mo it ny b gy oc od l oe of">kubectl describe services kafka-client-msk -n kafka</span></pre><p id="e61e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要访问Apicurio Registry的基于web的UI，将<code class="fe oo op oq ny b">NodePort</code>添加到EKS节点的安全组，源是您的IP地址，一个<code class="fe oo op oq ny b">/32</code> CIDR块。</p><p id="bb2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要获取任何Amazon EKS工作节点的外部IP地址(<code class="fe oo op oq ny b">EXTERNAL-IP</code>)，请使用以下命令:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="d0eb" class="nl mo it ny b gy oc od l oe of">kubectl get nodes -o wide</span></pre><p id="6d57" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<code class="fe oo op oq ny b">&lt;NodeIP&gt;:&lt;NodePort&gt;</code>组合从网络浏览器访问用户界面，例如<code class="fe oo op oq ny b">http://54.237.41.128:30433</code>。在演示的这一点上，注册表将是空的。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/41dc6c9b999e6979a95cdbe63dc246d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TRYsY-jCu96O00G5aYpX_A.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">Apicurio注册表用户界面</figcaption></figure><h2 id="30d2" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">配置引导代理</h2><p id="baeb" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">在启动Kafka Connect之前，您需要修改Kafka Connect的配置文件。Kafka Connect能够以独立或分布式模式运行工作线程。因为我们将使用Kafka Connect的分布式模式，修改<code class="fe oo op oq ny b">config/connect-distributed.properties</code>文件。我在这篇文章中使用的配置文件的完整示例如下所示。</p><p id="5c68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Kafka Connect和schema registry将在亚马逊EKS上运行，而Kafka和Apache ZooKeeper将在亚马逊MSK上运行。更新<code class="fe oo op oq ny b">bootstrap.servers</code>属性以反映亚马逊MSK Kafka Bootstrap Brokers的逗号分隔列表。要获得Amazon MSK集群的引导代理列表，请使用AWS管理控制台或以下AWS CLI命令:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="e3ea" class="nl mo it ny b gy oc od l oe of"># get the msk cluster's arn<br/>aws kafka list-clusters --query 'ClusterInfoList[*].ClusterArn'</span><span id="0a53" class="nl mo it ny b gy pi od l oe of"># use msk arn to get the brokers<br/>aws kafka get-bootstrap-brokers --cluster-arn <strong class="ny iu">your-msk-cluster-arn</strong></span><span id="ff3a" class="nl mo it ny b gy pi od l oe of"># alternately, if you only have one cluster, then<br/>aws kafka get-bootstrap-brokers --cluster-arn $(<br/>  aws kafka list-clusters | jq -r '.ClusterInfoList[0].ClusterArn')</span></pre><p id="be04" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更新<code class="fe oo op oq ny b">config/connect-distributed.properties</code>文件。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="c0ce" class="nl mo it ny b gy oc od l oe of"><strong class="ny iu"># ***** CHANGE ME! *****<br/>bootstrap.servers=b-1.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098,b-2.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098, b-3.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098</strong></span><span id="fc71" class="nl mo it ny b gy pi od l oe of">group.id=connect-cluster</span><span id="66e7" class="nl mo it ny b gy pi od l oe of">key.converter.schemas.enable=true<br/>value.converter.schemas.enable=true</span><span id="9d66" class="nl mo it ny b gy pi od l oe of">offset.storage.topic=connect-offsets<br/>offset.storage.replication.factor=2<br/>#offset.storage.partitions=25</span><span id="6532" class="nl mo it ny b gy pi od l oe of">config.storage.topic=connect-configs<br/>config.storage.replication.factor=2</span><span id="7d28" class="nl mo it ny b gy pi od l oe of">status.storage.topic=connect-status<br/>status.storage.replication.factor=2<br/>#status.storage.partitions=5</span><span id="9fba" class="nl mo it ny b gy pi od l oe of">offset.flush.interval.ms=10000</span><span id="b22b" class="nl mo it ny b gy pi od l oe of">plugin.path=/usr/local/share/kafka/plugins</span><span id="751a" class="nl mo it ny b gy pi od l oe of"># kafka connect auth using iam<br/>ssl.truststore.location=/tmp/kafka.client.truststore.jks<br/>security.protocol=SASL_SSL<br/>sasl.mechanism=AWS_MSK_IAM<br/>sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;<br/>sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler</span><span id="fbd7" class="nl mo it ny b gy pi od l oe of"># kafka connect producer auth using iam<br/>producer.ssl.truststore.location=/tmp/kafka.client.truststore.jks<br/>producer.security.protocol=SASL_SSL<br/>producer.sasl.mechanism=AWS_MSK_IAM<br/>producer.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;<br/>producer.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler</span><span id="4f8a" class="nl mo it ny b gy pi od l oe of"># kafka connect consumer auth using iam<br/>consumer.ssl.truststore.location=/tmp/kafka.client.truststore.jks<br/>consumer.security.protocol=SASL_SSL<br/>consumer.sasl.mechanism=AWS_MSK_IAM<br/>consumer.sasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;<br/>consumer.sasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler</span></pre><p id="cef1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了在执行Kafka命令时方便起见，将<code class="fe oo op oq ny b">BBROKERS</code>环境变量设置为相同的Kafka引导程序代理的逗号分隔列表，例如:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="3438" class="nl mo it ny b gy oc od l oe of">export BBROKERS="b-1.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098,b-2.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098, b-3.your-cluster.123abc.c2.kafka.us-east-1.amazonaws.com:9098"</span></pre><h2 id="005f" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">确认从Kafka Connect访问亚马逊MSK</h2><p id="d0e7" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">要确认您可以从运行在亚马逊EKS上的Kafka Connect容器访问运行在亚马逊MSK上的Kafka，请尝试列出现有的Kafka主题:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="7b3b" class="nl mo it ny b gy oc od l oe of">bin/kafka-topics.sh --list \<br/>  --bootstrap-server $BBROKERS \<br/>  --command-config config/client-iam.properties</span></pre><p id="e977" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你也可以尝试列出现有的卡夫卡消费群体:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="bbe6" class="nl mo it ny b gy oc od l oe of">bin/kafka-consumer-groups.sh --list \<br/>  --bootstrap-server $BBROKERS \<br/>  --command-config config/client-iam.properties</span></pre><p id="d77a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果其中任何一个失败，你可能有网络或安全问题阻止从亚马逊EKS到亚马逊MSK的访问。检查您的VPC对等、路由表、IAM/IRSA和安全组入口设置。这些项目中的任何一个都可能导致容器和亚马逊MSK上运行的Kafka之间的通信问题。</p><h2 id="0195" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">卡夫卡连接</h2><p id="1320" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">配置完成后，作为后台进程启动Kafka Connect。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="bff8" class="nl mo it ny b gy oc od l oe of">bin/connect-distributed.sh \<br/>  config/connect-distributed.properties &gt; /dev/null 2&gt;&amp;1 &amp;</span></pre><p id="dd79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了确认Kafka Connect正确启动，请立即跟踪<code class="fe oo op oq ny b">connect.log</code>文件。该日志将捕获任何启动错误，以便进行故障排除。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="e534" class="nl mo it ny b gy oc od l oe of">tail -f logs/connect.log</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pl"><img src="../Images/33bccd8dfe3e3923a97ce2b703b42c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_NnPHDPCZtajPAwqN51dg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">显示Kafka Connect作为后台进程启动的Kafka Connect日志</figcaption></figure><p id="e843" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您还可以使用<code class="fe oo op oq ny b">ps</code>命令检查后台进程，以确认Kafka Connect正在运行。注意PID 4915的过程，如下所示。如有必要，使用<code class="fe oo op oq ny b">kill</code>命令和PID来停止Kafka Connect。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pl"><img src="../Images/74dbf61cb1a11e0301ad3cb723a21e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m0o4kmHYZOmaxPuIQtF1Ng.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">Kafka Connect作为后台进程运行</figcaption></figure><p id="f60a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果配置正确，当Kafka Connect启动时，Kafka Connect将创建三个新主题，称为Kafka Connect内部主题。主题在<code class="fe oo op oq ny b">config/connect-distributed.properties</code>文件中定义:<code class="fe oo op oq ny b">connect-configs</code>、<code class="fe oo op oq ny b">connect-offsets</code>和<code class="fe oo op oq ny b">connect-status</code>。根据<a class="ae le" href="https://docs.confluent.io/home/connect/userguide.html#kconnect-internal-topics" rel="noopener ugc nofollow" target="_blank">汇合</a>，Connect在这些主题中存储连接器和任务配置、偏移和状态。内部主题必须具有高复制系数、压缩清理策略和适当数量的分区。使用以下命令可以确认这些新主题。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="0a9e" class="nl mo it ny b gy oc od l oe of">bin/kafka-topics.sh --list \<br/>  --bootstrap-server $BBROKERS \<br/>  --command-config config/client-iam.properties \<br/>  | grep connect-</span></pre><h1 id="d8de" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">Kafka连接连接器</h1><p id="ae08" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">这篇文章演示了一组Kafka Connect源和接收器连接器的使用。源连接器基于PostgreSQL的Debezium源连接器和Apicurio注册表。sink连接器基于合流<a class="ae le" href="https://docs.confluent.io/kafka-connect-s3-sink/current/overview.html" rel="noopener ugc nofollow" target="_blank">亚马逊S3 Sink连接器</a>和Apicurio注册表。</p><h2 id="4163" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">连接器源</h2><p id="a9ca" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">创建或修改文件，<code class="fe oo op oq ny b">config/debezium_avro_source_connector_postgresql_05.json</code>。更新第3–6行，如下所示，以反映您的RDS实例连接细节。</p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="3447" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">源连接器从Pagila数据库的<code class="fe oo op oq ny b">public</code>模式中的六个相关表中导出现有数据和正在进行的更改:<code class="fe oo op oq ny b">actor</code>、<code class="fe oo op oq ny b">film</code>、<code class="fe oo op oq ny b">film_actor</code>、<code class="fe oo op oq ny b">category</code>、<code class="fe oo op oq ny b">film_category</code>和<code class="fe oo op oq ny b">language</code>。数据将被导入到相应的一组六个新的Kafka主题中:<code class="fe oo op oq ny b">pagila.public.actor</code>、<code class="fe oo op oq ny b">pagila.public.film</code>等等。(<em class="oj">见</em>上方第9行)。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pn"><img src="../Images/783d36e79b6f3e62faa18e0dd487af13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2rxhq_JFgMxbHXZUl0DvVg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">显示要导出的六个表的模式图</figcaption></figure><p id="6f76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">表中的数据以Apache Avro格式存储在Kafka中，模式单独存储在Apicurio注册表中(<em class="oj">第11-18行，见</em>)。</p><h2 id="3f65" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">连接器接收器</h2><p id="0f4a" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">创建或修改文件，<code class="fe oo op oq ny b">config/s3_sink_connector_05_debezium_avro.json</code>。更新第7行，如下所示，以反映您的亚马逊S3桶的名称。</p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="df67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接收器连接器每300条记录或60秒从六个Kafka主题向S3刷新新数据(<em class="oj">第4–5、9–10行，在</em>上面)。写入S3的数据的模式是从Apicurio注册表中提取的(<em class="oj">第17–24行，在</em>上面)。</p><p id="0828" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">sink connector通过将GZIP压缩的<a class="ae le" href="https://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Parquet </a>文件写入亚马逊S3，优化了导入S3的原始数据，以便进行下游处理。使用Parquet的列式文件格式和文件压缩应该有助于针对S3的原始数据优化<a class="ae le" href="https://www.stitchdata.com/resources/what-is-elt/" rel="noopener ugc nofollow" target="_blank">ELT</a>(<em class="oj">第12–13行，在</em>上面)。</p><h2 id="596b" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">部署连接器</h2><p id="5f1a" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">使用Kafka Connect REST接口部署源和接收器连接器:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="574b" class="nl mo it ny b gy oc od l oe of">curl -s -d @"config/<!-- -->debezium_avro_source_connector_postgresql_05.json<!-- -->" \<br/>    -H "Content-Type: application/json" \<br/>    -X PUT http://localhost:8083/connectors/<!-- -->debezium_avro_source_connector_postgresql_05<!-- -->/config | jq</span><span id="6a19" class="nl mo it ny b gy pi od l oe of">curl -s -d @"config/<!-- -->s3_sink_connector_05_debezium_avro<!-- -->.json" \<br/>    -H "Content-Type: application/json" \<br/>    -X PUT http://localhost:8083/connectors/<!-- -->s3_sink_connector_05_debezium_avro<!-- -->/config | jq</span></pre><h2 id="fc15" class="nl mo it bd mp nm nn dn mt no np dp mx kr nq nr mz kv ns nt nb kz nu nv nd nw bi translated">确认部署</h2><p id="b41d" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">使用以下命令确认新的连接器集已部署并正确运行。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="fcee" class="nl mo it ny b gy oc od l oe of">curl -s -X GET http://localhost:8083/connectors | jq</span><span id="eb0d" class="nl mo it ny b gy pi od l oe of">curl -s -H "Content-Type: application/json" \<br/>    -X GET http://localhost:8083/connectors/<!-- -->debezium_avro_source_connector_postgresql_05<!-- -->/status | jq</span><span id="3692" class="nl mo it ny b gy pi od l oe of">curl -s -H "Content-Type: application/json" \<br/>    -X GET http://localhost:8083/connectors/<!-- -->s3_sink_connector_05_debezium_avro<!-- -->/status | jq</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pl"><img src="../Images/612d34062918232f9a6c18b569c53e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eMiLH68FQi8MmM8RJeG_nQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">Kafka连接源和接收器连接器成功运行</figcaption></figure><p id="0997" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">存储在Apicurio注册表中的项目，例如事件模式和API设计，被称为注册表<em class="oj">工件</em>。如果我们重新访问Apicurio注册表的UI，我们应该观察到12个工件——从Pagila数据库导出的6个表中的每一个都有一个“key”和“value”工件。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/012a30eb8168eb8c3e83e035a47d6dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ron9xzKq404WtAFDYy1gpg.png"/></div></div></figure><p id="ffd4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">检查亚马逊S3，您应该注意到在按主题名组织的<code class="fe oo op oq ny b">/topics/</code>对象关键字前缀中有六组S3对象。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi po"><img src="../Images/04ab2caa9d1d62e91a4d2871de8dfd4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pcARNAt45Jktd_GKc44TXA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">亚马逊S3桶显示卡夫卡连接S3接收器连接器的结果，按主题名称组织</figcaption></figure><p id="a1f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在每个主题名称键中，应该有一组GZIP压缩的拼花文件。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/fd4782405fa4544037a76ec74a73b51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5SOv5BnHEW5Sd-eW427JSw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">显示GZIP压缩的Apache拼花格式文件的亚马逊S3桶</figcaption></figure><p id="b695" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次使用亚马逊S3控制台的“用S3选择查询”来查看拼花格式文件中包含的数据。或者，您可以将AWS CLI与<code class="fe oo op oq ny b">s3</code> API一起使用:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="d2b0" class="nl mo it ny b gy oc od l oe of">export SINK_BUCKET="your-s3-bucket"</span><span id="13f5" class="nl mo it ny b gy pi od l oe of">export KEY="topics/pagila.public.film/partition=0/pagila.public.film+0+0000000000.gz.parquet"</span><span id="175f" class="nl mo it ny b gy pi od l oe of">aws s3api select-object-content \<br/>    --bucket $SINK_BUCKET \<br/>    --key $KEY \<br/>    --expression "select * from s3object limit 5" \<br/>    --expression-type "SQL" \<br/>    --input-serialization '{"Parquet": {}}' \<br/>    --output-serialization '{"JSON": {}}' "output.json" \<br/>  &amp;&amp; cat output.json | jq \<br/>  &amp;&amp; rm output.json</span></pre><p id="9666" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的示例数据中，请注意基于日志的CDC消息的元数据丰富的结构，与我们在上一篇文章中观察到的基于查询的消息相比:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="ee73" class="nl mo it ny b gy oc od l oe of">{<br/>  "after": {<br/>    "special_features": [<br/>      "Deleted Scenes",<br/>      "Behind the Scenes"<br/>    ],<br/>    "rental_duration": 6,<br/>    "rental_rate": 0.99,<br/>    "release_year": 2006,<br/>    "length": 86,<br/>    "replacement_cost": 20.99,<br/>    "rating": "PG",<br/>    "description": "A Epic Drama of a Feminist And a Mad Scientist who must Battle a Teacher in The Canadian Rockies",<br/>    "language_id": 1,<br/>    "title": "ACADEMY DINOSAUR",<br/>    "original_language_id": null,<br/>    "last_update": "2017-09-10T17:46:03.905795Z",<br/>    "film_id": 1<br/>  },<br/>  "source": {<br/>    "schema": "public",<br/>    "sequence": "[null,\"1177089474560\"]",<br/>    "xmin": null,<br/>    "connector": "postgresql",<br/>    "lsn": 1177089474560,<br/>    "name": "pagila",<br/>    "txId": 18422,<br/>    "version": "1.6.1.Final",<br/>    "ts_ms": 1629340334432,<br/>    "snapshot": "true",<br/>    "db": "pagila",<br/>    "table": "film"<br/>  },<br/>  "op": "r",<br/>  "ts_ms": 1629340334434<br/>}</span></pre><h1 id="dc3b" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">基于日志的CDC的数据库更改</h1><p id="8912" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">当我们更改Debezium和Kafka Connect正在监控的表中的数据时会发生什么？为了回答这个问题，让我们对Pagila数据库做一些<a class="ae le" href="https://www.geeksforgeeks.org/sql-ddl-dql-dml-dcl-tcl-commands/" rel="noopener ugc nofollow" target="_blank"> DML </a>更改:插入、更新和删除:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="f86a" class="nl mo it ny b gy oc od l oe of">INSERT INTO public.category (name)<br/>VALUES ('Techno Thriller');</span><span id="3510" class="nl mo it ny b gy pi od l oe of">UPDATE public.film<br/>SET release_year = 2021,<br/>    rental_rate  = 2.99<br/>WHERE film_id = 1;</span><span id="0b56" class="nl mo it ny b gy pi od l oe of">UPDATE public.film<br/>SET rental_duration = 3<br/>WHERE film_id = 2;</span><span id="f0cb" class="nl mo it ny b gy pi od l oe of">UPDATE public.film_category<br/>SET category_id = (<br/>    SELECT DISTINCT category_id<br/>    FROM public.category<br/>    WHERE name = 'Techno Thriller')<br/>WHERE film_id = 3;</span><span id="c8ff" class="nl mo it ny b gy pi od l oe of">UPDATE public.actor<br/>SET first_name = upper('Kate'),<br/>    last_name  = upper('Winslet')<br/>WHERE actor_id = 6;</span><span id="9866" class="nl mo it ny b gy pi od l oe of">DELETE<br/>FROM public.film_actor<br/>WHERE film_id = 375;</span></pre><p id="d2de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要了解这些变化是如何传播的，首先，检查Kafka Connect日志。下面，我们将看到与上面显示的一些数据库更改相对应的示例日志事件。Kafka Connect源连接器检测更改，然后将更改从PostgreSQL导出到Kafka。然后，接收器连接器将这些更改写入亚马逊S3。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pl"><img src="../Images/7767c385a60b8e67d4a557fb3ebcb869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DORCMslrDxZKDMN0ZgfU7Q.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">Kafka连接日志显示正在导出/导入的Pagila数据库的更改</figcaption></figure><p id="5771" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以查看S3存储桶，它现在应该有与我们的更改相对应的新拼花文件。例如，我们用1的<code class="fe oo op oq ny b">film_id</code>对电影记录进行的两次更新。注意操作是更新(<code class="fe oo op oq ny b">"op": "u"</code>)和数据出现在<code class="fe oo op oq ny b">after</code>块中。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="c78d" class="nl mo it ny b gy oc od l oe of">{<br/>  "after": {<br/>    "special_features": [<br/>      "Deleted Scenes",<br/>      "Behind the Scenes"<br/>    ],<br/>    "rental_duration": 6,<br/><strong class="ny iu">    "rental_rate": 2.99,<br/>    "release_year": 2021,<br/></strong>    "length": 86,<br/>    "replacement_cost": 20.99,<br/>    "rating": "PG",<br/>    "description": "A Epic Drama of a Feminist And a Mad Scientist who must Battle a Teacher in The Canadian Rockies",<br/>    "language_id": 1,<br/>    "title": "ACADEMY DINOSAUR",<br/>    "original_language_id": null,<br/>    "last_update": "2021-08-19T03:19:57.073053Z",<br/>    "film_id": 1<br/>  },<br/>  "source": {<br/>    "schema": "public",<br/>    "sequence": "[\"1177693455424\",\"1177693455424\"]",<br/>    "xmin": null,<br/>    "connector": "postgresql",<br/>    "lsn": 1177693471392,<br/>    "name": "pagila",<br/>    "txId": 18445,<br/>    "version": "1.6.1.Final",<br/>    "ts_ms": 1629343197100,<br/>    "snapshot": "false",<br/>    "db": "pagila",<br/>    "table": "film"<br/>  },<br/><strong class="ny iu">  "op": "u",<br/></strong>  "ts_ms": 1629343197389<br/>}</span></pre><p id="ca14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在另一个例子中，我们看到在<code class="fe oo op oq ny b">film_actor</code>表中删除了<code class="fe oo op oq ny b">film_id</code>为375的记录。注意操作是删除(<code class="fe oo op oq ny b">"op": "d"</code>)和<code class="fe oo op oq ny b">before</code>程序块的存在，但没有<code class="fe oo op oq ny b">after</code>程序块。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="d15a" class="nl mo it ny b gy oc od l oe of">{<br/><strong class="ny iu">  "before": {<br/>    "last_update": "1970-01-01T00:00:00Z",<br/>    "actor_id": 5,<br/>    "film_id": 375<br/>  },<br/></strong>  "source": {<br/>    "schema": "public",<br/>    "sequence": "[\"1177693516520\",\"1177693516520\"]",<br/>    "xmin": null,<br/>    "connector": "postgresql",<br/>    "lsn": 1177693516520,<br/>    "name": "pagila",<br/>    "txId": 18449,<br/>    "version": "1.6.1.Final",<br/>    "ts_ms": 1629343198400,<br/>    "snapshot": "false",<br/>    "db": "pagila",<br/>    "table": "film_actor"<br/>  },<br/><strong class="ny iu">  "op": "d",<br/></strong>  "ts_ms": 1629343198426<br/>}</span></pre><h1 id="0409" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">Debezium事件扁平化SMT</h1><p id="8901" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">上面在S3展示的Debezium消息结构面临的挑战是负载的冗长性和数据的嵌套性。因此，针对此类记录开发SQL查询会很困难。例如，给定上面显示的消息结构，即使是Amazon Athena中最简单的查询也会变得非常复杂:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="bdce" class="nl mo it ny b gy oc od l oe of">SELECT after.actor_id, after.first_name, after.last_name, after.last_update<br/>FROM <br/>    (SELECT *,<br/>         ROW_NUMBER()<br/>        OVER ( PARTITION BY after.actor_id<br/>    ORDER BY  after.last_UPDATE DESC) AS row_num<br/>    FROM "pagila_kafka_connect"."pagila_public_actor") AS x<br/>WHERE x.row_num = 1<br/>ORDER BY after.actor_id;</span></pre><p id="f3ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了专门解决不同消费者的需求，Debezium提供了<a class="ae le" href="https://debezium.io/documentation/reference/configuration/event-flattening.html" rel="noopener ugc nofollow" target="_blank">事件扁平化单消息转换</a> (SMT)。事件扁平化转换是一个<a class="ae le" href="https://kafka.apache.org/documentation/#connect_transforms" rel="noopener ugc nofollow" target="_blank"> Kafka Connect SMT </a>。我们在上一篇文章中讨论了Kafka Connect SMTs。使用事件扁平化SMT，我们可以使Kafka接收到的消息更适合我们数据湖的特定消费者。要实现事件扁平化SMT，请修改并重新部署源连接器，添加额外的配置(<em class="oj">第19–23行，在</em>下面)。</p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="3686" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在转换后的消息中包含<code class="fe oo op oq ny b">op</code>、<code class="fe oo op oq ny b">db</code>、<code class="fe oo op oq ny b">schema</code>、<code class="fe oo op oq ny b">lsn</code>和<code class="fe oo op oq ny b">source.ts_ms</code>元数据字段，以及实际的记录数据(<code class="fe oo op oq ny b">table</code>)。这意味着我们已经选择从消息中排除所有其他字段。转换将使消息的嵌套结构变平。</p><p id="d2a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过添加转换对消息结构进行这种更改会导致消息模式的新版本被源连接器自动添加到Apicurio注册表中:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/3a75d9f4d8f55a1c795f21ff596db8f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gqaaFOVnQlQ-yYxssmv9dQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">显示修订版<code class="fe oo op oq ny b">pagila.public.film</code>模式的Apicurio注册表</figcaption></figure><p id="85cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于源连接器对SMT的事件扁平化，我们的消息结构得到了显著简化:</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="e90f" class="nl mo it ny b gy oc od l oe of">{<br/>  "actor_id": 7,<br/>  "first_name": "BOB",<br/>  "last_name": "MOSTEL",<br/>  "last_update": "2021-08-19T21:01:55.090858Z",<br/>  "__op": "u",<br/>  "__db": "pagila",<br/>  "__schema": "public",<br/>  "__table": "actor",<br/>  "__lsn": 1191920555344,<br/>  "__source_ts_ms": 1629406915091,<br/>  "__deleted": "false"<br/>}</span></pre><p id="a65e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意新的<code class="fe oo op oq ny b">__deleted</code>字段，它来自源连接器配置的第21–22行，如上所示。Debezium为事件流中的删除操作保留tombstone记录，并添加<code class="fe oo op oq ny b">__deleted</code>，设置为<code class="fe oo op oq ny b">true</code>或<code class="fe oo op oq ny b">false</code>。下面，我们看一个在<code class="fe oo op oq ny b">film_actor</code>表上的两个删除操作的例子。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="3469" class="nl mo it ny b gy oc od l oe of">{<br/>  "actor_id": 52,<br/>  "film_id": 376,<br/>  "last_update": "1970-01-01T00:00:00Z",<br/>  "__op": "d",<br/>  "__db": "pagila",<br/>  "__schema": "public",<br/>  "__table": "film_actor",<br/>  "__lsn": 1192390296016,<br/>  "__source_ts_ms": 1629408869556,<br/><strong class="ny iu">  "__deleted": "true"<br/></strong>}<br/>{<br/>  "actor_id": 60,<br/>  "film_id": 376,<br/>  "last_update": "1970-01-01T00:00:00Z",<br/>  "__op": "d",<br/>  "__db": "pagila",<br/>  "__schema": "public",<br/>  "__table": "film_actor",<br/>  "__lsn": 1192390298976,<br/>  "__source_ts_ms": 1629408869556,<br/><strong class="ny iu">  "__deleted": "true"<br/></strong>}</span></pre><h1 id="5fa4" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">查看数据湖中的数据</h1><p id="cf14" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">在我们的数据湖中检查现有数据和正在发生的数据变化的一种便捷方式是<a class="ae le" href="https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html" rel="noopener ugc nofollow" target="_blank">抓取</a>并用<a class="ae le" href="https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html" rel="noopener ugc nofollow" target="_blank"> AWS Glue </a>对S3桶的内容进行编目，然后用<a class="ae le" href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html" rel="noopener ugc nofollow" target="_blank"> Amazon Athena </a>查询结果。AWS Glue的数据目录是一个与Apache Hive兼容的、完全管理的、持久的元数据存储。AWS Glue可以在S3存储我们数据的模式、元数据和位置。Amazon Athena是一个无服务器的<a class="ae le" href="https://prestodb.io/" rel="noopener ugc nofollow" target="_blank">基于Presto </a>的(<a class="ae le" href="https://prestodb.io/" rel="noopener ugc nofollow" target="_blank"> PrestoDB </a> ) ad-hoc分析引擎，可以查询AWS Glue数据目录表和底层基于S3的数据。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/5ba9c66c12e0834a0f361cbbcec3fb4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkTzFWHE1AWsvo-KsDGENQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">显示六个新表格的AWS粘合数据目录(metastore)</figcaption></figure><p id="cadd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Glue中抓取并编目数据后，让我们对Pagila数据库的<code class="fe oo op oq ny b">film</code>表进行一些额外的更改。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="e04c" class="nl mo it ny b gy oc od l oe of">UPDATE public.film<br/>SET release_year = 2019,<br/>    rental_rate  = 3.99<br/>WHERE film_id = 1;<br/><br/>UPDATE public.film<br/>SET rental_duration = 4<br/>WHERE film_id = 2;<br/><br/>UPDATE public.film<br/>SET rental_duration = 7<br/>WHERE film_id = 2;</span><span id="7edc" class="nl mo it ny b gy pi od l oe of">INSERT INTO public.category (name)<br/>VALUES ('Steampunk');</span><span id="72f7" class="nl mo it ny b gy pi od l oe of">UPDATE public.film_category<br/>SET category_id = (<br/>    SELECT DISTINCT category_id<br/>    FROM public.category<br/>    WHERE name = 'Steampunk')<br/>WHERE film_id = 3;</span><span id="5a0f" class="nl mo it ny b gy pi od l oe of">UPDATE public.film<br/>SET release_year = 2017,<br/>    rental_rate  = 3.99<br/>WHERE film_id = 4;</span><span id="c426" class="nl mo it ny b gy pi od l oe of">UPDATE public.film_actor<br/>SET film_id = 100<br/>WHERE film_id = 5;<br/><br/>UPDATE public.film_category<br/>SET film_id = 100<br/>WHERE film_id = 5;<br/><br/>UPDATE public.inventory<br/>SET film_id = 100<br/>WHERE film_id = 5;<br/><br/>DELETE<br/>FROM public.film<br/>WHERE film_id = 5;</span></pre><p id="592c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过使用Amazon Athena执行查询，我们应该能够几乎立即观察到这些数据库变化。根据连接器配置，Kafka Connect会在几秒钟或更短的时间内将更改从PostgreSQL传播到Kafka，再传播到S3。在Athena中执行一个典型的查询将返回所有的原始记录以及我们所做的任何更新或删除，作为重复记录(记录等同于<code class="fe oo op oq ny b">film_id</code>主键)。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="3412" class="nl mo it ny b gy oc od l oe of">SELECT film_id, title, release_year, rental_rate, rental_duration,<br/>         date_format(from_unixtime(__source_ts_ms/1000), '%Y-%m-%d %h:%i:%s') AS timestamp<br/>FROM "pagila_kafka_connect"."pagila_public_film"<br/>ORDER BY film_id, timestamp</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/468a5b600832a1b966bdb4795caf431a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IGLUZVDyKxDxJGQDwXHiEg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">Amazon Athena显示SQL查询和带有重复记录的结果集</figcaption></figure><p id="3a21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意原始记录以及我们之前所做的每个更改。根据<a class="ae le" href="https://debezium.io/documentation/reference/1.6/connectors/postgresql.html#postgresql-meta-information" rel="noopener ugc nofollow" target="_blank"> Debezium </a>，从<code class="fe oo op oq ny b">__source_ts_ms</code>元数据字段导出的<code class="fe oo op oq ny b">timestamp</code>字段表示提交事务的服务器时间。另外，请注意查询结果中那些<code class="fe oo op oq ny b">film_id</code>为5的记录——我们从<code class="fe oo op oq ny b">film</code>表中删除的记录。在最新记录中，字段值(<em class="oj">大部分是</em>)为空，除了在Pagila表定义中具有默认值的任何字段。如果在字段上设置了默认值(例如，<code class="fe oo op oq ny b">rental_duration smallint default 3 not null</code>或<code class="fe oo op oq ny b">rental_rate numeric(4,2) default 4.99 not null</code>),则在使用事件平展SMT时，这些值最终会出现在已删除的记录中。除了给tombstone记录增加额外的大小之外，它不会产生任何负面影响(<em class="oj">不清楚这是Debezium的预期行为还是WAL条目的产物</em>)。</p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="pj pk l"/></div></figure><p id="8bb3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要仅查看<em class="oj">最新的数据并忽略已删除的记录，我们可以使用<code class="fe oo op oq ny b">ROW_NUMBER()</code> <a class="ae le" href="https://prestodb.io/docs/current/release/release-0.75.html#row-number-optimizations" rel="noopener ugc nofollow" target="_blank">函数</a>并添加一个谓词来检查<code class="fe oo op oq ny b">__deleted</code>字段的值:</em></p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="e94e" class="nl mo it ny b gy oc od l oe of">SELECT film_id, title, release_year, rental_rate, rental_duration,<br/>         date_format(from_unixtime(__source_ts_ms/1000), '%Y-%m-%d %h:%i:%s') AS timestamp<br/>FROM <br/>    (SELECT *,<br/>         ROW_NUMBER()<br/>        OVER ( PARTITION BY film_id<br/>    ORDER BY  __source_ts_ms DESC) AS row_num<br/>    FROM "pagila_kafka_connect"."pagila_public_film") AS x<br/>WHERE x.row_num = 1<br/>    AND __deleted != 'true'<br/>ORDER BY film_id</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/eeada0652f01509c22c971480aab808e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3AqSeikX8HJ5SI61rfoSSg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">Amazon Athena显示SQL查询和带有最新记录的结果集</figcaption></figure><p id="783f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们只看到最新的记录，包括删除任何已删除的记录。虽然这种方法对于单个记录集是有效的，但是在我看来，这个查询太复杂了，不适用于复杂的连接和聚合。</p><figure class="lz ma mb mc gt md"><div class="bz fp l di"><div class="pj pk l"/></div></figure><h1 id="1d94" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">数据传送</h1><p id="3667" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">使用Amazon Athena，我们可以轻松地将我们的<code class="fe oo op oq ny b">ROW_NUMBER()</code>查询结果写回到数据湖中，以便进一步丰富或分析。Athena的<code class="fe oo op oq ny b">CREATE TABLE AS SELECT </code> ( <a class="ae le" href="https://docs.aws.amazon.com/athena/latest/ug/ctas.html" rel="noopener ugc nofollow" target="_blank"> CTAS </a> ) SQL语句根据子查询中<code class="fe oo op oq ny b">SELECT</code>语句的结果在Athena(AWS粘合数据目录中的外部表)中创建新表。Athena将CTAS语句创建的数据文件存储在Amazon S3的指定位置，并创建一个新的AWS Glue数据目录表来存储结果集的模式和元数据信息。CTAS支持多种文件格式和存储选项。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/cf544287e7ab303360c79e6caf27be9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TpXpTlb8Y_Z6PIhdkl52Pw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">这篇文章演示的高级架构</figcaption></figure><p id="2b50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将最后一个查询包装在Athena的CTAS语句中，如下所示，我们可以将查询结果作为快速压缩的Parquet格式文件写入亚马逊S3桶中的一个新位置，该文件由电影<code class="fe oo op oq ny b">rating</code>进行分区。使用<a class="ae le" href="https://www.jamesserra.com/archive/2019/10/databricks-delta-lake/" rel="noopener ugc nofollow" target="_blank">通用数据湖术语</a>，我将通过Kafka将过滤和清理后的数据集称为<em class="oj">精炼的</em>或<em class="oj">银的</em>，而不是来自我们的数据源PostgreSQL的<em class="oj">原始摄取的</em>或<em class="oj">青铜的</em>数据。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="4fff" class="nl mo it ny b gy oc od l oe of">CREATE TABLE pagila_kafka_connect.pagila_public_film_refined<br/>WITH (<br/>  format='PARQUET',<br/>  parquet_compression='SNAPPY',<br/>  partitioned_by=ARRAY['rating'],<br/>  external_location='s3://my-s3-table/refined/film/'<br/>) AS<br/>SELECT film_id, title, release_year, rental_rate, rental_duration,<br/>         date_format(from_unixtime(__source_ts_ms/1000), '%Y-%m-%d %h:%i:%s') AS timestamp, rating<br/>FROM <br/>    (SELECT *,<br/>         ROW_NUMBER()<br/>        OVER ( PARTITION BY film_id<br/>    ORDER BY  __source_ts_ms DESC) AS row_num<br/>    FROM "pagila_kafka_connect"."pagila_public_film") AS x<br/>WHERE x.row_num = 1<br/>    AND __deleted = 'false'<br/>ORDER BY film_id</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi po"><img src="../Images/3afd95a2349cfc42c054398ad06f29ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQjufQFPDH0168wMtZMtBg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">Amazon Athena在左侧显示CTAS语句和生成的新表</figcaption></figure><p id="6b2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次检查亚马逊S3桶，您应该观察到在<code class="fe oo op oq ny b">/refined/film/</code>键路径中由<code class="fe oo op oq ny b">rating</code>划分的一组新的S3对象。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi po"><img src="../Images/bd7fc7d1a3d8ed0c02f0e3c3d729d190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*94RTbBzZY_KGOOcglimTpg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">显示CTAS报表结果的亚马逊S3存储桶</figcaption></figure><p id="8f2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还应该在同一个AWS Glue数据目录中看到一个新表，其中包含我们使用CTAS语句写入S3的数据的元数据、位置和模式信息。我们可以在<em class="oj">细化的</em>数据集上执行额外的查询。</p><pre class="lz ma mb mc gt nx ny nz oa aw ob bi"><span id="2421" class="nl mo it ny b gy oc od l oe of">SELECT *<br/>FROM "pagila_kafka_connect"."pagila_public_film_refined"<br/>ORDER BY film_id</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/08c6b50b8d1040025602cc0c7c4bda07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TukdziMDg6OUQA0LupTBYQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">Amazon Athena显示了来自精确电影数据的查询结果</figcaption></figure><h1 id="ae7c" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">数据湖中的CRUD操作</h1><p id="a4e4" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">为了充分利用CDC并最大化数据湖中数据的新鲜度，我们还需要采用现代数据湖文件格式，如<a class="ae le" href="https://hudi.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache胡迪</a>、<a class="ae le" href="https://iceberg.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache冰山</a>或<a class="ae le" href="https://delta.io/" rel="noopener ugc nofollow" target="_blank"> Delta湖</a>，以及分析引擎，如<a class="ae le" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>和<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark结构化流</a>来处理数据变化。使用这些技术，可以在像亚马逊S3这样的对象存储中执行记录级的数据增删。胡迪、Iceberg和Delta Lake提供的功能包括ACID事务、模式演化、增插、删除、时间旅行和数据湖中的增量数据消耗。像Spark这样的ELT引擎可以从Kafka读取流Debezium生成的CDC消息，并使用胡迪、冰山或三角洲湖处理这些变化。</p><h1 id="515b" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">结论</h1><p id="0838" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">这篇文章探讨了基于日志的CDC如何帮助我们将来自亚马逊RDS数据库的数据合并到基于亚马逊S3的数据湖中。我们利用了亚马逊MSK、亚马逊EKS、Apache Kafka Connect、Debezium、Apache Avro和Apicurio Registry的功能。在随后的帖子中，我们将了解数据湖文件格式，如Apache胡迪、Apache Iceberg和Delta Lake，以及Apache Spark结构化流，如何帮助我们积极管理数据湖中的数据。</p></div><div class="ab cl pp pq hx pr" role="separator"><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu"/></div><div class="im in io ip iq"><p id="bfcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇博客代表我自己的观点，而不是我的雇主亚马逊网络服务公司(AWS)的观点。所有产品名称、徽标和品牌都是其各自所有者的财产。</p></div></div>    
</body>
</html>