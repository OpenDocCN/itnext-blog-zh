<html>
<head>
<title>Refine Your Sparse PySC2 Agent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化您的稀疏PySC2代理</h1>
<blockquote>原文：<a href="https://itnext.io/refine-your-sparse-pysc2-agent-a3feb189bc68?source=collection_archive---------4-----------------------#2018-04-03">https://itnext.io/refine-your-sparse-pysc2-agent-a3feb189bc68?source=collection_archive---------4-----------------------#2018-04-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/1063d22ec2be9e6d503f5fb282e066cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Fu81P2kYDF_q_2PR6V6u8g.png"/></div></figure><blockquote class="ju jv jw"><p id="826a" class="jx jy jz ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://www.linkedin.com/cws/share?url=https%3A%2F%2Fitnext.io%2Frefine-your-sparse-pysc2-agent-a3feb189bc68%3Futm_source%3Dmedium_sharelink%26utm_medium%3Dsocial%26utm_campaign%3Dbuffer" rel="noopener ugc nofollow" target="_blank">点击这里在LinkedIn上分享这篇文章</a></p></blockquote><p id="4617" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">在我的<a class="ae kw" href="https://medium.com/@skjb/build-a-sparse-reward-pysc2-agent-a44e94ba5255" rel="noopener">上一篇教程</a>中，我展示了如何构建一个从稀疏回报中学习的PySC2代理。该代理能够赢得大约25%的时间，但会失去几乎50%的时间。</p><p id="42c9" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">在本教程中，我们将建立在以前的代理，但一些小的变化，我们将能够提高胜率超过70%。</p><h1 id="6ad0" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">1.状态不变时忽略学习</h1><p id="42f4" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">在处理之前的代理时，我将Q表数据输出到一个CSV:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="f38d" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">当代理学习时，我刷新了CSV，并提出了一些关于如何改进的理论。我注意到的第一件事是，因为我的状态非常简单，所以代理采取一个动作并返回到相同的状态是很常见的。每当代理人从一个状态前进到下一个状态时，它会获得该状态的最大折扣奖励。在这种状态下，该行为的价值会向奖励方向移动一点。</p><p id="73e7" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">虽然当一个动作将代理从一个状态移动到下一个状态时，这通常不是一个大问题，但是当它频繁地停留在同一个状态时，它倾向于将价值较低的动作向上推到价值最高的动作，而价值最高的动作被向下推。随着时间的推移，如果他们更频繁地停留在同一个状态，而不是另一个明显不同的状态，所有的回报将接近零。</p><p id="4b7d" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">为了解决这个问题，我们可以简单地阻止代理在动作没有改变状态的任何时候学习。代码非常简单，只需在<code class="fe mj mk ml mm b">QLearningTable</code>的<code class="fe mj mk ml mm b">learn()</code>方法的开头添加两行:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="e629" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">当状态相同时，这将立即中止学习步骤。</p><h1 id="18c6" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">2.防止无效操作</h1><p id="fbbc" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">在观察代理时，我注意到有几次它会被发现重复尝试无效的动作。通常可用动作的数量是可能动作的一半，所以代理花费时间试图从不会产生结果的动作中学习。</p><p id="1d0f" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">通过过滤这些无效的动作，我们可以让代理专注于尝试应该导致状态改变的动作，减少探索并改进学习时间。</p><p id="6596" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">首先我们修改<code class="fe mj mk ml mm b">QLearningTable</code>的构造函数:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="63fe" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">在最后一行，我们创建了一个不允许操作的列表。</p><p id="3954" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">然后，在<code class="fe mj mk ml mm b">choose_action()</code>方法中，我们接受给定状态的无效动作列表:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="d980" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">然后，我们从可能的选项中筛选出这些操作，这样代理就不会采取无效的操作:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="bb5f" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">然后我们做了一个非常重要的改变。因为无效的行为永远不会被选择，它们的回报永远不会改变。如果它们从0开始，那么它们可能成为该状态的最高值动作，如果所有其他动作具有负值的话。为了解决这个问题，我们在<code class="fe mj mk ml mm b">learn()</code>方法中从未来状态的奖励中过滤出无效的行为:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="8e28" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">接下来，我们要建立一个无效动作的列表。在代理的<code class="fe mj mk ml mm b">step()</code>方法中，收集一些我们将用于决策的数据:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="4bea" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">现在我们开始排除动作。首先，如果我们已经达到了2个补给站的限制，或者我们没有工人来建造补给站，让我们取消代理建造补给站的能力:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="e031" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">如果我们没有补给站，或者我们已经达到了2个兵营的上限，或者我们没有建造兵营的工人，我们就不要让代理建造兵营了:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="d547" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">如果我们没有任何兵营或者我们已经达到了我们的供应极限，我们不想训练海军陆战队:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="f5bf" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">最后，如果我们没有海军陆战队，我们不想攻击任何东西:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="d6d2" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">现在我们有了无效操作的列表，我们可以在选择操作时输入它:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><h1 id="a4c3" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">3.将我们的单位位置添加到州</h1><p id="c519" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">这一个不是数据驱动的，而是一个理论。如果我们的代理不知道它的单位在哪里，它怎么知道哪个位置可能是最好的攻击？</p><p id="6076" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">将我们的单元位置添加到州非常简单，首先我们将州的大小增加到12:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="4e73" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">接下来，我们使用相同的逻辑来识别敌人的位置，除了我们为我们的单位过滤小地图:</p><figure class="md me mf mg gt jr"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="44a8" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">和敌人的位置一样，这将把小地图分成4份，任何包含友军单位的象限的值都设置为1。</p><h1 id="de20" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">4.试试看！</h1><p id="f137" class="pw-post-body-paragraph jx jy iq ka b kb ly kd ke kf lz kh ki kx ma kl km ky mb kp kq kz mc kt ku kv ij bi translated">来吧，试一试:</p><pre class="md me mf mg gt mn mm mo mp aw mq bi"><span id="e287" class="mr lb iq mm b gy ms mt l mu mv">python -m pysc2.bin.agent \<br/>--map Simple64 \<br/>--agent refined_agent.SparseAgent \<br/>--agent_race T \<br/>--max_agent_steps 0 \<br/>--norender</span></pre><p id="2a5d" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">信不信由你，我的经纪人在仅仅512场比赛后就取得了67%的胜率(平均过去100场比赛的胜率)。1965年游戏达到了71%,但你可以从本文开头的图表中看到，它在大部分时间里都做得很好。前一个代理人的峰值为59%。</p><p id="3114" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">这里有一个图表与之前的代理进行比较，显示了一段时间而不是过去100场比赛的胜率:</p><figure class="md me mf mg gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/a42ed6e3233732bed23eec2473c9dbac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*vky4H3CLJyQJVkYJOzCljw.png"/></div></figure><p id="6688" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">如你所见，胜率仍呈上升趋势。</p><p id="3c72" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">本教程的所有代码都可以在<a class="ae kw" href="https://github.com/skjb/pysc2-tutorial/tree/master/Refining%20the%20Sparse%20Reward%20Agent" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="88ba" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">如果你喜欢这个教程，请在<a class="ae kw" href="https://www.patreon.com/skjb" rel="noopener ugc nofollow" target="_blank"> Patreon </a>上支持我。也请和我一起上<a class="ae kw" href="https://discord.gg/qTZ65sh" rel="noopener ugc nofollow" target="_blank">不和谐</a>，或者关注我上<a class="ae kw" href="https://www.twitch.tv/skjb" rel="noopener ugc nofollow" target="_blank"> Twitch </a>、<a class="ae kw" href="https://medium.com/@skjb" rel="noopener"> Medium </a>、<a class="ae kw" href="https://github.com/skjb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>、<a class="ae kw" href="https://twitter.com/theskjb" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae kw" href="https://www.youtube.com/channel/UCZcEvhpV4_6llcrWrWQ2wsg" rel="noopener ugc nofollow" target="_blank"> YouTube </a>。</p></div></div>    
</body>
</html>