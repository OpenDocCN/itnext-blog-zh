<html>
<head>
<title>Multiline logs in OpenShift EFK stack</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenShift EFK堆栈中的多行日志</h1>
<blockquote>原文：<a href="https://itnext.io/multiline-logs-in-openshift-efk-stack-7a7bda4ed055?source=collection_archive---------2-----------------------#2018-07-31">https://itnext.io/multiline-logs-in-openshift-efk-stack-7a7bda4ed055?source=collection_archive---------2-----------------------#2018-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1dd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在容器中运行应用程序有很多优点，比如水平伸缩和有效的资源管理。开发<a class="ae kl" href="https://www.cncf.io/" rel="noopener ugc nofollow" target="_blank">云原生</a>应用需要不同的思维模式，熟悉新框架总是需要一个学习曲线。许多传统上由应用服务器提供的特性——部署管理、配置、指标、安全性、日志、监控、<a class="ae kl" href="https://developers.redhat.com/books/introducing-istio-service-mesh-microservices/" rel="noopener ugc nofollow" target="_blank">服务查找</a>—现在属于容器运行时环境。这很好，因为容器不是特定于语言的，应用程序可以专注于核心业务逻辑。另一方面，云原生应用程序是动态的，本质上是分布式的，这使得日志记录和跟踪等特定任务变得更加复杂。</p><p id="061a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a>可能是目前最知名的容器运行时。<a class="ae kl" href="https://www.openshift.com/" rel="noopener ugc nofollow" target="_blank"> Red Hat OpenShift </a>是一个基于Kubernetes的企业级容器应用平台。为了解决从容器中收集和搜索日志的问题，可以使用EFK (Elasticsearch，Fluentd，Kibana)栈<a class="ae kl" href="https://docs.openshift.com/container-platform/3.9/install_config/aggregate_logging.html" rel="noopener ugc nofollow" target="_blank">部署日志聚合</a>。理解和调优整个日志记录解决方案可能很复杂，需要深入了解每个组件，但一般目的是从容器的标准输出中收集日志行并编制索引，并用Kubernetes元数据标记它们。Docker日志由每个节点上的Fluentd进程收集，并转发给Elasticsearch进行存储，Kibana提供了一个UI用于查找。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/af79e3898f2277176fb8d2717841a27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_LD8Q4OlYGoZa2G1MFJYpA.png"/></div></div></figure><p id="b684" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦在集群上成功启动了第一个容器，您可能会注意到的一个问题是，标准输出中的每一行都被作为一个单独的日志事件来处理。只要您构建自己的日志行就可以了，但是在某些情况下(例如，请求负载、Java堆栈跟踪),您的应用程序最终会丢弃多行日志。如果它们被一行一行地分开，浏览或搜索它们是令人厌倦的，所以让我们试着找到一个解决方案。</p><p id="0f58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">问题从码头工人层面开始。无论Docker进程的标准输出最终出现在<a class="ae kl" href="https://docs.docker.com/config/containers/logging/journald/" rel="noopener ugc nofollow" target="_blank">日志</a>还是<a class="ae kl" href="https://docs.docker.com/config/containers/logging/json-file/" rel="noopener ugc nofollow" target="_blank"> json </a>日志中，它们都是逐行处理的。在这一层对多条线进行分组的特征请求是一个<a class="ae kl" href="https://github.com/moby/moby/issues/22920#issuecomment-264036710" rel="noopener ugc nofollow" target="_blank">死胡同</a>。在<a class="ae kl" href="https://github.com/fluent-plugins-nursery/fluent-plugin-concat" rel="noopener ugc nofollow" target="_blank"> fluentd </a>中通过一个regexp收集多行也许是可能的，但是目前似乎还没有通用的解决方案。虽然我希望有一天社区会提出一个可接受的方法，但是围绕这个主题的大多数讨论得出的结论是“你必须在你的应用程序中处理这个问题”。</p><p id="a6e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我们尝试了两种解决方法，它们或多或少地处理了这个问题，并且对应用程序的影响很小:</p><ul class=""><li id="d16d" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">'将行尾转换成CR '\r '</li><li id="4217" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">使用结构化的json日志</li></ul><blockquote class="lm ln lo"><p id="ee61" class="jn jo lp jp b jq jr js jt ju jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj kk ij bi translated">撰写本帖时使用的软件版本:<br/>-open shift v 3.9<br/>-Kubernetes v 1.9<br/>-Fluentd v 0 . 12 . 42<br/>-elastic search v 2 . 4 . 4<br/>-Kibana v 4 . 6 . 4<br/>-Spring Boot v 1 . 5 . 13<br/>-log back v 1 . 1 . 11<br/>-log stash-log back-encoder v 5.1</p></blockquote><h1 id="533c" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">行尾Hack</h1><p id="84cb" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">传统上，不同的操作系统使用不同的<a class="ae kl" href="https://en.wikipedia.org/wiki/Newline" rel="noopener ugc nofollow" target="_blank">换行符</a>。现在最常见的是Unix风格的LF <em class="lp"> (\n) </em>和Windows的CRLF<em class="lp">(\ r \ n)</em>但是CR <em class="lp"> (\r) </em>过去也经常使用。现代软件栈试图通过自动识别和转换不同的风格来减轻处理不同行尾的痛苦。</p><p id="1e1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">令人惊讶的是，对于Openshift EFK日志堆栈，使用CR作为属于同一个日志事件的行的换行符，并在日志事件之间使用常规LF，这是一个简单的方法。当我们谈论容器时，我们谈论带有LF <em class="lp"> (\n) </em>行尾的Linux环境。因此，我们需要修改应用程序的日志输出，将一个日志消息中的所有LF字符转换为CR，这可能很困难，也可能不困难，具体取决于所使用的软件栈。</p><p id="bb40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我主要是构建Java微服务，所以我尝试了一个使用Logback的Spring Boot应用程序(同样适用于Log4j)。这个日志框架有一个<a class="ae kl" href="https://logback.qos.ch/manual/layouts.html#replace" rel="noopener ugc nofollow" target="_blank"> <em class="lp">替换</em> </a>特性，可以用字符串替换正则表达式。这样的日志模式可以在<em class="lp"> application.yml </em>中定义如下:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="6176" class="nb lu iq mx b gy nc nd l ne nf">logging:<br/>  pattern:<br/>    console: %d %5p %logger: %replace(%m%wEx){'\\n','\r'}%nopex%n</span></pre><p id="9cdb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基本上是用CR (\r)代替LF (\n)，但是表达式需要一些解释。<em class="lp">替换</em>转换应用于消息<code class="fe ng nh ni mx b">%m</code>和java异常堆栈跟踪(如果有的话)。<code class="fe ng nh ni mx b">%wEx</code>是<a class="ae kl" href="https://docs.spring.io/spring-boot/docs/1.5.13.RELEASE/api/org/springframework/boot/logging/logback/WhitespaceThrowableProxyConverter.html" rel="noopener ugc nofollow" target="_blank"> Spring Boot特有的</a>，但像<code class="fe ng nh ni mx b">%n%ex</code>周围有一些空格的除外。这里应用了Java字符串转义，因此第一个参数是两个字符' \'+'n '的regexp，而第二个是一个带有一个CR字符的字符串(参见下面的<em class="lp"> logback.xml </em>)。<code class="fe ng nh ni mx b">%nopex</code>表示我们处理模式中的异常，否则默认情况下Logback会再次追加它。</p><p id="ee1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以使用CR的unicode值在回溯配置XML中设置类似的模式:</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="nj nk l"/></div></figure><blockquote class="lm ln lo"><p id="162e" class="jn jo lp jp b jq jr js jt ju jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj kk ij bi translated">注意:由于模式布局中的特殊CR字符，不容易通过OpenShift ConfigMap或容器环境变量(可能是b64编码的)来设置它。通常，日志模式是在应用程序打包的属性文件中设置的，所以这不是一个问题。</p></blockquote><p id="db2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://github.com/bszeti/multiline-log" rel="noopener ugc nofollow" target="_blank">这个示例应用程序</a>使用换行符记录一条消息(第31行),并在消息开始时抛出一个异常(第32行)。默认情况下，所有行都作为单独的日志事件处理。使用上面的日志模式，日志在OpenShift控制台上的pod页面上是可读的(尽管日志事件被分成2K长的段):</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nl"><img src="../Images/de6698925aa51abd2c565b86bd44fc88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktphH_HZLNJH2hZQZFodow.png"/></div></div></figure><p id="e134" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，在Kibana中，日志行属于预期的一个日志事件:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nm"><img src="../Images/fcbf3cbecc2052dda536e72a21812dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3CL-GKjNEot1lwzn5RvSg.png"/></div></div></figure><p id="707f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">日志事件的json视图显示了编码为' \r '的换行符，但是换行符在屏幕上的消息字段中看起来很好。</p><p id="0981" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们试图用OpenShift <code class="fe ng nh ni mx b">oc</code>客户端访问pod日志，CR字符会引起问题，但是可以使用一个简单的脚本(perl或其他)将CR转换回LF:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="9278" class="nb lu iq mx b gy nc nd l ne nf">oc logs -f multiline-log-9-sflkm | perl -pe 's/\R/\n/g'</span></pre><h1 id="6f26" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">结构化json日志</h1><p id="60ad" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">尽管上面解释的换行符转换在实践中有所帮助，但它确实感觉像是偶然发生的事情。作为一个“合适的解决方案”,我们应该使用结构化日志。在这种情况下，我们的应用程序需要将每个日志事件包装在带有特定字段名的json消息中，其中换行符被json编码为' \n '。</p><p id="7949" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，我们的两行初始化日志事件在标准输出中看起来像这样:</p><pre class="kn ko kp kq gt mw mx my mz aw na bi"><span id="b349" class="nb lu iq mx b gy nc nd l ne nf">{"<a class="ae kl" href="http://twitter.com/timestamp" rel="noopener ugc nofollow" target="_blank">@timestamp</a>":"2018-07-30T17:15:01.127-04:00", "<a class="ae kl" href="http://twitter.com/version" rel="noopener ugc nofollow" target="_blank">@version</a>":"1", "message":"<strong class="mx ir">Init GreetingController with message:\nHello %s from application.yaml</strong>!", "logger_name":"my.company.multilinelog.service.GreetingController", "thread_name":"main","level":"INFO","level_value":20000}</span></pre><p id="878c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">OpenShift <a class="ae kl" href="https://github.com/openshift/origin-aggregated-logging/tree/v3.9.0/fluentd" rel="noopener ugc nofollow" target="_blank"> Fluentd image </a>带有预配置的插件，这些插件解析这些json日志并将它们合并到转发给Elasticsearch的消息中。这不仅有利于多行日志，还保证了日志事件的其他字段(例如时间戳、严重性)在Elasticsearch中看起来不错。</p><p id="392e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于具有Logback或Log4j的Java应用程序，请看一下<a class="ae kl" href="https://developers.redhat.com/blog/2018/01/22/openshift-structured-application-logs/" rel="noopener ugc nofollow" target="_blank">这个详细的pos </a> t。要启用结构化日志，我们必须将<a class="ae kl" href="https://github.com/logstash/logstash-logback-encoder" rel="noopener ugc nofollow" target="_blank"><em class="lp">log stash-Logback-encoder</em></a><em class="lp"/>添加到类路径(参见<a class="ae kl" href="https://github.com/bszeti/multiline-log/blob/master/pom.xml" rel="noopener ugc nofollow" target="_blank"> pom.xml </a>)中，并使用<em class="lp"> LogstashEncoder </em>，如本<a class="ae kl" href="https://github.com/bszeti/multiline-log/blob/master/src/main/resources/logback-logstash.xml" rel="noopener ugc nofollow" target="_blank"> <em class="lp"> logback.xml </em> </a>所示:</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="744b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用这种配置，这些日志在Kibana中看起来很棒，但是作为一个代价，它们很难在OpenShift web控制台上或使用<code class="fe ng nh ni mx b">oc</code>客户端读取，因为它们是json消息。</p><p id="dd7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你知道如何在其他开发栈和语言中启用结构化日志，请在评论中添加它们或者写一篇关于它的帖子…</p><h1 id="57d2" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">摘要</h1><p id="92c9" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">尽管“能够处理多行日志”听起来像是一个简单的请求，但是用当前可用的工具集找到一个通用的解决方案是很复杂的。</p><p id="d6b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结构化日志记录可能是正确的方法，但是很难想象所有的软件开发栈都同意一个受支持的结构，并且不仅仅是在标准输出上“转储文本”。即使您使用支持这种模式的框架，它也只对生产环境有益。</p><p id="1c26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在开发或测试环境中，pod标准输出的可读性可能比搜索功能更重要。“line ending hack”是一个折中的解决方案，如果您没有现成的结构化json日志解决方案，它可能更容易实现。</p></div></div>    
</body>
</html>