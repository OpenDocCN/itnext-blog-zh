<html>
<head>
<title>Dive Deep into Resource Requests and Limits in Kubernetes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入探究Kubernetes中的资源请求和限制</h1>
<blockquote>原文：<a href="https://itnext.io/dive-deep-into-resource-requests-and-limits-in-kubernetes-3e99030697ec?source=collection_archive---------1-----------------------#2021-06-23">https://itnext.io/dive-deep-into-resource-requests-and-limits-in-kubernetes-3e99030697ec?source=collection_archive---------1-----------------------#2021-06-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/09a80d3ae4c747fe125e045cf46a1b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OdfPSwopCt9LnZBFosCzuQ.png"/></div></div></figure><p id="5089" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在Kubernetes集群中创建资源时，您可能会遇到以下情况:</p><ol class=""><li id="efc1" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">没有为工作负载指定CPU请求或指定较低的CPU请求，这意味着更多的单元“似乎”能够在同一个节点上工作。在流量突发期间，您的CPU会因较长的延迟而达到极限，而您的一些机器可能会出现CPU软锁定。</li><li id="94f3" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">同样，没有为工作负载指定内存请求或低内存请求。有些pod，尤其是运行Java业务应用的，在本地测试中实际上可以正常运行的时候，会一直重启。</li><li id="9ad8" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">在Kubernetes集群中，工作负载通常不会均匀地跨节点进行调度。特别是在大多数情况下，内存资源是不均匀分布的，这意味着一些节点可以看到比其他节点高得多的内存利用率。作为容器编排的事实上的标准，Kubernetes应该有一个有效的调度器来确保资源的均匀分布。但是，真的是这样吗？</li></ol><p id="bf9f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一般来说，如果当所有机器挂起并且SSH登录失败时，在流量激增的情况下发生上述问题，集群管理员除了重新启动集群之外什么也做不了。在本文中，我们将深入Kubernetes的请求和限制，分析可能出现的问题并讨论它们的最佳实践。如果你也对底层机制感兴趣，也可以找源代码角度的分析。希望这篇文章能帮助你理解Kubernetes的请求和限制是如何工作的，以及为什么它们能以预期的方式工作。</p><h1 id="f70d" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">概念</h1><p id="7c10" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">为了充分利用Kubernetes集群中的资源并提高调度效率，Kubernetes使用请求和限制来控制容器的资源分配。每个容器都有自己的请求和限制。这两个参数由<code class="fe mn mo mp mq b">resources.requests</code>和<code class="fe mn mo mp mq b">resources.limits</code>指定。一般来说，请求在调度中更重要，而限制在运行中更重要。</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="2676" class="mz ll iq mq b gy na nb l nc nd">resources:  <br/>    requests:    <br/>        cpu: 50m<br/>        memory: 50Mi<br/>    limits:    <br/>        cpu: 100m<br/>        memory: 100Mi</span></pre><p id="b3d8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请求定义了容器需要的最小资源量。例如，对于运行Spring Boot业务的容器，指定的请求必须是Java虚拟机(JVM)需要在容器映像中消耗的最小资源量。如果您只指定一个低内存请求，那么Kubernetes调度程序很可能会将Pod调度到没有足够资源来运行JVM的节点上。也就是说，Pod不能使用JVM启动过程所需的更多内存。结果，吊舱不断重启。</p><p id="a6d5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一方面，限制决定了容器可以使用的最大资源量，防止由于过度消耗资源而导致资源短缺或机器崩溃。如果设置为<code class="fe mn mo mp mq b">0</code>，表示容器没有资源限制。特别是，如果您设置了<code class="fe mn mo mp mq b">limits</code>而没有指定<code class="fe mn mo mp mq b">requests</code>，Kubernetes默认认为<code class="fe mn mo mp mq b">requests</code>的值与<code class="fe mn mo mp mq b">limits</code>的值相同。</p><p id="61a6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请求和限制适用于两种类型的资源—可压缩资源(例如CPU)和不可压缩资源(例如内存)。对于不可压缩资源，适当的限制是极其重要的。</p><p id="d4bd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下是对请求和限制的简要总结:</p><ul class=""><li id="060d" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv ne lc ld le bi translated">如果Pod中的服务使用的CPU资源超过指定的限制，Pod将受到限制，但不会被终止。如果没有设置限制，Pod可以使用所有空闲的CPU资源。</li><li id="1695" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv ne lc ld le bi translated">如果Pod使用的内存资源超过指定的限制，Pod中的容器进程将由于OOM而被终止。在这种情况下，Kubernetes倾向于在原始节点上重启容器，或者简单地创建另一个Pod。</li><li id="954a" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv ne lc ld le bi translated">0 &lt;=请求&lt; =可分配的节点；请求&lt; =限制&lt; =无穷大。</li></ul><h1 id="9a87" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">情况分析</h1><p id="7e98" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">在我们看了请求和限制的概念之后，让我们回到开头提到的三个场景。</p><h1 id="39d5" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">场景1</h1><p id="0be2" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">首先也是最重要的，你需要知道CPU资源和内存资源是完全不同的。CPU资源是可压缩的。CPU的分配和管理基于完全公平调度器(CFS)和Cgroups。简单地说，如果Pod中的服务使用的CPU资源超过了指定的CPU限制，那么Kubernetes就会对其进行节流。对于没有CPU限制的pod，一旦闲置的CPU资源用完，之前分配的CPU资源量就会逐渐减少。在这两种情况下，最终，pod将无法处理外部请求，导致更长的延迟和响应时间。</p><h1 id="8bed" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">场景2</h1><p id="bf4a" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">相反，内存无法压缩，Pods无法共享内存资源。这意味着如果内存耗尽，新内存资源的分配肯定会失败。</p><p id="0c17" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Pod中的一些进程在初始化时专门需要一定量的内存。例如，JVM在启动时会申请一定数量的内存。如果指定的内存请求小于JVM应用的内存，内存应用将失败(OOM-kill)。因此，Pod会不断重启和失败。</p><h1 id="6d6e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">场景3</h1><p id="c2b8" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">当创建一个Pod时，Kubernetes需要以平衡和全面的方式分配或提供不同的资源，包括CPU和内存。同时，Kubernetes调度算法需要多种因素，如<code class="fe mn mo mp mq b">NodeResourcesLeastAllocated</code>和Pod亲和力。内存资源经常分布不均的原因是，对于app来说，内存被认为比其他资源更稀缺。</p><p id="be44" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，Kubernetes调度器基于集群的当前状态工作。换句话说，当创建新的Pods时，调度程序会根据当时集群的资源规格为Pods选择一个最佳节点来运行。这是潜在问题发生的地方，因为Kubernetes集群是高度动态的。例如，为了维护一个节点，您可能需要封锁它，并且在它上面运行的所有pod将被调度到其他节点。问题是，在维护之后，这些pod不会自动调度回原始节点。这是因为一个正在运行的Pod一旦在开始时绑定到一个节点，就不能由Kubernetes自己重新调度到另一个节点。</p><h1 id="3630" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">最佳实践</h1><p id="d08d" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">从上面的分析我们可以知道，集群的稳定性对你的应用程序的性能有直接的影响。暂时的资源短缺通常是集群不稳定的主要原因，这意味着应用程序故障甚至节点故障。这里，我们想介绍两种提高集群稳定性的方法。</p><p id="6f89" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，通过<a class="ae nf" href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/" rel="noopener ugc nofollow" target="_blank">编辑kubelet配置文件</a>，预留一定的系统资源。当您处理不可压缩的计算资源(如内存或磁盘空间)时，这一点尤其重要。</p><p id="241d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其次，为pod配置合适的<a class="ae nf" href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/" rel="noopener ugc nofollow" target="_blank">服务质量(QoS)等级</a>。Kubernetes使用QoS等级来确定pod的调度和驱逐优先级。不同的pod可以被分配不同的QoS等级，包括<code class="fe mn mo mp mq b">Guaranteed</code>(最高优先级)、<code class="fe mn mo mp mq b">Burstable</code>和<code class="fe mn mo mp mq b">BestEffort</code>(最低优先级)。</p><ul class=""><li id="9373" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv ne lc ld le bi translated"><code class="fe mn mo mp mq b">Guaranteed</code>。Pod中的每个容器(包括init容器)都必须有为CPU和内存指定的请求和限制，并且它们必须相等。</li><li id="69b3" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv ne lc ld le bi translated"><code class="fe mn mo mp mq b">Burstable</code>。Pod中至少有一个容器具有为CPU或内存指定的请求。</li><li id="7aae" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv ne lc ld le bi translated"><code class="fe mn mo mp mq b">BestEffort</code>。Pod中的任何容器都没有为CPU和内存指定请求和限制。</li></ul><p id="c33c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ng">注:</em></p><p id="bcce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ng">使用Kubelet的CPU管理策略，您可以为特定的Pod设置CPU关联。有关更多信息，请参见Kubernetes文档</em><em class="ng"/><a class="ae nf" href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/" rel="noopener ugc nofollow" target="_blank"><em class="ng">。</em></a></p><p id="ba77" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当资源耗尽时，您的集群将首先杀死QoS等级为<code class="fe mn mo mp mq b">BestEffort</code>的Pods，然后是<code class="fe mn mo mp mq b">Burstable</code>。换句话说，具有最低优先级的pod首先被终止。如果你有足够的资源，你可以把所有的吊舱都分配到<code class="fe mn mo mp mq b">Guaranteed</code>这个等级。这可以被认为是计算资源与性能和稳定性之间的权衡。您可能期望更高的开销，但是您的集群可以更高效地工作。同时，为了提高资源利用率，您可以将运行业务服务的pod分配给<code class="fe mn mo mp mq b">Guaranteed</code>类。对于其他服务，根据其优先级将它们分配到<code class="fe mn mo mp mq b">Burstable</code>或<code class="fe mn mo mp mq b">BestEffort</code>类。</p><p id="58c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们将使用<a class="ae nf" href="https://kubesphere.io/" rel="noopener ugc nofollow" target="_blank"> Kubernetes平台</a>作为例子，来看看如何为Pods优雅地配置资源。</p><h1 id="21af" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">使用KubeSphere分配资源</h1><p id="1d4c" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">如上所述，请求和限制是集群稳定性的两个重要组成部分。作为Kubernetes的主要发行版之一，KubeSphere拥有一个简洁、清晰和交互式的用户界面，大大减少了Kubernetes的学习曲线。</p><h1 id="0d9e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">开始之前</h1><p id="ad88" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">KubeSphere具有一个高功能的多租户系统，用于不同用户的细粒度访问控制。在KubeSphere 3.0中，可以分别为名称空间(ResourceQuotas)和容器(LimitRanges)设置请求和限制。要执行这些操作，您需要创建一个工作区、一个项目(即命名空间)和一个帐户(<code class="fe mn mo mp mq b">ws-admin</code>)。有关更多信息，请参见<a class="ae nf" href="https://kubesphere.io/docs/quick-start/create-workspace-and-project/" rel="noopener ugc nofollow" target="_blank">创建工作空间、项目、账户和角色</a>。</p><h1 id="43e3" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">设置资源配额</h1><ol class=""><li id="f68a" class="kw kx iq ka b kb mi kf mj kj nh kn ni kr nj kv lb lc ld le bi translated">进入您项目的<strong class="ka ir">概述</strong>页面，在<strong class="ka ir">项目设置</strong>中导航到<strong class="ka ir">基本信息</strong>，在<strong class="ka ir">管理项目</strong>下拉菜单中选择<strong class="ka ir">编辑额度</strong>。</li></ol><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/0680fc7a3aab4cbd9c2c8ae91c033702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H1pGz8EcoYZYOr00.png"/></div></div></figure><p id="9de6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.在出现的对话框中，为您的项目设定请求和限制。</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/cc32f7a260f52e1f6a0ea3ef6f853201.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*08KtmvY5654k8r_v.png"/></div></div></figure><p id="a352" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请记住:</p><ul class=""><li id="f130" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv ne lc ld le bi translated">在此页面上设置的请求或限制必须大于为项目中所有窗格指定的请求或限制总数。</li><li id="d2c1" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv ne lc ld le bi translated">当您在项目中创建容器而没有指定请求或限制时，您会在创建时看到一条错误消息(记录在事件中)。</li></ul><p id="7adb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦您配置了项目配额，就需要为项目中创建的所有容器指定<code class="fe mn mo mp mq b">requests</code>和<code class="fe mn mo mp mq b">limits</code>。正如我们常说的，“代码就是法律”。项目配额设定了所有容器都要遵守的规则。</p><p id="cf1a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ng">注:</em></p><p id="52a5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ng">kube sphere中的项目配额与Kubernetes中的</em> <a class="ae nf" href="https://kubernetes.io/docs/concepts/policy/resource-quotas/" rel="noopener ugc nofollow" target="_blank"> <em class="ng">资源配额</em> </a> <em class="ng">相同。除了CPU和内存，您还可以单独为其他对象(如部署和配置图)设置资源配额。详见</em> <a class="ae nf" href="https://kubesphere.io/docs/workspace-administration/project-quotas/" rel="noopener ugc nofollow" target="_blank"> <em class="ng">项目定额</em> </a> <em class="ng">。</em></p><h1 id="c16b" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">设置默认请求和限制</h1><p id="b93d" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">如上所述，如果指定了项目配额，您需要相应地配置pod的请求和限制。事实上，在测试甚至生产中，<code class="fe mn mo mp mq b">requests</code>的值和<code class="fe mn mo mp mq b">limits</code>的值非常接近，甚至对大多数吊舱来说是等价的。为了简化创建工作负载的过程，KubeSphere允许用户预先为容器设置默认请求和限制。这样，您就不需要在每次创建pod时都设置请求和限制。</p><p id="0317" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要设置默认请求和限制，请执行以下步骤:</p><ol class=""><li id="1979" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">同样在<strong class="ka ir">基本信息</strong>页面，从<strong class="ka ir">管理项目</strong>下拉菜单中点击<strong class="ka ir">编辑资源默认请求</strong>。</li><li id="de7c" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">在出现的对话框中，配置容器的默认请求和限制。</li></ol><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/5c8cacd69ca7bd9724cfc37b34d3a38a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zU2XtPScM0FWWVz_.png"/></div></div></figure><p id="99f0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ng">注</em></p><p id="81e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ng">kube sphere中的默认容器请求和限制在Kubernetes中称为</em> <a class="ae nf" href="https://kubernetes.io/docs/concepts/policy/limit-range/" rel="noopener ugc nofollow" target="_blank"> <em class="ng">限制范围</em> </a> <em class="ng">。更多信息请参见</em> <a class="ae nf" href="https://kubesphere.io/docs/project-administration/container-limit-ranges/" rel="noopener ugc nofollow" target="_blank"> <em class="ng">集装箱限制范围</em> </a> <em class="ng">。</em></p><p id="fdbd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">3.当您稍后创建工作负荷时，将自动预填充请求和限制。有关如何在KubeSphere中创建工作负载的更多信息，请参见<a class="ae nf" href="https://kubesphere.io/docs/project-user-guide/application-workloads/deployments/" rel="noopener ugc nofollow" target="_blank">KubeSphere文档</a>。</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/a3b7a6e4c8e148a554f3f08364d0a509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YvSIUf3rMM-FQRiA.jpg"/></div></div></figure><p id="804d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于运行关键业务流程的容器，它们需要处理比其他容器更多的流量。实际上，没有灵丹妙药，您需要对这些容器的请求和限制做出谨慎和全面的决定。思考以下问题:</p><ol class=""><li id="c3f6" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">您的容器是CPU密集型还是IO密集型？</li><li id="1c55" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">它们的可用性高吗？</li><li id="db9c" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">你的服务的上游和下游对象是什么？</li></ol><p id="31c2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你长期观察集装箱的装载量，你会发现它是周期性的。在这方面，历史监控数据可以作为配置请求和限制的重要参考。在集成到平台中的Prometheus的背面，KubeSphere具有强大的整体可观测性系统，可以在粒度级别上监控资源。纵向上，它涵盖了从集群到单元的数据。横向上，它跟踪关于CPU、内存、网络和存储的信息。通常，您可以基于历史数据的平均值指定请求，而限制需要高于平均值。也就是说，你可能需要根据需要对你的最终决定做一些调整。</p><h1 id="3a0a" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">源代码分析</h1><p id="0f27" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">现在，您已经知道了配置请求和限制的一些最佳实践，让我们更深入地研究源代码。</p><h1 id="92c4" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">请求和计划</h1><p id="38eb" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">以下代码显示了Pod请求和Pod中容器请求之间的关系。</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="7de3" class="mz ll iq mq b gy na nb l nc nd">func computePodResourceRequest(pod *v1.Pod) *preFilterState {<br/> result := &amp;preFilterState{}<br/> for _, container := range pod.Spec.Containers {<br/>  result.Add(container.Resources.Requests)<br/> }</span><span id="c151" class="mz ll iq mq b gy no nb l nc nd">// take max_resource(sum_pod, any_init_container)<br/> for _, container := range pod.Spec.InitContainers {<br/>  result.SetMaxResource(container.Resources.Requests)<br/> }</span><span id="e109" class="mz ll iq mq b gy no nb l nc nd">// If Overhead is being utilized, add to the total requests for the pod<br/> if pod.Spec.Overhead != nil &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.PodOverhead) {<br/>  result.Add(pod.Spec.Overhead)<br/> }</span><span id="6ce3" class="mz ll iq mq b gy no nb l nc nd">return result<br/>}<br/>...<br/>func (f *Fit) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) *framework.Status {<br/> cycleState.Write(preFilterStateKey, computePodResourceRequest(pod))<br/> return nil<br/>}<br/>...<br/>func getPreFilterState(cycleState *framework.CycleState) (*preFilterState, error) {<br/> c, err := cycleState.Read(preFilterStateKey)<br/> if err != nil {<br/>  // preFilterState doesn't exist, likely PreFilter wasn't invoked.<br/>  return nil, fmt.Errorf("error reading %q from cycleState: %v", preFilterStateKey, err)<br/> }</span><span id="3d78" class="mz ll iq mq b gy no nb l nc nd">s, ok := c.(*preFilterState)<br/> if !ok {<br/>  return nil, fmt.Errorf("%+v  convert to NodeResourcesFit.preFilterState error", c)<br/> }<br/> return s, nil<br/>}<br/>...<br/>func (f *Fit) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {<br/> s, err := getPreFilterState(cycleState)<br/> if err != nil {<br/>  return framework.NewStatus(framework.Error, err.Error())<br/> }</span><span id="8b14" class="mz ll iq mq b gy no nb l nc nd">insufficientResources := fitsRequest(s, nodeInfo, f.ignoredResources, f.ignoredResourceGroups)</span><span id="bf9b" class="mz ll iq mq b gy no nb l nc nd">if len(insufficientResources) != 0 {<br/>  // We will keep all failure reasons.<br/>  failureReasons := make([]string, 0, len(insufficientResources))<br/>  for _, r := range insufficientResources {<br/>   failureReasons = append(failureReasons, r.Reason)<br/>  }<br/>  return framework.NewStatus(framework.Unschedulable, failureReasons...)<br/> }<br/> return nil<br/>}</span></pre><p id="ba53" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从上面的代码可以看出，调度器(调度线程)计算的是要调度的Pod所需的资源。具体来说，它根据Pod规范分别计算init容器的总请求数和工作容器的总请求数。将使用较大的一个。请注意，对于轻量级虚拟机(例如kata-container)，它们自己的虚拟化资源消耗需要计入缓存中。在接下来的<code class="fe mn mo mp mq b">Filter</code>阶段，将检查所有节点是否满足条件。</p><p id="6c28" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ng">注:</em></p><p id="6fa8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ng">调度流程包含不同的阶段，包括</em><code class="fe mn mo mp mq b"><em class="ng">Pre filter</em></code><em class="ng"/><code class="fe mn mo mp mq b"><em class="ng">Filter</em></code><em class="ng"/><code class="fe mn mo mp mq b"><em class="ng">Post filter</em></code><em class="ng"/><code class="fe mn mo mp mq b"><em class="ng">Score</em></code><em class="ng">。有关更多信息，请参见</em> <a class="ae nf" href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation" rel="noopener ugc nofollow" target="_blank"> <em class="ng">过滤器和分数节点</em> </a> <em class="ng">。</em></p><p id="39d4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在过滤之后，如果只有一个适用的节点，则Pod将被调度给它。如果有多个适用的pod，调度程序将选择加权分数总和最高的节点。评分基于各种因素，因为<a class="ae nf" href="https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins" rel="noopener ugc nofollow" target="_blank">调度插件</a>实现了一个或多个扩展点。注意，<code class="fe mn mo mp mq b">requests</code>的值和<code class="fe mn mo mp mq b">limits</code>的值直接影响插件<code class="fe mn mo mp mq b">NodeResourcesLeastAllocated</code>的最终结果。下面是源代码:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="46df" class="mz ll iq mq b gy na nb l nc nd">func leastResourceScorer(resToWeightMap resourceToWeightMap) func(resourceToValueMap, resourceToValueMap, bool, int, int) int64 {<br/>	return func(requested, allocable resourceToValueMap, includeVolumes bool, requestedVolumes int, allocatableVolumes int) int64 {<br/>		var nodeScore, weightSum int64<br/>		for resource, weight := range resToWeightMap {<br/>			resourceScore := leastRequestedScore(requested[resource], allocable[resource])<br/>			nodeScore += resourceScore * weight<br/>			weightSum += weight<br/>		}<br/>		return nodeScore / weightSum<br/>	}<br/>}<br/>...<br/>func leastRequestedScore(requested, capacity int64) int64 {<br/>	if capacity == 0 {<br/>		return 0<br/>	}<br/>	if requested &gt; capacity {<br/>		return 0<br/>	}</span><span id="86e0" class="mz ll iq mq b gy no nb l nc nd">	return ((capacity - requested) * int64(framework.MaxNodeScore)) / capacity<br/>}</span></pre><p id="5c40" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于<code class="fe mn mo mp mq b">NodeResourcesLeastAllocated</code>，如果一个节点有更多的资源用于相同的Pod，它将获得更高的分数。换句话说，Pod将更有可能被调度到具有足够资源的节点。</p><p id="3724" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">创建Pod时，Kubernetes需要分配不同的资源，包括CPU和内存。每种资源都有一个权重(源代码中的<code class="fe mn mo mp mq b">resToWeightMap</code>结构)。总的来说，它们告诉Kubernetes调度程序实现资源平衡的最佳决策是什么。在<code class="fe mn mo mp mq b">Score</code>阶段，调度器除了<code class="fe mn mo mp mq b">NodeResourcesLeastAllocated</code>还使用其他插件进行评分，比如<code class="fe mn mo mp mq b">InterPodAffinity</code>。</p><h1 id="ce8e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">QoS和调度</h1><p id="e2a3" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">作为Kubernetes中的一种资源保护机制，QoS主要用于控制内存等不可压缩资源。它还影响不同舱和容器的OOM分数。当一个节点内存不足时，内核(OOM黑仔)会杀死低优先级的Pods(分数越高意味着优先级越低)。下面是源代码:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="7971" class="mz ll iq mq b gy na nb l nc nd">func GetContainerOOMScoreAdjust(pod *v1.Pod, container *v1.Container, memoryCapacity int64) int {<br/>	if types.IsCriticalPod(pod) {<br/>		// Critical pods should be the last to get killed.<br/>		return guaranteedOOMScoreAdj<br/>	}</span><span id="db5b" class="mz ll iq mq b gy no nb l nc nd">	switch v1qos.GetPodQOS(pod) {<br/>	case v1.PodQOSGuaranteed:<br/>		// Guaranteed containers should be the last to get killed.<br/>		return guaranteedOOMScoreAdj<br/>	case v1.PodQOSBestEffort:<br/>		return besteffortOOMScoreAdj<br/>	}</span><span id="5eb8" class="mz ll iq mq b gy no nb l nc nd">	// Burstable containers are a middle tier, between Guaranteed and Best-Effort. Ideally,<br/>	// we want to protect Burstable containers that consume less memory than requested.<br/>	// The formula below is a heuristic. A container requesting for 10% of a system's<br/>	// memory will have an OOM score adjust of 900. If a process in container Y<br/>	// uses over 10% of memory, its OOM score will be 1000. The idea is that containers<br/>	// which use more than their request will have an OOM score of 1000 and will be prime<br/>	// targets for OOM kills.<br/>	// Note that this is a heuristic, it won't work if a container has many small processes.<br/>	<br/>memoryRequest := container.Resources.Requests.Memory().Value()<br/>oomScoreAdjust := 1000 - (1000*memoryRequest)/memoryCapacity<br/>	<br/>        // A guaranteed pod using 100% of memory can have an OOM score of 10. Ensure<br/>	// that burstable pods have a higher OOM score adjustment.<br/>	<br/>       if int(oomScoreAdjust) &lt; (1000 + guaranteedOOMScoreAdj) {<br/>           return (1000 + guaranteedOOMScoreAdj)<br/>	}<br/>	// Give burstable pods a higher chance of survival over besteffort pods.<br/>	if int(oomScoreAdjust) == besteffortOOMScoreAdj {<br/>	    return int(oomScoreAdjust - 1)<br/>	}<br/>	return int(oomScoreAdjust)<br/>}</span></pre><h1 id="d3ae" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">摘要</h1><p id="34c5" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">作为一个可移植和可扩展的开源平台，Kubernetes是为管理容器化的工作负载和服务而生的。它拥有一个全面、快速增长的生态系统，这有助于确保其作为容器编排事实上的标准的地位。也就是说，用户学习Kubernetes并不总是容易的，这就是KubeSphere发挥作用的地方。KubeSphere使用户能够在其仪表板上执行几乎所有的操作，同时他们还可以选择使用内置的web kubectl工具来运行命令。本文主要关注请求和限制、它们在Kubernetes中的底层逻辑，以及如何使用KubeSphere来配置它们，以便于集群的操作和维护。</p><h1 id="35e7" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">关于KubeSphere</h1><p id="2c66" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">KubeSphere是一个基于Kubernetes的开源容器平台，其核心是应用程序。它提供全栈It自动化操作和简化的开发运维工作流。</p><p id="8366" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae nf" href="https://kubesphere.io/" rel="noopener ugc nofollow" target="_blank"> KubeSphere </a>已被全球数千家企业采用，如<strong class="ka ir"> Aqara、新浪、奔来、中国太平、华夏银行、国药控股、微众银行、Geko Cloud、VNG公司、Radore </strong>。KubeSphere为运维提供向导界面和各种企业级功能，包括Kubernetes资源管理、<a class="ae nf" href="https://kubesphere.io/devops/" rel="noopener ugc nofollow" target="_blank">、DevOps (CI/CD) </a>、应用生命周期管理、服务网格、多租户管理、<a class="ae nf" href="https://kubesphere.io/observability/" rel="noopener ugc nofollow" target="_blank">监控</a>、日志记录、警报、通知、存储和网络管理以及GPU支持。有了KubeSphere，企业能够快速建立一个强大且功能丰富的容器平台。</p><p id="1a30" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">欲了解更多信息，请访问<a class="ae nf" href="https://kubesphere.io/" rel="noopener ugc nofollow" target="_blank"> https://kubesphere.io </a></p></div></div>    
</body>
</html>