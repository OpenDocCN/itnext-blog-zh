<html>
<head>
<title>PeopleBlending: Science Art Using Cognitive Services and a Bit of Creativity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人类融合:使用认知服务和一点创造力的科学艺术</h1>
<blockquote>原文：<a href="https://itnext.io/peopleblending-science-art-using-cognitive-services-and-a-bit-of-creativity-5393300dbe00?source=collection_archive---------8-----------------------#2019-11-21">https://itnext.io/peopleblending-science-art-using-cognitive-services-and-a-bit-of-creativity-5393300dbe00?source=collection_archive---------8-----------------------#2019-11-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f339" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我非常相信跨学科的东西，不同领域的知识可以融合在一起，创造出独特的东西。因为我女儿喜欢艺术，而我喜欢技术——我经常研究这两个领域的交集，或者所谓的科学艺术。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/7d4982ad02a8c7479eb492d1560c3eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tdelncK7mR1dbHzhJebBKQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">人群融合</figcaption></figure><p id="0184" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这一次，我想和大家分享一个艺术实验的结果，我称之为<a class="ae lb" href="http://bit.do/peopleblending" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">认知人交融</strong> </a>:</p><div class="km kn ko kp gt ab cb"><figure class="lc kq ld le lf lg lh paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/657aeffc29d353761caef02cb7e405ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*XX48hvovCxqZyCYn.png"/></div></figure><figure class="lc kq ld le lf lg lh paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/d61213bb44bc2f539ecab27416dc979d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*N7Vq7juPeXCTNRav.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk li di lj lk translated">Vickie Rotator(左)，玻璃女孩(右)— 2019，<a class="ae lb" href="http://bit.do/peopleblending" rel="noopener ugc nofollow" target="_blank"> PeopleBlending </a></figcaption></figure></div><p id="56d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些照片的创作方式是根据他们的眼睛排列几幅肖像，从而创造出看起来像混合脸的东西。虽然你当然可以在PhotoShop中手动完成，但这会很耗时，而且你快速实验的能力会受到严重限制。我将向你展示如何使用<a class="ae lb" href="http://aka.ms/cognitive_serv" rel="noopener ugc nofollow" target="_blank">微软认知服务</a>和一点创意来轻松创建这样的图片。您可以使用这里的代码<a class="ae lb" href="http://github.com/CloudAdvocacy/CreepyFaces" rel="noopener ugc nofollow" target="_blank">来遵循我在下面描述的过程。如果您使用代码创建自己的图像，请参考<strong class="jp ir">认知人融合</strong>技术和链接:</a><a class="ae lb" href="http://bit.do/peopleblending" rel="noopener ugc nofollow" target="_blank">http://bit.do/peopleblending</a>。</p><h1 id="7460" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">主要思想</h1><p id="4147" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">这个过程从一些肖像开始——它们可以是同一个人的照片，也可以是我们想要融合在一起的不同人的照片。你有越多的图片，你会得到越有趣的结果。从10张图片开始，你就可以创作出一些有趣的东西。</p><p id="6ac1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了找出眼睛在每张照片上的位置，我们将使用<a class="ae lb" href="https://azure.microsoft.com/services/cognitive-services/face/?WT.mc_id=personal-blog-dmitryso" rel="noopener ugc nofollow" target="_blank">面部API </a> —它可以提取面部关键点的坐标，即所谓的<em class="mo">面部标志</em>。然后我们会用一个聪明的东西叫做<em class="mo">仿射变换</em>和OpenCV库来根据关键点旋转图像。最后，我们需要使用简单的平均将图像混合在一起。</p><h1 id="6359" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">提取面部标志</h1><p id="23aa" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">让我们从学习如何从图片中提取面部标志开始。<a class="ae lb" href="https://azure.microsoft.com/services/cognitive-services/face/?WT.mc_id=personal-blog-dmitryso" rel="noopener ugc nofollow" target="_blank">微软Face API </a>提供了一个简单的REST API，可以从人脸图像中提取很多有用的信息，包括那些地标:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/e513d31bc942415670187a513569a7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/0*c_pZ4UN6FPYJ0YA3.jpg"/></div></figure><p id="f51b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过REST直接调用Face API相当容易，但使用现有的SDK更好，它作为<a class="ae lb" href="https://docs.microsoft.com/ru-ru/azure/cognitive-services/face/quickstarts/python-sdk/?WT.mc_id=personal-blog-dmitryso" rel="noopener ugc nofollow" target="_blank"> Azure SDK库</a>的一部分可用于Python。你可以<a class="ae lb" href="https://docs.microsoft.com/ru-ru/azure/cognitive-services/face/index/?wt.mc_id=personal-blog-dmitryso" rel="noopener ugc nofollow" target="_blank">访问微软文档</a>获取更多关于Face API的详细信息，以及在Python之外的语言中使用它。</p><p id="44ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用以下命令安装SDK(以及我们将需要的OpenCV库):</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="91aa" class="mv lm iq mr b gy mw mx l my mz">pip install azure-cognitiveservices-vision-face opencv-python</span></pre><p id="e85d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要使用Face API，我们需要提供一个<strong class="jp ir">键</strong>和<strong class="jp ir">端点URL </strong>(因为在不同地区可用，URL可以不同)。有许多方法可以获得Face API密钥:</p><ul class=""><li id="fbf7" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated">如果你有Azure订阅，最好的选择是<a class="ae lb" href="https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account/?WT.mc_id=personal-blog-dmitryso" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">创建认知服务资源</strong> </a>，并从那里获取密钥/url</li><li id="51d8" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated">你可以随时<a class="ae lb" href="https://azure.microsoft.com/free/?WT.mc_id=personal-blog-dmitryso" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">创建免费试用订阅</strong> </a>(你需要一张信用卡)</li><li id="5396" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated">如果你没有Azure订阅，你可以免费试用Face API在这里 申请你的试用密钥<a class="ae lb" href="https://azure.microsoft.com/try/cognitive-services/my-apis/?api=face-api&amp;WT.mc_id=personal-blog-dmitryso" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">。</strong></a></li></ul><p id="62a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">重要提示:</strong>如果您使用试用密钥，您的Face API在您可以处理的图像数量和API调用频率方面会有一些限制(每分钟不超过20次调用)。为了让20张以上的图片也能工作，你需要在通话之间插入一些停顿。</p><p id="8fa7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">获得密钥和端点后，我们会将它们放入代码中:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="2035" class="mv lm iq mr b gy mw mx l my mz">key <strong class="mr ir">=</strong> ‘ — INSERT YOUR KEY HERE — ‘ <br/>endpoint <strong class="mr ir">=</strong> ‘https://westus2.api.cognitive.microsoft.com'</span></pre><p id="b047" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Face API的大部分调用都是通过静态模块<code class="fe no np nq mr b">cognitive_face</code>完成的，为了简便起见，我们称之为<code class="fe no np nq mr b">cf</code>:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="4b14" class="mv lm iq mr b gy mw mx l my mz"><strong class="mr ir">import </strong>azure.cognitiveservices.vision.face <strong class="mr ir">as</strong> cf <br/><strong class="mr ir">from </strong>msrest.authentication <strong class="mr ir">import </strong>CognitiveServicesCredentials <br/>cli <strong class="mr ir">=</strong> cf<strong class="mr ir">.</strong>FaceClient(endpoint,CognitiveServicesCredentials(key))</span></pre><p id="92bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">面部检测的主要功能称为<code class="fe no np nq mr b">face.detect_with_url</code>或<code class="fe no np nq mr b">face.detect_with_stream</code>。根据您指定的参数，它可以从面部提取许多有用的信息——在我们的例子中，我们需要面部标志:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="f32a" class="mv lm iq mr b gy mw mx l my mz">im_url<strong class="mr ir">='</strong>https://site.com/path/to/image.jpg' <br/>res <strong class="mr ir">=</strong> cli<strong class="mr ir">.</strong>face<strong class="mr ir">.</strong>detect_with_url(im_url,return_face_landmarks<strong class="mr ir">=</strong>True) <strong class="mr ir">print</strong>(res[0])</span></pre><p id="6d44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这段代码中，<code class="fe no np nq mr b">res</code>将是一个数组，其中的每个元素对应于图片中的一张脸。我们将总是假设我们正在处理包含一个且只有一个面部的肖像，因此使用<code class="fe no np nq mr b">res[0]</code>将给出该面部的信息:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="b2b2" class="mv lm iq mr b gy mw mx l my mz">{‘face_rectangle’: &lt;azure.cognitiveservices.vision.face.models._models_py3.FaceRectangle object at 0x7f72f23570b8&gt;, ‘additional_properties’: {}, ‘face_attributes’: None, ‘face_id’: ‘59dc97ef-b5e2–4c83–99c0–75cdb69048fa’, ‘face_landmarks’: &lt;azure.cognitiveservices.vision.face.models._models_py3.FaceLandmarks object at 0x7f72f2357080&gt;, ‘recognition_model’: None}</span></pre><p id="7dab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了得到面部标志的字典，我们将使用<code class="fe no np nq mr b">res[0].facial_landmarks.as_dict()</code>:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="bf2c" class="mv lm iq mr b gy mw mx l my mz">{'nose_left_alar_top': {'y': 147.4, 'x': 131.9}, 'eyebrow_right_inner': {'y': 106.3, 'x': 157.2}, <br/>'pupil_right': {'y': 118.9, 'x': 170.9}, <br/>'eye_right_outer': {'y': 118.5, 'x': 181.5}, <br/>...}</span></pre><h1 id="dfeb" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">处理图像</h1><p id="4589" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">此时，我们需要一组图像。我建议你从15-20张自己的照片开始。不过如果你跃跃欲试又懒得收集照片，可以下载一些从<a class="ae lb" href="https://docs.microsoft.com/azure/cognitive-services/bing-image-search/index/?WT.mc_id=personal-blog-dmitryso" rel="noopener ugc nofollow" target="_blank">必应图片搜索</a>获得的比尔盖茨的图片。我们将它们放入<code class="fe no np nq mr b">images</code>目录:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="7847" class="mv lm iq mr b gy mw mx l my mz">mkdir images wget https://github.com/shwars/NeuroWorkshopData   <br/>                         /raw/master/Data/Gates50.zip <br/>unzip -q Gates50.zip -d images rm Gates50.zip</span></pre><p id="c3c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你使用Azure Notebook并且想要你自己的照片——在你的项目中创建<code class="fe no np nq mr b">images</code>目录，然后<a class="ae lb" href="https://docs.microsoft.com/azure/notebooks/work-with-project-data-files/?WT.mc_id=personal-blog-dmitryso" rel="noopener ugc nofollow" target="_blank">手动上传你的照片到那里</a>。</p><p id="8c69" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，是玩得开心的时候了！我们将加载所有图像并调用Face API来获取面部标志:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="294d" class="mv lm iq mr b gy mw mx l my mz">import glob <br/>filenames <strong class="mr ir">=</strong> [] <br/>images <strong class="mr ir">=</strong> [] <br/>imagepoints <strong class="mr ir">=</strong> [] <br/><strong class="mr ir">for</strong> fn <strong class="mr ir">in</strong> glob<strong class="mr ir">.</strong>glob(“images/*”): <br/> <strong class="mr ir">print</strong>(“Processing {}”<strong class="mr ir">.</strong>format(fn)) <br/> <strong class="mr ir">with</strong> open(fn,’rb’) <strong class="mr ir">as</strong> f: <br/>  res<strong class="mr ir">=</strong>cli<strong class="mr ir">.</strong>face<strong class="mr ir">.</strong>detect_with_stream(f,return_face_landmarks<strong class="mr ir">=</strong>True)    <br/> <strong class="mr ir">if</strong> len(res)<strong class="mr ir">&gt;</strong>0: <br/>  filenames<strong class="mr ir">.</strong>append(fn) <br/>  images<strong class="mr ir">.</strong>append(cv2<strong class="mr ir">.</strong>cvtColor(cv2<strong class="mr ir">.</strong>imread(fn),cv2<strong class="mr ir">.</strong>COLOR_BGR2RGB))     <br/>  imagepoints<strong class="mr ir">.</strong>append(res[0]<strong class="mr ir">.</strong>face_landmarks<strong class="mr ir">.</strong>as_dict())</span></pre><p id="34fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了查看结果，让我们在图像上绘制地标:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="f522" class="mv lm iq mr b gy mw mx l my mz"><strong class="mr ir">def</strong> <strong class="mr ir">decorate</strong>(i): <br/>   img <strong class="mr ir">=</strong> images[i]<strong class="mr ir">.</strong>copy() <br/>   <strong class="mr ir">for</strong> k,v <strong class="mr ir">in</strong> imagepoints[i]<strong class="mr ir">.</strong>items(): <br/>     cv2<strong class="mr ir">.</strong>circle(img,(int(v[‘x’]),int(v[‘y’])),7,(255,255,0),5)     <br/>   <strong class="mr ir">return</strong> img </span><span id="e9a1" class="mv lm iq mr b gy nr mx l my mz">display_images([decorate(i) <strong class="mr ir">for</strong> i <strong class="mr ir">in</strong> range(1,5)])</span></pre><p id="cf93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这段代码中，函数<code class="fe no np nq mr b">display_images</code>用于绘制一系列图像，这里我将省略这段代码，您可以在资源库中找到它<a class="ae lb" href="http://github.com/CloudAdvocacy/CreepyFaces" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ns"><img src="../Images/b5510232264539084995998e66fb52c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AdRxsRqnupXt7eSe.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">在实际图像上绘制的面部标志</figcaption></figure><h1 id="3296" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">仿射变换</h1><p id="7e31" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">现在我们有了这些点，我们需要对齐图像，这样眼睛就可以移动到所有图像的完全相同的位置。要做到这一点，我们需要缩放图像，旋转它，可能还需要做一些倾斜。在数学上，图像的这种变换叫做<a class="ae lb" href="https://en.wikipedia.org/wiki/Affine_transformation" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">仿射变换</strong> </a>。众所周知，仿射变换由三个点的变换唯一定义。</p><p id="c999" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们的例子中，我们知道眼睛的位置，并且我们知道我们想要将它们移动到位置(130，120)和(170，120)，如果我们的目标图像尺寸是300x300，这听起来是一个不错的位置。然而，除了眼睛之外，我们还需要一点来完整地定义转换。</p><p id="2157" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然我们可以选择任何一个点，但选择嘴的中部是很方便的——因为它在某种程度上与眼睛相对，而三角形的眼睛——嘴的中部覆盖了脸的很大一部分。我们没有中嘴的面部标志，但是我们可以取<code class="fe no np nq mr b">mouth_left</code>和<code class="fe no np nq mr b">mouth_right</code>之间的平均值来代替。</p><p id="7a6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用矩阵在2D空间中定义仿射变换。OpenCV包含一个函数<code class="fe no np nq mr b">getAffineTransform</code>，它可以计算这样一个矩阵，给定变换前后的3个点的坐标，正如我们上面所描述的。然后，我们使用<code class="fe no np nq mr b">warpAffine</code>对原始图像进行变换——它还剪切掉图像的剩余部分，使其适合指定大小的矩形。</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="4278" class="mv lm iq mr b gy mw mx l my mz">target_triangle <strong class="mr ir">=</strong> np<strong class="mr ir">.</strong>float32(<br/>                      [[130.0,120.0],[170.0,120.0],[150.0,160.0]]) <br/>size <strong class="mr ir">=</strong> 300 <br/><strong class="mr ir">def</strong> affine_transform(img,attrs): <br/>  mc_x<strong class="mr ir">=</strong>(attrs['mouth_left']['x']<strong class="mr ir">+</strong>attrs['mouth_right']['x'])<strong class="mr ir">/</strong>2.0    <br/>  mc_y<strong class="mr ir">=</strong>(attrs['mouth_left']['y']<strong class="mr ir">+</strong>attrs['mouth_right']['y'])<strong class="mr ir">/</strong>2.0<br/>  tr <strong class="mr ir">=</strong> cv2<strong class="mr ir">.</strong>getAffineTransform(np<strong class="mr ir">.</strong>float32([<br/>   (attrs['pupil_left']['x'],attrs['pupil_left']['y']),   <br/>   (attrs['pupil_right']['x'],attrs['pupil_right']['y']), <br/>      (mc_x,mc_y)]), target_triangle) <br/>  <strong class="mr ir">return</strong> cv2<strong class="mr ir">.</strong>warpAffine(img,tr,(size,size))</span></pre><p id="4483" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们定义了这个函数，我们就可以变换我们所有的图像:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="d6e1" class="mv lm iq mr b gy mw mx l my mz">img_aligned <strong class="mr ir">=</strong> [affine_transform(i,a) <br/>                             <strong class="mr ir">for</strong> i,a <strong class="mr ir">in </strong>zip(images,imagepoints)] <br/>display_images(img_aligned[:5])</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ns"><img src="../Images/c2903e55978c895edf09b3f396f32071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yh0M5HYp9_9nyWf6.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">应用仿射变换后的图像</figcaption></figure><h1 id="2de5" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">然后…瞧！</h1><p id="cd53" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">为了得到最终的结果，我们基本上需要将图像融合在一起。为此，我们只需要对相应的<code class="fe no np nq mr b">numpy</code>数组求平均值，这可以通过一个简单的操作来完成:</p><pre class="km kn ko kp gt mq mr ms mt aw mu bi"><span id="3736" class="mv lm iq mr b gy mw mx l my mz">imgs <strong class="mr ir">= </strong>np<strong class="mr ir">.</strong>array(img_aligned,dtype<strong class="mr ir">=</strong>np<strong class="mr ir">.</strong>float32)<strong class="mr ir">/</strong>255. plt<strong class="mr ir">.</strong>imshow(np<strong class="mr ir">.</strong>average(imgs,axis<strong class="mr ir">=</strong>0))</span></pre><p id="0594" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的一个技巧是，我们需要将图像数据从整数矩阵转换为浮点(在0..1)为了获得正确的平均。一旦我们做到了——这就是结果:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/09e5bb8ddd48ac7cac3cff5e05971348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*pRgSRxEkd3FOjOoo.jpg"/></div></figure><h1 id="5a0d" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">现在是人们融合的时候了！</h1><p id="2f33" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">现在你知道了创建你自己的人所需的一切！你这里有示例代码<a class="ae lb" href="http://github.com/CloudAdvocacy/CreepyFaces" rel="noopener ugc nofollow" target="_blank">这里有</a>，你甚至不需要安装Python，因为你可以使用<a class="ae lb" href="http://bit.do/whyaznb" rel="noopener ugc nofollow" target="_blank"> Azure笔记本</a>。所以你没有借口不自己尝试！</p><p id="1937" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了给你多一个尝试的理由(除了<em class="mo">好奇心</em>和<em class="mo">创意</em>，我要宣布一个小小的<strong class="jp ir">圣诞竞赛</strong>！请在1月1日之前在评论中留下您的结果(例如，作为社交媒体帖子的链接)或<a class="ae lb" href="http://t.me/shwars" rel="noopener ugc nofollow" target="_blank">将它们发送给我</a> <strong class="jp ir">，我将在本博客中发布最佳图片，并给获胜者一本<a class="ae lb" href="http://bit.do/fsharpbook" rel="noopener ugc nofollow" target="_blank">我的F#书</a>(这现在很少见，因为它已经相当过时了)。</strong></p><p id="99c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">快乐的人融合！</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="ed80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mo">原载于2019年11月21日</em><a class="ae lb" href="https://soshnikov.com/scienceart/peopleblending/" rel="noopener ugc nofollow" target="_blank"><em class="mo">【https://soshnikov.com】</em></a><em class="mo">。</em></p></div></div>    
</body>
</html>