<html>
<head>
<title>Kubernetes: running metrics-server in AWS EKS for a Kubernetes Pod AutoScaler</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kubernetes:在EKS AWS中为Kubernetes Pod自动缩放器运行metrics-server</h1>
<blockquote>原文：<a href="https://itnext.io/kubernetes-running-metrics-server-in-aws-eks-for-a-kubernetes-pod-autoscaler-140481f54459?source=collection_archive---------6-----------------------#2020-02-15">https://itnext.io/kubernetes-running-metrics-server-in-aws-eks-for-a-kubernetes-pod-autoscaler-140481f54459?source=collection_archive---------6-----------------------#2020-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/b507bdfa1910ba0d8b988a12cd2d2352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sAiJ7eA5ub8jzlSKjKum6g.png"/></div></div></figure><p id="facd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们已经有了一个带有工作节点的AWS EKS集群。</p><p id="5766" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本文中，我们将连接到一个新创建的集群，使用HPA创建一个测试部署—<a class="ae kw" href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" rel="noopener ugc nofollow" target="_blank">Kubernetes Horizontal Pod auto scaler</a>，并尝试使用<code class="fe kx ky kz la b"><a class="ae kw" href="https://www.mankier.com/1/kubectl-top" rel="noopener ugc nofollow" target="_blank">kubectl top</a></code>获取有关资源使用情况的信息。</p><h2 id="1ceb" class="lb lc iq bd ld le lf dn lg lh li dp lj kj lk ll lm kn ln lo lp kr lq lr ls lt bi translated">库伯内特星团</h2><p id="6cc9" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">使用<code class="fe kx ky kz la b"><a class="ae kw" href="https://eksctl.io/" rel="noopener ugc nofollow" target="_blank">eksctl</a></code>创建一个测试集群:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="0981" class="lb lc iq la b gy mh mi l mj mk">$ eksctl create cluster — profile arseniy — region us-east-2 — name eks-dev-1<br/>…<br/>[ℹ] node “ip-192–168–54–141.us-east-2.compute.internal” is ready<br/>[ℹ] node “ip-192–168–85–24.us-east-2.compute.internal” is ready<br/>[ℹ] kubectl command should work with “/home/setevoy/.kube/config”, try ‘kubectl get nodes’<br/>[✔] EKS cluster “eks-dev-1” in “us-east-2” region is ready</span></pre><p id="b3c7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后切换到它。</p><h2 id="89ff" class="lb lc iq bd ld le lf dn lg lh li dp lj kj lk ll lm kn ln lo lp kr lq lr ls lt bi translated">Kubernetes集群上下文</h2><p id="2b1e" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">配置您的<code class="fe kx ky kz la b">kubectl</code>:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="80a5" class="lb lc iq la b gy mh mi l mj mk">$ aws eks — profile arseniy — region us-east-2 update-kubeconfig — name eks-dev-1<br/>Added new context arn:aws:eks:us-east-2:534***385:cluster/eks-dev-1 to /home/setevoy/.kube/config</span></pre><p id="5f15" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在<em class="ml">美国东部-2 </em>地区查看您的AWS帐户中可用的EKS集群:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="bfb1" class="lb lc iq la b gy mh mi l mj mk">$ aws eks — profile arseniy — region us-east-2 list-clusters — output text<br/>CLUSTERS eksctl-bttrm-eks-production-1<br/>CLUSTERS mobilebackend-dev-eks-0-cluster<br/>CLUSTERS eks-dev-1</span></pre><p id="ab0c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并检查当前使用的配置文件:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="5866" class="lb lc iq la b gy mh mi l mj mk">$ kubectl config current-context<br/>arn:aws:eks:us-east-2:534***385:cluster/eks-dev-1</span></pre><p id="49fe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe kx ky kz la b">aws eks</code>已经为我们配置了<code class="fe kx ky kz la b">kubectl</code>的最后一个集群。</p><p id="1bbb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果需要，您可以使用<code class="fe kx ky kz la b">get-contexts</code>随时获取所有可用的配置文件:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="f9b9" class="lb lc iq la b gy mh mi l mj mk">$ kubectl config get-contexts</span></pre><p id="65b2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并切换到一个必要的:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="26f2" class="lb lc iq la b gy mh mi l mj mk">$ kubectl config use-context arn:aws:eks:us-east-2:534***385:cluster/eks-dev-1<br/>Switched to context “arn:aws:eks:us-east-2:534***385:cluster/eks-dev-1”.</span></pre><h2 id="87f4" class="lb lc iq bd ld le lf dn lg lh li dp lj kj lk ll lm kn ln lo lp kr lq lr ls lt bi translated">部署</h2><p id="617c" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">出于测试目的，让我们创建一个带有自动缩放器的部署。</p><p id="d7fe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">HPA将使用从<code class="fe kx ky kz la b">mertics-server</code>收集的指标来获取关于节点和单元上的资源使用情况的数据，以了解何时必须扩大或缩小特定部署的单元。</p><p id="aba2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">创建HPA和部署:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="9f7a" class="lb lc iq la b gy mh mi l mj mk">---<br/>apiVersion: autoscaling/v2beta2<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/>  name: hello-hpa<br/>spec:<br/>  scaleTargetRef:<br/>    apiVersion: apps/v1<br/>    kind: Deployment<br/>    name: hello<br/>  minReplicas: 1<br/>  maxReplicas: 2<br/>  metrics:<br/>  - type: Resource<br/>    resource:<br/>      name: cpu<br/>      target:<br/>        type: Utilization<br/>        averageUtilization: 80<br/>---<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: hello<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      app: hello<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: hello<br/>    spec:<br/>      containers:<br/>        - name: hello<br/>          image: gcr.io/google-samples/node-hello:1.0<br/>          resources:<br/>            limits:<br/>              cpu: "0.1"<br/>            requests:<br/>              cpu: "0.1"</span></pre><p id="5e55" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">应用它们:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="4058" class="lb lc iq la b gy mh mi l mj mk">$ kubectl apply -f example-deployment.yml<br/>horizontalpodautoscaler.autoscaling/hello-hpa created<br/>deployment.apps/hello created</span></pre><p id="1593" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">检查:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="bedc" class="lb lc iq la b gy mh mi l mj mk">$ kubectl get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>hello-hpa Deployment/hello &lt;unknown&gt;/80% 1 2 1 26s</span></pre><h2 id="cc45" class="lb lc iq bd ld le lf dn lg lh li dp lj kj lk ll lm kn ln lo lp kr lq lr ls lt bi translated">HPA —目标<targets>无法获取指标</targets></h2><p id="3c2d" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">如果您现在检查HPA，您会发现它无法收集有关其目标(节点和单元)的数据:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="aabd" class="lb lc iq la b gy mh mi l mj mk">$ kubectl get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>hello-hpa Deployment/hello &lt;unknown&gt;/80% 1 2 1 26s</span></pre><p id="7c38" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在<em class="ml">条件</em>和<em class="ml">事件</em>中，您可以找到关于该问题的更多详细信息:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="c07e" class="lb lc iq la b gy mh mi l mj mk">$ kubectl describe hpa hello-hpa<br/>…<br/>Conditions:<br/>Type Status Reason Message<br/> — — — — — — — — — — — -</span><span id="6b80" class="lb lc iq la b gy mm mi l mj mk">AbleToScale True SucceededGetScale the HPA controller was able to get the target’s current scale</span><span id="b826" class="lb lc iq la b gy mm mi l mj mk">ScalingActive False FailedGetResourceMetric the HPA was unable to compute the replica count: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)<br/>…<br/>Events:</span><span id="e875" class="lb lc iq la b gy mm mi l mj mk">Type Reason Age From Message<br/> — — — — — — — — — — — — -<br/>Warning FailedGetResourceMetric 12s (x3 over 43s) horizontal-pod-autoscaler unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)</span><span id="624a" class="lb lc iq la b gy mm mi l mj mk">Warning FailedComputeMetricsReplicas 12s (x3 over 43s) horizontal-pod-autoscaler failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)</span></pre><p id="af85" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">事实上，我们的目标是:</p><blockquote class="mn mo mp"><p id="427a" class="jy jz ml ka b kb kc kd ke kf kg kh ki mq kk kl km mr ko kp kq ms ks kt ku kv ij bi translated">无法获取cpu利用率:无法获取资源cpu的度量</p></blockquote><h2 id="e9a6" class="lb lc iq bd ld le lf dn lg lh li dp lj kj lk ll lm kn ln lo lp kr lq lr ls lt bi translated">服务器出错(未找到):服务器找不到请求的资源(获取服务http:heapster:)</h2><p id="1fc6" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">另外，如果现在尝试对节点和pod使用<code class="fe kx ky kz la b">top</code>——Kubernetes将抛出另一个错误:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="8662" class="lb lc iq la b gy mh mi l mj mk">$ kubectl top node<br/>Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)</span><span id="c1b6" class="lb lc iq la b gy mm mi l mj mk">$ kubectl top pod<br/>Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)</span></pre><p id="c423" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了使<code class="fe kx ky kz la b">top</code>能够显示它尝试连接到<a class="ae kw" href="https://github.com/kubernetes-retired/heapster" rel="noopener ugc nofollow" target="_blank">堆</a>服务时的资源使用情况，请参见<a class="ae kw" href="https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/top/top_pod.go#L143" rel="noopener ugc nofollow" target="_blank">源代码</a>:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="dac2" class="lb lc iq la b gy mh mi l mj mk">...<br/>o.Client = metricsutil.NewHeapsterMetricsClient(clientset.CoreV1(), o.HeapsterOptions.Namespace, o.HeapsterOptions.Scheme, o.HeapsterOptions.Service, o.HeapsterOptions.Port)<br/>...</span></pre><p id="8bd3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是Heaspter是不赞成使用的服务，以前用于收集指标。</p><p id="1038" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如今，对于CPU和内存指标，使用的是<code class="fe kx ky kz la b"><a class="ae kw" href="https://github.com/kubernetes-incubator/metrics-server" rel="noopener ugc nofollow" target="_blank">metrics-server</a></code>服务。</p><h2 id="f463" class="lb lc iq bd ld le lf dn lg lh li dp lj kj lk ll lm kn ln lo lp kr lq lr ls lt bi translated">运行指标-服务器</h2><p id="5932" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">克隆存储库:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="f61b" class="lb lc iq la b gy mh mi l mj mk">$ git clone <a class="ae kw" href="https://github.com/kubernetes-sigs/metrics-server.git" rel="noopener ugc nofollow" target="_blank">https://github.com/kubernetes-sigs/metrics-server.git</a><br/>$ cd metrics-server/</span></pre><h2 id="2995" class="lb lc iq bd ld le lf dn lg lh li dp lj kj lk ll lm kn ln lo lp kr lq lr ls lt bi translated">度量-AWS EKS的服务器配置</h2><p id="7746" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">要使metrics-server能够找到AWS Elastic Kubernetes服务集群中的所有资源，请编辑其部署文件<code class="fe kx ky kz la b">deploy/kubernetes/metrics-server-deployment.yaml</code>，并添加带有四个参数的<code class="fe kx ky kz la b">command</code>:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="e18c" class="lb lc iq la b gy mh mi l mj mk">...<br/>        command:<br/>          - /metrics-server<br/>          - --logtostderr<br/>          - --kubelet-insecure-tls=true<br/>          - --kubelet-preferred-address-types=InternalIP<br/>          - --v=2<br/>...</span></pre><ul class=""><li id="20c3" class="mt mu iq ka b kb kc kf kg kj mv kn mw kr mx kv my mz na nb bi translated"><code class="fe kx ky kz la b">kubelet-insecure-tls</code> -不检查节点上的kubelet-clients CA证书</li><li id="8815" class="mt mu iq ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated"><code class="fe kx ky kz la b">kubelet-preferred-address-types</code> -如何在Kubernetes空间中找到资源-通过使用<em class="ml">主机名</em>、<em class="ml">内部域名</em>、<em class="ml">内部域名</em>、<em class="ml">外部域名</em>或<em class="ml">外部域名</em>，对于EKS将其设置为<em class="ml">内部域名</em>值</li><li id="7eec" class="mt mu iq ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated"><code class="fe kx ky kz la b">v=2</code> -日志详细程度</li></ul><p id="e8c4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">保存并部署它:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="f5c1" class="lb lc iq la b gy mh mi l mj mk">$ kubectl apply -f deploy/kubernetes/<br/>clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created<br/>clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created<br/>rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created<br/>apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created<br/>serviceaccount/metrics-server created<br/>deployment.apps/metrics-server created<br/>service/metrics-server created<br/>clusterrole.rbac.authorization.k8s.io/system:metrics-server created<br/>clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created</span></pre><p id="9201" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">检查:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="b2c5" class="lb lc iq la b gy mh mi l mj mk">$ kubectl -n kube-system get pod<br/>NAME READY STATUS RESTARTS AGE<br/>aws-node-mt9pq 1/1 Running 0 2m5s<br/>aws-node-rl7t2 1/1 Running 0 2m2s<br/>coredns-74dd858ddc-xmrhj 1/1 Running 0 7m33s<br/>coredns-74dd858ddc-xpcwx 1/1 Running 0 7m33s<br/>kube-proxy-b85rv 1/1 Running 0 2m5s<br/>kube-proxy-n647l 1/1 Running 0 2m2s<br/>metrics-server-546565fdc9–56xwl 1/1 Running 0 6s</span></pre><p id="a8b9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">或者通过使用:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="8614" class="lb lc iq la b gy mh mi l mj mk">$ kubectl get apiservices | grep metr<br/>v1beta1.metrics.k8s.io kube-system/metrics-server True 91s</span></pre><p id="9371" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">服务日志:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="97e7" class="lb lc iq la b gy mh mi l mj mk">kubectl -n kube-system logs -f metrics-server-546565fdc9-qswck<br/>I0215 11:59:05.896014 1 serving.go:312] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)<br/>I0215 11:59:06.439725 1 manager.go:95] Scraping metrics from 0 sources<br/>I0215 11:59:06.439749 1 manager.go:148] ScrapeMetrics: time: 2.728µs, nodes: 0, pods: 0<br/>I0215 11:59:06.450735 1 secure_serving.go:116] Serving securely on [::]:4443<br/>E0215 11:59:10.096632 1 reststorage.go:160] unable to fetch pod metrics for pod default/hello-7d6c85c755-r88xn: no metrics known for pod<br/>E0215 11:59:25.109059 1 reststorage.go:160] unable to fetch pod metrics for pod default/hello-7d6c85c755-r88xn: no metrics known for pod</span></pre><p id="09cc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">尝试<code class="fe kx ky kz la b">top</code>获取节点:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="9df8" class="lb lc iq la b gy mh mi l mj mk">$ kubectl top node<br/>error: metrics not available yet</span></pre><p id="3d72" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于吊舱:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="a86a" class="lb lc iq la b gy mh mi l mj mk">$ kubectl top pod<br/>W0215 13:59:58.319317 4014051 top_pod.go:259] Metrics not available for pod default/hello-7d6c85c755-r88xn, age: 4m51.319306547s<br/>error: Metrics not available for pod default/hello-7d6c85c755-r88xn, age: 4m51.319306547s</span></pre><p id="3158" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">1-2分钟后，再次检查<code class="fe kx ky kz la b">metrics-server</code>日志:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="468d" class="lb lc iq la b gy mh mi l mj mk">I0215 12:00:06.439839 1 manager.go:95] Scraping metrics from 2 sources<br/>I0215 12:00:06.447003 1 manager.go:120] Querying source: kubelet_summary:ip-192–168–54–141.us-east-2.compute.internal<br/>I0215 12:00:06.450994 1 manager.go:120] Querying source: kubelet_summary:ip-192–168–85–24.us-east-2.compute.internal<br/>I0215 12:00:06.480781 1 manager.go:148] ScrapeMetrics: time: 40.886465ms, nodes: 2, pods: 8<br/>I0215 12:01:06.439817 1 manager.go:95] Scraping metrics from 2 sources</span></pre><p id="9612" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并再次尝试<code class="fe kx ky kz la b">top</code>:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="efcc" class="lb lc iq la b gy mh mi l mj mk">$ kubectl top node<br/>NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%<br/>ip-192–168–54–141.us-east-2.compute.internal 25m 1% 406Mi 5%<br/>ip-192–168–85–24.us-east-2.compute.internal 26m 1% 358Mi 4%</span></pre><p id="6932" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">豆荚:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="b894" class="lb lc iq la b gy mh mi l mj mk">$ kubectl top pod<br/>NAME CPU(cores) MEMORY(bytes)<br/>hello-7d6c85c755-r88xn 0m 8Mi</span></pre><p id="7bda" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以及HPA服务:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="2d4b" class="lb lc iq la b gy mh mi l mj mk">$ kubectl describe hpa hello-hpa<br/>…<br/>Conditions:<br/>Type Status Reason Message<br/> — — — — — — — — — — — -</span><span id="eaa3" class="lb lc iq la b gy mm mi l mj mk">AbleToScale True ReadyForNewScale recommended size matches current size</span><span id="f03b" class="lb lc iq la b gy mm mi l mj mk">ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)<br/>ScalingLimited True TooFewReplicas the desired replica count is more than the maximum replica count</span><span id="8ca8" class="lb lc iq la b gy mm mi l mj mk">Events:<br/>Type Reason Age From Message<br/> — — — — — — — — — — — — -<br/>Warning FailedComputeMetricsReplicas 4m23s (x12 over 7m10s) horizontal-pod-autoscaler failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)</span><span id="fb8b" class="lb lc iq la b gy mm mi l mj mk">Warning FailedGetResourceMetric 4m8s (x13 over 7m10s) horizontal-pod-autoscaler unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)</span></pre><p id="b3f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请注意以下信息:</p><ul class=""><li id="b128" class="mt mu iq ka b kb kc kf kg kj mv kn mw kr mx kv my mz na nb bi translated"><em class="ml">HPA能够根据cpu资源利用率成功计算副本数量</em> — HPA现在能够收集指标</li><li id="8116" class="mt mu iq ka b kb nc kf nd kj ne kn nf kr ng kv my mz na nb bi translated"><em class="ml">无法获取资源cpu的指标</em> —检查<em class="ml">年龄</em>，—其计数器必须停止增长(保持[13]的数字)</li></ul><p id="26ca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">实际上，运行一个<code class="fe kx ky kz la b">metrics-server</code>来使用HPA就需要这些了。</p><p id="2aac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下图—使用HPA和<code class="fe kx ky kz la b">metrics-server</code>时的一个常见错误。</p><h2 id="82cf" class="lb lc iq bd ld le lf dn lg lh li dp lj kj lk ll lm kn ln lo lp kr lq lr ls lt bi translated">HPA无法计算副本数:缺少对cpu的请求</h2><p id="f116" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">有时，HPA会报告以下错误:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="135c" class="lb lc iq la b gy mh mi l mj mk">$ kubectl describe hpa hello-hpa<br/>…<br/>Conditions:<br/>Type Status Reason Message<br/> — — — — — — — — — — — -<br/>AbleToScale True SucceededGetScale the HPA controller was able to get the target’s current scale</span><span id="bc2f" class="lb lc iq la b gy mm mi l mj mk">ScalingActive False FailedGetResourceMetric the HPA was unable to compute the replica count: missing request for cpu<br/>…</span><span id="611e" class="lb lc iq la b gy mm mi l mj mk">Warning FailedGetResourceMetric 2s (x2 over 17s) horizontal-pod-autoscaler missing request for cpu</span></pre><p id="1ed3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果部署中的pod模板没有定义<code class="fe kx ky kz la b">resources</code>和<code class="fe kx ky kz la b"><a class="ae kw" href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-requests-and-limits-of-pod-and-container" rel="noopener ugc nofollow" target="_blank">requests</a></code>，可能会发生这种情况。</p><p id="0805" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在当前情况下，我注释掉了这些行:</p><pre class="lz ma mb mc gt md la me mf aw mg bi"><span id="d316" class="lb lc iq la b gy mh mi l mj mk">...<br/>---<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: hello<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      app: hello<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: hello<br/>    spec:<br/>      containers:<br/>        - name: hello<br/>          image: gcr.io/google-samples/node-hello:1.0<br/>#          resources:<br/>#            limits:<br/>#              cpu: "0.1"<br/>#            requests:<br/>#              cpu: "0.1"</span></pre><p id="ce08" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并且在没有<code class="fe kx ky kz la b"><a class="ae kw" href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-requests-and-limits-of-pod-and-container" rel="noopener ugc nofollow" target="_blank">requests</a></code>的情况下部署了pod，这导致了“<strong class="ka ir"> <em class="ml">缺少对cpu </em> </strong>的请求”错误消息。</p><p id="7c58" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将<code class="fe kx ky kz la b"><a class="ae kw" href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-requests-and-limits-of-pod-and-container" rel="noopener ugc nofollow" target="_blank">requests</a></code>拨回——它现在肯定可以工作了。</p><p id="fed7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">完成了。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="5560" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ml">最初发布于</em> <a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-running-metrics-server-in-aws-eks-for-a-kubernetes-pod-autoscaler/" rel="noopener ugc nofollow" target="_blank"> <em class="ml"> RTFM: Linux、DevOps和系统管理</em> </a> <em class="ml">。</em></p></div></div>    
</body>
</html>