<html>
<head>
<title>Provision Volumes on Kubernetes and Nomad using Ceph CSI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Ceph CSI提供关于Kubernetes和Nomad的资料</h1>
<blockquote>原文：<a href="https://itnext.io/provision-volumes-from-external-ceph-storage-on-kubernetes-and-nomad-using-ceph-csi-7ad9b15e9809?source=collection_archive---------1-----------------------#2021-05-07">https://itnext.io/provision-volumes-from-external-ceph-storage-on-kubernetes-and-nomad-using-ceph-csi-7ad9b15e9809?source=collection_archive---------1-----------------------#2021-05-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e1c7ca575325226b3879e0bc057905c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zPIm7M71j7wido3DV4--JQ.png"/></div></div></figure><p id="865d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您想在Kubernetes上运行有状态的应用程序，CSI(容器存储接口)在动态供应卷方面有着重要的作用。一般来说，CSI不仅用于为Kubernetes提供存储卷，还用于所有其他容器编排器，如Mesos、Nomad。<br/>在这里，我将讨论使用外部Ceph存储中的Ceph CSI为Hashicorp的Kubernetes和Nomad进行卷配置。</p><p id="ae00" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我在这篇文章中使用的重要组件如下。</p><ul class=""><li id="2edb" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">Kubernetes v1.17版</li><li id="302e" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated"><a class="ae lk" href="https://github.com/hashicorp/nomad/tree/v1.0.4" rel="noopener ugc nofollow" target="_blank"> Nomad v1.0.4 </a></li><li id="85ba" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated"><a class="ae lk" href="https://docs.ceph.com/en/latest/releases/nautilus/" rel="noopener ugc nofollow" target="_blank"> Ceph存储v14(鹦鹉螺)</a></li><li id="a0a5" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated"><a class="ae lk" href="https://github.com/ceph/ceph-csi/tree/v3.3.1" rel="noopener ugc nofollow" target="_blank"> Ceph CSI v3.3.1 </a></li></ul></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="ac09" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">Kubernetes上的供应卷</h1><p id="9e2a" class="pw-post-body-paragraph jy jz iq ka b kb mq kd ke kf mr kh ki kj ms kl km kn mt kp kq kr mu kt ku kv ij bi translated">在本例中，您将看到如何使用Ceph CSI从Kubernetes上的外部Ceph存储供应卷。</p><p id="40a8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，让我们安装helm在Kubernetes上部署Ceph CSI图表。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="732b" class="ne lt iq na b gy nf ng l nh ni">curl -fsSL -o get_helm.sh <a class="ae lk" href="https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3</a><br/>chmod 700 get_helm.sh<br/>./get_helm.sh</span></pre><p id="88bf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为Ceph CSI创建一个命名空间。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="10a5" class="ne lt iq na b gy nf ng l nh ni">kubectl create namespace ceph-csi-rbd; </span></pre><p id="6d3f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">克隆Ceph CSI并切换到v3.3.1。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="78b9" class="ne lt iq na b gy nf ng l nh ni">git clone <a class="ae lk" href="https://github.com/ceph/ceph-csi.git" rel="noopener ugc nofollow" target="_blank">https://github.com/ceph/ceph-csi.git</a>;<br/>cd ceph-csi;<br/>git checkout v3.3.1;</span><span id="1625" class="ne lt iq na b gy nj ng l nh ni"># move to rbd chart.<br/>cd charts/ceph-csi-rbd;</span></pre><p id="0bf4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">或者，您可以将Kubernetes v1.17.x的csi版本更改为<code class="fe nk nl nm na b">v1beta1</code></p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="0d72" class="ne lt iq na b gy nf ng l nh ni">sed -i 's/storage.k8s.io\/betav1/storage.k8s.io\/v1beta1/g' templates/csidriver-crd.yaml;</span></pre><p id="c006" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，我们需要图表值文件来部署ceph csi。看起来是这样的。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="d280" class="ne lt iq na b gy nf ng l nh ni">cat &lt;&lt;EOF &gt; ceph-csi-rbd-values.yaml<br/>csiConfig:<br/>  - clusterID: "c628ebf1-d03f-4806-9941-8b5840338b14"<br/>    monitors:<br/>      - "10.0.0.3:6789"<br/>      - "10.0.0.4:6789"<br/>      - "10.0.0.5:6789"<br/>provisioner:<br/>  name: provisioner<br/>  replicaCount: 2<br/>EOF</span></pre><p id="fd6e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe nk nl nm na b">clusterID</code>可以通过命令获得。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="7de9" class="ne lt iq na b gy nf ng l nh ni">sudo ceph fsid;</span></pre><p id="be6e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">ceph监视器的<code class="fe nk nl nm na b">monitors</code>可以用这个获得。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="b835" class="ne lt iq na b gy nf ng l nh ni">sudo ceph mon dump;</span></pre><p id="4f19" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们在Kuberntes上安装Ceph CSI图表。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="04bb" class="ne lt iq na b gy nf ng l nh ni">helm install \<br/>--namespace ceph-csi-rbd \<br/>ceph-csi-rbd \<br/>--values ceph-csi-rbd-values.yaml \<br/>./;<br/>kubectl rollout status deployment ceph-csi-rbd-provisioner -n ceph-csi-rbd;<br/>helm status ceph-csi-rbd -n ceph-csi-rbd;</span></pre><p id="b673" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，您已经准备好ceph csi驱动程序来配置卷。</p><p id="b53a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在创建Ceph存储类之前，需要在Ceph存储中创建几个资源。</p><p id="dd2e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要从外部Ceph存储在Kubernetes上挂载卷，首先需要创建一个池。在ceph中创建一个池。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="a344" class="ne lt iq na b gy nf ng l nh ni">sudo ceph osd pool create kubePool 64 64</span></pre><p id="5ce7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并将池初始化为块设备。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="2940" class="ne lt iq na b gy nf ng l nh ni">sudo rbd pool init kubePool</span></pre><p id="35e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要使用策略访问池，您需要一个用户。在此示例中，将创建池的管理员用户。让我们为池<code class="fe nk nl nm na b">kubePool</code>创建一个管理员用户，并将生成的密钥编码为base64。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="7479" class="ne lt iq na b gy nf ng l nh ni">sudo ceph auth get-or-create-key client.kubeAdmin mds 'allow *' mgr 'allow *' mon 'allow *' osd 'allow * pool=kubePool' | tr -d '\n' | base64;<br/>QVFEMWxvTmdzMTZyRVJBQTZJalBHcDBWUi8wcUd6TW9sSmlaTXc9PQ==</span></pre><p id="d1ef" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将管理员用户<code class="fe nk nl nm na b">kubeAdmin</code>编码为base64。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="8ce2" class="ne lt iq na b gy nf ng l nh ni">echo "kubeAdmin" | tr -d '\n' | base64;<br/>a3ViZUFkbWlu</span></pre><p id="99b4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，已经为池<code class="fe nk nl nm na b">kubePool</code>创建了管理员用户和密钥。</p><p id="57a8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们为admin用户创建一个秘密资源。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="0f4c" class="ne lt iq na b gy nf ng l nh ni">cat &gt; ceph-admin-secret.yaml &lt;&lt; EOF<br/>apiVersion: v1<br/>kind: Secret<br/>metadata:<br/>  name: ceph-admin<br/>  namespace: default<br/>type: kubernetes.io/rbd<br/>data:<br/>  userID: a3ViZUFkbWlu<br/>  userKey: QVFEMWxvTmdzMTZyRVJBQTZJalBHcDBWUi8wcUd6TW9sSmlaTXc9PQ==<br/>EOF</span><span id="524f" class="ne lt iq na b gy nj ng l nh ni"># create a secret for admin user.<br/>kubectl apply -f ceph-admin-secret.yaml;</span></pre><p id="d987" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们准备创建Ceph存储类。让我们创造它。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="1cd8" class="ne lt iq na b gy nf ng l nh ni"># ceph storage class.<br/>cat &gt; ceph-rbd-sc.yaml &lt;&lt;EOF<br/>apiVersion: storage.k8s.io/v1<br/>kind: StorageClass<br/>metadata:<br/>  name: ceph-rbd-sc<br/>  annotations:<br/>    storageclass.kubernetes.io/is-default-class: "true"<br/>provisioner: rbd.csi.ceph.com<br/>parameters:<br/>   clusterID: c628ebf1-d03f-4806-9941-8b5840338b14<br/>   pool: kubePool<br/>   imageFeatures: layering<br/>   csi.storage.k8s.io/provisioner-secret-name: ceph-admin<br/>   csi.storage.k8s.io/provisioner-secret-namespace: default<br/>   csi.storage.k8s.io/controller-expand-secret-name: ceph-admin<br/>   csi.storage.k8s.io/controller-expand-secret-namespace: default<br/>   csi.storage.k8s.io/node-stage-secret-name: ceph-admin<br/>   csi.storage.k8s.io/node-stage-secret-namespace: default<br/>reclaimPolicy: Delete<br/>allowVolumeExpansion: true<br/>mountOptions:<br/>   - discard<br/>EOF</span><span id="f924" class="ne lt iq na b gy nj ng l nh ni">kubectl apply -f ceph-rbd-sc.yaml;</span></pre><p id="41b6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe nk nl nm na b">clusterID</code>是<code class="fe nk nl nm na b">sudo ceph fsid</code>的值，<code class="fe nk nl nm na b">pool</code>是上面已经创建的<code class="fe nk nl nm na b">kubePool</code>的值。</p><p id="8324" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，您已经准备好使用Ceph存储类轻松地为有状态应用程序提供卷了。</p><p id="240e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们用pvc创建一个pod，以使用ceph存储类从外部ceph存储挂载卷。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="453c" class="ne lt iq na b gy nf ng l nh ni">## create pvc and pod.<br/>cat &lt;&lt;EOF &gt; pv-pod.yaml   <br/>---<br/>kind: PersistentVolumeClaim<br/>apiVersion: v1<br/>metadata:<br/>  name: ceph-rbd-sc-pvc<br/>spec:<br/>  accessModes:<br/>    - ReadWriteOnce<br/>  resources:<br/>    requests:<br/>      storage: 2Gi<br/>  storageClassName: ceph-rbd-sc<br/>---    <br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: ceph-rbd-pod-pvc-sc<br/>spec:<br/>  containers:<br/>  - name:  ceph-rbd-pod-pvc-sc<br/>    image: busybox<br/>    command: ["sleep", "infinity"]<br/>    volumeMounts:<br/>    - mountPath: /mnt/ceph_rbd<br/>      name: volume<br/>  volumes:<br/>  - name: volume<br/>    persistentVolumeClaim:<br/>      claimName: ceph-rbd-sc-pvc<br/>EOF<br/> <br/>kubectl apply -f pv-pod.yaml;</span></pre><p id="602e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">看一下<code class="fe nk nl nm na b">storageClassName</code>也就是<code class="fe nk nl nm na b">ceph-rbd-sc</code>的值。将使用ceph存储类动态配置该卷，此示例pod将使用路径<code class="fe nk nl nm na b">/mnt/ceph_rbd</code>挂载配置的卷。</p><p id="77c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们检查一下容器中的容量。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="a6f6" class="ne lt iq na b gy nf ng l nh ni">kubectl exec pod/ceph-rbd-pod-pvc-sc -- df -k | grep rbd;<br/>/dev/rbd0              1998672      6144   1976144   0% /mnt/ceph_rbd</span></pre><p id="e764" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如图所示，ceph rbd卷已经安装到容器中的路径<code class="fe nk nl nm na b">/mnt/ceph_rbd</code>上。</p><p id="64bd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并且，检查是否在ceph池中创建了映像。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="ae58" class="ne lt iq na b gy nf ng l nh ni">sudo rbd ls -p kubePool;<br/>csi-vol-c545c641-a4b3-11eb-b242-26d41aad22d3</span></pre><p id="fd07" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，您已经看到了如何在Kubernetes上使用CSI从外部Ceph存储供应卷。</p><h1 id="a731" class="ls lt iq bd lu lv nn lx ly lz no mb mc md np mf mg mh nq mj mk ml nr mn mo mp bi translated">提供关于游牧的书籍</h1><p id="29e7" class="pw-post-body-paragraph jy jz iq ka b kb mq kd ke kf mr kh ki kj ms kl km kn mt kp kq kr mu kt ku kv ij bi translated">正如您在上面看到的，在kubernetes上动态供应卷很简单。游牧部落怎么样？</p><p id="53a9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，您需要将以下内容添加到nomad客户端配置中。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="9087" class="ne lt iq na b gy nf ng l nh ni">plugin "docker" {<br/>  config {<br/>    allow_privileged = true<br/>  }<br/>}</span></pre><p id="ca32" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这样，docker容器可以在Nomad客户机节点上以特权身份运行。</p><p id="8e4d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如前所述，CSI不仅可以在Kubernetes上运行，还可以在所有其他容器编排器上运行。CSI由<code class="fe nk nl nm na b">controller</code>和<code class="fe nk nl nm na b">node</code>组成。让我们首先创建一个Ceph CSI控制器作业。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="a849" class="ne lt iq na b gy nf ng l nh ni">cat &lt;&lt;EOC &gt; ceph-csi-plugin-controller.nomad<br/>job "ceph-csi-plugin-controller" {<br/>  datacenters = ["dc1"]</span><span id="e622" class="ne lt iq na b gy nj ng l nh ni">group "controller" {<br/>    network {    <br/>      port "metrics" {}<br/>    }<br/>    task "ceph-controller" {</span><span id="6a2e" class="ne lt iq na b gy nj ng l nh ni">template {<br/>        data        = &lt;&lt;EOF<br/>[{<br/>    "clusterID": "62c42aed-9839-4da6-8c09-9d220f56e924",<br/>    "monitors": [<br/>        "10.0.0.3:6789",<br/>  "10.0.0.4:6789",<br/>  "10.0.0.5:6789" <br/>    ]<br/>}]<br/>EOF<br/>        destination = "local/config.json"<br/>        change_mode = "restart"<br/>      }<br/>      driver = "docker"<br/>      config {<br/>        image = "quay.io/cephcsi/cephcsi:v3.3.1"<br/>        volumes = [<br/>          "./local/config.json:/etc/ceph-csi-config/config.json"<br/>        ]    <br/>        args = [<br/>          "--type=rbd",<br/>          "--controllerserver=true",<br/>          "--drivername=rbd.csi.ceph.com",         <br/>          "--endpoint=unix://csi/csi.sock",<br/>          "--nodeid=\${node.unique.name}",<br/>    "--instanceid=\${node.unique.name}-controller", <br/>          "--pidlimit=-1",<br/>    "--logtostderr=true",<br/>          "--v=5",    <br/>          "--metricsport=\$\${NOMAD_PORT_metrics}"<br/>        ]  <br/>      }   <br/>   resources {<br/>        cpu    = 500<br/>        memory = 256<br/>      }<br/>      service {<br/>        name = "ceph-csi-controller"<br/>        port = "metrics"<br/>        tags = [ "prometheus" ]<br/>      }</span><span id="af92" class="ne lt iq na b gy nj ng l nh ni">csi_plugin {<br/>        id        = "ceph-csi"<br/>        type      = "controller"<br/>        mount_dir = "/csi"<br/>      }<br/>    }<br/>  }<br/>}<br/>EOC</span></pre><p id="8b2b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe nk nl nm na b">clusterID</code>和<code class="fe nk nl nm na b">monitors</code>应该改成你的值。该Ceph CSI控制器将被创建为<code class="fe nk nl nm na b">service</code>类型。</p><p id="3477" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并创建一个Ceph CSI节点作业。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="844a" class="ne lt iq na b gy nf ng l nh ni">cat &lt;&lt;EOC &gt; ceph-csi-plugin-nodes.nomad<br/>job "ceph-csi-plugin-nodes" {<br/>  datacenters = ["dc1"]<br/>  type        = "system"<br/>  group "nodes" {<br/>    network {    <br/>      port "metrics" {}<br/>    }<br/> <br/>    task "ceph-node" {<br/>      driver = "docker"<br/>      template {<br/>        data        = &lt;&lt;EOF<br/>[{<br/>    "clusterID": "62c42aed-9839-4da6-8c09-9d220f56e924",<br/>    "monitors": [<br/>        "10.0.0.3:6789",<br/>  "10.0.0.4:6789",<br/>  "10.0.0.5:6789" <br/>    ]<br/>}]<br/>EOF<br/>        destination = "local/config.json"<br/>        change_mode = "restart"<br/>      }<br/>      config {<br/>        image = "quay.io/cephcsi/cephcsi:v3.3.1"<br/>        volumes = [<br/>          "./local/config.json:/etc/ceph-csi-config/config.json"<br/>        ]<br/>        mounts = [<br/>          {<br/>            type     = "tmpfs"<br/>            target   = "/tmp/csi/keys"<br/>            readonly = false<br/>            tmpfs_options = {<br/>              size = 1000000 # size in bytes<br/>            }<br/>          }<br/>        ]<br/>        args = [<br/>          "--type=rbd",<br/>          "--drivername=rbd.csi.ceph.com",        <br/>          "--nodeserver=true",<br/>          "--endpoint=unix://csi/csi.sock",<br/>          "--nodeid=\${node.unique.name}",<br/>          "--instanceid=\${node.unique.name}-nodes", <br/>          "--pidlimit=-1",<br/>    "--logtostderr=true",<br/>          "--v=5",       <br/>          "--metricsport=\$\${NOMAD_PORT_metrics}"<br/>        ]  <br/>        privileged = true<br/>      }   <br/>   resources {<br/>        cpu    = 500<br/>        memory = 256<br/>      }<br/>      service {<br/>        name = "ceph-csi-nodes"<br/>        port = "metrics"<br/>        tags = [ "prometheus" ]<br/>      }</span><span id="bf53" class="ne lt iq na b gy nj ng l nh ni">csi_plugin {<br/>        id        = "ceph-csi"<br/>        type      = "node"<br/>        mount_dir = "/csi"<br/>      }<br/>    }<br/>  }<br/>}<br/>EOC</span></pre><p id="6a01" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个Ceph节点作业是<code class="fe nk nl nm na b">system</code>的类型，也就是说，ceph csi节点容器将在所有的nomad客户机节点上创建。</p><p id="80cb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们在Nomad上运行Ceph CSI控制器和节点作业。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="50ac" class="ne lt iq na b gy nf ng l nh ni">nomad job run ceph-csi-plugin-controller.nomad;<br/>nomad job run ceph-csi-plugin-nodes.nomad;</span></pre><p id="671b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">查看ceph csi插件的状态。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="cf00" class="ne lt iq na b gy nf ng l nh ni">nomad plugin status ceph-csi;<br/>ID                   = ceph-csi<br/>Provider             = rbd.csi.ceph.com<br/>Version              = v3.3.1<br/>Controllers Healthy  = 1<br/>Controllers Expected = 1<br/>Nodes Healthy        = 2<br/>Nodes Expected       = 2</span><span id="beb6" class="ne lt iq na b gy nj ng l nh ni">Allocations<br/>ID        Node ID   Task Group  Version  Desired  Status   Created    Modified<br/>b6268d6d  457a8291  controller  0        run      running  1d21h ago  1d21h ago<br/>ec265d25  709ee9cc  nodes       0        run      running  1d21h ago  1d21h ago<br/>4cd7dffa  457a8291  nodes       0        run      running  1d21h ago  1d21h ago</span></pre><p id="b9be" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，它可以使用ceph csi驱动程序从外部ceph存储挂载卷了。</p><p id="5771" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在继续之前，在所有的nomad客户机节点上加载RBD模块。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="150c" class="ne lt iq na b gy nf ng l nh ni">sudo modprobe rbd;<br/>sudo lsmod |grep rbd;<br/>rbd                    83733  0<br/>libceph               306750  1 rbd</span></pre><p id="7b27" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们创建一个ceph池<code class="fe nk nl nm na b">myPool</code>和管理员用户<code class="fe nk nl nm na b">myPoolAdmin</code>。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="db21" class="ne lt iq na b gy nf ng l nh ni"># Create a ceph pool:<br/>sudo ceph osd pool create myPool 64 64<br/>sudo rbd pool init myPool;</span><span id="39fc" class="ne lt iq na b gy nj ng l nh ni"># create admin user for pool.<br/>sudo ceph auth get-or-create-key client.myPoolAdmin mds 'allow *' mgr 'allow *' mon 'allow *' osd 'allow * pool=myPool'<br/>AQCKf4JgHPVxAxAALZ8ny4/R7s6/3rZWC2o5vQ==</span></pre><p id="7fa4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们需要在Nomad上注册一个卷，创建一个卷。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="40c9" class="ne lt iq na b gy nf ng l nh ni">cat &lt;&lt;EOF &gt; ceph-volume.hcl<br/>type = "csi"<br/>id   = "ceph-mysql"<br/>name = "ceph-mysql"<br/>external_id     = "0001-0024-62c42aed-9839-4da6-8c09-9d220f56e924-0000000000000009-00000000-1111-2222-bbbb-cacacacacaca"                          <br/>access_mode     = "single-node-writer"<br/>attachment_mode = "file-system"<br/>mount_options {<br/>  fs_type = "ext4"<br/>}<br/>plugin_id       = "ceph-csi"<br/>secrets {<br/>  userID  = "myPoolAdmin"<br/>  userKey = "AQCKf4JgHPVxAxAALZ8ny4/R7s6/3rZWC2o5vQ=="  <br/>}<br/>context {  <br/>  clusterID = "62c42aed-9839-4da6-8c09-9d220f56e924"<br/>  pool      = "myPool" <br/>  imageFeatures = "layering"<br/>}<br/>EOF</span></pre><p id="e324" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe nk nl nm na b">userID</code>和<code class="fe nk nl nm na b">userKey</code>是上面创建的值，<code class="fe nk nl nm na b">clusterID</code>是Ceph Cluster ID，<code class="fe nk nl nm na b">pool</code>是之前创建的<code class="fe nk nl nm na b">myPool</code>的值。</p><p id="88d2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">看一下<code class="fe nk nl nm na b">external_id</code>，它是单个卷的唯一ID。该ID基于<a class="ae lk" href="https://github.com/ceph/ceph-csi/blob/71ddf51544be498eee03734573b765eb04480bb9/internal/util/volid.go#L27" rel="noopener ugc nofollow" target="_blank"> CSI ID格式</a>。让我们看看<code class="fe nk nl nm na b">external_id</code>的约定。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="668c" class="ne lt iq na b gy nf ng l nh ni">&lt;csi-id-version&gt;-&lt;cluster-id-length&gt;-&lt;cluster-id&gt;-&lt;pool-id&gt;-&lt;uuid&gt;</span></pre><p id="8605" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">按照这个惯例，下面的<code class="fe nk nl nm na b">external_id</code>可以分成独立的部分。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="8ceb" class="ne lt iq na b gy nf ng l nh ni">0001-0024-62c42aed-9839-4da6-8c09-9d220f56e924-0000000000000009-00000000-1111-2222-bbbb-cacacacacaca</span></pre><ul class=""><li id="0ed3" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">CSI ID版本:<code class="fe nk nl nm na b">0001</code></li><li id="5cf1" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">集群ID长度:<code class="fe nk nl nm na b">0024</code></li><li id="2c15" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">集群ID: <code class="fe nk nl nm na b">62c42aed-9839–4da6–8c09–9d220f56e924</code></li><li id="9988" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">人才库ID: <code class="fe nk nl nm na b">0000000000000009</code></li><li id="d513" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">UUID: <code class="fe nk nl nm na b">00000000–1111–2222-bbbb-cacacacacaca</code></li></ul><p id="acef" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于池ID，您可以在ceph中获得池的ID<code class="fe nk nl nm na b">myPool</code>。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="a188" class="ne lt iq na b gy nf ng l nh ni">sudo ceph osd lspools<br/>1 cephfs_data<br/>2 cephfs_metadata<br/>3 foo<br/>4 bar<br/>5 .rgw.root<br/>6 default.rgw.control<br/>7 default.rgw.meta<br/>8 default.rgw.log<br/>9 myPool<br/>10 default.rgw.buckets.index<br/>11 default.rgw.buckets.data</span></pre><p id="9d91" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe nk nl nm na b">myPool</code>的ID是9。</p><p id="d1db" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">UUID是卷的唯一ID。如果要从同一个池创建新卷，需要设置新UUID。</p><p id="ba87" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在Nomad上注册卷之前，需要先在池<code class="fe nk nl nm na b">myPool</code>中创建一个映像。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="3417" class="ne lt iq na b gy nf ng l nh ni">sudo rbd create csi-vol-00000000-1111-2222-bbbb-cacacacacaca --size 1024 --pool myPool --image-feature layering;</span></pre><p id="82ea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">看一下图片的名字<code class="fe nk nl nm na b">csi-vol-00000000–1111–2222-bbbb-cacacacacaca</code>。Ceph CSI参数<code class="fe nk nl nm na b">volumeNamePrefix</code>的默认值为<code class="fe nk nl nm na b">csi-vol-</code>。剩下的就是上面提到的基于CSI ID格式的<code class="fe nk nl nm na b">external_id</code>的UUID。要创建的图像的名称有以下约定。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="a1b2" class="ne lt iq na b gy nf ng l nh ni">&lt;volume-name-prefix&gt;&lt;uuid&gt;</span></pre><p id="51d1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，在Nomad上注册音量。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="6ca2" class="ne lt iq na b gy nf ng l nh ni">nomad volume register ceph-volume.hcl;</span></pre><p id="8ba1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">查看注册卷的状态。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="f2d4" class="ne lt iq na b gy nf ng l nh ni">nomad volume status;<br/>Container Storage Interface<br/>ID        Name        Plugin ID  Schedulable  Access Mode<br/>ceph-mys  ceph-mysql  ceph-csi   true         single-node-writer</span></pre><p id="6d1a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们通过在Nomad上运行示例MySQL服务器作业来挂载Ceph CSI提供的卷。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="fa15" class="ne lt iq na b gy nf ng l nh ni">cat &lt;&lt;EOF &gt; mysql-server.nomad<br/>job "mysql-server" {<br/>  datacenters = ["dc1"]<br/>  type        = "service"</span><span id="fda4" class="ne lt iq na b gy nj ng l nh ni">group "mysql-server" {<br/>    count = 1</span><span id="3ae6" class="ne lt iq na b gy nj ng l nh ni">volume "ceph-mysql" {<br/>      type      = "csi"<br/>      read_only = false<br/>      source    = "ceph-mysql"<br/>    }</span><span id="68a4" class="ne lt iq na b gy nj ng l nh ni">network {<br/>      port "db" {<br/>        static = 3306<br/>      }<br/>    }</span><span id="ed22" class="ne lt iq na b gy nj ng l nh ni">restart {<br/>      attempts = 10<br/>      interval = "5m"<br/>      delay    = "25s"<br/>      mode     = "delay"<br/>    }</span><span id="5756" class="ne lt iq na b gy nj ng l nh ni">task "mysql-server" {<br/>      driver = "docker"</span><span id="e58f" class="ne lt iq na b gy nj ng l nh ni">volume_mount {<br/>        volume      = "ceph-mysql"<br/>        destination = "/srv"<br/>        read_only   = false<br/>      }</span><span id="f042" class="ne lt iq na b gy nj ng l nh ni">env {<br/>        MYSQL_ROOT_PASSWORD = "password"<br/>      }</span><span id="a56c" class="ne lt iq na b gy nj ng l nh ni">config {<br/>        image = "hashicorp/mysql-portworx-demo:latest"<br/>        args  = ["--datadir", "/srv/mysql"]<br/>        ports = ["db"]<br/>      }</span><span id="73d2" class="ne lt iq na b gy nj ng l nh ni">resources {<br/>        cpu    = 500<br/>        memory = 1024<br/>      }</span><span id="58ae" class="ne lt iq na b gy nj ng l nh ni">service {<br/>        name = "mysql-server"<br/>        port = "db"</span><span id="9edd" class="ne lt iq na b gy nj ng l nh ni">check {<br/>          type     = "tcp"<br/>          interval = "10s"<br/>          timeout  = "2s"<br/>        }<br/>      }<br/>    }<br/>  }<br/>}<br/>EOF</span><span id="0ae4" class="ne lt iq na b gy nj ng l nh ni"># run mysql job.<br/>nomad job run mysql-server.nomad;</span></pre><p id="c663" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们检查来自Ceph RBD的卷是否在进入分配的mysql服务器容器时被挂载。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="038c" class="ne lt iq na b gy nf ng l nh ni">nomad alloc exec bfe37c92 sh<br/># df -h<br/>Filesystem      Size  Used Avail Use% Mounted on<br/>...<br/>/dev/rbd0       976M  180M  781M  19% /srv<br/>...</span></pre><p id="34ec" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如图所示，卷<code class="fe nk nl nm na b">ceph-mysql</code>已经安装到路径<code class="fe nk nl nm na b">/srv</code>中。</p><p id="f794" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，我们将检查在Nomad上重新提交mysql服务器作业后，MySQL数据是否丢失。</p><p id="00c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们连接到容器中的MySQL服务器。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="3576" class="ne lt iq na b gy nf ng l nh ni">mysql -u root -p -D itemcollection;<br/>... type the password of MYSQL_ROOT_PASSWORD<br/></span><span id="d013" class="ne lt iq na b gy nj ng l nh ni">mysql&gt; select * from items;<br/>+----+----------+<br/>| id | name     |<br/>+----+----------+<br/>|  1 | bike     |<br/>|  2 | baseball |<br/>|  3 | chair    |<br/>+----+----------+</span></pre><p id="c2a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们添加一些行。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="9f2d" class="ne lt iq na b gy nf ng l nh ni">INSERT INTO items (name) VALUES ('glove');<br/>INSERT INTO items (name) VALUES ('hat');<br/>INSERT INTO items (name) VALUES ('keyboard');</span></pre><p id="5386" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">确保成功插入行。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="b81f" class="ne lt iq na b gy nf ng l nh ni">mysql&gt; select * from items;<br/>+----+----------+<br/>| id | name     |<br/>+----+----------+<br/>|  1 | bike     |<br/>|  2 | baseball |<br/>|  3 | chair    |<br/>|  4 | glove    |<br/>|  5 | hat      |<br/>|  6 | keyboard |<br/>+----+----------+<br/>6 rows in set (0.00 sec)</span></pre><p id="c633" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，停止mysql服务器的工作。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="df74" class="ne lt iq na b gy nf ng l nh ni">nomad stop -purge mysql-server;</span></pre><p id="b7d5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并且，再次提交mysql服务器作业。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="7558" class="ne lt iq na b gy nf ng l nh ni">nomad job run mysql-server.nomad</span></pre><p id="b50b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">检查mysql数据是否无损存在。进入分配的mysql服务器容器后，像前面一样键入以下内容。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="4f0a" class="ne lt iq na b gy nf ng l nh ni">mysql -u root -p -D itemcollection;<br/>... type the password of MYSQL_ROOT_PASSWORD</span><span id="4062" class="ne lt iq na b gy nj ng l nh ni">mysql&gt; select * from items;<br/>+----+----------+<br/>| id | name     |<br/>+----+----------+<br/>|  1 | bike     |<br/>|  2 | baseball |<br/>|  3 | chair    |<br/>|  4 | glove    |<br/>|  5 | hat      |<br/>|  6 | keyboard |<br/>+----+----------+<br/>6 rows in set (0.00 sec)</span></pre><p id="1dcd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">即使mysql服务器作业在Nomad上重新启动，mysql服务器也不会丢失数据。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="22db" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您已经看到了目前如何使用相同的Ceph CSI在Kubernetes和Nomad上提供卷。</p><p id="16d7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要使用CSI从外部Ceph存储供应卷，在Kubernetes上比在Nomad上更简单。您可以在Kubernetes上动态配置卷，但是目前，Nomad还不支持动态卷配置。最近，<a class="ae lk" href="https://www.hashicorp.com/blog/announcing-hashicorp-nomad-1-1-beta" rel="noopener ugc nofollow" target="_blank"> Nomad 1.1 Beta版</a>宣布了对CSI支持的改进，例如，<code class="fe nk nl nm na b">nomad volume create &lt;volume-hcl&gt;</code>将自动在ceph中创建一个池的图像。</p><h2 id="9ebf" class="ne lt iq bd lu ns nt dn ly nu nv dp mc kj nw nx mg kn ny nz mk kr oa ob mo oc bi translated">参考</h2><ul class=""><li id="c88a" class="kw kx iq ka b kb mq kf mr kj od kn oe kr of kv lb lc ld le bi translated"><a class="ae lk" href="https://rancher.com/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/ceph/" rel="noopener ugc nofollow" target="_blank">https://rancher . com/docs/rancher/v2 . x/en/cluster-admin/volumes-and-storage/ceph/</a></li><li id="5ee5" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated"><a class="ae lk" href="https://learn.hashicorp.com/tutorials/nomad/stateful-workloads-csi-volumes?in=nomad/stateful-workloads" rel="noopener ugc nofollow" target="_blank">https://learn . hashi corp . com/tutorials/nomad/stateful-workloads-CSI-volumes？in =移动/有状态工作负载</a></li><li id="138d" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated"><a class="ae lk" href="https://github.com/hashicorp/nomad/tree/main/demo/csi/ceph-csi-plugin" rel="noopener ugc nofollow" target="_blank">https://github . com/hashicorp/nomad/tree/main/demo/CSI/ceph-CSI-plugin</a></li></ul></div></div>    
</body>
</html>