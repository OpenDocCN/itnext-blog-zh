<html>
<head>
<title>The baseline for Precision-Recall curve: A Bayesian approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">精确召回曲线的基线:贝叶斯方法</h1>
<blockquote>原文：<a href="https://itnext.io/the-baseline-for-precision-recall-curve-a-bayesian-approach-1611c690607?source=collection_archive---------2-----------------------#2022-10-27">https://itnext.io/the-baseline-for-precision-recall-curve-a-bayesian-approach-1611c690607?source=collection_archive---------2-----------------------#2022-10-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3281" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与受试者工作特征(ROC)曲线不同，精确召回没有0.5的通用基线。但是为什么会这样呢？让我们后退一步，看看这种情况下的基线是什么。</p><p id="b67f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给定我们要分类的数据，基线有助于与随机分类器进行比较。这很重要，因为如果不知道基线，我们就无法真正判断一个分类器。想象一个精确回忆(AUPR)曲线下面积为0.3的分类器。有多大用处？如果基数是0.003，我们有一个杰出的分类器；但是，如果基线是0.25，我们并没有提高基线太多。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="kq kr l"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">资料来源:giphy.com</figcaption></figure><h1 id="b81e" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">从贝叶斯定理的角度看</h1><p id="11fb" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">让我们计算一下精确召回曲线的基线，但在此之前，让我们回过头来看看贝耶定理。</p><p id="cada" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据贝叶定理，条件概率如下:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/f12f8aa6d2b6367e2aa4e241225efe8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zEu75-OVmF1LD-KR.png"/></div></div></figure><p id="d53c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中X代表我们的数据，Y是样本的真实类别。本质上，Baye定理将我们的先验信息更新为P(Y)，给定新数据X，返回后验概率密度P(Y|X)。</p><p id="584f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分母可以用X所有可能结果的总和来代替。这也称为归一化因子，用于将结果保持在[0，1]之间。</p><p id="47d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于二元分类器，我们可以使用预测类的两个结果进行总结。</p><p id="b2c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于第1类，我们有以下内容:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/84bc66829f6635020d1a120e874d3918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vlh7ATjX1wcaD06Z.png"/></div></div></figure><p id="8c19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似地，对于0类:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/96aa52c50cc4c94cc932c4148bc110d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FNaMzyp2cNzh8rLB.png"/></div></div></figure><h1 id="f121" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">将分类器性能量化为贝叶斯定理</h1><p id="7b88" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">在上一节中，我们推导了二元分类器的贝叶斯定理的基本形式。现在我们需要注入我们的分类器预测，而不是x。这里，让我们将Y '作为分类器预测引入定理。因此，下面定义了当真实类别是一个给定的预测类别0时的后验概率。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/2f47be192cb36a7cc8ccd734a63e34c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KC7qQvZXZgJk2UP-.png"/></div></div></figure><p id="fcb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分类器通常为每个类别输出一个分数，然后我们应用一个阈值来得到预测的类别。考虑到这一点，我们可以将我们的后验概率定义为P(Y=1|q&gt;qt)。</p><p id="0409" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">定义了分类器预测后，我们对真阳性率(回忆)、真阴性率(特异性)、假阳性率(假警报率)和假阴性率(遗漏率)有以下定义。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mg"><img src="../Images/5df6ad35c81b7d8e17ae2b62c390379c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9WbIq4qAnxhdEDcR.png"/></div></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mh"><img src="../Images/d278983eb2c83f32a1dfef31eb17ca54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rd9_3R3Ozro7xanO.png"/></div></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mi"><img src="../Images/66fba7e2ce3fabde7abd7d58b5a22cf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w9TmODrNCnSxgl0H.png"/></div></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mj"><img src="../Images/bceda719cef8a28fdb93b014b9ae2db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ex4Nb2cl9lbSGITo.png"/></div></div></figure><h1 id="aa5a" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">计算随机分类器的精确召回基线</h1><p id="0ff2" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">最后，配备了我们需要的所有工具，我们可以计算出精确召回曲线的基线。让我们从检查精确度开始，精确度也被称为阳性预测值。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/3b727e38335ce91e9e09ab5ae80401e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bvtbOgVFP-APZVkr.png"/></div></div></figure><p id="38c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们也假设我们的分类器分配均匀随机的问题给我们数据中的每个样本。然后，我们使用<code class="fe mk ml mm mn b">qt</code>作为阈值来确定预测类别。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/a7678a2f5955ae8e03de044e2f3ce196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2etkEm3zg-DMQPOL.png"/></div></div></figure><p id="37ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经知道，如果两个变量在统计上是独立的，那么它们事件的交集概率将等于它们概率的乘积。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mo"><img src="../Images/018074a3c7e307db48e9b19a1d127c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*63l7PIC6vX4BqLVh.png"/></div></div></figure><p id="825d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们有:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/0b130ddeaefcd58dffd789ad95a2271b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5BIXyNe62SaN-6WP.png"/></div></div></figure><p id="a55c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">或者在我们的分类器中:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/69c76a98e726bfa760808140016ab854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jP3lmIXhbauKsjpV.png"/></div></div></figure><p id="0f93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个先验概率是从我们的数据中抽取一个正面例子的概率，称为患病率。<strong class="jp ir">患病率是我们数据的一个属性，等于阳性样本总数与样本总数的比率</strong>。换句话说，随机分类器的PR曲线的y轴随着P/N的值是恒定的。这成为PR曲线的主要缺点之一，因为不具有与类别分布无关的通用基线。Flach等人在他们的论文“Precision-Recall-Gain Curves:PR Analysis Done Right”中弥补了这一特性，其中他们引入了PR曲线的一种新的推导方法，将“总是正分类器”作为基线。</p><h1 id="62ad" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">Python实验</h1><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="kq kr l"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">资料来源:giphy.com</figcaption></figure><p id="d2b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们通过使用一些Python代码来试验一下。</p><p id="9b37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我从创建一个随机分类器开始，该分类器将来自正态或高斯分布的概率分配给每个数据点，而不管其值如何。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="mp kr l"/></div></figure><p id="ad59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如你所见，<code class="fe mk ml mm mn b">n_component</code>是我们的分类器将要预测的类的数量，在二进制分类器的情况下是2。</p><p id="2865" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我准备了三个实验，以下为正值和负值的个数。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="mp kr l"/></div></figure><p id="03e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用0作为平均值，1作为分布参数的方差。</p><p id="4c13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们看看我们的随机分类器的概率密度图。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="mp kr l"/></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mq"><img src="../Images/94d1d0002b1d649781313f32a849fa74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEmrS2DZMa4g6AREE9k6FQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">实验中使用的三种随机分类器的密度图</figcaption></figure><p id="b46b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如所料，人口越多，曲线越像钟形曲线。</p><p id="bd22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们可以为我们的分类器绘制精确-召回曲线。</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="mp kr l"/></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mr"><img src="../Images/f6736654f03daa4fc21c17de449ad775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f8FiW3QSauH1BA9dWngl4A.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">三种随机分类器的精度-召回曲线</figcaption></figure><p id="236e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">红线表示作为PR曲线基线的P/N比。随机分类器快速收敛到阳性样本与样本总数的比率(P/N)。</p><p id="3eb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae ms" href="https://github.com/realsarm/deepLearningRecipes" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上可以找到这个模拟的代码和一些实际操作的例子。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="d241" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我会多写CS的文章；因此，如果你觉得有趣和有帮助，请跟随我的媒介。另外，请随时通过<a class="ae ms" href="https://www.linkedin.com/in/ali--moezzi/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>直接联系我。</p></div></div>    
</body>
</html>