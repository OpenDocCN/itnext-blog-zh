<html>
<head>
<title>Kubernetes Resources and Autoscaling — From Basics to Greatness</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kubernetes资源和自动缩放—从基础到卓越</h1>
<blockquote>原文：<a href="https://itnext.io/kubernetes-resources-and-autoscaling-from-basics-to-greatness-7cae17fbf27b?source=collection_archive---------0-----------------------#2020-10-22">https://itnext.io/kubernetes-resources-and-autoscaling-from-basics-to-greatness-7cae17fbf27b?source=collection_archive---------0-----------------------#2020-10-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="8379" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">介绍</h1><p id="a02b" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">自动缩放应用程序运行的资源并不是一个新的想法。我们可以在AWS中通过使用<a class="ae lq" href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">自动缩放组</em> </a>来实现多年。当应用程序直接在EC2实例上运行时，我们可以根据负载的变化增加或减少实例的数量。</p><p id="52b4" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">用Kubernetes部署和运行我们的应用程序给自动伸缩带来了不同程度的复杂性。我们不仅可以通过添加节点来扩展应用程序，还可以通过添加单元来扩展应用程序。此外，在没有Kubernetes控制平面参与的情况下添加新节点(例如EC2实例)可能是徒劳的。</p><p id="bb25" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在这篇博客中，我们将尝试描述Kubernetes用来实现自动缩放的构件。我们将看到Kubernetes如何收集和使用指标来自动缩放pod和节点。我们将使用AWS <a class="ae lq" href="https://aws.amazon.com/eks/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc&amp;eks-blogs.sort-by=item.additionalFields.createdDate&amp;eks-blogs.sort-order=desc" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> EKS </em> </a>来展示用于在pod和节点级别实现自动伸缩的模式和最佳实践。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="4ebd" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">收集指标</h1><p id="7cc6" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">那么Kubernetes控制平面和其他相关组件如何知道所有节点上运行的pod和容器所使用的内存、CPU和其他资源的数量呢？嗯，它从集群中每个工作节点上运行的Kubelets获得帮助。Kubelet将<a class="ae lq" href="https://github.com/google/cadvisor" rel="noopener ugc nofollow" target="_blank"> cAdvisor </a>捆绑到其二进制文件中，该文件从节点上运行的容器中收集相关的指标。Kubelet将这些指标作为API端点公开。在EKS集群上，您可以通过首先执行以下代理命令来查看这些指标:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="4885" class="mg jv iq mc b gy mh mi l mj mk">kubectl proxy --port=8080 &amp;</span></pre><p id="c373" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">然后调用下面的API端点(用集群中的一个真实节点名替换<strong class="ku ir"> {NODE_NAME} </strong>)</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="ceca" class="mg jv iq mc b gy mh mi l mj mk"># curl <a class="ae lq" href="http://localhost:8080/api/v1/nodes/ip-10-16-10-212.ec2.internal/proxy/metrics/cadvisor" rel="noopener ugc nofollow" target="_blank">http://localhost:8080/api/v1/nodes/{NODE_NAME}/proxy/metrics/cadvisor</a></span></pre><p id="a835" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">从结果中可以看出，所有需要的指标都由API公开了(而且是普罗米修斯指标格式！).</p><p id="4c07" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">按照Kubernetes的逻辑，我们期望这些指标可以通过暴露在<a class="ae lq" href="https://kubernetes.io/docs/concepts/overview/components/#kube-apiserver" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> API-server </em> </a>上的API端点获得。为了实现这一点，API-server需要一种机制来聚集来自所有节点的指标。然而，API-server本身并不这样做，它使用一个名为<a class="ae lq" href="https://github.com/kubernetes-sigs/metrics-server" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> metrics-server </em> </a>的特殊集群插件，该插件需要部署到集群上。</p><p id="483b" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">度量服务器充当Kubelets和API服务器之间的中介(图1)。首先，指标服务器发现集群中的节点，然后收集和汇总指标。API-server使用由metrics-server聚合的指标，通过API端点向集群的其他部分公开所需的信息。</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ml"><img src="../Images/e6be9e081d8805fd68dd57138e5c56ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nZfW6tk3zSEtYpBVObUXPQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">图1 </strong>。metrics-server从运行在worker节点上的Kubelets收集指标。API服务器向控制器展示指标。</figcaption></figure><p id="4f3f" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在EKS集群上，默认情况下不部署度量服务器。您可以使用此<a class="ae lq" href="https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">向导</em> </a>来部署它。</p><p id="8636" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在部署metric-server之后，我们可以使用之前使用的相同的代理命令，然后调用以下命令从API-server获取pods的CPU和内存使用情况:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="0dff" class="mg jv iq mc b gy mh mi l mj mk"># curl <a class="ae lq" href="http://localhost:8080/apis/metrics.k8s.io/v1beta1/pods/" rel="noopener ugc nofollow" target="_blank">http://localhost:8080/apis/metrics.k8s.io/v1beta1/pods/</a></span></pre></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="d40e" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">Pod调度—请求和限制</h1><p id="efa8" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在声明一个新的pod时，我们可以指定其专用的<a class="ae lq" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">资源</em> </a>(通常是内存和CPU)。正如我们将要看到的，我们可以指定所需的<strong class="ku ir"><em class="lr"/></strong>最小资源，称为<strong class="ku ir"><em class="lr"/></strong><em class="lr">。</em>我们还可以指定<strong class="ku ir"><em class="lr"/></strong>的最大资源使用量，称为<strong class="ku ir"><em class="lr"/></strong>。重要的是要记住，资源是为pod中的每个容器单独声明的，而不是作为一个整体为每个pod声明的。pod所需的总资源是其所有容器所需资源的总和。</p><p id="c0b5" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">请求</strong></p><p id="8484" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">让我们来看看下面的吊舱减速:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="e0c3" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: two-containers-pod<br/>spec:<br/>  containers:<br/>  - image: ubuntu<br/>    command: ["/bin/bash", "-c", "--"]<br/>    args: ["while true; do sleep 30; done;"]<br/>    name: container1<br/>    resources:<br/>      <strong class="mc ir">requests:<br/>        cpu: 500m<br/>        memory: 300Mi</strong><br/>  - image: ubuntu<br/>    command: ["/bin/bash", "-c", "--"]<br/>    args: ["while true; do sleep 30; done;"]<br/>    name: container2<br/>    resources:<br/>      <strong class="mc ir">requests:<br/>        cpu: 100m<br/>        memory: 400Mi</strong></span></pre><p id="b41a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">此pod减速指定了两个容器。第一个容器要求至少500m个CPU(半个CPU内核的时间)和300Mi的RAM。第二个容器请求最少1亿个CPU(1/10的CPU内核时间)和400兆内存。pod要求至少600兆CPU和700兆内存。一种方法是描述pod运行的节点(<em class="lr"> kubectl describe nodes </em>命令)。这样做将向我们展示以下内容:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mx"><img src="../Images/5e5144a0ad39e8678248cbf9fdc8e005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPS4kyZVFHaUZzEp7mqLJQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> <em class="my"> kubectl描述节点</em> </strong></figcaption></figure><p id="9f73" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">但是我们为什么要关心一个pod需要的最小资源，难道我们不应该只关心它能消耗的最大资源吗？嗯，这与Kubernetes <a class="ae lq" href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/#:~:text=kube%2Dscheduler%20is%20the%20default,manage%20the%20lifecycle%20of%20containers.%20.&amp;text=In%20a%20cluster%2C%20Nodes%20that,Pod%20are%20called%20feasible%20nodes." rel="noopener ugc nofollow" target="_blank"> <em class="lr">调度器</em> </a>的工作方式有关。让我们看看在描述节点时得到的更多信息:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mz"><img src="../Images/0d6fe2d7460b22e34701a2ab55788efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uVFR1q84fJHpvr24LzYNPg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> <em class="my"> kubectl描述节点</em> </strong></figcaption></figure><p id="045d" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我们可以看到，节点有36%的CPU和20%的内存是由运行的pods请求的。这正是调度程序在需要调度一个新的pod时要检查的内容。调度器将pod请求的请求与每个节点上的空闲资源量(未请求)进行比较，并选择具有足够可用资源的节点。如果我们要求将一个CPU请求为3000m个CPU的pod调度到一个只有两个CPU的工作节点的集群，我们可以看到这一点。pod将卡在<a class="ae lq" href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#my-pod-stays-pending" rel="noopener ugc nofollow" target="_blank"> <em class="lr">待定</em> </a>状态。描述pod(使用<em class="lr"> kubectl describe pod </em>命令)将向我们展示以下内容:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi na"><img src="../Images/dac1fcd927e268a499097fe3d14a5371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*USEWVaU_Lbrjwa2OIBvtwA.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">T21【kubectl】形容荚T23】</strong></figcaption></figure><p id="1a4a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">Kubernetes集群中的每个工作节点都有自己的资源容量(CPU和内存的总量)。但是，被安排到一个节点的单元不能使用所有的节点容量。一部分CPU和内存需要保留给OS和运行在节点上的其他Kubernetes组件(比如Kubelet)。当描述一个节点(<em class="lr"> kubectl describe nodes </em>命令)时，我们会注意到对于<em class="lr">容量</em>和<em class="lr">可分配:</em>有一个输出</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nb"><img src="../Images/d016a1da4716b4d7d03df17752f035e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VznBg5cpggWmUd9rKlxbvg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> <em class="my"> kubectl描述节点</em> </strong></figcaption></figure><p id="34ed" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><em class="lr">容量</em>是节点的总容量(我们从云提供商那里得到的)。<em class="lr">可分配的</em>是在我们为操作系统和其他Kubernetes组件预留了所需资源后，pod可用的资源总量。调度程序在调度pod时只考虑<em class="lr">可分配的</em>资源。</p><p id="0530" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">需要记住的最后一件重要事情是，调度程序不会评估节点的当前资源利用率来做出决定。调度器仅基于节点上运行的pods已经请求的资源总量来做出他的决定。例如，假设有一个2个CPU的工作节点，运行着总共有1700m个CPU请求的pod。当前实际的CPU利用率是10% (2亿个CPU)。即使当前有1800m个CPU可用，尝试调度向该节点请求400m个CPU的pod也会失败。</p><p id="fe01" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">极限值</strong></p><p id="22fc" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我们通过在每个容器上设置<em class="lr">限制</em>来设置pod可以利用的最大资源:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="85b2" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: limits-pod<br/>spec:<br/>  containers:<br/>  - image: ubuntu<br/>    command: ["/bin/bash", "-c", "--"]<br/>    args: ["while true; do sleep 30; done;"]<br/>    name: limits-container<br/>    resources:<br/>      requests:<br/>        cpu: 300m<br/>        memory: 300Mi<br/>      <strong class="mc ir">limits:<br/>        cpu: 1000m<br/>        memory: 500Mi</strong></span></pre><p id="c5da" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在上面的示例中，我们部署了一个容量限制为1000m CPUs个CPU内核)和500Mi RAM的pod。现在，当容器试图消耗比它的<em class="lr">限制</em>更多的资源时会发生什么？结果取决于它试图过度消耗的资源。</p><p id="938e" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">CPU是<em class="lr">可压缩的</em>资源，而内存是<em class="lr">不可压缩的</em>。在容器内运行的进程不能使用超过其限制的CPU。因为CPU是一种可压缩的资源，结果可能是响应时间变慢，但是进程本身通常不会崩溃。另一方面，试图过度消耗内存会导致进程终止(臭名昭著的<a class="ae lq" href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> OOMKilled </em> </a>错误)。当一个内存片被一个进程消耗掉时，它就归这个进程所有，不先杀死它就不能被取走(因此有了<em class="lr"> OOMKilled </em>)。这就是为什么内存是一种不可压缩的资源。</p><p id="8e97" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">服务质量</strong></p><p id="8992" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">正如我们已经看到的，在陈述请求时，我们必须考虑节点的<em class="lr">可分配的</em>资源(在2个CPU的节点上不能请求3000m个CPU)。但是限制可以被过量使用，这意味着所有pod的限制的总和可以大于节点<em class="lr">可分配的</em>资源。这样，一个节点可以耗尽它的资源，Kubernetes将需要决定哪些pod应该被杀死，以便该节点继续运行。为了做出这些决定，Kubernetes使用了<a class="ae lq" href="https://www.replex.io/blog/everything-you-need-to-know-about-kubernetes-quality-of-service-qos-classes" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> QoS </em> </a>(服务质量)。pod的QoS是从其容器请求和限制中推导出来的。有三类服务质量:</p><ol class=""><li id="7dd5" class="nc nd iq ku b kv ls kz lt ld ne lh nf ll ng lp nh ni nj nk bi translated"><em class="lr">保底</em></li><li id="8597" class="nc nd iq ku b kv nl kz nm ld nn lh no ll np lp nh ni nj nk bi translated"><em class="lr">可爆发</em></li><li id="649c" class="nc nd iq ku b kv nl kz nm ld nn lh no ll np lp nh ni nj nk bi translated"><em class="lr">最大努力</em></li></ol><p id="2b79" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">对于为其所有容器配置了请求和限制的pod，实现了<em class="lr">保证的</em> QoS等级。必须为CPU和内存指定请求和限制，并且它们必须相同(提示:如果忽略请求而只指定限制，Kubernetes会将其视为请求=限制)。此QoS类别中的pod将最后被逐出。</p><p id="a653" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><em class="lr">可突发的</em> QoS等级是为不具有等于所有容器上的限制的请求的pod或具有至少一个没有指定限制的容器的pod实现的。因为限制不等于请求，所以pod可以使用比它们所请求的(突发)更多的资源，但是达到限制。此QoS类别中的pod将在<em class="lr">保证的</em>pod之前被驱逐，但仅在<em class="lr">尽力而为的pod之后被驱逐。</em></p><p id="77f8" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">对于绝对没有为其任何容器配置请求或限制的pod，实现了best effort QoS类别。这种QoS级别的pod将首先被淘汰。</p><p id="962a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">极限范围</strong></p><p id="efa7" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">对容器没有限制的豆荚是危险的生物，在某些情况下，它们可以让一个节点屈服。想象一个应用程序遭受<a class="ae lq" href="https://en.wikipedia.org/wiki/Memory_leak#:~:text=In%20computer%20science%2C%20a%20memory,accessed%20by%20the%20running%20code." rel="noopener ugc nofollow" target="_blank"> <em class="lr">内存泄漏</em> </a>问题，在一个没有内存限制的pod内运行。pod将开始消耗越来越多的内存。因为它可以消耗的内存量没有限制，所以它可能会对与其一起运行的所有其他进程产生严重影响。没有CPU限制的pod也会出现同样的问题，消耗越来越多的CPU时间。</p><p id="5199" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">处理这个问题的一个方法是对我们部署的所有pod的所有容器设置限制。集群管理员可以通过强制实施每个pod或容器的最小和最大请求和限制来增加另一层保护。这是使用一个名为<a class="ae lq" href="https://kubernetes.io/docs/concepts/policy/limit-range/" rel="noopener ugc nofollow" target="_blank"><em class="lr">limit range</em></a><em class="lr"/>的准入控制器在一个<em class="lr">名称空间内完成的。</em></p><p id="1972" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">LimitRange将为缺少默认值的容器声明注入指定的默认值。这将保护我们免受上述情况的影响。此外，LimitRange在新配置或现有配置的准入阶段强制实施默认值和限制。如果某些资源声明违反了LimitRange中指定的约束，它将被拒绝。</p><p id="6472" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">让我们通过在limit-range名称空间中声明以下LimitRange对象来看看这一点:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="bb93" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: v1<br/>kind: LimitRange<br/>metadata:<br/>  name: limit-range-obj<br/>  namespace: limit-range<br/>spec:<br/>  limits:<br/>  - default:<br/>      cpu: 500m<br/>      memory: 1Gi<br/>    max:<br/>      cpu: 1000m<br/>      memory: 2Gi    <br/>    defaultRequest:<br/>      cpu: 100m<br/>      memory: 200Mi<br/>    type: Container</span></pre><p id="829b" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">这个LimitRange指定了500m CPUs和1Gi RAM的默认限制。它还将默认请求设置为100m CPUs和200 mi RAM。LimitRange还设置了1000m CPUs和2Gi RAM的最大值。现在让我们部署一个没有声明资源的pod(注意，我们将pod部署到限制范围的名称空间):</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="846f" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: limit-range-pod<br/>  <strong class="mc ir">namespace: limit-range</strong><br/>spec:<br/>  containers:<br/>  - image: ubuntu<br/>    command: ["/bin/bash", "-c", "--"]<br/>    args: ["while true; do sleep 30; done;"]<br/>    name: limit-range-container</span></pre><p id="86e0" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">运行<em class="lr">ku bectl-n limit-range get pod limit-range-pod-o YAML</em>命令将产生以下结果:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nq"><img src="../Images/79ba2be0cf820ce157b2dd9141633053.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sejNp7Vf3meZsJTEph3X8g.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"><em class="my">ku bectl-n limit-range get pod limit-range-pod-o YAML</em></strong></figcaption></figure><p id="505e" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">正如我们所看到的，LimitRange准入控制器识别出我们没有声明任何请求或限制，并且已经注入了指定的缺省值。现在，让我们部署一个具有指定请求但没有限制的pod:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="1e5e" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: limit-range-pod<br/>  namespace: limit-range<br/>spec:<br/>  containers:<br/>  - image: ubuntu<br/>    command: ["/bin/bash", "-c", "--"]<br/>    args: ["while true; do sleep 30; done;"]<br/>    name: limit-range-container<br/>    resources:<br/>      <strong class="mc ir">requests:<br/>        cpu: 300m<br/>        memory: 300Mi</strong></span></pre><p id="f950" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">运行<em class="lr">ku bectl-n limit-range get pod limit-range-pod-o YAML</em>命令将产生以下结果:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nr"><img src="../Images/31497d6b8d2a9ccaeb7020a6cbf8a548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywQ-0pJcC3MVrbN-2gIRxg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"><em class="my">ku bectl-n limit-range get pod limit-range-pod-o YAML</em></strong></figcaption></figure><p id="580a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">如我们所见，这些请求是pod减速中指定的请求，而不是LimitRange中指定的请求。然而，未设置的限制由LimitRange许可控制器实施。最后一件事，让我们尝试部署一个请求的资源超过LimitRange中配置的最大值(1000m CPUs和2Gi RAM)的pod:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="8701" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: limit-range-pod<br/>  namespace: limit-range<br/>spec:<br/>  containers:<br/>  - image: ubuntu<br/>    command: ["/bin/bash", "-c", "--"]<br/>    args: ["while true; do sleep 30; done;"]<br/>    name: limit-range-container<br/>    resources:<br/>      <strong class="mc ir">requests:<br/>        cpu: 2000m<br/>        memory: 2.5Gi</strong></span></pre><p id="66a7" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">尝试部署此pod将导致以下错误:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ns"><img src="../Images/1c32642e8e1112ed196c0eeeef1cafa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_tQlO8EvrNi6m4SQkbWeWw.png"/></div></div></figure><p id="5d72" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">RangeLimit也可以强制执行我们在pod级别上看到的相同限制。在这种情况下，为容器配置的资源总和不得超过为RangeLimit中的pod配置的限制。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="dde0" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">水平Pod自动缩放器(HPA)</h1><p id="98d8" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">水平自动缩放的第一个实现是传统的AWS <a class="ae lq" href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">自动缩放组</em> </a> (ASGs)。对于AWS ASGs，我们有一组相同的EC2实例(使用相同的<a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> AMI </em> </a>)。然后，EC2服务根据我们指定的度量阈值添加或删除EC2实例。例如，您可以将CPU阈值设置为80%。当观察到的当前运行实例的平均CPU利用率在定义的时间范围内高于80%时，将启动一个新的EC2实例，平均CPU利用率将下降。另一方面，如果当前运行的实例的平均CPU利用率在定义的时间范围内低于80%，一个或多个EC2实例将被终止。kubernetes<a class="ae lq" href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" rel="noopener ugc nofollow" target="_blank"><em class="lr">Horizontal Pod auto scaler</em></a>(HPA)以非常相似的方式工作，但是作用于Pod而不是EC2实例。</p><p id="ecab" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">让我们通过关注CPU利用率来继续我们的讨论。对于ASGs，我们声明一个特定的CPU利用率阈值，根据该阈值做出自动伸缩决策。但是，在Kubernetes中，没有地方声明这个特定的CPU利用率阈值。对于HPA，pod的请求被用作阈值。HPA计算当前pod的CPU利用率与pod的原始请求的比率。</p><p id="b72b" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">考虑一个具有两个pod <a class="ae lq" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">部署</em> </a> <em class="lr"> </em>和控制部署的副本(pod)数量的HPA的示例。一个pod目前使用9亿个CPU，另一个使用6亿个CPU。在这种情况下，两个pod的总CPU利用率为1500m CPU(900+600 = 1500m CPU)。如果CPU请求设置为每个机架500m个CPU，HPA需要将当前所需的机架数量设置为3 (1500/500=3)。这意味着HPA需要强制部署将其复制副本数量更改为3，以便可以添加另一个单元。</p><p id="4fed" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">请注意，所需pod数量的计算结果总是四舍五入到下一个大整数。因此，如果上例中当前的总CPU利用率是1100m个CPU(而不是1500m个CPU)，HPA仍然会向部署中多添加一个单元(1100/500=2.2，四舍五入为3)。</p><p id="da00" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">总之，我们需要记住一个重要的事实。因为HPA需要pod的请求作为其考虑的阈值，所以我们必须用请求声明pod的所有容器。</p><p id="94cc" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">HPA本身由两部分组成，Kubernetes API资源和控制器。控制器使用metric-server聚集的指标(在<em class="lr"> metrics.k8s.io </em>、<em class="lr"> custom.metrics.k8s.io </em>和<em class="lr">external . metrics . k8s . io</em>API中公开)来定期调整pod的数量。API资源声明控制器做出调整决策所依据的参数。</p><p id="8cc0" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">HPA可以调整以下Kubernetes对象的副本(pod)数量:</p><ul class=""><li id="2daf" class="nc nd iq ku b kv ls kz lt ld ne lh nf ll ng lp nt ni nj nk bi translated">部署</li><li id="cc5b" class="nc nd iq ku b kv nl kz nm ld nn lh no ll np lp nt ni nj nk bi translated">复制集</li><li id="8bff" class="nc nd iq ku b kv nl kz nm ld nn lh no ll np lp nt ni nj nk bi translated">复制控制器</li><li id="34c3" class="nc nd iq ku b kv nl kz nm ld nn lh no ll np lp nt ni nj nk bi translated">状态集</li></ul><p id="963e" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">通过修改<a class="ae lq" href="https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/design/horizontal-pod-autoscaler.md#scale-subresource" rel="noopener ugc nofollow" target="_blank"> <em class="lr">比例子资源</em> </a>来动态调整吊舱数量。HPA控制器通过更改ScaleSpec中的副本字段来更新所需pod的数量。HPA本身不会改变吊舱的数量。它只更新所需的状态。自动缩放对象的控制器(例如，部署控制器)是负责对变更采取行动并相应地更新窗格数量的人。</p><p id="2ccc" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">让我们看看颐康保障户口的运作。我们将从以下两个副本部署开始:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="9cd7" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: hpa-deployment<br/>spec:<br/>  <strong class="mc ir">replicas: 2</strong><br/>  selector:<br/>    matchLabels:<br/>      app: hpa-deployment<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: hpa-deployment<br/>    spec:<br/>      containers:<br/>      - image: ubuntu<br/>        command: ["/bin/bash", "-c", "--"]<br/>        args: ["while true; do sleep 30; done;"]<br/>        name: hpa-container<br/>        resources:<br/>          <strong class="mc ir">requests:<br/>            cpu: 500m</strong></span></pre><p id="a12a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">如您所见，单个容器pod的请求被设置为500m CPUs。接下来，我们声明HPA:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="b56c" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: autoscaling/v2beta2<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/>  name: hpa-test<br/>spec:<br/>  scaleTargetRef:<br/>    <strong class="mc ir">apiVersion: apps/v1<br/>    kind: Deployment<br/>    name: hpa-deployment</strong><br/>  minReplicas: 1<br/>  maxReplicas: 4<br/>  metrics:<br/>  - type: Resource<br/>    resource:<br/>      name: cpu<br/>      target:<br/>        type: Utilization<br/>        averageUtilization: 80</span></pre><p id="455a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">稳定的HPA <em class="lr"> autoscaling/v1 </em> API版本仅支持基于CPU利用率的扩展。我们在这里使用的<em class="lr"> autoscaling/v2beta2 </em>版本也增加了对内存和自定义指标的支持。我们已经将上面创建的部署设置为HPA的目标(<em class="lr"> scaleTargetRef </em>)。我们将最小副本数设置为1 ( <em class="lr"> minReplicas </em>)，最大副本数设置为4 ( <em class="lr"> maxReplicas </em>)。HPA将根据CPU利用率进行扩展，阈值设置为80% ( <em class="lr">平均利用率</em>)。</p><p id="7e35" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">使用<em class="lr"> kubectl get hpa </em>命令，我们将看到以下内容:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nu"><img src="../Images/d7a62d0017523c8bf7992496f950d1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*76_t8qV8xckm4EnRiP-emQ.png"/></div></div></figure><p id="cc17" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">最初的部署是一个2副本的部署，但是因为pod的CPU利用率基本上是空闲的，HPA已经将副本的数量减少到一个副本的最低配置。我们可以通过描述HPA(使用<em class="lr"> kubectl describe hpa </em>命令)来看到它的作用:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nv"><img src="../Images/197458309ce079ad1c232cc26f8197ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hPVpAKMTScapbw8JDAxHZw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl描述hpa </strong></figcaption></figure><p id="6f25" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了测试HPA如何增加单元的数量，我们将从正在运行的单元中运行<a class="ae lq" href="https://www.hecticgeek.com/stress-test-your-ubuntu-computer-with-stress/" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> stress </em> </a>命令来加载整个CPU:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="40b5" class="mg jv iq mc b gy mh mi l mj mk"># stress -c 1</span></pre><p id="bb2d" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">现在HPA将显示以下内容(使用<em class="lr"> kubectl get hpa </em>命令):</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nw"><img src="../Images/f205335a729df78c3c3667a8a4dbc21d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pG9ra5oTIQa0dE-E5lfzqg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> <em class="my"> kubectl获取hpa </em> </strong></figcaption></figure><p id="fe96" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在上面的部署中，我们将CPU请求设置为500m个CPU。HPA阈值设置为4亿个CPU(5亿个CPU的80%，如上面的<strong class="ku ir">目标</strong>输出所示)。<em class="lr">压力</em>命令创建了一个完整CPU的负载(1000m个CPU)。处理负载所需的吊舱数量是3 (1000/400=2.5，四舍五入为3，如上面的<strong class="ku ir">副本</strong>输出所示)。结果，HPA迫使部署增加了两个吊舱。每个pod上的平均负载现在将是3.33亿个CPU(1000/3 ~ 3.33亿个CPU)，这是部署中声明的5亿个CPU请求的66%(如上面的<strong class="ku ir">目标</strong>输出所示)。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="37d2" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">集群自动缩放器</h1><p id="567d" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">HPA是一个很好的解决方案，可以处理当前运行的pod上负载的动态增加或减少。但是，我们可能会达到无法再添加更多单元的地步，因为节点本身已满负荷。</p><p id="3d7c" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">当HPA注意到阈值被突破时，它将尝试添加一个或多个pod。最后，调度程序将负责将pod调度到节点上，但是因为节点处于满负荷状态，所以这将失败。结果将是当前正在运行的pod上的高负载，而新的pod则停留在挂起状态。摆脱这种死锁的唯一方法是向集群添加更多的节点，这正是<a class="ae lq" href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler" rel="noopener ugc nofollow" target="_blank"> <em class="lr">集群自动缩放器</em> </a> (CA)的用途。</p><p id="88eb" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">集群自动缩放器(CA)基础知识</strong></p><p id="47a5" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">CA为我们完成两项任务。首先，它将监视由于缺少可用资源而处于未决状态的pod。然后，它将向Kubernetes集群添加节点以减轻负载。其次，它将通过终止未充分利用的节点来降低云成本。</p><p id="dbe7" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">CA在API-server上初始化一个<a class="ae lq" href="https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes" rel="noopener ugc nofollow" target="_blank"> <em class="lr">观察器</em> </a>。通过<em class="lr">手表</em>机制，寻找<em class="lr">可调度</em> PodCondition设置为<em class="lr"> false </em>且<em class="lr">原因</em>设置为<em class="lr">不可调度</em>的pod。当CA注意到一个pod处于这种状态时，它将尝试向群集中添加一个节点。</p><p id="d863" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在CA的术语中，<em class="lr">节点组</em>是一组具有相同容量(CPU、内存)的虚拟机(AWS中的EC2实例)。在AWS中，CA使用自动扩展组(ASG)作为其节点组。</p><p id="d50d" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">当一个节点需要被添加到ASG以容纳新的pod时，CA增加ASG <a class="ae lq" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html#cfn-as-group-desiredcapacity" rel="noopener ugc nofollow" target="_blank"> <em class="lr">期望容量</em> </a>。DesiredCapacity的增加将启动一个扩展事件，这将导致启动一个新的EC2实例。将会添加新的EC2实例，直到ASG中的当前实例数量达到CA设置的所需容量，或者达到ASG的<a class="ae lq" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html#cfn-as-group-maxsize" rel="noopener ugc nofollow" target="_blank">T5】MaxSize</a>(图2)</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nx"><img src="../Images/fce4b97620bb7ad9ff06398a467e0b03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qY-OHgudOCuGc9kyCOObvg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">图二。行动中的CA。<em class="my"> 1。</em> </strong> <em class="my"> HPA尝试向集群添加一个pod。</em> <strong class="bd jw"> <em class="my"> 2。</em> </strong> <em class="my">由于节点上缺乏资源，pod无法调度。</em> <strong class="bd jw"> <em class="my"> 3。</em></strong><em class="my">CA通过API-server上的监视器，注意到未决的pod。</em> <strong class="bd jw"> <em class="my"> 4。</em></strong><em class="my">CA增加了ASG的期望容量。</em> <strong class="bd jw"> <em class="my"> 5。</em> </strong> <em class="my">一个新的EC2实例被添加到ASG中。</em> <strong class="bd jw"> <em class="my"> 6。</em> </strong> <em class="my">新的pod被调度到新的EC2实例上。</em></figcaption></figure><p id="f7b6" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">向群集中添加新节点是一个耗时耗力的过程。为了避免启动不合适的节点，CA预先检查节点与pod请求的资源的兼容性。例如，添加一个2 CPU节点来容纳一个具有2500m CPUs的CPU请求的pod不会解决挂起pod的问题。</p><p id="fb47" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">CA还会寻找未充分利用的节点，并尝试终止它们以节省云成本。CA启动指定节点的驱逐和终止必须满足两个条件。首先，已经在指定节点上运行的pod的请求资源的总和必须小于该节点的<em class="lr">可分配</em>资源的50%。第二，在指定节点上运行的pod可以被调度到不同的节点。具有<a class="ae lq" href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets" rel="noopener ugc nofollow" target="_blank"><em class="lr">pod disruption budget</em></a>【PDB】的pod、kube-system pod、直接创建的pod(例如，不通过部署)、具有本地存储的pod以及与节点具有特定亲缘关系的pod是可以禁止终止指定节点的一些示例。</p><p id="eda0" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">如果满足上述所有条件的时间超过特定的时间段(默认为10分钟)，CA将启动驱逐和终止序列。CA一次终止一个节点，以避免出现pod没有合适的节点运行的情况。节点的驱逐是通过pod驱逐API完成的。CA将等待一段时间(默认为10分钟)让pod正常终止。在此期间之后，即使某些pod仍在运行，该节点也会终止。</p><p id="aa69" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir"> CA扩展器</strong></p><p id="b91a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">Kubernetes集群可以由一个或几个助理秘书长组成。正如我们将看到的，一些ASG可以基于<a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">按需</em> </a>实例，而一些基于<a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html#:~:text=A%20Spot%20Instance%20is%20an,is%20called%20a%20Spot%20price." rel="noopener ugc nofollow" target="_blank"> <em class="lr">当场</em> </a>实例。在多个ASG的情况下，CA需要决定应该扩大其中的哪一个。为了实现这一点，CA使用<a class="ae lq" href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-expanders" rel="noopener ugc nofollow" target="_blank"> <em class="lr">扩展器</em> </a>，它规定了CA在选择要扩展的ASG时使用的逻辑。目前，我们可以从五个可用的<a class="ae lq" href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-expanders" rel="noopener ugc nofollow" target="_blank"> <em class="lr">扩展器</em> </a>中进行选择:</p><ol class=""><li id="9857" class="nc nd iq ku b kv ls kz lt ld ne lh nf ll ng lp nh ni nj nk bi translated"><em class="lr"> Random Expander </em>:顾名思义，这个扩展器会以随机的方式将CA扩展为可用的ASG之一。这是CA使用的默认扩展器。</li><li id="4f05" class="nc nd iq ku b kv nl kz nm ld nn lh no ll np lp nh ni nj nk bi translated"><em class="lr"> Most-pods </em> <em class="lr">扩展器</em>:当要调度的pods有<a class="ae lq" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> nodeSelector </em> </a>或<a class="ae lq" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> nodeAffinity </em> </a>绑定到特定ASG的节点时，此扩展器很有用。如果CA要扩展这个特定的ASG，它将向集群引入一个新的节点，在该节点上可以调度挂起的pod。</li><li id="c2d5" class="nc nd iq ku b kv nl kz nm ld nn lh no ll np lp nh ni nj nk bi translated"><em class="lr">最少浪费</em> <em class="lr">扩展器</em>:当要调度的pods是CPU<strong class="ku ir">T5或者T7】内存受限时，这个扩展器很有用。对于CPU绑定的pod，ASG可以使用<a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">计算优化的</em> </a> EC2实例，而对于内存绑定的pod，ASG可以使用<a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/memory-optimized-instances.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">内存优化的</em> </a> EC2实例。CA将根据pods CPU或内存请求扩大具有最佳匹配的ASG。</strong></li><li id="d74a" class="nc nd iq ku b kv nl kz nm ld nn lh no ll np lp nh ni nj nk bi translated"><em class="lr">价格</em> <em class="lr">扩展器</em>:这个扩展器使用一个<a class="ae lq" href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/pricing.md" rel="noopener ugc nofollow" target="_blank"> <em class="lr">算法</em> </a>来预测哪个节点组在扩大规模时将是最划算的。目前仅在<a class="ae lq" href="https://cloud.google.com/compute" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> GCE </em> </a>和<a class="ae lq" href="https://cloud.google.com/kubernetes-engine" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> GKE </em> </a>上可用。</li><li id="8c4c" class="nc nd iq ku b kv nl kz nm ld nn lh no ll np lp nh ni nj nk bi translated"><em class="lr">优先级</em> <em class="lr">扩展器</em>:该扩展器使集群管理员能够为不同的ASG设置优先级。稍后，我们将看到如何使用这个扩展器。</li></ol><p id="b43c" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">操场</strong></p><p id="0cfa" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了查看我们的CA的运行情况，我们使用Kubernetes版本1.17部署了一个EKS集群。集群由三个<a class="ae lq" href="https://docs.aws.amazon.com/eks/latest/userguide/worker.html" rel="noopener ugc nofollow" target="_blank"><em class="lr"/></a><em class="lr"/>自管理的ASGs工作节点组成。第一组由三个按需t3.medium实例组成。第二组由来自m5.large系列(2 CPU/8GB RAM)的spot实例组成。第三个ASG由m5.xlarge系列的spot实例组成(4c pu/16GB RAM)。该架构的概述可以在图3中找到。</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ny"><img src="../Images/70a54922bc5c241d5a66a19019f6df48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aKfPSazsv90A7oVETDMoXg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">图3。Kubernetes集群由3个助理秘书长组成。</strong>每组跨越3个az。其中一个ASG由按需EC2实例组成(DesiredCapacity设置为3)。另外两个ASG由具有不同CPU和内存(2 CPU/8GB RAM和4 CPU/16GB RAM)的spot实例(<strong class="bd jw">初始所需容量设置为零</strong>)组成</figcaption></figure><p id="6490" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">使用spot实例是节省云成本的一个好方法，但是需要了解spot实例的运行方式以及如何将它们与Kubernetes集成。一个很好的起点是这个<a class="ae lq" href="https://aws.amazon.com/blogs/compute/cost-optimization-and-resilience-eks-with-spot-instances/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">博客</em> </a>，它是我们在集群<em class="lr">的配置和部署期间流动的。</em></p><p id="916d" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了区分3个ASG，我们使用Kubernetes <a class="ae lq" href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">节点标签</em> </a>。我们使用EC2 <a class="ae lq" href="https://aws.amazon.com/premiumsupport/knowledge-center/eks-worker-nodes-cluster/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">用户数据</em> </a> <em class="lr"> </em>来标注节点。对于点播组，我们使用以下用户数据:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="9774" class="mg jv iq mc b gy mh mi l mj mk">#!/bin/bash<br/>set -o xtrace<br/>/etc/eks/bootstrap.sh us-east-1-devops-eks --kubelet-extra-args --node-labels=<strong class="mc ir">env=devops,node_type=compute</strong>,subnet=private</span></pre><p id="29e2" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">对于2 CPU/8GB RAM spot ASG，用户数据为:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="d01e" class="mg jv iq mc b gy mh mi l mj mk">#!/bin/bash<br/>set -o xtrace<br/>/etc/eks/bootstrap.sh us-east-1-devops-eks --kubelet-extra-args --node-labels=<strong class="mc ir">env=devops,node_type=Spot2Cpu8Gb</strong>,subnet=private</span></pre><p id="7653" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">对于4CPUs/16GB RAM spot ASG，用户数据为:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="148c" class="mg jv iq mc b gy mh mi l mj mk">#!/bin/bash<br/>set -o xtrace<br/>/etc/eks/bootstrap.sh us-east-1-devops-eks --kubelet-extra-args --node-labels=<strong class="mc ir">env=devops,node_type=Spot4Cpu16Gb</strong>,subnet=private</span></pre><p id="17b6" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">由于spot实例可以在任何时候被AWS终止(<a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> Spot实例中断</em> </a>),我们希望我们的CA被部署在稳定的随需应变ASG上。为了实现这一点，我们将下面的<a class="ae lq" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity" rel="noopener ugc nofollow" target="_blank"><em class="lr">node affinity</em></a><em class="lr"/>添加到CA部署清单<em class="lr"> : </em></p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="0b09" class="mg jv iq mc b gy mh mi l mj mk"><strong class="mc ir">...</strong><br/>affinity:<br/>  nodeAffinity:<br/>    requiredDuringSchedulingIgnoredDuringExecution:<br/>      nodeSelectorTerms:<br/>      - matchExpressions:<br/>        - key: env<br/>          operator: In<br/>          values:<br/>          - <strong class="mc ir">devops</strong><br/>        - key: node_type<br/>          operator: In<br/>          values:<br/>          - <strong class="mc ir">compute<br/>...</strong></span></pre><p id="5290" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我们需要用两个特定的标记来标记ASG，这样CA就会发现它们，并将它们添加到其管理的ASG中。这两个标签是:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="d534" class="mg jv iq mc b gy mh mi l mj mk">k8s.io/cluster-autoscaler/<strong class="mc ir">&lt;cluster-name&gt;</strong>=owned<br/>k8s.io/cluster-autoscaler/enabled=true</span></pre><p id="3c38" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir"> &lt;集群名称&gt; </strong>应该替换为EKS集群名称。</p><p id="672f" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在我们的例子中，我们希望CA只管理基于现场的ASG。为了实现这一点，我们用上面的两个标记只标记了基于点的ASG。</p><p id="ec33" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在我们的示例中，我们还希望现场助理从零开始横向扩展，然后纵向扩展到零。为此，我们将两个现场助理秘书长的<em class="lr">所需容量</em>和<em class="lr">最小尺寸</em>设置为零。此外，我们将只希望特定的吊舱部署到现场助理秘书长。为此，我们将再次使用<em class="lr"> nodeAffinity </em>。下面显示了一个<em class="lr"> nodeAffinity </em>配置，该配置限制pod在标签为<strong class="ku ir"> env=devops </strong>和<strong class="ku ir"> node_type </strong>等于Spot2Cpu8Gb <strong class="ku ir">或</strong> Spot4Cpu16Gb的节点上运行:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="26e9" class="mg jv iq mc b gy mh mi l mj mk"><strong class="mc ir">...</strong><br/>affinity:<br/>  nodeAffinity:<br/>    requiredDuringSchedulingIgnoredDuringExecution:<br/>      nodeSelectorTerms:<br/>      - matchExpressions:<br/>        - key: env<br/>          operator: In<br/>          values:<br/>          <strong class="mc ir">- devops</strong><br/>        - key: node_type<br/>          operator: In<br/>          values:<br/>          <strong class="mc ir">- Spot2Cpu8Gb<br/>          - Spot4Cpu16Gb<br/>...</strong></span></pre><p id="e9ed" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">当CA从零开始缩放ASG时，我们需要记住一件重要的事情。Kubernetes控制平面只有在ASG节点标签启动并注册到API服务器后才会知道它们。因此，如果CA需要从零开始横向扩展ASG，它不知道ASG节点标签。在我们希望CA考虑节点标签的情况下(就像上面的<em class="lr"> nodeAffinity </em>一样)，我们需要用相关的节点标签<a class="ae lq" href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-scale-a-node-group-to-0" rel="noopener ugc nofollow" target="_blank"> <em class="lr">标记</em></a>ASG本身。在我们的示例中，对于2CPUs/8GB RAM spot ASG，我们添加了以下标记:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="a1db" class="mg jv iq mc b gy mh mi l mj mk">k8s.io/cluster-autoscaler/node-template/label/env=devops<br/>k8s.io/cluster-autoscaler/node-template/label/node_type=Spot2Cpu8Gb</span></pre><p id="70e8" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">对于4CPUs/16GB RAM spot ASG，我们添加了以下标签:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="f573" class="mg jv iq mc b gy mh mi l mj mk">k8s.io/cluster-autoscaler/node-template/label/env=devops<br/>k8s.io/cluster-autoscaler/node-template/label/node_type=Spot4Cpu16Gb</span></pre><p id="a013" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">总结一下ASG的标记，2 CPU/8GB RAM spot ASG的最终结果是:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nz"><img src="../Images/ce2018c055841a2330d0b6fa9191c520.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*83EEOTXqZy088sMu94oesg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">2 CPUs/8GB现货ASG标签</strong></figcaption></figure><p id="26ae" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">而对于4CPUs/16GB RAM spot ASG，标签为:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oa"><img src="../Images/5245523455cace13eb144d02844fe1f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HglfZpTzsW3r0oWTapYWVg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">4c pus/16GB现货ASG标签</strong></figcaption></figure><p id="9f15" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了处理spot实例中断，我们使用<a class="ae lq" href="https://github.com/aws/aws-node-termination-handler#helm" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> AWS节点终止处理程序</em> </a>。节点终止处理程序是一个<a class="ae lq" href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/" rel="noopener ugc nofollow" target="_blank"><em class="lr">DaemonSet</em></a><em class="lr"/>部署到集群中的所有节点。DaemonSet的pod利用EC2 <a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">实例元数据服务</em> </a> <em class="lr"> </em>来获得关于<a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> EC2维护事件</em> </a> <em class="lr"> </em>和<a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> EC2现场中断</em> </a> <em class="lr">的通知。</em>当在特定节点上运行的DaemonSet <em class="lr"> </em>的特定pod注意到实例即将被终止时，它将首先<a class="ae lq" href="https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration" rel="noopener ugc nofollow" target="_blank"> <em class="lr">封锁</em> </a>该节点，这将阻止调度程序将新的pod调度到它上面。然后节点终止处理器盒将<a class="ae lq" href="https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration" rel="noopener ugc nofollow" target="_blank"> <em class="lr">排空</em> </a>节点。有了这个设置，我们可以优雅地处理现场实例中断，避免应用程序的突然中断。</p><p id="752a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">测试最少浪费的膨胀机</strong></p><p id="bfc0" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我们首先部署了配置了最少浪费扩展器的CA:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="cc75" class="mg jv iq mc b gy mh mi l mj mk"><strong class="mc ir">...</strong><br/>command:<br/>  - ./cluster-autoscaler<br/>  - --v=4<br/>  - --stderrthreshold=info<br/>  - --cloud-provider=aws<br/>  - --skip-nodes-with-local-storage=false<br/>  - <strong class="mc ir">--expander=least-waste</strong><br/>  - --scale-down-unneeded-time=1m<br/>  - --<strong class="mc ir">node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/us-east-1-devops-eks<br/>...</strong></span></pre><p id="17bb" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">如前所述，CA目标的两个ASG的初始DesiredCapacity设置为零。为了测试新节点的启动，我们声明了以下部署(低cpu部署):</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="87cb" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: low-cpu-deployment<br/>spec:<br/>  <strong class="mc ir">replicas: 3</strong><br/>  strategy:<br/>    rollingUpdate:<br/>      maxSurge: 1<br/>      maxUnavailable: 0<br/>  selector:<br/>    matchLabels:<br/>      app: low-cpu-deployment<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: low-cpu-deployment<br/>    spec:<br/>      containers:<br/>      - image: ubuntu:18.04<br/>        command: ["/bin/bash", "-c", "--"]<br/>        args: ["while true; do sleep 30; done;"]<br/>        name: hpa-container<br/>        resources:<br/>          <strong class="mc ir">requests:<br/>            cpu: 500m</strong><br/>      affinity:<br/>        nodeAffinity:<br/>          requiredDuringSchedulingIgnoredDuringExecution:<br/>            nodeSelectorTerms:<br/>            - matchExpressions:<br/>              - key: env<br/>                operator: In<br/>                values:<br/>                <strong class="mc ir">- devops</strong><br/>              - key: node_type<br/>                operator: In<br/>                values:<br/>                <strong class="mc ir">- Spot2Cpu8Gb<br/>                - Spot4Cpu16Gb</strong></span></pre><p id="d5cb" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">部署声明了一个3副本，具有500m CPUs的CPU请求和将pod绑定到我们的spot ASGs的nodeAffinity。当部署应用到集群时，我们可以看到，pod最初处于挂起状态，因为没有可以部署pod的节点。几分钟后，CA完成其工作并启动一个新节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ob"><img src="../Images/f20dff375bbeb928c323fb88d3448f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6F5sO2axhmz4EqfWL6570g.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get node -Lnode_type，env </strong></figcaption></figure><p id="b9f3" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">pod本身很快被调度和部署到该节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oc"><img src="../Images/fa774c4beaaecfedbe4088ce5169a701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3gPEK46fVtqJhU5PzpsJw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get pod -owide </strong></figcaption></figure><p id="7047" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">由于每个pod请求5亿个CPU，CA从2CPUs/8GB ASG启动了一个EC2实例。接下来，我们声明了以下部署(高cpu部署):</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="e3d0" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: high-cpu-deployment<br/>spec:<br/>  <strong class="mc ir">replicas: 3</strong><br/>  strategy:<br/>    rollingUpdate:<br/>      maxSurge: 1<br/>      maxUnavailable: 0<br/>  selector:<br/>    matchLabels:<br/>      app: high-cpu-deployment<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: high-cpu-deployment<br/>    spec:<br/>      containers:<br/>      - image: ubuntu:18.04<br/>        command: ["/bin/bash", "-c", "--"]<br/>        args: ["while true; do sleep 30; done;"]<br/>        name: hpa-container<br/>        resources:<br/>          requests:<br/>            <strong class="mc ir">cpu: 3000m</strong><br/>      affinity:<br/>        nodeAffinity:<br/>          requiredDuringSchedulingIgnoredDuringExecution:<br/>            nodeSelectorTerms:<br/>            - matchExpressions:<br/>              - key: env<br/>                operator: In<br/>                values:<br/>                <strong class="mc ir">- devops</strong><br/>              - key: node_type<br/>                operator: In<br/>                values:<br/>                <strong class="mc ir">- Spot2Cpu8Gb<br/>                - Spot4Cpu16Gb</strong></span></pre><p id="9740" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">除了一个例外，此部署与之前的相同，CPU请求现在设置为3000个CPU。为了容纳单元，CA需要添加三个4CPU/16GB节点，每个节点一个单元。通过检查群集中的节点，我们可以看到三个4CPU/16GB节点如预期的那样添加到了群集中:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi od"><img src="../Images/29e4ebf9fe698046d1bea693a95c2e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*54yw8lTbHzA7tRVr0CVzBA.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get node -Lnode_type，env </strong></figcaption></figure><p id="b92c" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">部署的3个单元已计划并部署到节点，每个节点一个单元:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oe"><img src="../Images/89b5cde2b738aaf9f0a2c0ba6e8fbe03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipZ9nqce21xI-VTlge-cXQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">库贝克特尔获得pod -owide </strong></figcaption></figure><p id="6709" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了测试未充分利用的节点的终止，我们将高cpu部署副本数量从3个减少到2个。正如预测的那样，CA最终发现其中一个4CPU/16GB节点未得到充分利用，并将其终止，只剩下两个4CPU/16GB节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi of"><img src="../Images/9d1fed41bb5a17d02b6ef603e6006bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OqLuucZ8PKcuPMqSz1CCIw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">库贝克获取节点类型，环境</strong></figcaption></figure><p id="26c8" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">最后，我们删除两个部署，这最终导致CA终止两个现场ASG的所有节点，只留下按需EC2节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi og"><img src="../Images/efad4d4916951d14a4e05d959f1f3545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CgDWcgv9TU_66qfcsPu7FA.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get node -Lnode_type，env </strong></figcaption></figure><p id="2ebf" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">将HPA与CA相结合</strong></p><p id="4210" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了了解HPA如何与CA一起工作，我们首先部署了以下部署:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="c160" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: high-cpu-deployment<br/>spec:<br/>  <strong class="mc ir">replicas: 1</strong><br/>  strategy:<br/>    rollingUpdate:<br/>      maxSurge: 1<br/>      maxUnavailable: 0<br/>  selector:<br/>    matchLabels:<br/>      app: high-cpu-deployment<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: high-cpu-deployment<br/>    spec:<br/>      containers:<br/>      - image: ubuntu:18.04<br/>        command: ["/bin/bash", "-c", "--"]<br/>        args: ["while true; do sleep 30; done;"]<br/>        name: hpa-container<br/>        resources:<br/>          requests:<br/>            <strong class="mc ir">cpu: 3000m</strong><br/>      affinity:<br/>        nodeAffinity:<br/>          requiredDuringSchedulingIgnoredDuringExecution:<br/>            nodeSelectorTerms:<br/>            - matchExpressions:<br/>              - key: env<br/>                operator: In<br/>                values:<br/>                <strong class="mc ir">- devops</strong><br/>              - key: node_type<br/>                operator: In<br/>                values:<br/>                <strong class="mc ir">- Spot2Cpu8Gb<br/>                - Spot4Cpu16Gb</strong></span></pre><p id="0b13" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">该部署有一个单元，其CPU请求为3000m CPU。这触发了CA，CA添加了一个4CPU/16GB节点，并导致pod被调度并启动到该节点上。</p><p id="ec98" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">其次，我们针对上述部署配置了以下HPA:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="9518" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: autoscaling/v2beta2<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/>  name: hpa-high-cpu-deployment<br/>spec:<br/>  scaleTargetRef:<br/>    apiVersion: apps/v1<br/>    kind: Deployment<br/>    name: <strong class="mc ir">high-cpu-deployment</strong><br/>  <strong class="mc ir">minReplicas: 1<br/>  maxReplicas: 4</strong><br/>  metrics:<br/>  - type: Resource<br/>    resource:<br/>      name: cpu<br/>      target:<br/>        type: Utilization<br/>        <strong class="mc ir">averageUtilization:</strong> <strong class="mc ir">20</strong></span></pre><p id="4c90" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我们将<strong class="ku ir"> averageUtilization </strong>字段设置为所请求的CPU (600m个CPU)的20%。</p><p id="e633" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了加载pod CPU，我们使用运行pod中的<a class="ae lq" href="https://www.hecticgeek.com/stress-test-your-ubuntu-computer-with-stress/" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> stress </em> </a>命令，加载一个完整的CPU(1000m CPU):</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="31ac" class="mg jv iq mc b gy mh mi l mj mk"># stress -c 1</span></pre><p id="6d33" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">不久之后，HPA注意到了负载，并将所需的复制副本数量更改为2:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oh"><img src="../Images/9b054be7597a680f931a9e5c12dc058e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5a3D0xBGS0E_cJaz0bOE9Q.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">ku bectl get HPA HPA-高CPU-部署</strong></figcaption></figure><p id="34ab" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">因为pod需要3000m CPUs，所以新的pod需要启动一个单独的4CPU/16GB节点。CA注意到挂起的pod，并向当前运行的节点添加了另一个4CPU/16GB节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oi"><img src="../Images/fb24014557e6d2dded952b446701b094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YWY0jP-2P_drabrqUmJTMA.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get node -Lnode_type，env </strong></figcaption></figure><p id="71f6" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">最终，新的单元被调度并部署到新的节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oj"><img src="../Images/c8b9aa4483bf1fca40ce115ebe250bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mx6M4B3OGRqY_Is_p3hK5Q.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get pod -owide </strong></figcaption></figure><p id="f4b4" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">查看HPA，我们可以看到添加新pod的结果:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ok"><img src="../Images/309a46e03fd89c2b9c4fa575ea02520f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T2q9YFofhZtyxU8d5vMcPQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">ku bectl get HPA HPA-高CPU-部署</strong></figcaption></figure><p id="a51b" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">现在，我们将停止第一个节点上的应力，这将导致两个闲置的吊舱。HPA将注意到这一变化，并最终将所需的副本数量减少到1:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ol"><img src="../Images/3dce25019968fb06d7c6d091ee2984f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KkLqxbTJguGYQ_OEwYHj7g.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">ku bectl get HPA HPA-高CPU-部署</strong></figcaption></figure><p id="e2cd" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">这将留给我们一个pod和一个不需要的4CPU/16GB节点。不久之后，CA将注意到空闲节点并终止它，只给我们留下一个4CPU/16GB节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi om"><img src="../Images/a3ff7d5c550589fa396858b9ec37374f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bNLPpa9-txEFALfyeqa8Hw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get node -Lnode_type，env </strong></figcaption></figure><p id="62c7" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">基于优先级的扩展器</strong></p><p id="a2aa" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">想象一下这样一种情况，我们需要部署具有500m CPUs的CPU请求，但是能够突发到3000m CPUs的pod。我们希望不惜任何代价调度pod，即使它们被调度到无法提供3000m CPUs最大突发的节点。如果我们应该部署以下部署(突发cpu部署):</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="b332" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: burst-cpu-deployment<br/>spec:<br/>  replicas: 1<br/>  strategy:<br/>    rollingUpdate:<br/>      maxSurge: 1<br/>      maxUnavailable: 0<br/>  selector:<br/>    matchLabels:<br/>      app: burst-cpu-deployment<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: burst-cpu-deployment<br/>    spec:<br/>      containers:<br/>      - image: ubuntu:18.04<br/>        command: ["/bin/bash", "-c", "--"]<br/>        args: ["while true; do sleep 30; done;"]<br/>        name: hpa-container<br/>        resources:<br/>          <strong class="mc ir">requests:<br/>            cpu: 500m<br/>          limits:<br/>            cpu: 3000m</strong>  <br/>      affinity:<br/>        nodeAffinity:<br/>          requiredDuringSchedulingIgnoredDuringExecution:<br/>            nodeSelectorTerms:<br/>            - matchExpressions:<br/>              - key: env<br/>                operator: In<br/>                values:<br/>                - devops<br/>              - key: node_type<br/>                operator: In<br/>                values:<br/>                - Spot2Cpu8Gb<br/>                - Spot4Cpu16Gb</span></pre><p id="850d" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我们会发现，在当前的CA配置下，已经启动的节点是2CPU/8GB节点。CA查看pod(500m CPU)的请求，并选择2CPU/8GB节点。我们需要一种方法来使4CPU/16GB ASG优先于2CPU/8GB，这样我们将有一个可以承受CPU突发的节点。这正是<a class="ae lq" href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/expander/priority/readme.md" rel="noopener ugc nofollow" target="_blank"> <em class="lr">基于优先级的扩展器</em> </a>的用途。</p><p id="bdcc" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">使用基于优先级的扩展器，我们可以规定CA的优先级，其中一个ASG应该扩展到哪个CA。此扩展器需要一个用于声明ASGs优先级的配置映射。我们可以使用正则表达式来匹配ASG的名称。配置图必须命名为<em class="lr">cluster-auto scaler-priority-expander</em>。这个ConfigMap必须部署到我们已经部署了CA的同一个名称空间中(在我们的例子中是kube-system)。对配置映射的更改会即时应用到CA(无需在更改配置映射后重新部署CA)。</p><p id="0576" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我们已经声明了以下配置映射:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="d567" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: v1<br/>kind: ConfigMap<br/>metadata:<br/>  <strong class="mc ir">name: cluster-autoscaler-priority-expander<br/>  namespace: kube-system</strong><br/>data:<br/>  <strong class="mc ir">priorities: |-<br/>    100: <br/>      - Spot2Cpu8Gb<br/>    200: <br/>      - Spot4Cpu16Gb</strong></span></pre><p id="7288" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">优先级</strong>是正整数，其中较高的值意味着较高的优先级。我们为2CPU/8GB ASG设置了100的优先级，为4CPU/16GB ASG设置了200的优先级。我们还将CA配置更改为以下内容:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="c6ac" class="mg jv iq mc b gy mh mi l mj mk"><strong class="mc ir">...<br/></strong>command:<br/>  - ./cluster-autoscaler<br/>  - --v=4<br/>  - --stderrthreshold=info<br/>  - --cloud-provider=aws<br/>  - --skip-nodes-with-local-storage=false<br/>  - <strong class="mc ir">--expander=priority</strong><br/>  - --scale-down-unneeded-time=1m<br/>  - --<strong class="mc ir">node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/us-east-1-devops-eks</strong><br/><strong class="mc ir">...</strong></span></pre><p id="6364" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">接下来，我们必须重新部署可突发的pod(突发cpu部署),不久之后，我们可以看到CA已经启动了一个4CPU/16GB节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi on"><img src="../Images/e0afb64bab78c1242e410e95a7faa638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOKxDvXYCmunJuiWsrRMeQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get node -Lnode_type，env </strong></figcaption></figure><p id="1fac" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">pod已计划并部署到此节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oo"><img src="../Images/f382d4a0b6b6cff41ca1ef27359b54a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GeHzFIFFwljw9w2OmLdOyw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl获取pod -owide </strong></figcaption></figure><p id="fddf" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir"> Pod优先级和抢占</strong></p><p id="edf0" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">有些情况下，我们需要Kubernetes调度程序从一个节点中驱逐pods，以便可以调度其他更重要的pods。这是通过设置<a class="ae lq" href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> pod优先级</em> </a>来实现的。当具有较高优先级的pod需要被调度到具有低资源的节点上时，调度器将<em class="lr">抢占</em>(驱逐)较低优先级的pod以释放资源。</p><p id="88bf" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">通过在减速箱内的<em class="lr"> priorityClassName </em>中指定<a class="ae lq" href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass" rel="noopener ugc nofollow" target="_blank"><em class="lr">priority class</em></a><em class="lr"/>对象名称，将优先级附加到减速箱。让我们看一个<em class="lr">优先级</em>对象减速的例子:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="7714" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: scheduling.k8s.io/v1<br/>kind: PriorityClass<br/>metadata:<br/>  name: cluster-default<br/><strong class="mc ir">value: 50</strong><br/><strong class="mc ir">globalDefault: true</strong><br/>description: "Cluster default priority class."</span></pre><p id="dc05" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">优先级在<strong class="ku ir">值</strong>整数字段中设置，可以设置为小于或等于10亿的值。值越大，优先级越高。在上面的例子中，我们已经将布尔型<strong class="ku ir">全局默认</strong>字段设置为真。这将对集群中没有配置<em class="lr"> priorityClassName </em>的任何pod强制执行<strong class="ku ir"> value </strong>字段中指定的优先级(在我们的示例中为50)。如果集群缺少<em class="lr"> PriorityClass </em>且<strong class="ku ir"> globalDefault </strong>设置为true，则没有定义<em class="lr"> priorityClassName </em>的pod的优先级将默认为零。</p><p id="46ee" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">优先级也会影响pod的调度顺序。当需要调度pod时，它们被放入调度队列中。Kubernetes调度程序从队列中挑选pod，并尝试为它们安装合适的节点。在较低和较高优先级的单元被添加到调度队列的情况下，调度器将首先尝试调度较高优先级的单元。如果由于任何原因较高优先级的pod调度失败(例如，缺少节点资源和没有可用于抢占的pod)，调度器将对pod应用回退，并且调度器将尝试调度较低优先级的pod。</p><p id="c2a4" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在某些情况下，调度程序不会考虑<a class="ae lq" href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> Pod中断预算</em> </a> (PDB，我们稍后会看到)<em class="lr"/>Pod被抢占。例如，如果调度程序找不到替代的方法，一个pod将被驱逐以支持更高优先级的方法，即使它的抢占违反了它的PDB。</p><p id="d8b5" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">可能存在这样的情况，即我们不希望特定的挂起pod触发CA扩展序列。为了解决这个问题，CA支持优先级切断。如果处于未决状态的pod的优先级低于截止值(默认情况下为10)，CA将不会启动它们的扩大。此外，如果节点被CA指定为终止，则优先级低于截止的pod将不会阻止规模缩小。</p><p id="3182" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">现在，让我们来看看pod优先级和抢占是如何用于过度配置集群节点的。</p><p id="09c8" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated"><strong class="ku ir">延迟敏感型应用和过度配置</strong></p><p id="a583" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在具有大量不可预测的负载峰值的延迟敏感型应用程序中，CA本身不会给出足够快的响应。纵向扩展延迟(从pod转换到挂起状态到它开始处理请求的时间)可能相当长(以分钟为单位)。纵向扩展延迟可以分为三个部分。</p><p id="a194" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">第一部分是CA注意到有一个pod处于挂起状态并决定扩大哪个ASG所需的时间。这对于小型集群(少于100个节点，每个节点30个机架)平均需要5秒，对于大型集群(100–1000个节点，每个节点30个机架)平均需要15秒。Pod <em class="lr">亲和</em>和<em class="lr">反亲和</em>可能会严重影响这个阶段。第二部分是云提供商调配节点所需的时间，可能需要几分钟。第三部分是Kubelet向集群注册、提取pod的容器映像并启动它们所花费的时间。这也可能需要几分钟，取决于容器图像的大小和应用程序的启动时间。</p><p id="0c0a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">我们可以通过减少第二和第三部分来减少纵向扩展延迟。对于第三部分，我们可以减小容器图像大小并优化应用程序启动时间。第二部分将要求我们保留一个热节点池(<a class="ae lq" href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler" rel="noopener ugc nofollow" target="_blank"> <em class="lr">过度供应</em> </a>)，以便可以立即调度和启动应用程序容器。为了实现这一点，我们利用pod优先级和先占权。</p><p id="44ed" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了超额供应，我们使用一个低优先级的部署运行pod(图4)。CA将添加节点来容纳低优先级的pods(热节点池)。如果出现峰值，HPA将增加实际应用程序部署的副本数量。优先级高于当前正在运行的pod的应用程序pod将抢占正在运行的pod，并启动到热池节点上。因此，我们将让低优先级的pod处于未决状态。注意到处于未决状态的pod，CA将向热池添加更多的节点，以容纳低优先级的pod。</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi op"><img src="../Images/e3c5bc2747f44e1b78ca82f3db91afe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EDADHMdd2u1xc94I"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">图4。</strong> <strong class="bd jw">过度配置。</strong> <strong class="bd jw"> 1。</strong>由于负载激增，HPA向应用程序部署添加了一个副本。<strong class="bd jw"> 2。</strong>新的pod被调度到一个热池节点上。<strong class="bd jw"> 3。</strong>由于新的pod比当前运行的pod具有更高的优先级，低优先级的pod被抢占。<strong class="bd jw"> 4。</strong>低优先级pod转换到待定状态。<strong class="bd jw"> 5。</strong>CA注意到有一个待定的pod。<strong class="bd jw"> 6。</strong>CA向热池添加一个新节点。<strong class="bd jw"> 7。</strong>低优先级pod被调度并启动到新的热池节点。</figcaption></figure><p id="261c" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">让我们创建一个上面的例子。我们首先声明下面的<strong class="ku ir"> PriorityClass </strong>:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="97d9" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: scheduling.k8s.io/v1<br/>kind: PriorityClass<br/>metadata:<br/>  <strong class="mc ir">name: overprovisioning</strong><br/><strong class="mc ir">value: -1</strong><br/>globalDefault: false<br/>description: "Warm pool overprovision nodes priority class."</span></pre><p id="892a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在这个例子中，我们将优先级<strong class="ku ir">值</strong> we设置为-1。请确保不要将该值设置为低于CA截止值(默认情况下为-10)，因为在这种情况下CA不会启动扩大。</p><p id="56fd" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">接下来，我们为低优先级pod声明一个部署。我们使用<a class="ae lq" href="https://groups.google.com/g/kubernetes-users/c/jVjv0QK4b_o" rel="noopener ugc nofollow" target="_blank"> <em class="lr">暂停</em> </a>容器作为pod的唯一容器，并将<strong class="ku ir"> priorityClassName </strong>设置为我们在上面声明的<em class="lr"> PriorityClass </em>的名称。</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="3fee" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: overprovisioning-low-priority-deployment<br/>spec:<br/>  replicas: 1<br/>  strategy:<br/>    rollingUpdate:<br/>      maxSurge: 1<br/>      maxUnavailable: 0<br/>  selector:<br/>    matchLabels:<br/>      app: overprovisioning-low-priority-deployment<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: overprovisioning-low-priority-deployment<br/>    spec:<br/>      <strong class="mc ir">priorityClassName: overprovisioning</strong><br/>      containers:<br/>      <strong class="mc ir">- image: 602401143452.dkr.ecr.us-east-   1.amazonaws.com/eks/pause-amd64:3.1</strong><br/>        name: overprovisioning-low-priority<br/>        resources:<br/>          requests:<br/>            cpu: 3000m<br/>      affinity:<br/>        nodeAffinity:<br/>          requiredDuringSchedulingIgnoredDuringExecution:<br/>            nodeSelectorTerms:<br/>            - matchExpressions:<br/>              - key: env<br/>                operator: In<br/>                values:<br/>                - devops<br/>              - key: node_type<br/>                operator: In<br/>                values:<br/>                - Spot2Cpu8Gb<br/>                - Spot4Cpu16Gb</span></pre><p id="1dde" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">不久之后，CA将启动一个节点，低优先级pod被调度并启动到该节点上。接下来，我们声明以下部署:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="1635" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: high-cpu-deployment<br/>spec:<br/>  replicas: 1<br/>  strategy:<br/>    rollingUpdate:<br/>      maxSurge: 1<br/>      maxUnavailable: 0<br/>  selector:<br/>    matchLabels:<br/>      app: high-cpu-deployment<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: high-cpu-deployment<br/>    spec:<br/>      containers:<br/>      - image: ubuntu:18.04<br/>        command: ["/bin/bash", "-c", "--"]<br/>        args: ["while true; do sleep 30; done;"]<br/>        name: hpa-container<br/>        resources:<br/>          requests:<br/>            cpu: 3000m<br/>      affinity:<br/>        nodeAffinity:<br/>          requiredDuringSchedulingIgnoredDuringExecution:<br/>            nodeSelectorTerms:<br/>            - matchExpressions:<br/>              - key: env<br/>                operator: In<br/>                values:<br/>                - devops<br/>              - key: node_type<br/>                operator: In<br/>                values:<br/>                - Spot2Cpu8Gb<br/>                - Spot4Cpu16Gb</span></pre><p id="2569" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">发生了两件事。首先，低优先级pod被抢占，并转换到挂起状态。其次，应用程序窗格被调度并启动到节点:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oq"><img src="../Images/0c87fa9b41580e413676fae250e19a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i8-ubaALEMF3uSq9yvh_CA.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl获取pod -owide </strong></figcaption></figure><p id="90d6" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">注意到低优先级pod处于挂起状态的CA已经向热池添加了一个节点，并且低优先级pod被调度并启动到这个新节点上:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi or"><img src="../Images/3ff9867a8576d5c93b1fbb6556a8b796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHY9PeFpbGgF5lKqvwBOpA.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl获取pod -owide </strong></figcaption></figure><p id="80f0" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">接下来，我们针对应用程序部署声明了以下HPA:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="ab52" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: autoscaling/v2beta2<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/>  name: hpa-high-cpu-deployment<br/>spec:<br/>  scaleTargetRef:<br/>    apiVersion: apps/v1<br/>    kind: Deployment<br/>    name: <strong class="mc ir">high-cpu-deployment</strong><br/>  minReplicas: 1<br/>  maxReplicas: 4<br/>  metrics:<br/>  - type: Resource<br/>    resource:<br/>      name: cpu<br/>      target:<br/>        type: Utilization<br/>        averageUtilization: 20</span></pre><p id="4cde" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">现在我们将加载应用程序窗格，从运行窗格中使用<a class="ae lq" href="https://www.hecticgeek.com/stress-test-your-ubuntu-computer-with-stress/" rel="noopener ugc nofollow" target="_blank"><em class="lr">stress</em></a><em class="lr"/>命令来加载一个完整的CPU (1000m个CPU):</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="1669" class="mg jv iq mc b gy mh mi l mj mk"># stress -c 1</span></pre><p id="1c20" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">HPA注意到了负载的变化，并向部署中添加了第二个副本:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi os"><img src="../Images/2b6c4484696a6e614a3abd1bfded39e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NGsY3SQWAA3RLLSbgKjgVg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">ku bectl get HPA HPA-高CPU-部署</strong></figcaption></figure><p id="3f3d" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">这将强制应用程序部署再添加一个pod。通过抢占低优先级pod，新pod被调度并启动到热池节点上:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oj"><img src="../Images/7399e5207e4b948b9a1080c71668e38d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6HjRYhPvnINhIBvJwyBqQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get pod -owide </strong></figcaption></figure><p id="0f32" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">同样，CA将向热池添加另一个节点，保持稳定的热池大小:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ot"><img src="../Images/3a3a6640cc111fb967a95b7d911004f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dui0ghE72qeLm-SgknCoPg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"> kubectl get node -Lenv，node_type </strong></figcaption></figure></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="0787" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">Pod中断预算(PDB)</h1><p id="40d9" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/High_availability#:~:text=High%20availability%20(HA)%20is%20a,increased%20reliance%20on%20these%20systems." rel="noopener ugc nofollow" target="_blank"> <em class="lr">高可用性</em> </a> (HA)是关键任务应用的支柱之一。在AWS中，最佳实践是将一个应用程序的几个副本分散到在不同的<a class="ae lq" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">可用性区域</em> </a> (AZs)中运行的EC2实例上(通常通过使用ASG)，并在它们前面放置一个负载平衡器。如果一个实例崩溃，EC2服务将启动一个新的EC2实例作为替代。它还将尝试<a class="ae lq" href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html#arch-AutoScalingMultiAZ" rel="noopener ugc nofollow" target="_blank"> <em class="lr">在az之间重新平衡</em></a>EC2实例，这样我们将尽可能均匀地分布EC2实例。</p><p id="8a66" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了在Kubernetes中部署HA应用程序，我们通常使用一个ASG将工作节点启动到不同的az中。然后，我们尝试通过使用<a class="ae lq" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity" rel="noopener ugc nofollow" target="_blank"><em class="lr">【pod antiaffinity】</em></a><em class="lr"/>(图5) <em class="lr">，在不同az中的节点之间均匀分布应用程序的副本(pod)。</em>但是我们如何确保我们总是运行最小数量的副本呢？集群管理员和<a class="ae lq" href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler" rel="noopener ugc nofollow" target="_blank"> <em class="lr">集群自动缩放器</em> </a> (CA)可以<a class="ae lq" href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">驱逐</em> </a>节点。<a class="ae lq" href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> Pod中断预算</em> </a> (PDB)可以保护我们的HA应用。</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ou"><img src="../Images/a70fafb6b6ff6af2695417a445e0d427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dQggYWSaYLAxkB4jCwH1Rg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">图5。</strong>三个副本的Kubernetes HA应用程序在三个可用性区域自动缓存组上的每个可用性区域运行一个副本。</figcaption></figure><p id="3a63" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">有了PDB，我们可以确保在给定时间内始终有最少数量的pod可用。如果这违反了PDB ( <a class="ae lq" href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#voluntary-and-involuntary-disruptions" rel="noopener ugc nofollow" target="_blank"> <em class="lr">自愿中断</em> </a>)，PDB将阻止CA甚至管理员驱逐pod。当然，PDB不会在物理机崩溃或网络分区(<a class="ae lq" href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#voluntary-and-involuntary-disruptions" rel="noopener ugc nofollow" target="_blank"> <em class="lr">非自愿中断</em> </a>)等事件中提供帮助。因此，在为应用程序设置PDB时，我们必须考虑pods在不受Kubernetes自身控制的情况下终止的情况。</p><p id="bb26" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在图6中，我们可以看到一个示例，由于资源消耗较低，HPA终止了两个副本应用程序单元中的一个，只剩下一个副本可用。</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ov"><img src="../Images/88fb7d1485ab3389b7513f56f7ad0d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*84dLacfDkka2WG02"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">图6。</strong>由于资源消耗较低，两个复制副本应用程序箱中的一个被HPA终止。</figcaption></figure><p id="8882" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">现在，假设在应用程序进入几分钟后，CA指定最后一个pod正在其上运行的节点终止。CA将开始从该节点驱逐所有的pod。在我们没有PDB的情况下，应用程序的最后一个复制副本将被终止，并且应用程序将不可用，直到pod部署到另一个节点。如果我们为PDB配置了最少一个pod，那么CA操作将会被阻止(图7)。</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div class="ab gu cl ow"><img src="../Images/7499d3c62e1305667cc6fab7a7f05ddb.png" data-original-src="https://miro.medium.com/v2/0*cmnvWprAryjujHVa"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">图7。</strong>CA指定终止的节点，并试图驱逐pod。PDB阻止pod驱逐并维护一个副本工作应用程序。</figcaption></figure><p id="5f16" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">让我们通过部署以下3个副本部署来看一个示例:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="e505" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: pdb-deployment<br/>  <strong class="mc ir">labels:<br/>    app: pdb-deployment</strong><br/>spec:<br/>  replicas: 3<br/>  strategy:<br/>    rollingUpdate:<br/>      maxSurge: 1<br/>      maxUnavailable: 0<br/>  selector:<br/>    matchLabels:<br/>      app: pdb-deployment<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: pdb-deployment<br/>    spec:<br/>      containers:<br/>      - image: ubuntu:18.04<br/>        command: ["/bin/bash", "-c", "--"]<br/>        args: ["while true; do sleep 30; done;"]<br/>        name: pdb-container<br/>        <strong class="mc ir">resources:<br/>          requests:<br/>            cpu: 500m<br/>          limits:<br/>            cpu: 500m</strong></span></pre><p id="98e9" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">部署被标记为<strong class="ku ir"> app: pdb-deployment </strong>，并且容器具有CPU请求和500m CPUs的限制。接下来，我们配置PDB:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="c80d" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: policy/v1beta1<br/>kind: PodDisruptionBudget<br/>metadata:<br/>  name: test-pdb<br/>spec:<br/>  <strong class="mc ir">minAvailable: 2</strong><br/>  <strong class="mc ir">selector:<br/>    matchLabels:<br/>      app: pdb-deployment</strong></span></pre><p id="6126" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在PDB中，我们可以将可用副本的最小数量或不可用副本的最大数量设置为整数或百分比。我们用<strong class="ku ir"> minAvailable </strong>字段设置了可用副本的最小数量(在上面的例子中，它被设置为2)。我们用<strong class="ku ir"> maxUnavailable </strong>字段设置不可用副本的最大数量。</p><p id="3a55" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">PDB保护通过部署(如我们的示例)、副本集和状态集部署的副本。我们使用<strong class="ku ir"> matchLabels </strong>选择器来创建两者之间的绑定(在我们的示例中，这被设置为<strong class="ku ir"> app: pdb-deployment </strong>，以匹配部署标签<strong class="ku ir"> </strong>)。</p><p id="e05a" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">描述PDB(使用<em class="lr"> kubectl describe pdb </em>命令)将向我们展示以下内容:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ox"><img src="../Images/6631951401881675015ae27cba56a166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KgMCO4fqpeHRCaZ5fL1vEg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw">T3】ku bectl描述pdb测试-pdbT5】</strong></figcaption></figure><p id="8a8d" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">因为我们有3个副本部署，并且PDB <strong class="ku ir"> minAvailable </strong>字段设置为2，所以<strong class="ku ir"> Allowed disruptions </strong>字段的值计算为1。这意味着我们一次最多可以从部署中删除1个复制副本。</p><p id="11d6" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">为了完成我们的示例，让我们部署以下HPA配置:</p><pre class="lx ly lz ma gt mb mc md me aw mf bi"><span id="6ba4" class="mg jv iq mc b gy mh mi l mj mk">apiVersion: autoscaling/v2beta2<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/>  name: hpa-pdb<br/>spec:<br/>  scaleTargetRef:<br/>    apiVersion: apps/v1<br/>    kind: Deployment<br/>    name: pdb-deployment<br/>  <strong class="mc ir">minReplicas: 1</strong><br/>  maxReplicas: 4<br/>  metrics:<br/>  - type: Resource<br/>    resource:<br/>      name: cpu<br/>      target:<br/>        type: Utilization<br/>        averageUtilization: 80</span></pre><p id="dca2" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">HPA将副本的最小数量设置为1，并以我们的部署为目标。由于部署中的单元处于CPU空闲状态，HPA将尝试将复制副本的数量缩减到最少1个。过了一会儿，我们会看到只剩下部署的一个吊舱。这与我们配置的PDB相矛盾。描述PDB(使用<em class="lr"> kubectl describe pdb </em>命令)将向我们展示以下内容:</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oy"><img src="../Images/bc6caab78f5bd85b0cdbac369b70b478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lPfpKRXe8IzJge1PVVPpOA.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><em class="my"> kubectl描述pdb测试-pdb </em></figcaption></figure><p id="156e" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">如我们所见，HPA不支持PDB，因此我们在为HA应用程序配置HPA时需要小心。现在，让我们尝试清空剩下最后一个副本的节点(使用<em class="lr"> kubectl drain </em>命令):</p><figure class="lx ly lz ma gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oz"><img src="../Images/efbd4d5a87de3f734f9e9c66f6f70a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*maay9KQiZeEwyLHKsNHyKw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><strong class="bd jw"><em class="my"/></strong></figcaption></figure><p id="5695" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">在这种情况下，PDB阻止了我们对节点进行排水的尝试。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="7542" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结论</h1><p id="3725" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Kubernetes让我们管理从容器级别到节点级别的资源的粒度是惊人的。使用我们上面描述的工具和模式，集群管理员可以达到很高的控制水平。</p><p id="0f9c" class="pw-post-body-paragraph ks kt iq ku b kv ls kx ky kz lt lb lc ld lu lf lg lh lv lj lk ll lw ln lo lp ij bi translated">稳定性、低延迟和高可用性通常是现代应用程序的必备条件。将Kubernetes的正确附加组件和最佳实践与云的力量相结合，使我们能够构建支持这些应用的基础架构。</p></div></div>    
</body>
</html>