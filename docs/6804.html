<html>
<head>
<title>The Magic of Machine Learning: Gradient Descent Explained Simply but With All Math</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的魔力:梯度下降解释简单，但所有的数学</h1>
<blockquote>原文：<a href="https://itnext.io/the-magic-of-machine-learning-gradient-descent-explained-simply-but-with-all-math-f19352f5e73c?source=collection_archive---------0-----------------------#2022-03-03">https://itnext.io/the-magic-of-machine-learning-gradient-descent-explained-simply-but-with-all-math-f19352f5e73c?source=collection_archive---------0-----------------------#2022-03-03</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><p id="64b6" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">从头开始用梯度下降代码</p><p id="64ce" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated"><strong class="jq is">简介</strong></p><p id="5df7" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">梯度下降算法是许多机器学习算法的本质，尤其是在神经网络以及任何预测任务中。这种算法用于许多人工智能应用，从人脸识别到其他计算机视觉产品，用于不同的预测，既用于连续目标的预测(即回归，如价格)，也用于分类目标的预测(即某些结果的概率)。</p><p id="1ed7" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">在梯度下降算法中，部分是梯度，也就是说，简单地说，两个参数之间的变化和下降之间的关系，这种关系中的某个点使某个期望的参数最小化。但是我们想要最小化什么呢？成本函数。成本函数是什么？</p><p id="a827" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated"><strong class="jq is">成本函数</strong></p><p id="c592" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">在具有连续目标的任务中，成本函数是损失误差函数(通常称为均方误差)，而在具有分类目标的任务中，损失函数是交叉熵。我们将在这里集中讨论第一个问题。</p><p id="badb" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">利用模型m，我们得到每行(数据点)的一些预测值<strong class="jq is">y’</strong>。对于我们的每个数据点，我们可以看到每个数据点的预测与真实值的差异(<strong class="jq is"> y </strong>)。</p><p id="6d17" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">J = (∑(y'-y) )/n</p><p id="6881" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">其中n是数据点的数量(即我们样本的大小)</p><p id="6990" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated"><strong class="jq is">梯度下降的目标是找到最佳权重</strong></p><p id="b70a" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">假设我们有一个特征(预测值)— x。我们的预测值(<strong class="jq is">y’)</strong>取决于该预测，但是我们必须找到给出预测值的权重(𝛉):</p><p id="87cc" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">y' = 𝛉⁰ + 𝛉*x</p><p id="53ed" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">该公式适用于每个数据点(即，如果我们希望根据学习时间预测学生成绩，则适用于每个学生)。但是最优𝛉对于每个数据点(学生，<em class="km"> i </em>)都是相同的，因为我们可以将成本函数改写为:</p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj kn"><img src="../Images/67bd1e04d2cecc7ff4904e14dd4236c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*Xpw4Gqt2UxGm5aq5E4GWYQ.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">加权𝛉.的成本函数</figcaption></figure><p id="6e92" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">因为在这个例子中我们只有一个特征，所以总成本函数的导数和成本函数的偏导数是相同的，但是实际上，通常成本函数的总导数是由所有偏导数构成的。但是导数是什么呢？我们如何获得最佳𝛉，使我们的误差函数最小(本例中为MSE)？通过衍生！</p><p id="117d" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated"><strong class="jq is">函数的导数</strong></p><p id="b3e7" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">梯度基本上是某个函数的斜率，函数变化的度量。为了得到梯度，我们必须计算两个参数的导数，在这种情况下是成本函数(j)和(𝛉).)基本上，我们想找到误差函数最小的𝛉的值。</p><p id="3675" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">函数的斜率是𝛥𝛉和𝛥J (𝛥𝛉/𝛥J).)之间的比率我们希望相对于𝛥J的梯度尽可能小，因为在这一点上，我们将得到我们想要的j的最小值，因为我们希望误差函数尽可能小。在这个简单的例子中，可能是这样的，但是即使对于更复杂的情况，规则也是相似的。</p><p id="82f5" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">更一般地，线性和非线性函数的导数可以用以下公式计算:</p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj kz"><img src="../Images/04b1c3c4c0f0e5cffa4d3609f5c2f1dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*oGcuOIRFEaQcEyePvarRUw.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">导数𝛥x的计算</figcaption></figure><p id="4f75" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">其中<em class="km"> df(x)/dx </em>是我们将用于表示函数相对于x的导数的符号。我们可以注意到，当𝛥x接近0时，这是正确的。另一方面，(f(x)+𝛥x ) — f(x)对应于给定x和𝛥x.的𝛥y</p><p id="1e0b" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated"><strong class="jq is">常数、线性和非线性函数的梯度。</strong></p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj la"><img src="../Images/06e24def202756f35faa080cf05bf20a.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*UrfnXBqsRJ_Cs9AtiUwhpQ.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">简单线性函数:y = 3x + 1</figcaption></figure><p id="6d2d" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">线性函数的导数始终等于回归权重，在本例中为3。这意味着<strong class="jq is"> y </strong>比x变化快3倍，这个梯度值是恒定的，不变，因为我们有线性函数，即y的线性恒定变化依赖于x</p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj lb"><img src="../Images/e37a9964071e87b78d0f68199d92f8eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*lyqGRxgnH1b9keKU5v-y1A.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">上述线性函数(y = 3x + 1)的梯度计算。</figcaption></figure><p id="4703" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">对于非线性二次函数，我们最终会有梯度(斜率)的变化:</p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj lc"><img src="../Images/1def4e815601875bb5b1b2b0a4ddaea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*H14I9uZSvmirJeMLhkqs7g.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">蓝色—原始功能；橙色—一阶导数；红色-0渐变</figcaption></figure><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gi gj ld"><img src="../Images/e7d1f293e49d04623bcc31e763bc3ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1pk5WxKHOjZJZgIYDA6ktw.png"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">x的导数。</figcaption></figure><p id="9897" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">从上图可以看出，斜率取决于x的值，并且它会不断变化，取决于x。因此，我们可以使用梯度作为方向的标志，以便最小化y(或成本函数)。当x为负时，斜率为负。当x很大且为负时，该值很大且为负，当x =0时，该值增加直至达到0。因此，我们可以看到，只要看一下图，<strong class="jq is">x和斜率(梯度)之间有关系。在ML问题中，我们希望找到使成本函数</strong>最小化的 𝛉 <strong class="jq is">的值，即最小梯度。现在让我们看看我们真正感兴趣的是什么:成本函数的导数。</strong></p><p id="6958" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated"><strong class="jq is">成本函数的导数</strong></p><p id="156f" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">让我们区分这个成本函数J(𝛉).可以写成dJ(𝛉)/d𝛉，意思是对𝛉:做了区分</p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj li"><img src="../Images/4d9d221221ebe1900fcaac6da5da6459.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*K6GFNOb7UvsfrXD5-pDTDQ.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">成本函数的导数:第一个方程。</figcaption></figure><p id="7f78" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">我们可以从导数中得出，因为乘法按常数规律进行。也可以使用求和规则从导数中提取总和。</p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj li"><img src="../Images/77f311b553792f61c2287388467a1d66.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*HtTRbKs2CV-fpX5Euf4a1g.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">下一步</figcaption></figure><p id="12ca" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">为了区分这个函数，我们需要把它分离为两个函数的组合。因此，让我们创建以下函数:</p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj lj"><img src="../Images/bab8a706ad052c89ed34e97f5a739254.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*2OCa9tTuYSRKMlLWKwCmKw.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">每个数据点的变换。</figcaption></figure><p id="3d3a" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">进一步求解，使用链式法则计算每个数据点，我们将计算dJ(𝛉相对于u(𝛉,i):的导数</p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj lk"><img src="../Images/006c0a255ffbb9195b9ed4b2aefd4650.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*d19v8jqHUTZYP6eKyo3_jQ.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">关于每个数据点(du)。</figcaption></figure><p id="862c" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">现在它变得有点复杂，但逻辑是清晰而简单的。完整的方程式你可以在这里看到。</p><p id="4ad8" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">最后，我们得到:</p><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj lk"><img src="../Images/607bbff2aebb5d64abf8792f02204074.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*vK-4wseL80FicOXupXtu4A.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">成本函数的最终导数。</figcaption></figure><p id="003a" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">dJ(𝛉)/d𝛉函数是我们关注的重点，因为它采用了我们想要优化的参数𝛉的值，并返回了𝛉.值的成本函数的正切斜率斜率告诉我们最小化成本的方向。</p><p id="da30" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated"><strong class="jq is">编程梯度下降从零开始</strong></p><p id="60e7" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">现在，我们将创建一个简单的函数来实现线性回归的所有功能。这远比你想象的简单！</p><p id="bd3f" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">让我们首先简单地写出误差的计算，即成本函数的导数:</p><pre class="ko kp kq kr gu lm ln lo lp aw lq bi"><span id="3c29" class="lr ls ir ln b gz lt lu l lv lw"><strong class="ln is">def</strong> calculate_error_j_(theta1, x_i, y_i):<br/>    <strong class="ln is">return</strong> ((theta1 <strong class="ln is">*</strong> x_i) <strong class="ln is">-</strong> y_i) <strong class="ln is">*</strong> x_i</span></pre><p id="93fe" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">它需要一些权重、预测特征和目标。简单。</p><p id="cdbd" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">好的，但是现在我们必须提到几个新概念。这种斜率计算可以在不同的时间间隔进行，即𝛉中的“跳跃”，这可以通过称为<strong class="jq is">学习率</strong>的参数来调节。该参数基本上规定了在𝛉观测斜率变化时的跳跃幅度。这是有代价的——学习率越高，跳跃越大，计算越快，但它可以过冲并监督最优点，即成本函数的最小值，因此它可以有不太精确的解决方案。此外，这些具有不同跳转的更新是通过被称为<strong class="jq is">时期</strong>的迭代来完成的。一些高级梯度下降算法，如ADAM、RMSProp等，改变学习速率，当斜率较小时，学习速率变小，从而围绕最佳𝛉值进行一些微调。</p><p id="e689" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">好了，我们准备好了，我们将创建一个函数，它将在几个不同的步骤中计算和更新权重。</p><pre class="ko kp kq kr gu lm ln lo lp aw lq bi"><span id="19ba" class="lr ls ir ln b gz lt lu l lv lw"><strong class="ln is">def</strong> linear_regression(x, y, b<strong class="ln is">=</strong>0, b0<strong class="ln is">=</strong>0, epochs<strong class="ln is">=</strong>1000, learning_rate<strong class="ln is">=</strong>0.001):<br/>    N <strong class="ln is">=</strong> float(len(y))<br/>    <strong class="ln is">for</strong> i <strong class="ln is">in</strong> range(epochs):<br/>        y_predicted <strong class="ln is">=</strong> b0 <strong class="ln is">+</strong> (b <strong class="ln is">*</strong> X)<br/>        cost <strong class="ln is">=</strong> sum([data<strong class="ln is">**</strong>2 <strong class="ln is">for</strong> data <strong class="ln is">in</strong> (y<strong class="ln is">-</strong>y_predicted)]) <strong class="ln is">/</strong> N<br/>        b_gradient <strong class="ln is">=</strong> <strong class="ln is">-</strong>(2<strong class="ln is">/</strong>N) <strong class="ln is">*</strong> sum(X <strong class="ln is">*</strong> (y <strong class="ln is">-</strong> y_predicted))<br/>        b0_gradient <strong class="ln is">=</strong> <strong class="ln is">-</strong>(2<strong class="ln is">/</strong>N) <strong class="ln is">*</strong> sum(y <strong class="ln is">-</strong> y_predicted)<br/>        b <strong class="ln is">=</strong> b <strong class="ln is">-</strong> (learning_rate <strong class="ln is">*</strong> b_gradient)<br/>        b0 <strong class="ln is">=</strong> b0 <strong class="ln is">-</strong> (learning_rate <strong class="ln is">*</strong> b0_gradient)<br/><em class="km">#         print(f'b0:{b0}', f'b:{b}')</em><br/>    <strong class="ln is">return</strong> b, b0, cost</span></pre><p id="4e14" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">该函数采用真实的x(特征预测)和y(目标)值，以及时期数和学习率，然后在每个时期结束时进行迭代和𝛉更新。𝛉在代码中表示为<strong class="jq is"> b </strong>，而b0是线性函数中的截距。首先计算预测值(y_predicted)，然后计算成本函数(MSE)，然后根据上述公式计算梯度。然后<strong class="jq is"> b </strong>由该梯度更新，并且该更新的强度由学习率调节。然后，该过程针对新的时期重复自身，直到时期数结束，而在这种情况下，我们预计梯度将为0，直到现在，因此它将达到b的最优值。这是机器学习的揭示秘密，尽管在更复杂的示例中它可能复杂得多。</p><p id="bbc3" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">所以，让我们制造一些假数据，看看这个简单的算法收敛到𝛉.的最优值我们将看到他移动不同的迭代次数(时期)。</p><p id="bacb" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">先来点假数据:</p><pre class="ko kp kq kr gu lm ln lo lp aw lq bi"><span id="eb20" class="lr ls ir ln b gz lt lu l lv lw"><strong class="ln is">import</strong> numpy <strong class="ln is">as</strong> np<br/><strong class="ln is">import</strong> matplotlib.pyplot <strong class="ln is">as</strong> plt<br/>predictor <strong class="ln is">=</strong> np<strong class="ln is">.</strong>random<strong class="ln is">.</strong>randn(1000)<br/>target <strong class="ln is">=</strong> [2<strong class="ln is">*</strong>x <strong class="ln is">+</strong> 4 <strong class="ln is">+</strong> 0.02<strong class="ln is">*</strong>np<strong class="ln is">.</strong>random<strong class="ln is">.</strong>uniform(1000) <strong class="ln is">for</strong> x <strong class="ln is">in</strong> predictor] <em class="km"># we create some fake data</em><br/>plt<strong class="ln is">.</strong>figure(figsize<strong class="ln is">=</strong>(4,4))<br/>plt<strong class="ln is">.</strong>scatter(predictor, target);</span></pre><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj lx"><img src="../Images/3e0f240320e7a859645b8fdec3cdfaf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*WrvWO7bqwfvp9BmruRoa1Q.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">伪造简单数据。</figcaption></figure><p id="0532" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">现在，我们将绘制不同时段大小的预测值，我们将看到𝛉如何缓慢收敛到最佳值(红色):</p><pre class="ko kp kq kr gu lm ln lo lp aw lq bi"><span id="104c" class="lr ls ir ln b gz lt lu l lv lw">plt<strong class="ln is">.</strong>figure(figsize<strong class="ln is">=</strong>(8,8))<br/>plt<strong class="ln is">.</strong>xlabel('x')<br/>plt<strong class="ln is">.</strong>ylabel('y')<br/><br/>epochs <strong class="ln is">=</strong> {'y':10, 'orange':20, 'blue':30, 'green':100, 'red':1000}<br/><br/><strong class="ln is">for</strong> color, epoch <strong class="ln is">in</strong> epochs<strong class="ln is">.</strong>items():<br/>    b, b0, cost <strong class="ln is">=</strong> linear_regression(predictor, target, b<strong class="ln is">=</strong>0, b0<strong class="ln is">=</strong>0, epochs<strong class="ln is">=</strong>epoch, learning_rate<strong class="ln is">=</strong>0.01) <br/>    predicted <strong class="ln is">=</strong> [b0<strong class="ln is">+</strong>b<strong class="ln is">*</strong>x <strong class="ln is">for</strong> x <strong class="ln is">in</strong> predictor]<br/>   <br/>    plt<strong class="ln is">.</strong>scatter(predictor, target);<br/>    plt<strong class="ln is">.</strong>plot(predictor, predicted, color<strong class="ln is">=</strong>color, label<strong class="ln is">=</strong>f'fitted line - prediction for {epoch} epoch')<br/>    plt<strong class="ln is">.</strong>legend(loc<strong class="ln is">=</strong>'best')<br/>    plt<strong class="ln is">.</strong>xlabel('x')<br/>    plt<strong class="ln is">.</strong>ylabel('y');</span></pre><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div class="gi gj ly"><img src="../Images/11846ef94a713e59789dbfc4ae04d815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*qUZPDROhrEX8oBpNoHeRRQ.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk translated">将重量𝛉转换为最佳值。</figcaption></figure><p id="8396" class="pw-post-body-paragraph jo jp ir jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl ik bi translated">这是一个非常简单但本质上非常强大的算法，可确保在其他强力或其他方法无法处理大量数据和要素的情况下找到最佳解决方案。因此，ML可以被视为优化。希望这对你的学习有所帮助。</p></div></div>    
</body>
</html>