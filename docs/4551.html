<html>
<head>
<title>How To Start with Apache Spark and Apache Cassandra</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从Apache Spark和Apache Cassandra开始</h1>
<blockquote>原文：<a href="https://itnext.io/how-to-start-with-apache-spark-and-apache-cassandra-886a648bd2fb?source=collection_archive---------0-----------------------#2020-07-23">https://itnext.io/how-to-start-with-apache-spark-and-apache-cassandra-886a648bd2fb?source=collection_archive---------0-----------------------#2020-07-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bbacd483a7834e0b96648c8e38d085b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lF7HnzHyw7I7g9kI.jpg"/></div></div></figure><div class=""/><p id="4636" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Apache Cassandra是一个特定的数据库，可以线性扩展。这是有代价的:特定的表格模型、可配置的一致性和有限的分析。苹果每秒在超过160，000个Cassandra实例上执行数百万次操作，同时收集超过100 PBs的数据。您可以通过Apache Spark和DataStax连接器绕过这些有限的分析，这就是本文的内容。</p><figure class="la lb lc ld gt iv"><div class="bz fp l di"><div class="le lf l"/></div></figure><h1 id="cd6a" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">设置</h1><p id="9e5a" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">我在Docker上使用过一个Apache Cassandra节点</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="37c2" class="mo lh je mk b gy mp mq l mr ms">version: '3'<br/> <br/>services:<br/>  cassandra:<br/>    image: cassandra:latest<br/>    ports:<br/>      - "9042:9042"</span></pre><p id="4f36" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Apache Spark 3.0作为shell推出，带有连接器和Cassandra的客户端库，这对于timeuuid类型转换将很有用。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="1871" class="mo lh je mk b gy mp mq l mr ms">./spark-shell --packages com.datastax.spark:spark-cassandra-connector_2.12:3.0.0-beta,com.datastax.cassandra:cassandra-driver-core:3.9.0</span></pre><p id="7023" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果Cassandra没有在本地运行，您需要配置它的地址。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="1063" class="mo lh je mk b gy mp mq l mr ms">spark.conf.set("spark.cassandra.connection.host", "127.0.0.1")</span></pre><h1 id="365b" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">数据</h1><p id="d009" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">为了测试Spark + Cassandra组合，我使用mockaroo.com生成了一些日期。这是一个传感器列表和这些传感器的测量值列表。你可以在GitHub的<a class="ae kz" href="https://github.com/zorteran/wiadro-danych-spark-cassandra-101" rel="noopener ugc nofollow" target="_blank">库</a>中找到它们。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="57fd" class="mo lh je mk b gy mp mq l mr ms">maciej@ubuntu:~/Desktop/spark_and_cassandra$ head sensor_reads.csv <br/>date,sensor_id,temperature,wind_speed,wind_direction<br/>2020-02-20 13:00:57,11,90.42,72.91,153<br/>2020-05-28 21:31:03,9,51.62,20.07,255<br/>2020-06-04 16:32:02,3,6.68,89.31,309<br/>...</span></pre><h1 id="9d35" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">在Apache Cassandra中创建表格</h1><p id="c677" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">Apache Cassandra有一个叫做<em class="mt"> cqlsh </em>的专用工具。在Docker的情况下，您必须使用以下命令。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="8bb4" class="mo lh je mk b gy mp mq l mr ms">sudo<!-- --> <!-- -->docker exec<!-- --> <!-- -->-it container_name cqlsh</span></pre><p id="c3bc" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，我们必须创建一个keyspace，这是一个用来存放我们的表的包。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="ec15" class="mo lh je mk b gy mp mq l mr ms"><strong class="mk jf">CREATE</strong> <!-- -->KEYSPACE spark_playground <strong class="mk jf">WITH</strong> <!-- -->replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };</span></pre><p id="a3b2" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有关传感器的信息将保存在传感器表中。没什么异常:id，位置，组id。注意，主键是id。因此，在没有完整的表扫描的情况下，我们不会对其他列执行WHERE操作(我提到具体的数据建模了吗？)，但是按键过滤会超级快。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="d98c" class="mo lh je mk b gy mp mq l mr ms"><strong class="mk jf">CREATE</strong> <strong class="mk jf">TABLE</strong> <!-- -->sensors ( sensor_id <strong class="mk jf">int</strong>, location text, group_id <strong class="mk jf">int</strong>, <strong class="mk jf">PRIMARY</strong> <strong class="mk jf">KEY</strong> <!-- -->(sensor_id ));</span></pre><p id="7e69" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">测量值将在sensors_reads表中。密钥由传感器id(分区密钥)和日期组成<a class="ae kz" href="https://docs.datastax.com/en/cql-oss/3.3/cql/cql_reference/uuid_type_r.html" rel="noopener ugc nofollow" target="_blank"> timeuuid </a>(聚类密钥)。分区键指示记录的位置(在哪个节点中)。Timeuuid作为一个聚类键允许对记录进行排序。如果你迷路了，看看<a class="ae kz" href="https://stackoverflow.com/questions/24949676/difference-between-partition-key-composite-key-and-clustering-key-in-cassandra" rel="noopener ugc nofollow" target="_blank">关于</a>栈溢出的解释。您必须小心选择分区键。<strong class="kd jf">错误的选择会导致节点上的负载不均匀</strong>。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="bd25" class="mo lh je mk b gy mp mq l mr ms"><strong class="mk jf">CREATE</strong> <strong class="mk jf">TABLE</strong> <!-- -->sensors_reads ( sensor_id <strong class="mk jf">int</strong>, <strong class="mk jf">date</strong> <!-- -->timeuuid, <strong class="mk jf">temp</strong> <strong class="mk jf">double</strong>, humidity <strong class="mk jf">double</strong>, wind_speed <strong class="mk jf">double</strong>, wind_direction <strong class="mk jf">double</strong>, <strong class="mk jf">PRIMARY</strong> <strong class="mk jf">KEY</strong> <!-- -->(sensor_id, <strong class="mk jf">date</strong> <!-- -->));</span></pre><h1 id="f833" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">使用Spark向Cassandra添加数据</h1><h2 id="e6ae" class="mo lh je bd li mu mv dn lm mw mx dp lq km my mz lu kq na nb ly ku nc nd mc ne bi translated">加载CSV文件</h2><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="66ad" class="mo lh je mk b gy mp mq l mr ms">val sensors = spark.read.format("csv").option("header", "true").load("/home/maciej/Desktop/spark_and_cassandra/sensors.csv")<br/>val sensorReads = spark.read.format("csv").option("header", "true").load("/home/maciej/Desktop/spark_and_cassandra/sensor_reads.csv")</span></pre><h2 id="debc" class="mo lh je bd li mu mv dn lm mw mx dp lq km my mz lu kq na nb ly ku nc nd mc ne bi translated">写入传感器表</h2><p id="b14f" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">列的类型和名称都一致，所以操作很简单。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="0804" class="mo lh je mk b gy mp mq l mr ms">sensors.write<br/>    .format("org.apache.spark.sql.cassandra")<br/>    .option("keyspace","spark_playground")<br/>    .option("table","sensors")<br/>    .mode("append")<br/>    .save()</span></pre><h2 id="bea6" class="mo lh je bd li mu mv dn lm mw mx dp lq km my mz lu kq na nb ly ku nc nd mc ne bi translated">写入传感器读取表</h2><p id="b265" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">这里变得更加复杂:</p><ul class=""><li id="c4a9" class="nf ng je kd b ke kf ki kj km nh kq ni ku nj ky nk nl nm nn bi translated">温度列中的列名不一致。温度= &gt;温度</li><li id="2990" class="nf ng je kd b ke no ki np km nq kq nr ku ns ky nk nl nm nn bi translated">日期列中的类型不一致。我们从CSV中读取字符串形式的日期，而Cassandra中的列类型是timeuuid。我们需要使用Cassandra的客户端库来进行转换。</li></ul><p id="4fd7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">类型不一致的问题可以通过适当的UDF(用户定义的函数)来解决。顺便说一下，我们将检查反向函数是否返回相同的日期。通常我会用IntelliJ编写一个测试，但是让我们用spark-shell的方式来做😊。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="1be1" class="mo lh je mk b gy mp mq l mr ms">import spark.implicits._<br/>import com.datastax.driver.core.utils.UUIDs<br/>import org.apache.spark.sql.functions.udf<br/> <br/>val toTimeuuid: java.sql.Timestamp =&gt; String = x =&gt; UUIDs.startOf(x.getTime()).toString()<br/>val fromTimeuuid: String =&gt; java.sql.Timestamp = x =&gt; new java.sql.Timestamp(UUIDs.unixTimestamp(java.util.UUID.fromString(x)))<br/> <br/>val toTimeuuidUDF = udf(toTimeuuid)<br/>val fromTimeuuidUDF = udf(fromTimeuuid)<br/> <br/>sensorsReads<br/>    .withColumn("date_as_timestamp", to_timestamp($"date"))<br/>    .withColumn("date_as_timeuuid", toTimeuuidUDF($"date_as_timestamp"))<br/>    .withColumn("timestamp_from_timeuuid",fromTimeuuidUDF($"date_as_timeuuid"))<br/>    .show(false)</span></pre><figure class="la lb lc ld gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/25e2141eca7f9fc1ea0b1e688c73d535.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/0*a1g7m2gWTj5qIPlu.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk translated">看起来不错。</figcaption></figure><p id="8fe7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们去掉不必要的列，重命名温度列，并将数据保存在Cassandra中</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="c684" class="mo lh je mk b gy mp mq l mr ms">val sensorsReads_fixed = sensorsReads<br/>                            .withColumn("date_as_timestamp", to_timestamp($"date"))<br/>                            .withColumn("date_as_timeuuid", toTimeuuidUDF($"date_as_timestamp"))<br/>                            .drop("date").drop("date_as_timestamp")<br/>                            .withColumnRenamed("date_as_timeuuid","date")<br/>                            .withColumnRenamed("temperature","temp")<br/>sensorsReads_fixed.write<br/>    .format("org.apache.spark.sql.cassandra")<br/>    .option("keyspace","spark_playground")<br/>    .option("table","sensors_reads")<br/>    .mode("append")<br/>    .save()</span></pre><h1 id="6a46" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">阅读卡珊德拉的作品</h1><p id="5ce9" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">在Cassandra中可以简化对表的引用。您只需要配置Spark上下文。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="2dc9" class="mo lh je mk b gy mp mq l mr ms">spark.conf.set("spark.sql.catalog.casscatalog","com.datastax.spark.connector.datasource.CassandraCatalog")</span></pre><p id="b6fd" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们可以使用<em class="mt">cas catalog . key space _ name . table _ name</em>符号来引用这些表。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="d6ee" class="mo lh je mk b gy mp mq l mr ms">scala&gt; spark.read.table("casscatalog.spark_playground.sensors").show()<br/>+---------+--------+--------------------+<br/>|sensor_id|group_id|            location|<br/>+---------+--------+--------------------+<br/>|        2|       4|       027 Heath Way|<br/>|       13|       3|  42585 Ramsey Alley|<br/>|        4|       1|     676 Marcy Point|<br/>|        5|       3|260 Steensland Cr...|<br/>|        9|       3|9385 Comanche Ter...|<br/>|        8|       2|291 Meadow Ridge ...|<br/>|        7|       2|     716 Randy Point|<br/>|       14|       2|    331 Mcbride Road|<br/>|       15|       3|     91 Gateway Hill|<br/>|        1|       3|      66 Vera Avenue|<br/>|        6|       4|87212 Lake View S...|<br/>|       12|       2|    12 Montana Place|<br/>|       10|       3|      60 Spohn Plaza|<br/>|       11|       1|    48 Redwing Court|<br/>|        3|       3|        930 Almo Way|<br/>+---------+--------+--------------------+</span><span id="5dfd" class="mo lh je mk b gy ny mq l mr ms"><strong class="mk jf">val</strong> <!-- -->sensors<strong class="mk jf">_</strong>table <strong class="mk jf">=</strong> <!-- -->spark.read.table("casscatalog.spark_playground.sensors")</span><span id="26fb" class="mo lh je mk b gy ny mq l mr ms"><strong class="mk jf">val</strong> <!-- -->sensors<strong class="mk jf">_</strong>reads<strong class="mk jf">_</strong>table <strong class="mk jf">=</strong> <!-- -->spark.read.table("casscatalog.spark_playground.sensors_reads")</span></pre><h2 id="e876" class="mo lh je bd li mu mv dn lm mw mx dp lq km my mz lu kq na nb ly ku nc nd mc ne bi translated">手术费用</h2><p id="6a0b" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">有了Spark，一切都显得轻松愉快。我们必须记住，我们操作的是Cassandra，它有自己处理查询的方式。我主要关心通过键和值检索记录的速度。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="fe77" class="mo lh je mk b gy mp mq l mr ms">scala&gt; sensors_table.filter("sensor_id in (1,2,3)").select("sensor_id","location").explain<br/>20/07/21 11:09:50 INFO V2ScanRelationPushDown:<br/>Pushing operators to sensors<br/>Pushed Filters: In(sensor_id, [1,2,3])<br/>Post-Scan Filters:<br/>Output: sensor_id#749, location#751<br/>          <br/>== Physical Plan ==<br/>*(1) Project [sensor_id#749, location#751]<br/>+- BatchScan[sensor_id#749, location#751] Cassandra Scan: spark_playground.sensors<br/> - Cassandra Filters: [["sensor_id" IN (?, ?, ?), 1]]<br/> - Requested Columns: [sensor_id,location]</span></pre><p id="595a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">通过sensor_id过滤发生在从Cassandra检索数据的阶段。按group_id过滤需要按Spark扫描整个表。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="ca37" class="mo lh je mk b gy mp mq l mr ms">scala&gt; sensors_table.filter("group_id in (1,2,3)").select("sensor_id","location").explain<br/>20/07/21 11:14:34 INFO V2ScanRelationPushDown:<br/>Pushing operators to sensors<br/>Pushed Filters:<br/>Post-Scan Filters: group_id#750 IN (1,2,3)<br/>Output: sensor_id#749, group_id#750, location#751<br/>          <br/>== Physical Plan ==<br/>*(1) Project [sensor_id#749, location#751]<br/>+- *(1) Filter group_id#750 IN (1,2,3)<br/>   +- BatchScan[sensor_id#749, group_id#750, location#751] Cassandra Scan: spark_playground.sensors<br/> - Cassandra Filters: []<br/> - Requested Columns: [sensor_id,group_id,location]</span></pre><h2 id="f1a1" class="mo lh je bd li mu mv dn lm mw mx dp lq km my mz lu kq na nb ly ku nc nd mc ne bi translated">简单聚合</h2><p id="e6cf" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">让我们假设需要计算传感器组的数量。我们没有在数据库设计级别预测到这一点，CQL查询也不会被执行。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="6646" class="mo lh je mk b gy mp mq l mr ms">cqlsh:spark_playground&gt; SELECT group_id, count(1) FROM sensors GROUP BY group_id;<br/>InvalidRequest: Error from server: code=2200 [Invalid query] message="Group by is currently only supported on the columns of the PRIMARY KEY, got group_id"</span></pre><p id="0233" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，我们要么在客户端应用程序端进行，要么使用Apache Spark。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="b8e9" class="mo lh je mk b gy mp mq l mr ms">scala&gt; sensors_table.groupBy("group_id").count.show<br/>+--------+-----+                                                                <br/>|group_id|count|<br/>+--------+-----+<br/>|       1|    2|<br/>|       3|    7|<br/>|       4|    2|<br/>|       2|    4|<br/>+--------+-----+</span></pre><h2 id="ab5d" class="mo lh je bd li mu mv dn lm mw mx dp lq km my mz lu kq na nb ly ku nc nd mc ne bi translated">连接joinWithCassandraTable</h2><p id="d508" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">最简单的连接方法是取两个集合，做一个笛卡尔积。然而，Cassandra连接器提供了一个更快的解决方案。在RDD版本中有一个<a class="ae kz" href="https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md#using-joinwithcassandratable" rel="noopener ugc nofollow" target="_blank">joinwithcassandrable</a>方法，而在数据框架中有<a class="ae kz" href="https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md#direct-join" rel="noopener ugc nofollow" target="_blank"> Direct Join </a>，正如你所看到的，它的文档很差。joinWithCassandraTable为源RDD所需的每个分区执行一次查询。在数据帧的情况下，这是自动发生的。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="933e" class="mo lh je mk b gy mp mq l mr ms">scala&gt; sensors_reads_table.join(sensors_table).explain<br/>...    <br/>== Physical Plan ==<br/>CartesianProduct<br/>:- *(1) Project [date#755, sensor_id#756, humidity#757, temp#758, wind_direction#759, wind_speed#760]<br/>:  +- BatchScan[date#755, sensor_id#756, humidity#757, temp#758, wind_direction#759, wind_speed#760] Cassandra Scan: spark_playground.sensors_reads<br/> - Cassandra Filters: []<br/> - Requested Columns: [date,sensor_id,humidity,temp,wind_direction,wind_speed]<br/>+- *(2) Project [sensor_id#749, group_id#750, location#751]<br/>   +- BatchScan[sensor_id#749, group_id#750, location#751] Cassandra Scan: spark_playground.sensors<br/> - Cassandra Filters: []<br/> - Requested Columns: [sensor_id,group_id,location]</span></pre><p id="fc6d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">理论上，相同的操作，但是选择键控关节会生成更有效的执行计划。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="138a" class="mo lh je mk b gy mp mq l mr ms">scala&gt; sensors_reads_table.join(sensors_table, sensors_reads_table("sensor_id") === sensors_table("sensor_id"), "inner").explain<br/>...<br/>== Physical Plan ==<br/>*(5) SortMergeJoin [sensor_id#756], [sensor_id#749], Inner<br/>:- *(2) Sort [sensor_id#756 ASC NULLS FIRST], false, 0<br/>:  +- Exchange hashpartitioning(sensor_id#756, 200), true, [id=#812]<br/>:     +- *(1) Project [date#755, sensor_id#756, humidity#757, temp#758, wind_direction#759, wind_speed#760]<br/>:        +- BatchScan[date#755, sensor_id#756, humidity#757, temp#758, wind_direction#759, wind_speed#760] Cassandra Scan: spark_playground.sensors_reads<br/> - Cassandra Filters: []<br/> - Requested Columns: [date,sensor_id,humidity,temp,wind_direction,wind_speed]<br/>+- *(4) Sort [sensor_id#749 ASC NULLS FIRST], false, 0<br/>   +- Exchange hashpartitioning(sensor_id#749, 200), true, [id=#820]<br/>      +- *(3) Project [sensor_id#749, group_id#750, location#751]<br/>         +- BatchScan[sensor_id#749, group_id#750, location#751] Cassandra Scan: spark_playground.sensors<br/> - Cassandra Filters: []<br/> - Requested Columns: [sensor_id,group_id,location]</span></pre><h1 id="b4f7" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">编辑(2020年7月30日)</h1><p id="1658" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">Datastax的Alex Ott在Linkedin的评论中告诉我，在这个案例中没有直接加入。原来你需要配置spark.sql.extensions，更多详情可以在这里<a class="ae kz" href="https://www.datastax.com/blog/2020/05/advanced-apache-cassandra-analytics-now-open-all" rel="noopener ugc nofollow" target="_blank">找到</a>。</p><pre class="la lb lc ld gt mj mk ml mm aw mn bi"><span id="20fc" class="mo lh je mk b gy mp mq l mr ms">spark.conf.set("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions")</span></pre><h1 id="f451" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">贮藏室ˌ仓库</h1><p id="d1ed" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated"><a class="ae kz" href="https://github.com/zorteran/wiadro-danych-spark-cassandra-101" rel="noopener ugc nofollow" target="_blank">https://github . com/zorteran/wiadro-danych-spark-Cassandra-101</a>—docker-compose和CSV</p><h1 id="f846" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">摘要</h1><p id="c8d7" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">Spark和Cassandra合作的话题在这个词条中几乎没有提及。Cassandra是Hadoop生态系统的一个有趣的替代和/或补充。我们可以使用Spark进行分析，也可以维护和集成Cassandra的数据。毕竟，在第一种方法中很难找到理想的数据模型😉。</p></div></div>    
</body>
</html>