<html>
<head>
<title>How to set up local Apache Spark environment (5 ways)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何设置本地Apache Spark环境(5种方法)</h1>
<blockquote>原文：<a href="https://itnext.io/how-to-set-up-local-apache-spark-environment-5-ways-62910fa0e8ad?source=collection_archive---------0-----------------------#2020-08-15">https://itnext.io/how-to-set-up-local-apache-spark-environment-5-ways-62910fa0e8ad?source=collection_archive---------0-----------------------#2020-08-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e9fd2401d5e921b7f9ae4b19685248e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JCYZnLIV-yrkbBE5.jpg"/></div></div></figure><div class=""/><p id="8666" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Apache Spark是最流行的分布式数据处理和分析平台之一。虽然它与服务器场、Hadoop和云技术相关联，但您可以在您的机器上成功启动它。在本条目中，您将学习几种配置Apache Spark开发环境的方法。</p><p id="82e6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">假设</p><p id="eb30" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这种情况下，基本系统是Ubuntu桌面20.04 LTS。</p><h1 id="14eb" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">火花壳</h1><p id="7a8a" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">第一种方法是在终端中运行Spark。让我们从下载Apache Spark开始。你可以在这里下载。下载后，我们必须用tar打开包。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="3cf2" class="ml la je mh b gy mm mn l mo mp">wget ftp://ftp.task.gda.pl/pub/www/apache/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz<br/>tar zxvf spark-3.0.0-bin-hadoop3.2.tgz</span></pre><p id="caae" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Apache Spark是用Scala写的，这意味着我们需要一个Java虚拟机(JVM)。对于Spark 3.0，它将是Java 11。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="e10d" class="ml la je mh b gy mm mn l mo mp">sudo<!-- --> <!-- -->apt install<!-- --> <!-- -->default-jre</span></pre><p id="2a0b" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在你所要做的就是进入bin目录并运行spark-shell</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mq"><img src="../Images/bf03744efe6a7a98cdd82b71827796b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xAXKc3wCMR3Em2ol.png"/></div></div></figure><p id="4f0b" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果您需要一个库(例如，您想从MySQL下载数据，对其进行处理并保存到其他地方)，您可以手动附加jar(—jars)或从maven存储库下载它们(— packages)。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="4ae6" class="ml la je mh b gy mm mn l mo mp">./spark-shell<!-- --> <!-- -->--driver-memory 8G --packages mysql:mysql-connector-java:5.1.49</span></pre><h1 id="7109" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">pyspark</h1><p id="dae4" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">在spark-shell中我们用Scala编写，如果你更喜欢Python，你的选择将是PySpark。</p><p id="e00c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">系统里没有Python，我们就来一招。我们将安装pip3，Python将作为一个依赖项安装🙂。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="bd54" class="ml la je mh b gy mm mn l mo mp">sudo<!-- --> <!-- -->apt install<!-- --> <!-- -->python3-pip</span></pre><p id="4ca7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但事实证明这还不够。Pyspark没有找到变量python。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="bd65" class="ml la je mh b gy mm mn l mo mp">maciej@ubuntu:~/spark-3.0.0-bin-hadoop3.2/bin$ ./pyspark<br/>env: ‘python’: No such file or directory</span></pre><p id="b35c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们需要使用环境变量来指示Python版本。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="d4e0" class="ml la je mh b gy mm mn l mo mp">export<!-- --> <!-- -->PYSPARK_PYTHON=python3</span></pre><p id="8a1a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在pyspark在终端启动。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mr"><img src="../Images/18ef344cc88fb1a762d80dca2d616e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ug_usz3HZByvzDte.png"/></div></div></figure><h1 id="82dc" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">朱庇特笔记本里的pyspark</h1><p id="925a" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">大多数使用Python的人，而不是终端，更喜欢笔记本。最受欢迎的是Jupyter笔记本。让我们安装它。我们将使用pip3，然后添加<em class="ms"> /。local/bin </em>文件夹到路径。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="8d04" class="ml la je mh b gy mm mn l mo mp">pip3 install notebook<br/>export PATH=$PATH:~/.local/bin</span></pre><p id="e9b7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当您添加以下环境变量时…</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="5a8b" class="ml la je mh b gy mm mn l mo mp">export PYSPARK_DRIVER_PYTHON="jupyter"<br/>export PYSPARK_DRIVER_PYTHON_OPTS="notebook"</span></pre><p id="ab6c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">…Jupyter笔记本将与pyspark一起自动推出。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mt"><img src="../Images/3315eaef76598fc16eac121713342ee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WwRB4eJt0A1M6LQH.png"/></div></div></figure><p id="873e" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果需要添加库，请使用下面的环境变量。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="75bf" class="ml la je mh b gy mm mn l mo mp">export<!-- --> <!-- -->PYSPARK_SUBMIT_ARGS='--packages mysql:mysql-connector-java:5.1.49'</span></pre><h1 id="ee76" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">Jupter笔记本中的spylon (scala)</h1><p id="e629" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">但是，如果您更喜欢使用Scala，可以选择spylon内核。安装过程如下:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="6925" class="ml la je mh b gy mm mn l mo mp">pip3 install spylon-kernel<br/>python3 -m spylon_kernel install --user</span></pre><p id="6fe5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后设置环境变量SPARK_HOME。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="bf0e" class="ml la je mh b gy mm mn l mo mp">export<!-- --> <!-- -->SPARK_HOME=/home/maciej/spark-3.0.0-bin-hadoop3.2</span></pre><p id="4dbf" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们可以访问木星笔记本中的spylon内核了。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/cd6b360f636b72877a7edcb8f6a49a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/0*mCqwMlo_XFmpAT6e.png"/></div></figure><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mv"><img src="../Images/9f098b334729e75c3ef8339b92f98249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HWhC6_cwwRxom8OR.png"/></div></div></figure><p id="1848" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果您需要特定的包或配置，请使用%%init_spark</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mw"><img src="../Images/87a0acfedf6f15aa7f92d0d4b0151a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xwrluSHKgYZFDUtD.png"/></div></div></figure><h1 id="ef79" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">IntelliJ理念中的项目</h1><p id="8ec4" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">你需要Scala插件。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/7c23c8c82112cbc25c7cb321f14245f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/0*YZ_oqqEOCC-96e6R.png"/></div></figure><p id="c696" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们正在创建一个新的Scala -&gt; sbt项目。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c708498f3d6cd89a001a040839c2bf00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*Pf10NOCg-Mvi72e5.png"/></div></figure><p id="f18d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们选择Scala 2.12和JDK。我选择了亚马逊Corretto 11。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/173e0bc9f767f87c9b7b841a100113c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/0*TCeXbrtgqWsEORUy.png"/></div></div></figure><p id="458c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们将必要的包添加到build.sbt文件中。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="f95e" class="ml la je mh b gy mm mn l mo mp">libraryDependencies ++= Seq(<br/>  "org.apache.spark" %% "spark-core" % "3.0.0",<br/>  "org.apache.spark" %% "spark-sql" % "3.0.0"<br/>)</span></pre><p id="391d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在sbt加载更改后，我们可以开始在Spark中编写应用程序。在src/main/scala路径中创建一个对象并开始编码😎。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="01a5" class="ml la je mh b gy mm mn l mo mp">import org.apache.spark.sql._<br/> <br/>object MyAwesomeApp {<br/>  def main(args: Array[String]) {<br/> <br/>    val spark = SparkSession<br/>      .builder<br/>      .appName("MyAwesomeApp")<br/>      .master("local[*]")<br/>      .getOrCreate()<br/> <br/>    import spark.implicits._<br/> <br/>    val df = Seq(<br/>      ("x", 4),<br/>      ("y", 2),<br/>      ("z", 5)<br/>    ).toDF("some_id", "some_int")<br/> <br/>    df.show()<br/>  }<br/>}</span></pre></div></div>    
</body>
</html>