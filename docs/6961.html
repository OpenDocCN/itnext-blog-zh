<html>
<head>
<title>Building Neural Network from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头开始构建神经网络</h1>
<blockquote>原文：<a href="https://itnext.io/building-neural-network-from-scratch-in-python-71ed71d34588?source=collection_archive---------1-----------------------#2022-04-27">https://itnext.io/building-neural-network-from-scratch-in-python-71ed71d34588?source=collection_archive---------1-----------------------#2022-04-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1001" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">从零开始理解深度学习</strong></h2></div><p id="a55f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是<em class="lb">从零开始理解深度学习</em>系列的第二篇帖子，这是我们<a class="ae lc" href="https://akilahmed.medium.com/creating-a-single-neuron-model-perceptron-5731aaf36a54" rel="noopener">的第一篇帖子</a>，我们已经详细解释了如何创建一个单个神经元(感知器)，我们将在这里多次引用该帖子，所以请在另一个标签中打开它。您可以在这个<em class="lb"> building_dnn.ipynb </em>文件中的<a class="ae lc" href="https://github.com/akil-ahmed3/understanding_deep_learning" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> GitHub链接</em> </a> <em class="lb"> </em>上找到本文的代码，并且您可以在这个<a class="ae lc" href="https://drive.google.com/drive/folders/1OTmte5WhL97YrDHYsb7JSv_7E55JZeYG?usp=sharing" rel="noopener ugc nofollow" target="_blank">链接</a>中找到numpy数据集</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="8949" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本帖中，我们将从头开始用Python创建一个浅层神经网络。然后，我们训练一个分类模型来预测图像是狗还是猫。请参考我以前的帖子来理解数据集的设置。我们将跳过一些已经解释过的东西，如sigmoid函数、前向传播、后向传播、成本函数和梯度下降。我们将在这篇文章中讨论所有这些事情，但我们不会涉及太多细节。</p><p id="ea7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看我们模型的架构</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/cf586af8de79169c2fd5dca9a86576fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_AAxbsaTGFe0GzBC5m-MZQ.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">图片-1</figcaption></figure><p id="94c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与我们的感知器不同，这里的数据(X)不连接到我们的输出神经元，它连接到一个称为隐藏层的中间层(A)。<em class="lb"> X </em>的每个单元连接到<em class="lb"> A </em>的每个单元。这里我们将创建一个有1个输入、1个输出和1个隐藏层的网络。如果我们愿意，我们可以增加隐藏层的数量。A是这样计算的，</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/6c6029241154e334157750ca176dc1b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*HH8QkZ_yzDDqWPk3tNMw_Q.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">方程式- 1</figcaption></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/0e69865cd0fcf7d994d96a7d2d49593c.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*aIsLpK2dpTX0GM1l9CUCFw.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">方程式2</figcaption></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/10962a5447b3d88b8e869166751047e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*NNPF_CiDQUKw-ouevho7ww.jpeg"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">图片-2</figcaption></figure><p id="2e41" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像上次一样，我们用等式-1计算<em class="lb"> Z </em>向量，其中上标<em class="lb"> l </em>表示隐藏层数。<em class="lb"> W </em>、<em class="lb"> X </em>、<em class="lb"> b </em>分别是权重、样本数据和偏差。我们将在隐藏层使用Relu激活函数，在输出层使用sigmoid函数。</p><p id="eb03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，隐藏层的所有5个单元与我们的输出单元连接，输出单元像以前一样使用sigmoid函数来分类到正确的类中。</p><p id="2e97" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像以前一样，我们将首先初始化我们的权重和偏差，然后它将通过隐藏层，最后是输出层。让我们详细地看看并理解代码。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="md me l"/></div></figure><ul class=""><li id="a02a" class="mf mg iq kh b ki kj kl km ko mh ks mi kw mj la mk ml mm mn bi translated">首先在<em class="lb">initialize _ parameters _ deep</em>函数中，<em class="lb"> </em>我们在初始化参数，但与之前不同的是，我们将<em class="lb"> W </em>初始化为随机数。为什么？因为，如果我们为第一层的每个单元(神经元)从零开始设置<em class="lb"> W </em>的值，那么在向前和向后传播期间，每个层的每个神经元的<em class="lb"> W </em>的值将保持不变，每个单元的<em class="lb"> W </em>将以相同的方式更新，因为它们的起点是相同的，所以一般来说，它将不会学习到任何新的东西，这就像使用单个神经元进行更多的计算，这没有任何意义，对吗？</li><li id="2d33" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">如果我们使用随机数来初始化<em class="lb"> W </em>，那么所有的值在初始化时都将是不同的，并且每个神经元将学习一个我们想要的新东西。我们会看到多快。</li><li id="a2f2" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">我们正在创建一个参数字典，其中存储不同层的<em class="lb"> W </em>、<em class="lb"> b </em>(初始化为0)的值，并返回它们。</li><li id="ae31" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">然后我们创建Sigmoid和ReLu函数，分别用于输出层和隐藏层。</li></ul><p id="df8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的前向传播步骤与之前非常相似，唯一的区别是，之前我们只对一个神经元进行传播，但现在我们将对每一层的每个神经元进行传播。因此，如果模型有2层，每层有5个单元，那么总共有11个神经元(2 x 5 + 1个输出神经元)。</p><p id="7a01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们详细了解一下代码，</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="md me l"/></div></figure><ul class=""><li id="c980" class="mf mg iq kh b ki kj kl km ko mh ks mi kw mj la mk ml mm mn bi translated">在<em class="lb"> linear_forward </em>函数中，我们执行等式-1，将当前层的权重(W)与前一层(A_prev)或样本数据(X)的结果进行点积(仅针对第一层)。</li><li id="ddf4" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">然后在<em class="lb">linear _ activation _ forward</em>函数中，我们传递<em class="lb"> A_prev </em>(来自前几层的结果)<em class="lb"> W </em>、<em class="lb"> b </em>和<em class="lb"> activation </em>作为参数，这里activation将是一个字符串(<em class="lb"> sigmoid </em>或<em class="lb"> relu </em>)来决定在当前层使用哪个激活函数，我们将调用<em class="lb">linear _ forward</em></li><li id="33e6" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">然后在<em class="lb"> L_model_forward </em>函数中，我们将第一层的<em class="lb"> A </em>初始化为<em class="lb"> X </em>(数据集)，创建一个名为<em class="lb">cache</em>的空数组，它将跟踪我们的<em class="lb">参数(W和b) </em>、<em class="lb"> A_prev </em>和<em class="lb"> Z、</em>然后将层数存储在变量<em class="lb"> L. </em> I am中</li><li id="8bbb" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">在for循环中，所有隐藏层(A)的单元都是通过Relu激活来计算的，在下一层中，我们将A_prev更新为A(第28行)。因此，对于第一层<em class="lb"> A </em>是<em class="lb"> X </em>，但是对于所有其他层，<em class="lb"> A </em>成为前一层(A_prev)的输出，明白吗？</li><li id="52a1" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">循环之后，我们使用sigmoid激活函数计算最终层(al)的输出。</li><li id="0d04" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">L_model_forward函数将返回在整个正向传播步骤中追加的所有缓存。</li></ul><p id="2868" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能会问，为什么我们在隐藏层使用ReLu而不是sigmoid？因为ReLu是用<em class="lb"> max(0，Z) </em>公式计算的，所以它在计算上比sigmoid或tanh更有效，因为它不执行任何指数运算。此外，它解决了消失梯度问题。</p><p id="aa86" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们完成了正向传播步骤。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="7ee8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在是计算每层的成本函数(J)的时候了。在<a class="ae lc" href="https://akilahmed.medium.com/creating-a-single-neuron-model-perceptron-5731aaf36a54" rel="noopener">上一篇文章</a>中，我们已经详细解释了成本函数，所以在这里我们将只看到我们的公式对于多个单元和隐藏层是如何变化的。好吗？</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mt"><img src="../Images/d4ab9a3e1743a3562a6b0535a92134f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HAPLpaMxPVvkgAhK9GoJ7w.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">方程式3</figcaption></figure><p id="3b96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在这里看到的唯一变化是[L]上标，它表示网络的第1层。让我们看看代码，</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="md me l"/></div></figure><ul class=""><li id="fe3a" class="mf mg iq kh b ki kj kl km ko mh ks mi kw mj la mk ml mm mn bi translated">代码非常简单，我们取正向传播步骤(al)的最终结果，并用<em class="lb"> compute_cost </em>函数中的实际值(Y)计算成本。</li></ul></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="07e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们必须计算<em class="lb">反向传播</em>，其用于计算损失函数相对于参数的梯度。在向前和向后传播的过程中，我们会经历一个叫做计算图的过程。请参考<a class="ae lc" href="https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9" rel="noopener">本文</a>详细了解计算图。在该图中，想象我们在正向传播中所做的一切，我们必须反向进行，并计算损失函数相对于参数的<em class="lb">梯度。</em></p><p id="2a32" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们会找到激活函数(dZ)的梯度损失函数w.r.t，这是我们最后为前向传播计算的，所以在后向传播中，我们会先这样做，以此类推，好吗？所以<em class="lb"> dZ </em>是损失函数w.r.t <em class="lb"> dA </em>的导数和损失函数w.r.t <em class="lb"> Z </em>方程-4的导数的乘积。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/eaafec49ae63792f7896ff6e4c287f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*JDc7R79EUJYOKZqrbijYXg.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">方程式4</figcaption></figure><p id="853e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们知道第一层的激活函数是sigmoid，而其他层的激活函数是ReLu。此后，当激活函数为Sigmoid时，<em class="lb"> dZ = dA*A*(1-A) </em>当激活函数为ReLu时，<em class="lb">dZ = dA(Z&gt;0)</em><em class="lb">dZ = 0(Z≤0)。下面是代码，</em></p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="335a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面我就不详细介绍这些方程的推导了，如果你想让我说的话，请评论。这里<em class="lb"> l </em>是当前层，<em class="lb"> l-1 </em>是前一层。现在，权重(dW)、偏差(db)和前一层的输出(dA_prev)相对于损失函数(J)的导数将是，</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/5bfb14186da969d76fbb007019104c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*t0KTnfPgcCBTRiKFdgnyKA.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">方程式5</figcaption></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/3c1a57e5e91628ea3f7965cb72e95fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*87uH5TB8Jc4zwp9PYhid8A.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">方程式6</figcaption></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/6e8757447906fc233e10a2e86d534d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*TteN3lGNtmxsBAr8r83nuQ.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">方程式7</figcaption></figure><p id="50c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看代码，</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="md me l"/></div></figure><ul class=""><li id="bd1b" class="mf mg iq kh b ki kj kl km ko mh ks mi kw mj la mk ml mm mn bi translated">我们可以看到，linear_backward函数以<em class="lb"> dZ </em>、<em class="lb"> W </em>和<em class="lb"> b </em>作为参数，并将根据等式5、6和7返回<em class="lb"> dW </em>、<em class="lb"> db </em>和<em class="lb"> dA_prev </em>。</li><li id="883e" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">在正向传播中，我们取第1层的值<em class="lb"> W </em>，<em class="lb"> b </em>和第(L-1)层的值<em class="lb"> A </em>来计算第1层的值<em class="lb"> A </em>对吗？因此，在反向传播中，我们将采用第L层的<em class="lb"> dA </em>来计算第L层的<em class="lb"> dW </em>、第L层的<em class="lb"> db </em>和第(L-1)层的<em class="lb"> dA </em>，这个过程以相反的顺序发生。希望你能围绕它建立直觉。</li><li id="3167" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">在<em class="lb">linear _ activation _ backward</em>函数中，我们首先询问反向传播将发生在哪个激活函数上。</li><li id="3e7b" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">激活的Backprop返回<em class="lb"> dZ </em>，然后linear_backward计算并返回<em class="lb"> dA_prev </em>、<em class="lb"> dW </em>和<em class="lb"> db。</em></li></ul><p id="1c05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在前向传播步骤的<em class="lb">initialize _ parameters _ deep</em>函数中，我们已经将所有权重(W)和偏差(b)存储在参数字典中。同样，我们将把每一层的所有<em class="lb"> dW </em>、<em class="lb"> db </em>和<em class="lb"> dA_prev </em>存储在名为<em class="lb"> grads、</em>的字典中，其中字典的关键字是<em class="lb"> dW + str(l) </em>(字符串化后的数字)我以<em class="lb"> dW </em>为例，<em class="lb"> db </em>和<em class="lb"> dA </em>的存储方式也是一样的。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi my"><img src="../Images/21e37e4e5002e69caf50aa55abbadb09.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*wlGRNy2R_sNZvRJocyXK7Q.png"/></div></figure><p id="7578" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从正向传播中，我们得到了<em class="lb"> W </em>、<em class="lb"> b </em>、<em class="lb"> AL </em>。从反向传播中，我们得到了<em class="lb"> dW </em>，<em class="lb"> db </em>，<em class="lb"> dA_prev </em>。所以现在，我们必须更新每一层的<em class="lb"> W </em>和<em class="lb"> b </em>，以适应我们的数据。正如我们在上一篇文章中一样，我们通过分别从W和b中减去<em class="lb"> dW </em>和<em class="lb"> db </em>乘以学习率(alpha)来更新<em class="lb"> W </em>的值。参考<a class="ae lc" href="https://akilahmed.medium.com/creating-a-single-neuron-model-perceptron-5731aaf36a54" rel="noopener">上一篇</a>的渐变下降部分可以更好的理解。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ad07c12d197a86dd483cb579ab886d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*ji5LtZZp9GhCyH-UDr4J-g.png"/></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi na"><img src="../Images/531b59ae22eb19fe2fd842ef7a641145.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*95nWMEI5Vetc_JBhRmhS_w.png"/></div></figure><p id="6f36" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看梯度存储和参数更新的代码，</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="md me l"/></div></figure><ul class=""><li id="d82e" class="mf mg iq kh b ki kj kl km ko mh ks mi kw mj la mk ml mm mn bi translated">首先在<em class="lb"> L_model_backward </em>函数中，我们将grads初始化为一个空字典，它将存储<em class="lb"> dW </em>、<em class="lb"> db </em>和<em class="lb"> dA_prev </em>，然后<em class="lb"> L </em>是当前的层数，<em class="lb"> m </em>是概率向量的形状，前向传播的输出(<em class="lb"> AL </em>)，<em class="lb"> Y </em>是包含0或的标签向量</li><li id="c482" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated"><em class="lb"> dAL </em>是输出层损失函数的梯度，将在<em class="lb">linear _ activation _ backward</em>函数中用于计算最后一层(第10行)的梯度。</li><li id="ba63" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">正如你所记得的，在我们的前向属性的最后一层(输出层)中，我们使用了sigmoid函数，我们也在Sigmoid激活时进行后向属性，然后将其存储在grads字典中。</li><li id="25d7" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">然后，我们从倒数第二层循环到第一层，并计算和存储所有其他层的梯度。在这些层中，我们使用了ReLu激活，所以我们将使用我们之前创建的relu _ backwards函数。最后，我们把毕业生送回来了。</li><li id="63c1" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">在update_parameters函数中，我们传递参数(权重和偏差)、梯度(损失函数的梯度w.r.t参数dw，db)和学习率。</li><li id="d458" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">然后，从第一层循环到最后一层，并根据等式9和10更新权重和偏差。在上一篇文章中，我们在感知器模型的<em class="lb">梯度体面</em>函数中做了同样的事情。但是在这里，我们是针对多层和多个神经元(感知器)来做的</li></ul><p id="3cf9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们把到目前为止的所有函数集合起来，创建<em class="lb"> L_layer_model </em>函数，</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="md me l"/></div></figure><ul class=""><li id="e510" class="mf mg iq kh b ki kj kl km ko mh ks mi kw mj la mk ml mm mn bi translated">我们正在创建空的costs数组，它将存储每次迭代的成本函数。</li><li id="be16" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">用层维度初始化我们的参数(权重和偏差)，层维度将是一个类似于[3072，5，1]的数组，这里第一个是具有3072个单元(展平图像)的输入层，它有一个具有5个单元的隐藏层，最后一个是具有1个单元或神经元的输出层，它将给出我们分类的概率。我们可以通过改变层维度数组来增加隐藏层的数量，如果我们将它改为[3072，5，5，1]，那么它将有两个隐藏层，以此类推。</li><li id="2126" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">在循环内部，我们迭代我们提供的迭代次数。</li><li id="f0b4" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">首先，我们是<em class="lb">正向传播</em>，它给我们输出<em class="lb">层(a1)的结果，以及存储参数(权重和偏差)和每层(A)输出的缓存</em>。</li><li id="4ffd" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">然后，我们正在<em class="lb">计算AL和实际标签(Y)的成本。</em></li><li id="7d9d" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">之后，我们<em class="lb">计算我们的梯度(dW，db和dA_prev) </em>。</li><li id="c66b" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">接下来，我们将更新参数并打印成本函数。</li><li id="4239" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">在第13–14行，我在某些迭代之后降低了学习率，这样它就不会超调。</li><li id="af51" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">最后，我们返回更新的参数(W和b)和成本数组。</li></ul><p id="5a89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们训练和测试我们的模型，</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="md me l"/></div></figure><ul class=""><li id="9487" class="mf mg iq kh b ki kj kl km ko mh ks mi kw mj la mk ml mm mn bi translated">首先，我们调用L_layer_model函数，该函数将通过获取训练数据集来训练模型，层维度为[3072，5，5，1]，学习率为0.2，迭代次数为10000作为自变量，并返回更新的参数和成本。</li><li id="e1ac" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">然后在预测函数中，我们将概率值转换为0和1。如果前向传播或L_model_forward通过采用更新的参数得到的结果大于0.5，则为1，否则为0。</li><li id="8201" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">它将通过与实际值<em class="lb"> y. </em>进行比较来打印数据集的准确性</li></ul><pre class="ll lm ln lo gt nb nc nd ne aw nf bi"><span id="f2c9" class="ng nh iq nc b gy ni nj l nk nl">Cost after iteration 0: 0.6965011895457693<br/>Cost after iteration 1000: 0.6368951308414399 <br/>Cost after iteration 2000: 0.6219993389332111 <br/>Cost after iteration 3000: 0.6054223197992217 <br/>Cost after iteration 4000: 0.6010027596861128 <br/>Cost after iteration 5000: 0.5801166825629971 <br/>Cost after iteration 6000: 0.578504474487532 <br/>Cost after iteration 7000: 0.5665113489929966 <br/>Cost after iteration 8000: 0.5620262967779565 <br/>Cost after iteration 9000: 0.5597852629281578 <br/>Cost after iteration 9999: 0.5575603590876632`</span><span id="615b" class="ng nh iq nc b gy nm nj l nk nl">test Accuracy: 0.6150000000000001 <br/>train Accuracy: 0.7849999999999998</span></pre><p id="7681" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以清楚地看到，我们的具有多个神经元和层的模型仅比我们之前的模型提高了3%，这一点也不好。虽然我们比以前做了更多的工作。这是怎么回事？</p><p id="b39a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型在训练数据上过度拟合，为了避免这种情况，我们必须使用称为正则化方法的东西和一些超参数来优化我们的算法，这可以在本系列的第三篇文章中读到。感谢您的阅读，如果您有任何问题，请在评论区或任何社交媒体上发表。</p><p id="487f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">信用:【https://www.coursera.org/specializations/deep-learning T2】</p></div></div>    
</body>
</html>