<html>
<head>
<title>Build a Sparse Reward PySC2 Agent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建稀疏奖励PySC2代理</h1>
<blockquote>原文：<a href="https://itnext.io/build-a-sparse-reward-pysc2-agent-a44e94ba5255?source=collection_archive---------0-----------------------#2017-12-25">https://itnext.io/build-a-sparse-reward-pysc2-agent-a44e94ba5255?source=collection_archive---------0-----------------------#2017-12-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq"><div class="bz fp l di"><div class="jr js l"/></div></figure><p id="8c20" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated"><a class="ae kr" href="https://www.linkedin.com/cws/share?url=https%3A%2F%2Fitnext.io%2Fbuild-a-sparse-reward-pysc2-agent-a44e94ba5255" rel="noopener ugc nofollow" target="_blank"> <em class="ks">点击这里在LinkedIn </em>上分享这篇文章</a></p><p id="2726" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">在我的<a class="ae kr" href="https://medium.com/@skjb/add-smart-attacking-to-your-pysc2-agent-17fd5caad578" rel="noopener">最后几个教程</a>中，我讲述了如何构建一个PySC2代理，它使用机器学习来建造单位和攻击敌人。如果你跟着做，你可能已经注意到代理有有趣的结果，例如它经常在敌人基地外等待，等待单位出现，这样它就可以获得奖励。</p><p id="65f7" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">在本教程中，我们将转移到使用稀疏奖励，基本上代理人将获得奖励1赢得游戏，或-1输掉游戏。这可能需要更多的训练，但最终结果应该是更多的胜利。</p><p id="01d8" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">让我们开始吧。</p><p id="9739" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">注意:对于那些关注过我之前教程的人来说，本教程中的许多代码可能看起来很熟悉，但实际上有一些重要的区别。我会试着给你指出这些。</p><h1 id="fd12" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">1.建立</h1><p id="7ca8" class="pw-post-body-paragraph jt ju iq jv b jw lr jy jz ka ls kc kd ke lt kg kh ki lu kk kl km lv ko kp kq ij bi translated">首先，让我们设置我们将使用的导入和一些变量:</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="beb0" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">这里的最后一个项目是新的，它将允许我们指挥scv采集矿物。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="7a4c" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">我在这里添加了军队供应ID，以便更容易地遵循代码。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="f8ff" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">这里又有了一个新的单位ID，用来探测矿点的位置。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="fc7d" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">这里的最后一个动作类型是新的，它很酷，因为它允许我们在一个动作中选择屏幕上相同类型的所有单位。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="0b37" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">这是我们将存储Q学习表的文件的前缀。这将允许我们结束和恢复训练，当我们需要运行数百集时，这将派上用场。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="c382" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">你可能会注意到这里遗漏了几个动作，在本教程中，我们将把一系列动作(例如选择兵营，训练陆战队员)浓缩成一个动作(例如训练陆战队员)。我会边走边解释。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="d392" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">如果你读过我以前的教程，你可能会意识到这是把小地图分成四个象限。这是为了保持相对较小的动作空间，使代理更容易学习。</p><h1 id="0caf" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">2.添加Q学习表</h1><p id="b360" class="pw-post-body-paragraph jt ju iq jv b jw lr jy jz ka ls kc kd ke lt kg kh ki lu kk kl km lv ko kp kq ij bi translated">和我以前的教程一样，我们将使用Q学习表。请注意，这个已经更新，以支持一些熊猫的更新，这似乎导致一些人的问题。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="857a" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">这里的最后四行与我以前的教程不同，我们不是在每一点都应用全部奖励，而是在状态变得不稳定时才应用全部奖励，比如赢或输。所有其他的学习步骤都会打折扣。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="85f2" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">我总是惊讶于强化学习只需要这么少的代码就能奏效！</p><h1 id="0827" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">3.创建代理</h1><p id="f83b" class="pw-post-body-paragraph jt ju iq jv b jw lr jy jz ka ls kc kd ke lt kg kh ki lu kk kl km lv ko kp kq ij bi translated">代理的开始与之前的教程相同，我们保留所有QLearningTable设置的默认值。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="20a3" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">我们添加了几个属性<code class="fe mb mc md me b">cc_x</code>和<code class="fe mb mc md me b">cc_y</code>来跟踪指挥中心的位置。</p><p id="be84" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">另一个新属性<code class="fe mb mc md me b">move_number</code>将跟踪多步动作中的序列位置。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="6d26" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">这段代码很酷，如果<code class="fe mb mc md me b">sparse_agent_data.gz</code>存在，它将从这个文件中加载Q学习表数据，这允许您从以前的位置恢复学习。如果您需要停止培训，或者有某种错误导致您的代理崩溃，您可以简单地重新启动它，您的学习历史不会丢失。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="3ad2" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">前两种方法与之前的教程相同，如果底部位于右下角，它们基本上允许我们反转屏幕和小地图的位置，这样所有的操作都可以被视为从左上角执行，从而使我们的代理可以更快地学习。</p><p id="afd6" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">最后一个方法是一个实用程序，它允许is从我们选择的动作中提取我们需要的信息。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="1c7a" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">你可能以前没见过<code class="fe mb mc md me b">obs.first()</code>，这个方法本质上告诉你这是不是一集(游戏)的第一步，所以在这里你可以设置游戏剩余部分所需的任何数据。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="1913" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">在这里，我们只是设置了一些计数，可以用于状态和其他需要知道我们拥有什么的代码。你可以在这里阅读更详细的解释<a class="ae kr" href="https://medium.com/@skjb/how-to-locate-and-select-units-in-pysc2-2bb1c81f2ad3" rel="noopener">。</a></p><p id="55d5" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">好了，我们现在有了一个应该运行的基本代理，但是没有实现任何东西。</p><h1 id="35eb" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">4.添加多步骤操作的第一步</h1><p id="f4b6" class="pw-post-body-paragraph jt ju iq jv b jw lr jy jz ka ls kc kd ke lt kg kh ki lu kk kl km lv ko kp kq ij bi translated">正如我前面提到的，我们将把几个动作压缩成一个动作，这使得我们的动作空间更简单，这将有助于我们的代理更快地学习。我们所有的多步骤动作将消耗3个步骤，即使它们需要更少，因此学习调用总是每3个游戏步骤进行一次，以保持事情的一致性。</p><p id="c38c" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">我们将采取的主要行动是:</p><ul class=""><li id="af8d" class="mf mg iq jv b jw jx ka kb ke mh ki mi km mj kq mk ml mm mn bi translated">什么都不做——3步都不做</li><li id="ff84" class="mf mg iq jv b jw mo ka mp ke mq ki mr km ms kq mk ml mm mn bi translated">建造补给站—选择SCV，建造补给站，派SCV去采集矿物</li><li id="57e6" class="mf mg iq jv b jw mo ka mp ke mq ki mr km ms kq mk ml mm mn bi translated">建造兵营—选择SCV，建造兵营，派SCV去采集矿物</li><li id="0d57" class="mf mg iq jv b jw mo ka mp ke mq ki mr km ms kq mk ml mm mn bi translated">建造海军陆战队——选择所有兵营，训练海军陆战队，什么都不做</li><li id="9992" class="mf mg iq jv b jw mo ka mp ke mq ki mr km ms kq mk ml mm mn bi translated">攻击(x，y)-选择军队，攻击坐标，什么都不做</li></ul><p id="1115" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">不幸的是，将scv送回矿点并不完美，但似乎相当有效。这样做的目的是为了防止scv被选中，而不是被兵营选中。</p><p id="499b" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">在上一步的<code class="fe mb mc md me b">barracks_count</code>行后插入以下代码:</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="513d" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">首先，我们检查这是否是多步操作中的第一步，由值为0的<code class="fe mb mc md me b">self.move_number</code>表示。我们增加数字，以便在下一个游戏步骤中，我们将继续进行多步动作的第二步。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="1c31" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">接下来，我们设置州包括我们的每种建筑类型的计数和我们的海军计数。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="b23d" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">现在我们把小地图分成四个象限，如果其中包含任何敌人单位，就标记一个象限为“热点”。如果底座在右下角，我们将象限颠倒，这样所有游戏都可以从左上角底座的角度来看，而不管实际的底座位置。这应该有助于我们的代理更快地学习。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="936c" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">如果我们不在整个游戏的第一步，我们调用Q学习表上的<code class="fe mb mc md me b">learn()</code>方法。因为这仅在每个多步骤动作的第一步完成，所以将在每第三个游戏步骤采取状态并执行学习。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="fbb7" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">接下来，我们选择一个动作，如果存在任何<code class="fe mb mc md me b">x</code>和<code class="fe mb mc md me b">y</code>坐标，则将其分离出来。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="594d" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">建造补给站或兵营的第一步是选择一个SCV。我们通过识别屏幕上的所有SCV点并随机点击一个点来做到这一点。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="3bd4" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">建造海军陆战队的第一步是选择兵营。事实上，通过发送<code class="fe mb mc md me b">_SELECT_ALL</code>值，我们可以同时选择屏幕上的所有兵营。这样做的主要好处是，游戏会自动将下一个陆战队员排在最不繁忙的兵营，首先考虑选择正确兵营的需要。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="dc93" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">攻击一个地点的第一步是选择军队。</p><h1 id="100d" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">5.添加多步骤操作的第二步</h1><p id="aecd" class="pw-post-body-paragraph jt ju iq jv b jw lr jy jz ka ls kc kd ke lt kg kh ki lu kk kl km lv ko kp kq ij bi translated">将以下代码直接添加到上一步中的代码下方:</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="fcc6" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">我们从增加移动数和提取动作细节开始。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="71be" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">建造补给站的第二步是命令SCV在给定的地点建造补给站。通过使用我们储存的指挥中心位置，即使我们的指挥中心被摧毁了，我们也可以建造一个补给站。</p><p id="e8de" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">为了让我们的代理更容易，每个补给站的位置都是硬编码的，所以它只需要知道它决定建立一个补给站，而不是补给站的位置。这使得我们的状态和动作空间更加简单，因为我们不需要跟踪这些细节。</p><p id="22de" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">这里可能出现的一个问题是，你的第一个补给站可能会在你的第二个补给站建成后被摧毁，但是你的代理会认为你的第一个补给站在第一个位置，因此会试图在已经有补给站的地方建立第二个补给站。虽然你可以试着解决这个问题，但我觉得没有必要，通常一旦敌人摧毁了你的一个兵营，你很可能就会失败。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="cd20" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">建造兵营的第二步与补给站非常相似，只是坐标不同。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="965e" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">训练陆战队员的第二步很简单，告诉兵营去训练陆战队员！我们把命令排队，这样兵营可以把几个陆战队员排成一排，在军队进攻需要增援的时候就方便了。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="215e" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">攻击的第二个动作是简单地命令军队攻击小地图上的一个地点。</p><p id="2ce7" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">为了防止意外选择scv并试图用它们攻击，我们检查了<code class="fe mb mc md me b">single_select</code>和<code class="fe mb mc md me b">multi_select</code>空格以确保我们没有选择scv。</p><p id="6051" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">一旦我们确定选择了军队，我们就在象限的中心随机选择一个位置。这允许我们保持我们的行动空间只有四个攻击坐标，但是帮助代理在象限周围攻击并且不留下任何敌人单位不被触动。</p><h1 id="b355" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">6.添加多步骤操作的最后一步</h1><p id="ec4f" class="pw-post-body-paragraph jt ju iq jv b jw lr jy jz ka ls kc kd ke lt kg kh ki lu kk kl km lv ko kp kq ij bi translated">将以下代码直接添加到上一步中的代码之后:</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="93d0" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">剩下唯一要做的就是把scv送回矿点。注意，这个动作是排队的，所以它会在SCV建造完补给站或兵营后执行。</p><p id="ce32" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">所有其他跳过的或无效的动作将通过一个<code class="fe mb mc md me b">_NO_OP</code>调用来实现。</p><h1 id="cec3" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">7.检测游戏完成</h1><p id="7be7" class="pw-post-body-paragraph jt ju iq jv b jw lr jy jz ka ls kc kd ke lt kg kh ki lu kk kl km lv ko kp kq ij bi translated">最后一步是检测游戏的结束，应用奖励，并为下一集清除任何属性。</p><p id="7997" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">将以下代码添加到代理的step()方法的开头，在<code class="fe mb mc md me b">super(SparseAgent, self).step(obs)</code>调用之后:</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="c51e" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">就像我们之前做的<code class="fe mb mc md me b">obs.first()</code>调用一样，<code class="fe mb mc md me b">obs.last()</code>调用允许我们检测一集的最后一个游戏步骤。</p><p id="eaac" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">幸运的是，作为观察的一部分，DeepMind以<code class="fe mb mc md me b">obs.reward</code>的形式提供了我们需要的奖励。这个值要么是1代表赢，要么是-1代表输，要么是0代表僵持或一集达到28，800步。</p><p id="c887" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">可以使用<code class="fe mb mc md me b">game_steps_per_episode</code>命令行参数增加该剧集步长限制，但这不是必需的。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="da3c" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">接下来，我们将这个奖励应用到我们的Q学习表中，但我们传入的不是当前状态，而是一个字符串<code class="fe mb mc md me b">terminal</code>，它表示一个特殊的状态，该状态应用完全奖励(乘以学习率)而不是折扣奖励。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="dd75" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">我们以gzipped pickle格式输出Q学习表数据，这样，如果我们的代理由于任何原因被停止，它可以被重新加载。</p><figure class="lw lx ly lz gt jq"><div class="bz fp l di"><div class="ma js l"/></div></figure><p id="99e0" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">然后我们重置我们的代理，使它可以重新开始。请注意，这些可以在第一步重置，但在我看来这样做更清晰。</p><p id="89e6" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">我们立即返回一个<code class="fe mb mc md me b">_NO_OP</code>调用，因为在我们的代码中继续下去没有任何价值。</p><h1 id="458b" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">8.运行代理</h1><p id="e71b" class="pw-post-body-paragraph jt ju iq jv b jw lr jy jz ka ls kc kd ke lt kg kh ki lu kk kl km lv ko kp kq ij bi translated">为了运行代理，请从命令行执行以下命令:</p><pre class="lw lx ly lz gt mt me mu mv aw mw bi"><span id="7ce2" class="mx ku iq me b gy my mz l na nb">python -m pysc2.bin.agent \<br/>--map Simple64 \<br/>--agent sparse_agent.SparseAgent \<br/>--agent_race T \<br/>--max_agent_steps 0 \<br/>--norender</span></pre><p id="81e6" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">1035场比赛后，我的经纪人的记录是这样的:</p><figure class="lw lx ly lz gt jq gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/71c0fd415146d8488593020b7e828295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*xD-D6vKpsCpH5-huCyUuEg.png"/></div></figure><p id="5c47" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">令我印象深刻的是，使用默认设置的代理丢失的时间不到50%。在我的下一个教程中，我将讲述如何让胜率超过70%。</p><p id="899a" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">你可以在这里找到奖励历史和最终Q学习表数据<a class="ae kr" href="https://docs.google.com/spreadsheets/d/10I4F4ONFo23DLp7kX5gHk62cOJ7JEzeFNj6d_GxDsqM/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="8624" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">本教程的所有代码都可以在<a class="ae kr" href="https://github.com/skjb/pysc2-tutorial/tree/master/Building%20a%20Sparse%20Reward%20Agent" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="207f" class="pw-post-body-paragraph jt ju iq jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq ij bi translated">如果你喜欢这个教程，请在<a class="ae kr" href="https://www.patreon.com/skjb" rel="noopener ugc nofollow" target="_blank"> Patreon </a>上支持我。也请和我一起上<a class="ae kr" href="https://discord.gg/qTZ65sh" rel="noopener ugc nofollow" target="_blank"> Discord </a>，或者关注我上<a class="ae kr" href="https://www.twitch.tv/skjb" rel="noopener ugc nofollow" target="_blank"> Twitch </a>、<a class="ae kr" href="https://medium.com/@skjb" rel="noopener"> Medium </a>、<a class="ae kr" href="https://github.com/skjb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>、<a class="ae kr" href="https://twitter.com/theskjb" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae kr" href="https://www.youtube.com/channel/UCZcEvhpV4_6llcrWrWQ2wsg" rel="noopener ugc nofollow" target="_blank"> YouTube </a>。</p></div></div>    
</body>
</html>