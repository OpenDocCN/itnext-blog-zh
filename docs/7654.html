<html>
<head>
<title>Simple Streaming Application to ingest Json to Iceberg Table in S3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单的流应用程序将Json导入S3的Iceberg表</h1>
<blockquote>原文：<a href="https://itnext.io/simple-streaming-application-to-ingest-json-to-iceberg-table-in-s3-9e385c17d793?source=collection_archive---------1-----------------------#2022-12-10">https://itnext.io/simple-streaming-application-to-ingest-json-to-iceberg-table-in-s3-9e385c17d793?source=collection_archive---------1-----------------------#2022-12-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/c63f3c02da0b5aa037906541c79fb5fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wPR-EF0LgzcrXzrB"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated"><a class="ae kc" href="https://unsplash.com/@simonppt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">西蒙·李</a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="6022" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想像S3一样建立流媒体应用程序来将事件摄取到存储中，你可以考虑kafka这样的事件流媒体平台，以及spark streaming application这样的流媒体应用程序来消费来自kafka主题的事件，并将其转换和保存到S3。</p><p id="a15c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是由于一些原因，如管理问题，如果您希望避免拥有如此庞大的kafka集群和spark流应用程序，这些应用程序需要在kubernetes、mesos和yarn等orchestrators上运行，您需要考虑更简单的方法来构建这样的摄取流应用程序。在这里，我将向你展示一个简单的方法。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="77e7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了理解这篇文章中的代码，复制下面的git repo:<a class="ae kc" href="https://github.com/mykidong/iceberg-example" rel="noopener ugc nofollow" target="_blank">https://github.com/mykidong/iceberg-example</a></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="e261" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">简单摄取流应用程序的架构</h1><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mg"><img src="../Images/cbea3c7d0baf10db8aa6335c911fe756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KS_YEdmtofQIPDhlqKPisw.png"/></div></div></figure><p id="dd2e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个简单的接收流应用程序基于spring boot。</p><ul class=""><li id="2137" class="ml mm iq kf b kg kh kk kl ko mn ks mo kw mp la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">Http Handler</code>处理传入的json事件和上传的excel，它们将被转换成json数组。它会将json推送到<code class="fe mu mv mw mx b">Disruptor</code>队列。</li><li id="5aaa" class="ml mm iq kf b kg my kk mz ko na ks nb kw nc la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">Disruptor</code>队列将json转发给<code class="fe mu mv mw mx b">LinkedQueue</code>。</li><li id="d58d" class="ml mm iq kf b kg my kk mz ko na ks nb kw nc la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">Embedded HTTP Server</code>正在处理来自<code class="fe mu mv mw mx b">Spark Http Receiver</code>的请求，以从<code class="fe mu mv mw mx b">LinkedQueue</code>获取json。</li><li id="8c1f" class="ml mm iq kf b kg my kk mz ko na ks nb kw nc la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">Spark HTTP Receiver</code>在spark上下文中运行将尝试向<code class="fe mu mv mw mx b">Embedded HTTP Server</code>发送请求以获取json。</li><li id="1298" class="ml mm iq kf b kg my kk mz ko na ks nb kw nc la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">Local Spark Executor</code>在本地模式下运行将从<code class="fe mu mv mw mx b">Spark HTTP Receiver</code>获取json，并将json保存到s3中的iceberg表。</li><li id="093e" class="ml mm iq kf b kg my kk mz ko na ks nb kw nc la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">Trino</code>将查询iceberg表中摄取的数据。</li></ul><h1 id="d752" class="li lj iq bd lk ll nd ln lo lp ne lr ls lt nf lv lw lx ng lz ma mb nh md me mf bi translated">先决条件</h1><p id="7075" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">在运行这个摄取应用程序之前，您需要以下内容。</p><ul class=""><li id="39d7" class="ml mm iq kf b kg kh kk kl ko mn ks mo kw mp la mq mr ms mt bi translated">Hive Metastore。</li><li id="3d29" class="ml mm iq kf b kg my kk mz ko na ks nb kw nc la mq mr ms mt bi translated">S3桶和凭证。</li><li id="4110" class="ml mm iq kf b kg my kk mz ko na ks nb kw nc la mq mr ms mt bi translated">崔诺。</li><li id="d8bc" class="ml mm iq kf b kg my kk mz ko na ks nb kw nc la mq mr ms mt bi translated">Java 11。</li><li id="ff45" class="ml mm iq kf b kg my kk mz ko na ks nb kw nc la mq mr ms mt bi translated">Maven 3。</li></ul><h2 id="5f29" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">安装配置单元Metastore</h2><p id="b5d5" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">有许多方法可以安装hive metastore。我将在kubernetes上安装hive metastore。</p><p id="b16d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">安装舵操作器。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="530b" class="od lj iq mx b be oe of l og oh">helm repo add dataroaster-helm-operator https://cloudcheflabs.github.io/helm-operator-helm-repo/<br/>helm repo update<br/><br/>helm upgrade \<br/>helm-operator \<br/>--install \<br/>--create-namespace \<br/>--namespace trino-controller \<br/>--version v1.1.1 \<br/>dataroaster-helm-operator/dataroasterhelmoperator;</span></pre><p id="ed50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并为hive metastore安装mysql。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="3b52" class="od lj iq mx b be oe of l og oh">cat &lt;&lt;EOF &gt; hive-metastore-mysql.yaml<br/>apiVersion: "helm-operator.cloudchef-labs.com/v1beta1"<br/>kind: HelmChart<br/>metadata:<br/>  name: hive-metastore-mysql<br/>  namespace: trino-controller<br/>spec:<br/>  repo: https://cloudcheflabs.github.io/mysql-helm-repo/<br/>  chartName: dataroaster-mysql<br/>  name: mysql<br/>  version: v1.0.1<br/>  namespace: hive-metastore<br/>  values: |<br/>    storage:<br/>      storageClass: &lt;storage-class&gt;<br/>      size: 2Gi<br/>EOF<br/><br/>kubectl apply -f hive-metastore-mysql.yaml;</span></pre><ul class=""><li id="1fe9" class="ml mm iq kf b kg kh kk kl ko mn ks mo kw mp la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">&lt;storage-class&gt;</code>需要改。</li></ul><p id="8f28" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，安装hive metastore。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="f59f" class="od lj iq mx b be oe of l og oh">cat &lt;&lt;EOF &gt; hive-metastore.yaml<br/>apiVersion: "helm-operator.cloudchef-labs.com/v1beta1"<br/>kind: HelmChart<br/>metadata:<br/>  name: hive-metastore<br/>  namespace: trino-controller<br/>spec:<br/>  repo: https://cloudcheflabs.github.io/hive-metastore-helm-repo/<br/>  chartName: dataroaster-hivemetastore<br/>  name: hive-metastore<br/>  version: v2.0.0<br/>  namespace: hive-metastore<br/>  values: |<br/>    image: cloudcheflabs/hivemetastore:v3.0.0<br/>    s3:<br/>      bucket: &lt;bucket&gt;<br/>      accessKey: &lt;access-key&gt;<br/>      secretKey: &lt;secret-key&gt;<br/>      endpoint: &lt;endpoint&gt;<br/>EOF<br/><br/>kubectl apply -f hive-metastore.yaml;</span></pre><p id="544d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您需要更改s3凭证，如<code class="fe mu mv mw mx b">&lt;bucket&gt;</code>、<code class="fe mu mv mw mx b">&lt;access-key&gt;</code>、<code class="fe mu mv mw mx b">&lt;secret-key&gt;</code>和<code class="fe mu mv mw mx b">&lt;endpoint&gt;</code>。</p><h2 id="05ae" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">端口转发配置单元metastore服务</h2><p id="6812" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">为了进行下面的演示，端口转发配置单元metastore服务。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="34e8" class="od lj iq mx b be oe of l og oh">kubectl port-forward svc/metastore 9083 -n hive-metastore</span></pre><h1 id="734a" class="li lj iq bd lk ll nd ln lo lp ne lr ls lt nf lv lw lx ng lz ma mb nh md me mf bi translated">演示:Json到冰山</h1><p id="aa40" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">对于这个简单的摄取流应用程序，您只需发送带有http请求的json，然后这个摄取流应用程序会将json转换为s3中的iceberg表。</p><h2 id="8de6" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">HTTP REST处理程序</h2><p id="a07e" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">主要的http rest处理程序是类<code class="fe mu mv mw mx b">CollectController</code>。下面的代码将使用参数<code class="fe mu mv mw mx b">schema</code>和<code class="fe mu mv mw mx b">table</code>处理传入的json。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="4f27" class="od lj iq mx b be oe of l og oh">    @PostMapping("/v1/event_log/create")<br/>    public String create(@RequestParam Map&lt;String, String&gt; params) {<br/>        return ControllerUtils.doProcess(() -&gt; {<br/>            String schema = params.get("schema");<br/>            String table = params.get("table");<br/>            String json = params.get("json");<br/><br/>            if(schema == null) {<br/>                throw new NullPointerException("schema is null!");<br/>            }<br/>            if(table == null) {<br/>                throw new NullPointerException("table is null!");<br/>            }<br/>            if(json == null) {<br/>                throw new NullPointerException("json is null!");<br/>            }<br/><br/>            // default catalog is iceberg.<br/>            String catalog = "iceberg";<br/><br/>            // user.<br/>            String user = bucket;<br/><br/>            // run spark streaming job.<br/>            CollectController.singletonEventLogToIceberg(disruptorHttpReceiver,<br/>                    user,<br/>                    hiveMetastoreUrl,<br/>                    bucket,<br/>                    accessKey,<br/>                    secretKey,<br/>                    endpoint);<br/><br/>            // publish event log to disruptor.<br/>            EventLogTranslator eventLogTranslator = new EventLogTranslator();<br/>            eventLogTranslator.setUser(user);<br/>            eventLogTranslator.setCatalog(catalog);<br/>            eventLogTranslator.setSchema(schema);<br/>            eventLogTranslator.setTable(table);<br/>            eventLogTranslator.setJson(json);<br/><br/>            eventLogDisruptor.publishEvent(eventLogTranslator);<br/><br/>            return ControllerUtils.successMessage();<br/>        });<br/>    }</span></pre><p id="1b56" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">传入的json将被发布到<code class="fe mu mv mw mx b">Disruptor</code>，在本地模式下运行的spark streaming executors将使用spark http receiver从<code class="fe mu mv mw mx b">LinkedQueue</code>获取json，并将json保存到iceberg表中。下面是在本地模式下运行的将json添加到iceberg表的spark流代码。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="067b" class="od lj iq mx b be oe of l og oh">      private static Thread saveEventLogToIceberg(DisruptorHttpReceiver receiver,<br/>                                                String user,<br/>                                                String metastoreUrl,<br/>                                                String bucket,<br/>                                                String accessKey,<br/>                                                String secretKey,<br/>                                                String endpoint) {<br/>        Thread t = new Thread(() -&gt; {<br/>            SparkStreamingInstance sparkStreamingInstance = SparkStreamingContextCreator.singleton(user, metastoreUrl, bucket, accessKey, secretKey, endpoint);<br/>            JavaStreamingContext ssc = sparkStreamingInstance.getJavaStreamingContext();<br/>            SparkSession spark = sparkStreamingInstance.getSpark();<br/><br/>            // receive event log from spark http receiver.<br/>            JavaDStream&lt;EventLog&gt; eventLogDStream = ssc.receiverStream(receiver);<br/><br/>            eventLogDStream.foreachRDD(rdd -&gt; {<br/>                try {<br/>                    List&lt;EventLog&gt; eventLogs = rdd.collect();<br/>                    Map&lt;String, List&lt;String&gt;&gt; eventMap = new HashMap&lt;&gt;();<br/>                    for (EventLog eventLog : eventLogs) {<br/>                        String schema = eventLog.getSchema();<br/>                        String table = eventLog.getTable();<br/>                        String tableName = "hive_prod." + schema + "." + table;<br/>                        String json = eventLog.getJson();<br/><br/>                        if (eventMap.containsKey(tableName)) {<br/>                            eventMap.get(tableName).add(json);<br/>                        } else {<br/>                            List&lt;String&gt; jsonList = new ArrayList&lt;&gt;();<br/>                            jsonList.add(json);<br/>                            eventMap.put(tableName, jsonList);<br/>                        }<br/>                    }<br/><br/>                    for (String tableName : eventMap.keySet()) {<br/>                        List&lt;String&gt; jsonList = eventMap.get(tableName);<br/>                        StructType schema = spark.table(tableName).schema();<br/>                        Dataset&lt;String&gt; jsonDs = spark.createDataset(jsonList, Encoders.STRING());<br/>                        Dataset&lt;Row&gt; df = spark.read().json(jsonDs);<br/><br/>                        // write to iceberg table.<br/>                        Dataset&lt;Row&gt; newDf = spark.createDataFrame(df.javaRDD(), schema);<br/>                        newDf.writeTo(tableName).append();<br/>                    }<br/>                } catch (Exception e) {<br/>                    e.printStackTrace();<br/>                    System.err.println(e.getMessage());<br/>                }<br/>            });<br/><br/>            ssc.start();<br/>            try {<br/>                ssc.awaitTermination();<br/>            } catch (InterruptedException e) {<br/>                e.printStackTrace();<br/>            }<br/><br/>        });<br/>        t.start();<br/><br/>        return t;<br/>    }</span></pre><h2 id="e342" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">Maven构建</h2><p id="cb3d" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">让我们构建所有的项目源。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="8b69" class="od lj iq mx b be oe of l og oh">cd &lt;iceberg-example-src-root&gt;;<br/>mvn -e -DskipTests=true clean install;</span></pre><h2 id="e243" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">运行简单的摄取流应用程序</h2><p id="8d7c" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">导出s3凭据环境变量。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="a305" class="od lj iq mx b be oe of l og oh">cd data-api;<br/><br/># export s3 credential environment variables.<br/>export S3_CREDENTIAL_BUCKET="&lt;bucket&gt;";<br/>export S3_CREDENTIAL_ACCESS_KEY="&lt;access-key&gt;";<br/>export S3_CREDENTIAL_SECRET_kEY="&lt;secret-key&gt;";<br/>export S3_CREDENTIAL_ENDPOINT="&lt;endpoint&gt;";</span></pre><p id="31c1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您需要更改s3凭证，如<code class="fe mu mv mw mx b">&lt;bucket&gt;</code>、<code class="fe mu mv mw mx b">&lt;access-key&gt;</code>、<code class="fe mu mv mw mx b">&lt;secret-key&gt;</code>和<code class="fe mu mv mw mx b">&lt;endpoint&gt;</code>。</p><p id="438c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和运行应用程序。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="f303" class="od lj iq mx b be oe of l og oh">mvn -e spring-boot:run;</span></pre><p id="473c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该应用程序正在监听<code class="fe mu mv mw mx b">8097</code>的端口。</p><h2 id="418c" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">使用Trino创建冰山表</h2><p id="c795" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">在将json事件推送到这个摄取流应用程序之前，您需要创建iceberg表。让我们使用trino创建冰山表。</p><p id="4459" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，表列名必须是字母数字序列。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="59e6" class="od lj iq mx b be oe of l og oh">-- create schema.<br/>CREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;<br/><br/>-- create table.<br/>CREATE TABLE iceberg.iceberg_db.test_iceberg (<br/>    baseproperties ROW(eventtype varchar, <br/>                       ts bigint, <br/>                       uid varchar, <br/>                       version varchar), <br/>    itemid varchar, <br/>    price bigint, <br/>    quantity bigint <br/>)<br/>WITH (<br/>    partitioning = ARRAY['itemid'],<br/>    format = 'PARQUET'<br/>);</span></pre><h2 id="430c" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">将json事件推送到简单的摄取流应用程序</h2><p id="42bf" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">将json事件推送到摄取流应用程序的测试用例如下所示。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="3c00" class="od lj iq mx b be oe of l og oh">    @Test<br/>    public void pushEventLogs() throws Exception {<br/>        String host = System.getProperty("host", "http://localhost:8097");<br/>        String eventCount = System.getProperty("eventCount", "1000");<br/><br/>        String schema = "iceberg_db";<br/>        String table = "test_iceberg";<br/>        String json = StringUtils.fileToString("data/test.json", true);<br/>        String lines[] = json.split("\\r?\\n");<br/><br/><br/>        String urlPath = host + "/v1/event_log/create";<br/><br/>        int MAX = Integer.valueOf(eventCount);<br/>        int count = 0;<br/>        while(true) {<br/>            for (String jsonLine : lines) {<br/>                FormBody.Builder builder = new FormBody.Builder();<br/>                builder.add("schema", schema);<br/>                builder.add("table", table);<br/>                builder.add("json", jsonLine);<br/><br/>                // parameters in body.<br/>                RequestBody body = builder.build();<br/>                Request request = new Request.Builder()<br/>                        .url(urlPath)<br/>                        .addHeader("Content-Length", String.valueOf(body.contentLength()))<br/>                        .post(body)<br/>                        .build();<br/>                RestResponse restResponse = ResponseHandler.doCall(simpleHttpClient.getClient(), request);<br/>                count++;<br/>            }<br/>            try {<br/>                //Thread.sleep(1000);<br/>            } catch (Exception e) {<br/>                e.printStackTrace();<br/>            }<br/>            if(count &gt; MAX) {<br/>                break;<br/>            }<br/>        }<br/>    }</span></pre><p id="6155" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行以下测试用例，将json事件推送到摄取流应用程序。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="0f3c" class="od lj iq mx b be oe of l og oh">mvn -e -Dtest=EventLogClientTestRunner test;</span></pre><h2 id="dcab" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">用trino查询冰山表中的数据</h2><p id="fc0f" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">您可以使用trino查询iceberg表中的json数据。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="91a9" class="od lj iq mx b be oe of l og oh">trino&gt; select * from iceberg.iceberg_db.test_iceberg limit 10;<br/>                           baseproperties                            |    itemid    | price | quantity<br/>---------------------------------------------------------------------+--------------+-------+----------<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/> {eventtype=cart-event, ts=1527304486873, uid=any-uid0, version=7.0} | any-item-id0 |  1000 |        2<br/>(10 rows)<br/><br/>Query 20221210_090448_00014_ti52p, FINISHED, 1 node<br/>Splits: 258 total, 258 done (100.00%)<br/>2.29 [3.02K rows, 809KB] [1.32K rows/s, 353KB/s]</span></pre><pre class="oi nz mx oa bn ob oc bi"><span id="d76c" class="od lj iq mx b be oe of l og oh">trino&gt; select count(*) from iceberg.iceberg_db.test_iceberg;<br/> _col0<br/>-------<br/>  1008<br/>(1 row)<br/><br/>Query 20221210_090643_00021_ti52p, FINISHED, 1 node<br/>Splits: 93 total, 93 done (100.00%)<br/>0.69 [1.01K rows, 217KB] [1.46K rows/s, 315KB/s]</span></pre><p id="752b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好的，工作正常。</p><h1 id="528b" class="li lj iq bd lk ll nd ln lo lp ne lr ls lt nf lv lw lx ng lz ma mb nh md me mf bi translated">演示:超越冰山</h1><p id="c4ad" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">将excel文件上传到该摄取流应用程序，然后该摄取流应用程序将excel转换为json，并最终将json保存到s3中的iceberg表。对于这个excel上传案例，您不必事先创建iceberg表。这个摄取流应用程序将自动为您做。</p><h2 id="2770" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">HTTP REST处理程序</h2><p id="0cec" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">下面是excel上传处理程序，与上面提到的json事件处理程序非常相似。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="9638" class="od lj iq mx b be oe of l og oh">    @RequestMapping(path = "/v1/excel/upload", method = POST, consumes = { MediaType.MULTIPART_FORM_DATA_VALUE })<br/>    public String uploadExcel(@RequestParam Map&lt;String, String&gt; params, @RequestPart MultipartFile file) throws Exception{<br/>        return ControllerUtils.doProcess(() -&gt; {<br/>            String schema = params.get("schema");<br/>            String table = params.get("table");<br/><br/>            // default catalog is iceberg.<br/>            String catalog = "iceberg";<br/><br/>            // schema name.<br/>            String schemaName = "hive_prod." + schema;<br/><br/>            // table name.<br/>            String tableName = schemaName + "." + table;<br/><br/>            // user.<br/>            String user = bucket;<br/>            // run spark streaming job.<br/>            CollectController.singletonEventLogToIceberg(disruptorHttpReceiver,<br/>                    user,<br/>                    hiveMetastoreUrl,<br/>                    bucket,<br/>                    accessKey,<br/>                    secretKey,<br/>                    endpoint);<br/><br/>            SparkStreamingInstance sparkStreamingInstance = SparkStreamingContextCreator.singleton(user, hiveMetastoreUrl, bucket, accessKey, secretKey, endpoint);<br/>            SparkSession spark = sparkStreamingInstance.getSpark();<br/><br/>            // convert excel to json list.<br/>            List&lt;Map&lt;String, Object&gt;&gt; rowList = excelToJson(file.getInputStream(), schemaName, tableName, spark, true);<br/><br/>            for(Map&lt;String, Object&gt; row : rowList) {<br/>                // publish event log to disruptor.<br/>                EventLogTranslator eventLogTranslator = new EventLogTranslator();<br/>                eventLogTranslator.setUser(user);<br/>                eventLogTranslator.setCatalog(catalog);<br/>                eventLogTranslator.setSchema(schema);<br/>                eventLogTranslator.setTable(table);<br/>                eventLogTranslator.setJson(JsonUtils.toJson(row));<br/><br/>                eventLogDisruptor.publishEvent(eventLogTranslator);<br/>            }<br/><br/><br/>            return ControllerUtils.successMessage();<br/>        });<br/>    }</span></pre><h2 id="cebe" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">将excel文件上传到摄取流应用程序</h2><p id="6e29" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">您可以将excel文件上传到摄取流应用程序，以便将excel数据添加到iceberg表中。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="c6dd" class="od lj iq mx b be oe of l og oh">curl -XPOST \<br/>http://localhost:8097/v1/excel/upload \<br/>--form  "schema=iceberg_db" \<br/>--form  "table=excel_to_json" \<br/>--form  "file=@/home/opc/iceberg-example/data-api/src/test/resources/data/excel-to-json2.xlsx" \<br/>;</span></pre><p id="c1b9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您需要更改<code class="fe mu mv mw mx b">file</code>参数，它是excel文件的路径。看看<code class="fe mu mv mw mx b">schema</code>和<code class="fe mu mv mw mx b">table</code>值，如果不存在，摄取流应用程序会自动创建这些值。</p><h2 id="a451" class="nn lj iq bd lk no np dn lo nq nr dp ls ko ns nt lw ks nu nv ma kw nw nx me ny bi translated">用Trino查询Iceberg表中的Excel数据</h2><p id="aaef" class="pw-post-body-paragraph kd ke iq kf b kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">您可以使用trino查询iceberg表中上传的excel摄取的数据。</p><pre class="mh mi mj mk gt nz mx oa bn ob oc bi"><span id="c7c4" class="od lj iq mx b be oe of l og oh">trino&gt; select * from iceberg.iceberg_db.excel_to_json;<br/>     name     | age  | address |    male<br/>--------------+------+---------+-------------<br/> Hamsongro 63 | 20.0 | true    | Kidong Lee<br/> í&lt;U+0095&gt;¨ì&lt;U+0086&gt;¡ë¡&lt;U+009C&gt; 63    | 21.0 | false   | ì&lt;U+009D&gt;´ê¸°ë&lt;U+008F&gt;&lt;U+0099&gt;<br/> í&lt;U+0095&gt;¨ì&lt;U+0086&gt;¡ë¡&lt;U+009C&gt; 63    | 21.0 | false   | ì&lt;U+009D&gt;´ê¸°ë&lt;U+008F&gt;&lt;U+0099&gt;2<br/> í&lt;U+0095&gt;¨ì&lt;U+0086&gt;¡ë¡&lt;U+009C&gt; 63    | 21.0 | false   | ì&lt;U+009D&gt;´ê¸°ë&lt;U+008F&gt;&lt;U+0099&gt;3<br/> í&lt;U+0095&gt;¨ì&lt;U+0086&gt;¡ë¡&lt;U+009C&gt; 63    | 21.0 | false   | ì&lt;U+009D&gt;´ê¸°ë&lt;U+008F&gt;&lt;U+0099&gt;4<br/> í&lt;U+0095&gt;¨ì&lt;U+0086&gt;¡ë¡&lt;U+009C&gt; 63    | 21.0 | false   | ì&lt;U+009D&gt;´ê¸°ë&lt;U+008F&gt;&lt;U+0099&gt;5<br/> í&lt;U+0095&gt;¨ì&lt;U+0086&gt;¡ë¡&lt;U+009C&gt; 63    | 21.0 | false   | ì&lt;U+009D&gt;´ê¸°ë&lt;U+008F&gt;&lt;U+0099&gt;6<br/> Hamsongro 63 | 20.0 | true    | Kidong Lee2<br/> Hamsongro 63 | 20.0 | true    | Kidong Lee3<br/>              | 20.0 | true    | Kidong Lee4<br/> í&lt;U+0095&gt;¨ì&lt;U+0086&gt;¡ë¡&lt;U+009C&gt; 63    | 21.0 | false   | ì&lt;U+009D&gt;´ê¸°ë&lt;U+008F&gt;&lt;U+0099&gt;7<br/>(11 rows)<br/><br/>Query 20221210_091713_00023_ti52p, FINISHED, 1 node<br/>Splits: 2 total, 2 done (100.00%)<br/>0.28 [11 rows, 3.78KB] [39 rows/s, 13.4KB/s]</span></pre><pre class="oi nz mx oa bn ob oc bi"><span id="b8ff" class="od lj iq mx b be oe of l og oh">trino&gt; select count(*) from iceberg.iceberg_db.excel_to_json;<br/> _col0<br/>-------<br/>    11<br/>(1 row)<br/><br/>Query 20221210_091746_00024_ti52p, FINISHED, 1 node<br/>Splits: 11 total, 11 done (100.00%)<br/>0.61 [11 rows, 2.92KB] [18 rows/s, 4.8KB/s]</span></pre></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="cf93" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如现在所看到的，有一种简单的方法来构建这样的摄取流应用程序，以便将json摄取到s3中的iceberg表，而不必在集群模式下运行如此大的kafka集群和spark流应用程序。要横向扩展这个摄取流应用程序(它只是一个处理更多json请求和excel上传的spring boot应用程序),有一种方法可以让您在kubernetes上轻松地横向扩展这个摄取流应用程序。</p></div></div>    
</body>
</html>