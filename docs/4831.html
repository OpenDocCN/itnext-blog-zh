<html>
<head>
<title>Migrating Apache Spark workloads from AWS EMR to Kubernetes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将Apache Spark工作负载从AWS EMR迁移到Kubernetes</h1>
<blockquote>原文：<a href="https://itnext.io/migrating-apache-spark-workloads-from-aws-emr-to-kubernetes-463742b49fda?source=collection_archive---------0-----------------------#2020-09-30">https://itnext.io/migrating-apache-spark-workloads-from-aws-emr-to-kubernetes-463742b49fda?source=collection_archive---------0-----------------------#2020-09-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/5ceac2c15af8a49da3ec8757c2b8e3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l231FCXKJUSMY6rtOFMPoQ.png"/></div></div></figure><h1 id="ec5a" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">介绍</h1><p id="ad01" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">ESG research <a class="ae lu" href="https://aws.amazon.com/big-data/what-is-spark/" rel="noopener ugc nofollow" target="_blank">发现，43% </a>的受访者认为云是Apache Spark的主要部署。这很有意义，因为云提供了可伸缩性、可靠性、可用性和巨大的规模经济。云部署的另一个强大卖点是托管服务形式的低准入门槛。“三大”云提供商中的每一家都有自己的产品来运行Apache Spark作为托管服务。你大概听说过<a class="ae lu" href="https://aws.amazon.com/emr/" rel="noopener ugc nofollow" target="_blank"> AWS弹性图还原</a>、<a class="ae lu" href="https://azure.microsoft.com/en-us/services/databricks/" rel="noopener ugc nofollow" target="_blank"> Azure Databricks </a>、<a class="ae lu" href="https://azure.microsoft.com/en-us/services/hdinsight/" rel="noopener ugc nofollow" target="_blank"> Azure HDInsight </a>、<a class="ae lu" href="https://cloud.google.com/dataproc" rel="noopener ugc nofollow" target="_blank"> Google Dataproc </a>。</p><p id="272f" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我将重点介绍AWS Elastic Map Reduce，因为我们在AWS上运行Spark工作负载。我们使用Apache Airflow进行工作流程编排。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/42110efacc00e35daf7e01da0e51d47d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73gRIkecKWjSr8GWqbmlgQ.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">数据流</figcaption></figure><p id="a9f7" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">数据来自不同的来源，分布在不同的地理区域，不一定运行在AWS云上。例如，一些数据源是在浏览器中运行的web应用程序，另一些是移动应用程序，还有一些是外部数据管道等。<a class="ae lu" href="https://medium.com/swlh/no-code-data-collect-api-on-aws-d79e3681d204" rel="noopener">此处</a>和<a class="ae lu" href="https://medium.com/swlh/no-code-data-collect-api-f8e934ed8535" rel="noopener">此处</a>您可以看到我们是如何实施数据摄取步骤的。所有输入数据都收集在S3桶中，并根据<a class="ae lu" href="https://aws.amazon.com/dynamodb/" rel="noopener ugc nofollow" target="_blank"> AWS DynamoDB </a>中的创建日期进行索引。这样做允许我们在任何给定的时间间隔内处理数据批次。我们每天处理2TB的数据，而在“特殊事件”日，数据量可能会大得多。</p><h1 id="7bdd" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">问题陈述</h1><p id="4690" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">总的来说，AWS EMR做得很好。它是管理Apache Spark集群的一个可靠、可伸缩和灵活的工具。AWS EMR以AWS Cloudwatch的形式提供开箱即用的监控，它提供了丰富的工具箱，包括<a class="ae lu" href="https://zeppelin.apache.org/" rel="noopener ugc nofollow" target="_blank"> Zeppelin </a>、<a class="ae lu" href="https://livy.apache.org/" rel="noopener ugc nofollow" target="_blank"> Livy </a>、<a class="ae lu" href="https://gethue.com/" rel="noopener ugc nofollow" target="_blank"> Hue </a>等，并且具有非常好的安全特性。但是AWS EMR也有自己的降级。</p><p id="8b3d" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><strong class="ky ir">可移植性</strong>:如果您正在构建一个多云或混合(云/本地)解决方案，请注意从AWS EMR迁移Spark应用程序可能是一件大事。在AWS EMR上运行一段时间后，您会发现自己与AWS的特定功能紧密结合。它可以很简单，比如日志和监控，也可以很复杂，比如自动伸缩机制、定制的主/从AMIs、AWS安全特性等等。</p><p id="4b23" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><strong class="ky ir">成本开销</strong>:亚马逊EMR价格是亚马逊EC2价格的补充。看一下定价示例<a class="ae lu" href="https://aws.amazon.com/emr/pricing/" rel="noopener ugc nofollow" target="_blank">这里</a></p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/a8b87b339eeceff0e26aed872eef6589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qE_CtWsDrc4CPjTdzfiUJA.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">AWS EMR定价</figcaption></figure><p id="968a" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">如您所见，在使用m5.xlarge、m5.2xlarge或m5.4xlarge机器时，您必须在基础EC2的按需价格上增加25%。当你使用大型机器时，EMR费用在12%到6%之间。当使用spot时，EMR价格可以高达基础EC2 spot实例价格的35%。例如，请参见附件中取自真实生产环境的3天账单。这里，AWS EMR集群由spot实例组成，您可能会看到EMR服务有些昂贵。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/41c42c2d89817794b5b6ff22b2abc350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ixkkRZMpIbJq0Ft885pKQ.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">三天期票据(美元)</figcaption></figure><p id="7ee4" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">通过对给定的三天账单进行年度预测，您可以看到，在这种特定情况下，EMR服务的成本可能高达每年300，000美元。</p><h1 id="cbd0" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">目标</h1><p id="5da1" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">综上所述，运行Apache Spark的理想框架通过以下目标实现了总体目标:</p><h2 id="87c5" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">可扩展性。</h2><p id="7a4f" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">随着计算需求的变化，新的解决方案应该提供扩展Apache Spark集群的能力。</p><h2 id="f903" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">可靠性:</h2><p id="0375" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">新的解决方案应该监控计算节点，并在出现故障时自动终止和替换实例。</p><h2 id="a05c" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">便携性:</h2><p id="de6e" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">新的解决方案应该消除对底层基础设施的依赖。</p><h2 id="a5ee" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">成本效益:</h2><p id="55ff" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">新的解决方案应该会降低托管服务的成本。</p><h1 id="8dfd" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">在Kubernetes上运行Apache Spark</h1><p id="5815" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">Apache Spark目前支持4种不同的集群管理器:<strong class="ky ir">独立、Apache Mesos </strong>、<strong class="ky ir"> Hadoop YARN和Kubernetes </strong>。前三个是众所周知的，已经存在了一段时间。点击这里，你可以找到一篇涵盖所有这些问题的精彩文章<a class="ae lu" href="https://data-flair.training/blogs/apache-spark-cluster-managers-tutorial/" rel="noopener ugc nofollow" target="_blank">。Kubernetes的支持是新的，它是在Spark 2.3(2018年2月)中引入的，从可移植性的角度来看，这是一大进步。Kubernetes为管理容器化的应用程序提供了强大的抽象。Kubernetes消除了基础架构锁定，允许在多个操作环境中运行应用程序，包括专用的本地服务器、虚拟化私有云以及公共云。</a></p><blockquote class="mx my mz"><p id="ac42" class="kw kx na ky b kz lv lb lc ld lw lf lg nb lx lj lk nc ly ln lo nd lz lr ls lt ij bi translated">"<a class="ae lu" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a>允许您在任何地方<a class="ae lu" href="https://www.infoworld.com/article/3173266/4-reasons-you-should-use-kubernetes.html#:~:text=Kubernetes%20allows%20you%20to%20deploy,exactly%20as%20you%20like%20everywhere&amp;text=With%20containers%2C%20it's%20easy%20to,lighter%20weight%20than%20virtual%20machines." rel="noopener ugc nofollow" target="_blank">部署云原生应用程序</a>并完全按照您喜欢的方式在任何地方管理它们"</p></blockquote><p id="f221" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">“Kubernetes”有两个运行Apache Spark应用程序的选项:通过<a class="ae lu" href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" rel="noopener ugc nofollow" target="_blank"> spark-submit </a>或使用<a class="ae lu" href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator#:~:text=The%20Kubernetes%20Operator%20for%20Apache%20Spark%20aims%20to%20make%20specifying,surfacing%20status%20of%20Spark%20applications." rel="noopener ugc nofollow" target="_blank"> Kubernetes操作符运行Spark </a>。后一种选择在管理和监控方面带来了很多。我将重点讨论第一个，因为它非常简单，由Spark提供本地支持，并且与Apache Airflow有很好的集成。</p><h2 id="a9e3" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">它是如何工作的</h2><p id="6554" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">spark的bin目录中的<a class="ae lu" href="https://spark.apache.org/docs/latest/submitting-applications.html" rel="noopener ugc nofollow" target="_blank"> spark-submit </a>脚本用于在使用各种资源管理器(YARN、Apache Mesos、Kubernetes)的集群上启动应用程序:</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="77ef" class="ml jz iq nf b gy nj nk l nl nm">./bin/spark-submit \<br/>  --class &lt;main-class&gt; \<br/>  --master &lt;master-url&gt; \<br/>  --deploy-mode &lt;deploy-mode&gt; \<br/>  --conf &lt;key&gt;=&lt;value&gt; \<br/>  ... # other options<br/>  &lt;application-jar&gt; \<br/>  [application-arguments]</span></pre><p id="e1c2" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">当master的URL以<strong class="ky ir"><em class="na">k8s</em></strong><em class="na">://【Kubernetes _ DNS】</em>开头时，Spark会在<em class="na"> Kubernetes_DNS </em>指定的集群上的<a class="ae lu" href="https://kubernetes.io/docs/concepts/workloads/pods/pod/" rel="noopener ugc nofollow" target="_blank"> Kubernetes pod </a>内创建一个运行的Spark驱动程序。驱动程序将在Kubernetes pods中创建执行器，并执行应用程序代码。当应用程序完成时，executor pods终止并被清理，但是driver pod保存日志并保持“完成”状态，直到它最终被垃圾收集或手动清理。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/fce08a185cbfd716effb605e4bca4069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*skc1DB6A5mb_yj02qxeofw.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">在K8s集群中执行spark-submit</figcaption></figure><p id="33e2" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">现在我们对在K8s上运行Apache Spark有了一个清晰的了解，我们有动力开始迁移过程。</p><h2 id="57b3" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">运行AWS EKS集群</h2><p id="deb6" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">第一步是创建Kubernetes集群。这里我们将使用EKS —一个完全管理的Kubernetes集群。AWS有如何开始使用EKS的可靠文档，例如，你可以在这里和这里看到逐步指南<a class="ae lu" href="https://aws.amazon.com/eks/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc&amp;eks-blogs.sort-by=item.additionalFields.createdDate&amp;eks-blogs.sort-order=desc" rel="noopener ugc nofollow" target="_blank">。我将总结整个过程。从克隆以下存储库开始:</a></p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="bc04" class="ml jz iq nf b gy nj nk l nl nm">git clone https://github.com/aws-samples/amazon-eks-apache-spark-etl-sample.git</span></pre><p id="f132" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">打开并编辑example/eksctl.yaml文件。它包含以下内容:</p><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="no np l"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">eksctl.yaml</figcaption></figure><p id="66b8" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">如果您想在us-east-1 (N.Virginia)以外的地区创建EKS集群，或者您更喜欢其他机器类型和容量，请编辑该文件。更改第22行并设置您的IAM_POLICY_ARN。然后，通过运行以下命令创建EKS集群</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="f4d8" class="ml jz iq nf b gy nj nk l nl nm">eksctl create cluster -f example/eksctl.yaml</span></pre><p id="e3a8" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">设置新的EKS集群大约需要15分钟。最后，您应该能够在EKS集群仪表板上看到以下信息:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/29692276fab4d3eedc9d7358a00f7c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U59o6W8mrsqJPJJuz_SvPw.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">EKS仪表板</figcaption></figure><p id="9933" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">API服务器端点(在橙色矩形中)很重要，Apache Airflow将使用它来提交spark作业。集群运行后，创建Kubernetes服务帐户和集群角色绑定，以授予Kubernetes对Spark作业的编辑权限:</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="cb60" class="ml jz iq nf b gy nj nk l nl nm">kubectl apply -f example/kubernetes/spark-rbac.yaml</span></pre><p id="26aa" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">注意，这里我们假设Spark应用程序将在默认名称空间中运行。如果要在专用名称空间中运行spark应用程序，请更改spark-rbac.yaml文件。</p><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="no np l"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">spark-rbac.yaml</figcaption></figure><h2 id="607b" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">为Spark应用程序构建Docker映像</h2><p id="2921" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">根据定义，Kubernetes是一个容器编排系统，因此我们必须为Spark应用程序创建一个docker映像。在这里，我将描述如何为Scala应用程序实现这一点。这个想法是创建docker映像，其中包含Apache Spark和Hadoop的发行版，以及一个Fat Jar，其中包含所有项目类文件和资源以及所有依赖项。第一步是向project.sbt文件添加一个程序集SBT插件(如果不存在的话)</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ba4b681d1fe4ab945ce5765f8891f595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*vB0wNBfvraVm5lKrG1TTrw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">project.sbt</figcaption></figure><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="b5f1" class="ml jz iq nf b gy nj nk l nl nm">addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.9")</span></pre><p id="5d8d" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">使用这个插件的目标很简单:创建一个包含所有依赖项的Scala项目的胖罐子。下一步是查看SBT文件中缺少的依赖项。AWS EMR附带了许多预装的库，在Kubernetes上运行时会缺少这些库。所以我们必须把它们都带到docker映像中。当然，库的数量和类型会因项目而异，但至少我们需要以下内容:</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="248a" class="ml jz iq nf b gy nj nk l nl nm">"com.amazonaws" % "aws-java-sdk" % "1.7.4"<br/>"org.apache.hadoop" % "hadoop-aws" % "2.7.1"</span></pre><p id="c66f" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">这些库将用于消费AWS服务，如S3、DynamoDB等。最后一步是创建docker文件。添加新文件</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f9dc40ebc37ebc4b0d243d6aeb33ef28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*HzT2iEiynt5PcLAV42fQTg.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">Dockerfile文件</figcaption></figure><p id="d17b" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">使用克隆存储库中Dockerfile的内容—<a class="ae lu" href="https://github.com/aws-samples/amazon-eks-apache-spark-etl-sample.git" rel="noopener ugc nofollow" target="_blank">https://github . com/AWS-samples/Amazon-eks-Apache-spark-ETL-sample . git</a>作为蓝本。这个docker文件安装了以下Apache Spark发行版:</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="5cc6" class="ml jz iq nf b gy nj nk l nl nm"><a class="ae lu" href="https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</a></span></pre><p id="b67a" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">在SBT文件中更改Spark或Hadoop版本时要小心。每个Apache Spark版本都与Hadoop的特定版本兼容。您应该知道的Other文件中的其他重要行:</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="bf24" class="ml jz iq nf b gy nj nk l nl nm"># Define working directory<br/>WORKDIR /opt/input<br/><br/># Project Definition layers change less often than application code<br/>COPY build.sbt ./<br/><br/>WORKDIR /opt/input/project<br/>COPY project/build.properties ./<br/>COPY project/*.sbt ./<br/><br/>WORKDIR /opt/input<br/><br/># Copy rest of application<br/>COPY . ./<br/>RUN SBT_OPTS="-Xms4096M -Xmx4096M -Xss1024M -XX:MaxMetaspaceSize=4096M" sbt clean assembly</span></pre><p id="cb5d" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">它将所有项目的源文件和资源复制到docker映像并构建Fat Jar。构建docker映像:</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="4b8d" class="ml jz iq nf b gy nj nk l nl nm">docker build -t &lt;REPO_NAME&gt;/&lt;IMAGE_NAME&gt;:v1.0 .</span></pre><p id="1990" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">确保您可以在docker映像中看到以下文件</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="6442" class="ml jz iq nf b gy nj nk l nl nm">//opt/spark/jars/*-assembly-v1.0.0.jar</span></pre><p id="742f" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">这个文件包含您的Spark应用程序代码。最后，您可以将docker图像上传到AWS ECR</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="9fef" class="ml jz iq nf b gy nj nk l nl nm">docker push &lt;REPO_NAME&gt;/&lt;IMAGE_NAME&gt;:v1.0</span></pre><h2 id="d7ac" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">气流库伯内特操作员</h2><p id="99a0" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">现在，我们准备从Apache Airflow中的代码执行spark-submit。Apache Airflow支持在底层Kubernetes集群或远程集群上启动新的Kubernetes pods。例如，您可以按以下方式使用KubernetesPodOperator:</p><figure class="mb mc md me gt jr"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="a840" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我猜代码本身非常简单，我将只关注重要的参数。我们将在“spark”服务帐户下的默认名称空间中的底层Kubernetes集群上运行Spark应用程序。</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="adea" class="ml jz iq nf b gy nj nk l nl nm">eks_in_cluster = true<br/>eks_namespace = "default"<br/>eks_service_account_name="spark"</span></pre><p id="9749" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">请注意，通常，隔离Spark应用程序并在专用名称空间中运行它们是有意义的。因此，如果其他应用程序正在使用默认名称空间，最好创建一个专用的spark apps名称空间。</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="bcc0" class="ml jz iq nf b gy nj nk l nl nm">eks_context = <em class="na">CLUSTER_ARN</em></span></pre><p id="592e" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">集群ARN可以在EKS仪表板上找到</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/29692276fab4d3eedc9d7358a00f7c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U59o6W8mrsqJPJJuz_SvPw.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">EKS仪表板</figcaption></figure><p id="0154" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">eks_command参数就是您的spark-submit命令，它应该具有以下形式:</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="a147" class="ml jz iq nf b gy nj nk l nl nm">/bin/sh,-c,/opt/spark/bin/spark-submit --master {} --deploy-mode cluster --name {} --conf spark.driver.memory={} --conf spark.executor.instances={} --conf spark.executor.memory={} --conf spark.executor.cores={}  --conf spark.kubernetes.container.image={}  --conf spark.kubernetes.authenticate.driver.serviceAccountName={} {} --class {} {} {}</span></pre><p id="3a1d" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">用户k8s://<em class="na">API _ SERVER _ ENDPOINT</em>(从EKS仪表板)作为集群的主服务器。</p><p id="ac7d" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">现在，当Apache Airflow执行KubernetesPodOperator时，您会看到为驱动程序和执行器创建了新的pods。所有日志都可以从pods stdout中获取。使用Kubernetes端口转发，通过Spark驱动程序上的Spark UI来监控作业。快跑吧</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="9982" class="ml jz iq nf b gy nj nk l nl nm">kubectl port-forward &lt;SPARK_DRIVER_POD_NAME&gt; 4040:4040</span></pre><p id="232d" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">并从浏览器导航到localhost:4040以查看Spark UI。</p><h2 id="88a9" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">与普罗米修斯和格拉夫纳一起监控</h2><p id="7707" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">EMR的一个非常强大的功能是内置的监控和警报功能，其形式为CloudWatch指标和警报。对于每个EMR集群，指标每五分钟更新一次，并自动收集和推送到CloudWatch。Amazon EMR报告的指标提供了可用于跟踪Apache Spark工作负载进度、分析内存和CPU使用情况、检测不健康节点等的信息。显然，当迁移到Kubernetes时，我们正在失去所有这些能力。幸运的是，有很多监控和可观察性工具可以安装在K8s集群上。最受欢迎的解决方案之一是普罗米修斯和格拉夫纳。这些工具的设置超出了本文的范围，但是您可以在K8s <a class="ae lu" href="https://medium.com/better-programming/monitor-your-kubernetes-cluster-with-prometheus-and-grafana-1f7d0195e59" rel="noopener">这里</a>找到Prometheus和Grafana的介绍。一旦设置好这些工具，我们就可以通过对Prometheus数据源使用Grafana查询来轻松定义Spark应用程序的<a class="ae lu" href="https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/" rel="noopener ugc nofollow" target="_blank">黄金</a>指标:延迟、流量、错误、饱和度。</p><ol class=""><li id="3dac" class="nt nu iq ky b kz lv ld lw lh nv ll nw lp nx lt ny nz oa ob bi translated">饱和度——衡量已用资源的指标</li></ol><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="d052" class="ml jz iq nf b gy nj nk l nl nm">node:node_cpu_saturation_load1:</span><span id="00f3" class="ml jz iq nf b gy oc nk l nl nm">node:node_memory_utilisation:ratio*100</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi od"><img src="../Images/158b0df9a50c409c72e149ec8b855f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bEbPG4zXpo6Riy9q2VFz1g.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">CPU和内存利用率</figcaption></figure><p id="e99d" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">2.延迟—处理一个请求(批处理)所需的时间</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="b46a" class="ml jz iq nf b gy nj nk l nl nm">avg(kube_pod_completion_time{namespace="default", pod=~".*driver.*"} - kube_pod_start_time{namespace="default", pod=~".*driver.*"})</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/75e05a4a0f00cac8fd0ba230447e2681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*Lq2T-AKgmXOkfMMjYahm3w.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">Spark应用持续时间</figcaption></figure><p id="bc19" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">3.流量被定义为自定义应用程序指标，并显示输入Spark数据帧中的行数</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e6d728f4b849b1b34628e26e73a06398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*yQSy2qDW-1QgcsllJP00uQ.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">火花输入大小</figcaption></figure><p id="7dc6" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">4.错误-计算失败的pod的数量。可以按原因汇总。</p><pre class="mb mc md me gt ne nf ng nh aw ni bi"><span id="78c3" class="ml jz iq nf b gy nj nk l nl nm">count(kube_pod_container_status_terminated_reason{namespace="default", container=~".*driver.*"})</span></pre><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1c35d04f3d7884716dcaad8f64984d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*pHfppG-6r3IqYDVrW5dsLw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">火花误差</figcaption></figure><h1 id="9f24" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">摘要</h1><p id="6dfb" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">迁移Apache Spark在可移植性、成本和资源共享效率方面带来了很多好处。</p><h2 id="cac9" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">费用</h2><p id="c1ca" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">与EMR相比，在Kubernetes上运行相同Spark工作负载的成本要低得多。AWS EKS集群每小时成本仅为0.10美元(每月72美元)。另一方面，AWS EMR价格始终是基础EC2机器成本的函数。正如本文开头所显示的，EMR对总成本的贡献很大，尤其是当Spark工作负载很大时。如果您需要一个粗略的估计，与在按需EC2上运行的EMR相比，您将节省高达20%的成本，与在现场运行的EMR相比，您将节省高达30%的成本。</p><h2 id="b002" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated">轻便</h2><p id="898e" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在EMR上运行时，Spark应用程序和支持它们的工具与底层AWS基础设施紧密相关，如专有的扩展技术、对ami的依赖性、安全特性、日志和监控系统等。Kubernetes通过为容器提供核心功能而不施加限制，消除了AWS的局限性。使用Kubernetes时，您可以跨多个操作环境运行Apache Spark工作负载，包括GCP、AWS、Azure、本地服务器和私有云。</p><h2 id="94b6" class="ml jz iq bd ka mm mn dn ke mo mp dp ki lh mq mr km ll ms mt kq lp mu mv ku mw bi translated"><strong class="ak">资源共享</strong></h2><p id="b4f8" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">Kubernetes高效地利用了底层基础设施，因为容器比虚拟机轻得多。在Spark应用程序和其他服务之间共享相同的EC2机器降低了对计算能力的需求。</p></div></div>    
</body>
</html>