<html>
<head>
<title>Azkaban on Kubernetes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">库贝内特斯岛上的阿兹卡班</h1>
<blockquote>原文：<a href="https://itnext.io/azkaban-on-kubernetes-149fde21aa50?source=collection_archive---------4-----------------------#2020-11-03">https://itnext.io/azkaban-on-kubernetes-149fde21aa50?source=collection_archive---------4-----------------------#2020-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f3a74b37c1fe6ebff5760e8d32666e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rgpQFrX9aeatfG-N"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">由<a class="ae kc" href="https://unsplash.com/@sarahdorweiler?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">莎拉·多维勒</a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="fe10" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Azkaban是一个流行的工作流引擎，我曾多次使用它来运行作业，尤其是在数据湖中。还有类似的工作流调度程序，如Oozie、Airflow，它们比Azkaban提供了更多的功能，但我更喜欢Azkaban，因为Azkaban的UI比其他的更有吸引力。</p><p id="ae14" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管Azkaban提供了几种作业类型，如hadoop、java、command、pig、hive等，但我在大多数情况下只使用了command作业类型。使用命令作业类型，您可以只键入一些shell命令来运行作业。这很简单，而且我认为它适用于大多数情况。在本文中，将只使用命令作业类型来运行作业。</p><p id="624f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">阿兹卡班由充当协调者的阿兹卡班Web服务器、充当工作者的阿兹卡班执行器和处理所有作业元数据的MySQL组成。我将在这里向你展示如何在kubernetes上运行阿兹卡班网络服务器、执行器和mysql。这里使用的所有代码都可以在我的git repo中找到:<a class="ae kc" href="https://github.com/mykidong/azkaban-on-kubernetes" rel="noopener ugc nofollow" target="_blank">https://github.com/mykidong/azkaban-on-kubernetes</a></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="522d" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">用源代码构建阿兹卡班(可选)</h1><p id="b1ad" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">因为我还没有找到预构建的azkaban 3.x，所以我打算用azkaban源代码来构建。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="84ae" class="mu lj iq mq b gy mv mw l mx my">cd ~;<br/><br/>git clone https://github.com/azkaban/azkaban.git<br/>cd azkaban;<br/><br/>git checkout tags/3.90.0;<br/><br/># Build and install distributions<br/>./gradlew installDist<br/><br/># package azkaban as tar files.<br/>## db.<br/>cd ~/azkaban/azkaban-db/build/install;<br/>tar -zcf azkaban-db-3.90.0.tar.gz azkaban-db;<br/><br/>## executor.<br/>cd ~/azkaban/azkaban-exec-server/build/install;<br/>tar -zcf azkaban-exec-server-3.90.0.tar.gz azkaban-exec-server;<br/><br/>## web.<br/>d ~/azkaban/azkaban-web-server/build/install;<br/>tar -zcf azkaban-web-server-3.90.0.tar.gz azkaban-web-server;</span></pre><p id="f0bc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要三个包，分别是azkaban db其中有sql脚本创建azkaban数据库和mysql中的表，azkaban executor和web服务器。把阿兹卡班打包成gz后，我已经把这些包上传到google drive了。</p><h1 id="5e05" class="li lj iq bd lk ll mz ln lo lp na lr ls lt nb lv lw lx nc lz ma mb nd md me mf bi translated">创建阿兹卡班码头形象</h1><p id="7524" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">在这里，我将为azkaban数据库、执行器和web服务器构建azkaban docker映像。</p><p id="ff21" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们看看azkaban db的Dockerfile，它用于创建azkaban数据库和mysql db的表。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="04b8" class="mu lj iq mq b gy mv mw l mx my">FROM java:8-jre<br/><br/>ENV <em class="ne">APP_HOME </em>/opt/azkaban-db<br/><br/>RUN echo "Asia/Seoul" &gt; /etc/timezone<br/>RUN dpkg-reconfigure -f noninteractive tzdata<br/><br/>RUN useradd -ms /bin/bash -d ${<em class="ne">APP_HOME</em>} db<br/><br/>RUN set -ex \<br/>    &amp;&amp; AZKABAN_DB_NAME=azkaban-db-3.90.0 \<br/>    &amp;&amp; fileId=1_oYPbDg3MKAu4RjL0P-_ZIl5ixlPgq04 \<br/>    &amp;&amp; fileName=${<em class="ne">AZKABAN_DB_NAME</em>}.tar.gz \<br/>    &amp;&amp; curl -sc /tmp/cookie "https://drive.google.com/uc?export=download&amp;id=${<em class="ne">fileId</em>}" &gt; /dev/null \<br/>    &amp;&amp; code="$(awk '/_warning_/ {print $<em class="ne">NF</em>}' /tmp/cookie)" \<br/>    &amp;&amp; curl -Lb /tmp/cookie "https://drive.google.com/uc?export=download&amp;confirm=${<em class="ne">code</em>}&amp;id=${<em class="ne">fileId</em>}" -o ${<em class="ne">fileName</em>} \<br/>    &amp;&amp; tar -zxf ${<em class="ne">fileName</em>} -C ${<em class="ne">APP_HOME</em>} \<br/>    &amp;&amp; cp -R ${<em class="ne">APP_HOME</em>}/azkaban-db/* ${<em class="ne">APP_HOME</em>}/ \<br/>    &amp;&amp; rm -rf ${<em class="ne">APP_HOME</em>}/azkaban-db \<br/>    &amp;&amp; rm -rf ${<em class="ne">fileName</em>}<br/><br/><br/>RUN chown db: -R ${<em class="ne">APP_HOME</em>}<br/><br/>RUN echo "deb [check-valid-until=no] http://cdn-fastly.deb.debian.org/debian jessie main" &gt; /etc/apt/sources.list.d/jessie.list<br/>RUN echo "deb [check-valid-until=no] http://archive.debian.org/debian jessie-backports main" &gt; /etc/apt/sources.list.d/jessie-backports.list<br/>RUN sed -i '/deb http:\/\/deb.debian.org\/debian jessie-updates main/d' /etc/apt/sources.list<br/>RUN apt-get -o Acquire::Check-Valid-Until=false update<br/><br/>RUN apt-get -y -f install mysql-client<br/><br/>USER db</span></pre><p id="15c1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上图所示，阿兹卡班db包是从google drive下载的，它是由tar提取的。注意<code class="fe nf ng nh mq b">mysql-client</code>安装在文件的末尾。</p><p id="5ba6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们来看看阿兹卡班的执行者Dockerfile。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="15f3" class="mu lj iq mq b gy mv mw l mx my">FROM java:8-jre<br/><br/>ENV <em class="ne">APP_HOME </em>/opt/azkaban-executor<br/><br/>RUN echo "Asia/Seoul" &gt; /etc/timezone<br/>RUN dpkg-reconfigure -f noninteractive tzdata<br/><br/>RUN useradd -ms /bin/bash -d ${<em class="ne">APP_HOME</em>} executor<br/><br/>RUN set -ex \<br/>    &amp;&amp; AZKABAN_EXEC_NAME=azkaban-exec-server-3.90.0 \<br/>    &amp;&amp; fileId=15jllIx3eAmAb9d-GZ_KISWxnZuAJiP5r \<br/>    &amp;&amp; fileName=${<em class="ne">AZKABAN_EXEC_NAME</em>}.tar.gz \<br/>    &amp;&amp; curl -sc /tmp/cookie "https://drive.google.com/uc?export=download&amp;id=${<em class="ne">fileId</em>}" &gt; /dev/null \<br/>    &amp;&amp; code="$(awk '/_warning_/ {print $<em class="ne">NF</em>}' /tmp/cookie)" \<br/>    &amp;&amp; curl -Lb /tmp/cookie "https://drive.google.com/uc?export=download&amp;confirm=${<em class="ne">code</em>}&amp;id=${<em class="ne">fileId</em>}" -o ${<em class="ne">fileName</em>} \<br/>    &amp;&amp; tar -zxf ${<em class="ne">fileName</em>} -C ${<em class="ne">APP_HOME</em>} \<br/>    &amp;&amp; cp -R ${<em class="ne">APP_HOME</em>}/azkaban-exec-server/* ${<em class="ne">APP_HOME</em>}/ \<br/>    &amp;&amp; rm -rf ${<em class="ne">APP_HOME</em>}/azkaban-exec-server \<br/>    &amp;&amp; rm -rf ${<em class="ne">APP_HOME</em>}/conf/azkaban.properties \<br/>    &amp;&amp; rm -rf ${<em class="ne">fileName</em>}<br/><br/>COPY activate-executor.sh ${<em class="ne">APP_HOME</em>}/bin/activate-executor.sh<br/>COPY start-exec.sh ${<em class="ne">APP_HOME</em>}/bin/start-exec.sh<br/>COPY start-and-activate-exec.sh ${<em class="ne">APP_HOME</em>}/bin/start-and-activate-exec.sh<br/><br/>RUN chmod a+x -R ${<em class="ne">APP_HOME</em>}/bin<br/>RUN chown executor: -R ${<em class="ne">APP_HOME</em>}<br/><br/>RUN echo "deb [check-valid-until=no] http://cdn-fastly.deb.debian.org/debian jessie main" &gt; /etc/apt/sources.list.d/jessie.list<br/>RUN echo "deb [check-valid-until=no] http://archive.debian.org/debian jessie-backports main" &gt; /etc/apt/sources.list.d/jessie-backports.list<br/>RUN sed -i '/deb http:\/\/deb.debian.org\/debian jessie-updates main/d' /etc/apt/sources.list<br/>RUN apt-get -o Acquire::Check-Valid-Until=false update<br/>RUN apt-get install -y openssh-client<br/><br/>USER executor<br/>RUN ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa<br/>WORKDIR ${<em class="ne">APP_HOME</em>}</span></pre><p id="eabb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类似于azkaban db dockerfile，下载并解压azkaban executor包。复制一些shell文件，最后生成ssh密钥，用于通过ssh连接远程机器。</p><p id="6781" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">阿兹卡班网络服务器:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="95cb" class="mu lj iq mq b gy mv mw l mx my">FROM java:8-jre<br/><br/>ENV <em class="ne">APP_HOME </em>/opt/azkaban-web<br/><br/>RUN echo "Asia/Seoul" &gt; /etc/timezone<br/>RUN dpkg-reconfigure -f noninteractive tzdata<br/><br/>RUN useradd -ms /bin/bash -d ${<em class="ne">APP_HOME</em>} web<br/><br/>RUN set -ex \<br/>    &amp;&amp; AZKABAN_WEB_NAME=azkaban-web-server-3.90.0 \<br/>    &amp;&amp; fileId=1GzVG5_aKlG8Mb38M3a10jF8X-VYpSxJx \<br/>    &amp;&amp; fileName=${<em class="ne">AZKABAN_WEB_NAME</em>}.tar.gz \<br/>    &amp;&amp; curl -sc /tmp/cookie "https://drive.google.com/uc?export=download&amp;id=${<em class="ne">fileId</em>}" &gt; /dev/null \<br/>    &amp;&amp; code="$(awk '/_warning_/ {print $<em class="ne">NF</em>}' /tmp/cookie)" \<br/>    &amp;&amp; curl -Lb /tmp/cookie "https://drive.google.com/uc?export=download&amp;confirm=${<em class="ne">code</em>}&amp;id=${<em class="ne">fileId</em>}" -o ${<em class="ne">fileName</em>} \<br/>    &amp;&amp; tar -zxf ${<em class="ne">fileName</em>} -C ${<em class="ne">APP_HOME</em>} \<br/>    &amp;&amp; cp -R ${<em class="ne">APP_HOME</em>}/azkaban-web-server/* ${<em class="ne">APP_HOME</em>}/ \<br/>    &amp;&amp; rm -rf ${<em class="ne">APP_HOME</em>}/azkaban-web-server \<br/>    &amp;&amp; rm -rf ${<em class="ne">APP_HOME</em>}/conf/azkaban.properties \<br/>    &amp;&amp; rm -rf ${<em class="ne">fileName</em>}<br/><br/>COPY start-web.sh ${<em class="ne">APP_HOME</em>}/bin/start-web.sh<br/><br/>RUN chmod a+x -R ${<em class="ne">APP_HOME</em>}/bin<br/>RUN chown web: -R ${<em class="ne">APP_HOME</em>}<br/><br/>EXPOSE 8081<br/>USER web<br/>WORKDIR ${<em class="ne">APP_HOME</em>}</span></pre><p id="7a23" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Azkaban web服务器将暴露端口8081。</p><p id="9d64" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以像这样构建阿兹卡班的所有组件。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="0416" class="mu lj iq mq b gy mv mw l mx my"># remove azkaban docker images.<br/>docker rmi -f $(docker images -a | grep azkaban | awk '{print $3}')<br/><br/># azkaban db docker image.<br/>cd &lt;src&gt;/docker/db;<br/>docker build . -t yourrepo/azkaban-db:3.90.0;<br/><br/>## push.<br/>docker push yourrepo/azkaban-db:3.90.0;<br/><br/><br/># azkaban executor image.<br/>cd &lt;src&gt;/docker/executor;<br/>docker build . -t yourrepo/azkaban-exec-server:3.90.0;<br/><br/>## push.<br/>docker push yourrepo/azkaban-exec-server:3.90.0;<br/><br/># azkaban web image.<br/>cd &lt;src&gt;/docker/web;<br/>docker build . -t yourrepo/azkaban-web-server:3.90.0;<br/><br/>## push.<br/>docker push yourrepo/azkaban-web-server:3.90.0;</span></pre><p id="415d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，你的知识库中有阿兹卡班的docker图像。</p><h1 id="0a7e" class="li lj iq bd lk ll mz ln lo lp na lr ls lt nb lv lw lx nc lz ma mb nd md me mf bi translated">在Kubernetes上运行阿兹卡班</h1><p id="4294" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">在源代码中找到了几个kubernetes yaml文件。</p><p id="5a1d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<code class="fe nf ng nh mq b">mysql.yaml</code>中，你必须改变存储等级。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="5888" class="mu lj iq mq b gy mv mw l mx my">storageClassName: direct.csi.min.io</span></pre><p id="a477" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在所有yaml文件中找到的azkaban映像的所有docker repo名称都应该更改为您的docker repo名称，例如在<code class="fe nf ng nh mq b">azkaban-executor.yaml</code>:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="b8df" class="mu lj iq mq b gy mv mw l mx my">image: yourrepo/azkaban-exec-server:3.90.0</span></pre><p id="85f2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，您已经准备好在kubernetes上运行azkaban了，让我们键入以下内容:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="d0c5" class="mu lj iq mq b gy mv mw l mx my">## ---- init.<br/># create mysql server.<br/>kubectl apply -f mysql.yaml;<br/><br/># wait for mysql pod being ready.<br/>while [[ $(kubectl get pods -n azkaban -l app=mysql -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True" ]]; do echo "waiting for mysql pod being ready" &amp;&amp; sleep 1; done<br/><br/># configmaps<br/>kubectl create configmap azkaban-cfg --dry-run --from-file=azkaban-executor.properties --from-file=azkaban-web.properties -o yaml -n azkaban | kubectl apply -f -<br/><br/># create db and tables.<br/>kubectl apply -f init-schema.yaml;<br/><br/># wait for job being completed.<br/>while [[ $(kubectl get pods -n azkaban -l job-name=azakban-initschema -o jsonpath={..status.phase}) != *"Succeeded"* ]]; do echo "waiting for finishing init schema job" &amp;&amp; sleep 2; done<br/><br/><br/>## ---- azkaban.<br/># create azkaban executor.<br/>kubectl apply -f azkaban-executor.yaml;<br/><br/># wait for azkaban executor being run<br/>while [[ $(kubectl get pods -n azkaban -l app=azkaban-executor -o jsonpath={..status.phase}) != *"Running"* ]]; do echo "waiting for executor being run" &amp;&amp; sleep 2; done<br/><br/><br/># create azkaban web.<br/>kubectl apply -f azkaban-web.yaml;</span></pre><p id="51ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看阿兹卡班命名空间中的豆荚。看起来是这样的:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="7dfe" class="mu lj iq mq b gy mv mw l mx my">kubectl get po -n azkaban;<br/>NAME                           READY   STATUS       RESTARTS   AGE<br/>azakban-initschema-hr4bn       0/1     Init:Error   0          4h3m<br/>azakban-initschema-kg75t       0/1     Completed    0          4h3m<br/>azakban-initschema-ppngd       0/1     Init:Error   0          4h3m<br/>azkaban-executor-0             1/1     Running      0          3h19m<br/>azkaban-executor-1             1/1     Running      0          3h18m<br/>azkaban-executor-2             1/1     Running      0          3h18m<br/>azkaban-web-664967cb99-xhmrf   1/1     Running      0          3h9m<br/>mysql-statefulset-0            1/1     Running      0          4h3m</span></pre><p id="d6c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如这里看到的，一个mysql服务器，三个executor服务器，一个web服务器运行在kubernetes上。</p><h1 id="9bd1" class="li lj iq bd lk ll mz ln lo lp na lr ls lt nb lv lw lx nc lz ma mb nd md me mf bi translated">访问用户界面</h1><p id="dd18" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">要访问UI，让我们看看阿兹卡班名称空间中的服务。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="e245" class="mu lj iq mq b gy mv mw l mx my">kubectl get svc -n azkaban;<br/>NAME               TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE<br/>azkaban-executor   ClusterIP      None            &lt;none&gt;          &lt;none&gt;           3h20m<br/>azkaban-web        LoadBalancer   10.233.49.152   52.231.165.73   8081:31538/TCP   3h9m<br/>mysql-service      ClusterIP      10.233.53.51    &lt;none&gt;          3306/TCP         4h4m</span></pre><p id="9fa0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用<code class="fe nf ng nh mq b">azkaban-web</code>服务的外部ip，您可以在浏览器中访问UI:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="dbd0" class="mu lj iq mq b gy mv mw l mx my">http://52.231.165.73:8081/</span></pre><h1 id="8e87" class="li lj iq bd lk ll mz ln lo lp na lr ls lt nb lv lw lx nc lz ma mb nd md me mf bi translated">阿兹卡班烟雾试验</h1><p id="1f72" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">您可以通过运行示例项目来测试azkaban。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="79d6" class="mu lj iq mq b gy mv mw l mx my"># install azkaban cli.<br/>sudo pip install --upgrade "urllib3==1.22" azkaban;<br/><br/># download sample projects and create project with azkaban cli.<br/>wget https://github.com/azkaban/azkaban/raw/master/az-examples/flow20-projects/basicFlow20Project.zip;<br/>wget https://github.com/azkaban/azkaban/raw/master/az-examples/flow20-projects/embeddedFlow20Project.zip;<br/><br/>azkaban upload -c -p basicFlow20Project -u azkaban@http://52.231.165.73:8081 ./basicFlow20Project.zip;<br/>azkaban upload -c -p embeddedFlow20Project -u azkaban@http://52.231.165.73:8081 ./embeddedFlow20Project.zip;</span></pre><h1 id="8027" class="li lj iq bd lk ll mz ln lo lp na lr ls lt nb lv lw lx nc lz ma mb nd md me mf bi translated">从阿兹卡班执行程序在远程机器上运行shell</h1><p id="f401" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">根据我的大多数经验，我使用位于远程机器上的shell，它将通过ssh在azkaban executors中远程调用。</p><p id="53c4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是azkaban executor调用远程shell来运行spark job的另一个例子。比方说，因为spark和kubectl安装在远程机器上，所以它准备向那里的kubernetes提交spark作业。为此，必须启用从azkaban executor到远程机器的ssh访问。</p><p id="88c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们把阿兹卡班执行者的公钥复制到远程机器上。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="8982" class="mu lj iq mq b gy mv mw l mx my"># list pods.<br/>kubectl get po -n azkaban<br/>NAME                           READY   STATUS       RESTARTS   AGE<br/>azakban-initschema-9bgbh       0/1     Completed    0          16h<br/>azakban-initschema-dtgg7       0/1     Init:Error   0          16h<br/>azakban-initschema-fw7gt       0/1     Init:Error   0          16h<br/>azkaban-executor-0             1/1     Running      0          16h<br/>azkaban-executor-1             1/1     Running      0          16h<br/>azkaban-executor-2             1/1     Running      0          16h<br/>azkaban-web-664967cb99-z8dzn   1/1     Running      0          16h<br/>mysql-statefulset-0            1/1     Running      0          16h<br/><br/># access executor pod to get public key.<br/>kubectl exec -it azkaban-executor-0 -n azkaban -- cat .ssh/id_rsa.pub;<br/>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0vuKKMz4dD0aBrJKtlVU8fDmYgqkwpkDXTzoUTqm57CqEmzHa5EDS90xGch1rAN4HucOR6dzUGvb2VlATBGIi5VZ6w0OuRR+r50KHqiC0TLdEXzX1/TRO/uHftI/xdUMFDHOWTuZnsYS5V7DCrw1yJnPzHTHktgXDyycM/iEspdfslzgZuIV4zT3HNVAYIplQPyy8TKRy7gojm7OYw5W2S14hqiY5/HL/CZ9CQpKV37qJvd3E4u/pOZCHH7r1Tm5E3bnUX9U8z7Nj0Fb+TZSkxiEbwoKB/Ib07Urc0il2f4mug2bKazZRsU+/bb1+VjoMW0ek+9Rvk1JTkaXIu8k/ executor@33842653d6db</span><span id="d303" class="mu lj iq mq b gy ni mw l mx my"># copy this executor public key and paste it to authorized_keys file in remote machine.<br/>## in remote machine.<br/>vi ~/.ssh/authorized_keys;<br/>... paste public key.<br/><br/># chmod 600.<br/>chmod 600 ~/.ssh/authorized_keys;</span></pre><p id="7d3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，在单独的阿兹卡班执行器中通过ssh登录到远程机器:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="6525" class="mu lj iq mq b gy mv mw l mx my">kubectl exec -it azkaban-executor-0 -n azkaban -- sh;<br/>ssh pcp@x.x.x.x;<br/>...<br/>exit;</span></pre><p id="dfd9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们创建shell来在远程机器上运行一个示例spark作业:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="4b2b" class="mu lj iq mq b gy mv mw l mx my">cat &gt; <!-- -->run-spark-example.sh <!-- -->&lt;&lt;'EOF'<br/>############## spark job: create delta table<br/><br/># submit spark job onto kubernetes.<br/>export MASTER=k8s://https://xxxx:6443;<br/>export NAMESPACE=ai-developer;<br/>export ENDPOINT=http://$(kubectl get svc s3g-service -n ai-developer -o jsonpath={.status.loadBalancer.ingress[0].ip}):9898;<br/>export HIVE_METASTORE=metastore.ai-developer:9083;<br/><br/>spark-submit \<br/>--master ${MASTER} \<br/>--deploy-mode cluster \<br/>--name spark-delta-example \<br/>--class io.spongebob.spark.examples.DeltaLakeExample \<br/>--packages com.amazonaws:aws-java-sdk-s3:1.11.375,org.apache.hadoop:hadoop-aws:3.2.0 \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-driver-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-exec-pvc \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-driver-localdir-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-exec-localdir-pvc \<br/>--conf spark.kubernetes.file.upload.path=s3a://mykidong/spark-examples \<br/>--conf spark.kubernetes.container.image.pullPolicy=Always \<br/>--conf spark.kubernetes.namespace=$NAMESPACE \<br/>--conf spark.kubernetes.container.image=xxx/spark:v3.0.0 \<br/>--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \<br/>--conf spark.hadoop.hive.metastore.client.connect.retry.delay=5 \<br/>--conf spark.hadoop.hive.metastore.client.socket.timeout=1800 \<br/>--conf spark.hadoop.hive.metastore.uris=thrift://$HIVE_METASTORE \<br/>--conf spark.hadoop.hive.server2.enable.doAs=false \<br/>--conf spark.hadoop.hive.server2.thrift.http.port=10002 \<br/>--conf spark.hadoop.hive.server2.thrift.port=10016 \<br/>--conf spark.hadoop.hive.server2.transport.mode=binary \<br/>--conf spark.hadoop.metastore.catalog.default=spark \<br/>--conf spark.hadoop.hive.execution.engine=spark \<br/>--conf spark.hadoop.hive.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.hadoop.hive.tez.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.sql.warehouse.dir=s3a:/mykidong/apps/spark/warehouse \<br/>--conf spark.hadoop.fs.defaultFS=s3a://mykidong \<br/>--conf spark.hadoop.fs.s3a.access.key=any-access-key \<br/>--conf spark.hadoop.fs.s3a.secret.key=any-secret-key \<br/>--conf spark.hadoop.fs.s3a.connection.ssl.enabled=true \<br/>--conf spark.hadoop.fs.s3a.endpoint=$ENDPOINT \<br/>--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \<br/>--conf spark.hadoop.fs.s3a.fast.upload=true \<br/>--conf spark.hadoop.fs.s3a.path.style.access=true \<br/>--conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \<br/>--conf spark.executor.instances=3 \<br/>--conf spark.executor.memory=2G \<br/>--conf spark.executor.cores=1 \<br/>--conf spark.driver.memory=1G \<br/>file:///home/pcp/xxx/examples/spark/target/spark-example-1.0.0-SNAPSHOT-spark-job.jar \<br/>--master ${MASTER};<br/>EOF</span><span id="b47c" class="mu lj iq mq b gy ni mw l mx my"># make it executable<br/>chmod a+x run-spark-example.sh;</span></pre><p id="574e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们创造阿兹卡班的流动脚本，<code class="fe nf ng nh mq b">spark.flow</code></p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="f40e" class="mu lj iq mq b gy mv mw l mx my">---<br/>config:<br/>  failure.emails: mykidong@gmail.com<br/><br/>nodes:<br/>- name: Start<br/>  type: noop<br/><br/><br/>- name: RunSparkJob<br/>  type: command<br/>  config:<br/>    command: ssh pcp@x.x.x.x "/home/pcp/run-spark-example.sh"<br/>  dependsOn:<br/>  - Start<br/><br/>- name: End<br/>  type: noop<br/>  dependsOn:<br/>  - RunSparkJob</span></pre><p id="9ecb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看一下上面流程中的命令。<code class="fe nf ng nh mq b">run-spark-example.sh</code>位于远程的机器将通过ssh被调用。</p><p id="00d1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们创建流元文件，命名为<code class="fe nf ng nh mq b">flow20.project</code>:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="8514" class="mu lj iq mq b gy mv mw l mx my">azkaban-flow-version: 2.0</span></pre><p id="4ab2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，让我们将azkaban项目压缩并上传到azkaban web服务器。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="3134" class="mu lj iq mq b gy mv mw l mx my"># build azkaban project. <br/>zip spark-job-example.zip azkaban/*;  </span><span id="fa49" class="mu lj iq mq b gy ni mw l mx my"># create azkaban project. <br/>azkaban upload -c -p spark-job-example -u azkaban@http://52.231.165.73:8081 ./spark-job-example.zip;</span></pre><h1 id="b014" class="li lj iq bd lk ll mz ln lo lp na lr ls lt nb lv lw lx nc lz ma mb nd md me mf bi translated">在阿兹卡班执行程序中运行Pod Runner来创建作业Pod</h1><p id="99fc" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">这是从阿兹卡班执行者那里运行作业的另一个场景。阿兹卡班执行者中的嵌入式pod runner将创建pod，该pod向kubernetes提交spark作业。</p><p id="7928" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面显示了在由azkaban executor的pod runner创建的pod中运行spark作业的示例场景。</p><figure class="ml mm mn mo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/afa2c083f67298dcc8b838a56594afbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ljatR7oHQ7o9PQHypf2QPQ.png"/></div></div></figure><p id="b75f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">阿兹卡班执行程序中的嵌入式Pod Runner将创建一个Spark作业提交Pod，其中所有必需的deps spark uber jar和run-job.sh脚本都是从S3下载的。请注意，在阿兹卡班运行job之前，您必须将所有必需的运行shell脚本和jar上传到S3。</p><p id="bfb5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了理解如何创建作业单元，让我们来看看<code class="fe nf ng nh mq b">ResourceController</code>类中的细节:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="210d" class="mu lj iq mq b gy mv mw l mx my">package io.mykidong.kubernetes;<br/><br/>import io.fabric8.kubernetes.api.model.HasMetadata;<br/>import io.fabric8.kubernetes.api.model.Pod;<br/>import io.fabric8.kubernetes.client.KubernetesClient;<br/>import io.fabric8.kubernetes.client.KubernetesClientException;<br/>import io.fabric8.kubernetes.client.dsl.LogWatch;<br/>import io.mykidong.domain.Kubeconfig;<br/>import io.mykidong.kubernetes.client.KubernetesClientUtils;<br/>import org.slf4j.Logger;<br/>import org.slf4j.LoggerFactory;<br/><br/>import java.io.InputStream;<br/>import java.util.List;<br/>import java.util.concurrent.TimeUnit;<br/><br/>public class ResourceController {<br/><br/>    private static Logger <em class="ne">LOG </em>= LoggerFactory.<em class="ne">getLogger</em>(ResourceController.class);<br/><br/>    public static void runPod(Kubeconfig kubeconfig, String namespace, InputStream jobResourceInputStream) {<br/>        try  {<br/>            KubernetesClient adminClient = KubernetesClientUtils.<em class="ne">newClient</em>(kubeconfig);<br/><br/>            List&lt;HasMetadata&gt; resources = adminClient.load(jobResourceInputStream).get();<br/>            HasMetadata resource = resources.get(0);<br/>            if (resource instanceof Pod){<br/>                Pod pod = (Pod) resource;<br/>                Pod result = adminClient.pods().inNamespace(namespace).create(pod);<br/>                String podName = result.getMetadata().getName();<br/>                <em class="ne">LOG</em>.info("podName: {}", podName);<br/><br/>                LogWatch watch = adminClient.pods().inNamespace(namespace).withName(podName).tailingLines(10).watchLog(System.<em class="ne">out</em>);<br/><br/>                long start = System.<em class="ne">currentTimeMillis</em>();<br/>                while(true) {<br/>                    String phase = adminClient.pods().inNamespace(namespace).withName(podName).get().getStatus().getPhase();<br/>                    <em class="ne">LOG</em>.info("phase: {}", phase);<br/>                    if(phase.equals("Failed")) {<br/>                        throw new RuntimeException(podName + " job failed!");<br/>                    } else if(phase.equals("Succeeded")) {<br/>                        break;<br/>                    }<br/><br/>                    Thread.<em class="ne">sleep</em>(5 * 1000);<br/>                    long elapsed = (System.<em class="ne">currentTimeMillis</em>() - start) / 1000;<br/>                    // more than 3 hours.<br/>                    if(elapsed &gt; (3 * 60 * 60)) {<br/>                        throw new RuntimeException(podName + " job takes too long time!");<br/>                    }<br/>                }<br/><br/>            } else {<br/>                System.<em class="ne">err</em>.println("Loaded resource is not a Pod! " + resource);<br/>            }<br/>        } catch (KubernetesClientException e) {<br/>            <em class="ne">LOG</em>.error(e.getMessage(), e);<br/>            throw new RuntimeException(e);<br/>        } catch (Exception e) {<br/>            <em class="ne">LOG</em>.error(e.getMessage(), e);<br/>            throw new RuntimeException(e);<br/>        }<br/>    }<br/>}</span></pre><p id="b3dd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该类创建一个作业窗格，并等待作业窗格完成。如果pod状态阶段是<code class="fe nf ng nh mq b">Failed</code>，那么将抛出一个异常，如果作业需要超过3个小时，那么将抛出一个异常。阿兹卡班会将此类异常情况解释为作业失败状态，并在阿兹卡班UI中显示作业失败消息。</p><p id="2ac9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们按照下面的步骤在pod中运行spark提交作业。</p><h2 id="8960" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">将本地文件，如加密器、pod runner jar和pod runner属性复制到阿兹卡班执行器</h2><p id="bb9d" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">首先，您必须通过kubectl cp将encryptor等文件复制到azkaban executor，以加密密钥，将pod runner jar复制到job pod，并将pod runner属性复制到azkaban executor。pod runner属性中的值都是加密的，使用encryptor，加密密钥将被解密，所有其他属性都将使用加密密钥解密。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="e1b1" class="mu lj iq mq b gy mv mw l mx my"># list azkaban pods.<br/>kubectl get po -n azkaban<br/>NAME                           READY   STATUS    RESTARTS   AGE<br/>azkaban-executor-0             1/1     Running   0          2d10h<br/>azkaban-executor-1             1/1     Running   0          5d22h<br/>azkaban-executor-2             1/1     Running   0          5d22h<br/>azkaban-web-664967cb99-z8dzn   1/1     Running   0          5d22h<br/>mysql-statefulset-0            1/1     Running   0          5d22h</span><span id="9f40" class="mu lj iq mq b gy ni mw l mx my"># cp encryptor to executor.<br/>kubectl cp &lt;src&gt;/encryptor/enc azkaban-executor-0:/opt/azkaban-executor -n azkaban;<br/>kubectl cp &lt;src&gt;/encryptor/enc azkaban-executor-1:/opt/azkaban-executor -n azkaban;<br/>kubectl cp &lt;src&gt;/encryptor/enc azkaban-executor-2:/opt/azkaban-executor -n azkaban;</span><span id="8dd9" class="mu lj iq mq b gy ni mw l mx my">## enc executable.<br/>kubectl exec -it azkaban-executor-0 -n azkaban -- chmod a+x enc;<br/>kubectl exec -it azkaban-executor-1 -n azkaban -- chmod a+x enc;<br/>kubectl exec -it azkaban-executor-2 -n azkaban -- chmod a+x enc;</span><span id="f6ba" class="mu lj iq mq b gy ni mw l mx my"># cp pod runner jar to executor.<br/>kubectl cp &lt;src&gt;/pod-runner/target/pod-runner-1.0.0-SNAPSHOT-fat.jar azkaban-executor-0:/opt/azkaban-executor -n azkaban;<br/>kubectl cp &lt;src&gt;/pod-runner/target/pod-runner-1.0.0-SNAPSHOT-fat.jar azkaban-executor-1:/opt/azkaban-executor -n azkaban;<br/>kubectl cp &lt;src&gt;/pod-runner/target/pod-runner-1.0.0-SNAPSHOT-fat.jar azkaban-executor-2:/opt/azkaban-executor -n azkaban;</span><span id="5a5a" class="mu lj iq mq b gy ni mw l mx my"># cp pod runner properties to executor.<br/>kubectl cp &lt;src&gt;/pod-runner/src/main/resources/pod-runner.properties azkaban-executor-0:/opt/azkaban-executor -n azkaban;<br/>kubectl cp &lt;src&gt;/pod-runner/src/main/resources/pod-runner.properties azkaban-executor-1:/opt/azkaban-executor -n azkaban;<br/>kubectl cp &lt;src&gt;/pod-runner/src/main/resources/pod-runner.properties azkaban-executor-2:/opt/azkaban-executor -n azkaban;</span></pre><h2 id="01da" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">构建Spark Job Runner Docker</h2><p id="ef1b" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">因为我要在pod中提交spark作业，所以我们必须构建一个spark作业运行器docker映像，其中安装了spark和kubectl。这张图片是基于我之前已经构建好的spark 3.x。</p><p id="1320" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看档案。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="b9b3" class="mu lj iq mq b gy mv mw l mx my">FROM mykidong/spark:v3.0.0<br/><br/>ENV <em class="ne">APP_HOME </em>/opt/spark<br/><br/>RUN echo "Asia/Seoul" &gt; /etc/timezone<br/>RUN dpkg-reconfigure -f noninteractive tzdata<br/><br/>RUN useradd -ms /bin/bash -d ${<em class="ne">APP_HOME</em>} spark<br/><br/>COPY enc ${<em class="ne">APP_HOME</em>}/<br/>COPY pod-runner-1.0.0-*-fat.jar ${<em class="ne">APP_HOME</em>}/<br/>COPY run.sh ${<em class="ne">APP_HOME</em>}/<br/>RUN chmod a+x ${<em class="ne">APP_HOME</em>}/*.sh<br/><br/>RUN chmod a+x -R ${<em class="ne">APP_HOME</em>}/bin<br/>RUN chown spark: -R ${<em class="ne">APP_HOME</em>}<br/><br/>RUN echo "deb [check-valid-until=no] http://cdn-fastly.deb.debian.org/debian jessie main" &gt; /etc/apt/sources.list.d/jessie.list<br/>RUN echo "deb [check-valid-until=no] http://archive.debian.org/debian jessie-backports main" &gt; /etc/apt/sources.list.d/jessie-backports.list<br/>RUN sed -i '/deb http:\/\/deb.debian.org\/debian jessie-updates main/d' /etc/apt/sources.list<br/>RUN apt-get -o Acquire::Check-Valid-Until=false update<br/>RUN apt-get install -y openssh-client<br/>RUN apt-get install -y curl<br/>RUN apt-get install -y unzip<br/>RUN curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" &amp;&amp; unzip awscliv2.zip &amp;&amp; ./aws/install<br/><br/>RUN apt-get install -y apt-transport-https gnupg2<br/>RUN curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -<br/>RUN echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | tee -a /etc/apt/sources.list.d/kubernetes.list<br/>RUN apt-get -o Acquire::Check-Valid-Until=false update<br/>RUN apt-get install -y kubectl<br/><br/><br/>USER spark<br/>RUN export SPARK_HOME=${<em class="ne">APP_HOME</em>} &amp;&amp; export PATH=$<em class="ne">PATH</em>:${<em class="ne">SPARK_HOME</em>}/bin<br/>WORKDIR ${<em class="ne">APP_HOME</em>}</span></pre><p id="17de" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，将安装aws cli和kubectl，因为这个映像基于spark 3.x，所以已经安装了spark。</p><p id="d3d9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们为spark作业提交构建docker映像:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="aa3d" class="mu lj iq mq b gy mv mw l mx my"># remove spark job runner docker.<br/>docker rmi -f $(docker images -a | grep spark-job-runner | awk '{print $3}')</span><span id="fd33" class="mu lj iq mq b gy ni mw l mx my"># build docker of spark job runner.<br/>cd &lt;src&gt;/pod-runner/docker/spark;</span><span id="ef27" class="mu lj iq mq b gy ni mw l mx my">## copy some files.<br/>cp &lt;src&gt;/encryptor/enc .;<br/>cp &lt;src&gt;/pod-runner/src/main/resources/shell/run.sh .;<br/>cp &lt;src&gt;/pod-runner/target/pod-runner-*-fat.jar .;</span><span id="77e3" class="mu lj iq mq b gy ni mw l mx my">## build docker.<br/>docker build . -t mykidong/spark-job-runner:v3.0.0;</span><span id="9359" class="mu lj iq mq b gy ni mw l mx my"># push docker of spark job runner.<br/>docker push mykidong/spark-job-runner:v3.0.0;</span></pre><h2 id="f256" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">创建运行spark作业外壳</h2><p id="877e" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">现在，创建一个run-job.sh来提交pod中的spark作业。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="5a63" class="mu lj iq mq b gy mv mw l mx my">cat &gt; run-spark.sh &lt;&lt;'EOF'</span><span id="a314" class="mu lj iq mq b gy ni mw l mx my"># export spark home and path.<br/>export SPARK_HOME=.;<br/>export PATH=$PATH:${SPARK_HOME}/bin;</span><span id="a735" class="mu lj iq mq b gy ni mw l mx my"># export kubeconfig.<br/>export KUBECONFIG=kubeconfig;</span><span id="de36" class="mu lj iq mq b gy ni mw l mx my"># export spark job jar.<br/>export SPARK_JOB_JAR=spark-example-1.0.0-SNAPSHOT-spark-job.jar;</span><span id="9571" class="mu lj iq mq b gy ni mw l mx my"># download spark job jar from s3.<br/>aws s3api --profile=minio --endpoint=$S3_ENDPOINT get-object --bucket mykidong --key test/${SPARK_JOB_JAR} ./${SPARK_JOB_JAR};</span><span id="8f6b" class="mu lj iq mq b gy ni mw l mx my">############## spark job: create delta table</span><span id="02e2" class="mu lj iq mq b gy ni mw l mx my"># submit spark job onto kubernetes.<br/>export MASTER=k8s://<a class="ae kc" href="https://xxx:6443" rel="noopener ugc nofollow" target="_blank">https://xxx:6443</a>;<br/>export NAMESPACE=ai-developer;<br/>export ENDPOINT=<a class="ae kc" href="https://any-s3-endpoint" rel="noopener ugc nofollow" target="_blank">https://any-s3-endpoint</a>;<br/>export HIVE_METASTORE=metastore.ai-developer:9083;</span><span id="fc77" class="mu lj iq mq b gy ni mw l mx my">spark-submit \<br/>--master ${MASTER} \<br/>--deploy-mode cluster \<br/>--name spark-delta-example \<br/>--class io.mykidong.spark.examples.DeltaLakeExample \<br/>--packages com.amazonaws:aws-java-sdk-s3:1.11.375,org.apache.hadoop:hadoop-aws:3.2.0 \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-driver-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-exec-pvc \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-driver-localdir-pvc \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.path=/localdir \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.mount.readOnly=false \<br/>--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-localdirpvc.options.claimName=spark-exec-localdir-pvc \<br/>--conf spark.kubernetes.file.upload.path=s3a://mykidong/spark-examples \<br/>--conf spark.kubernetes.container.image.pullPolicy=Always \<br/>--conf spark.kubernetes.namespace=$NAMESPACE \<br/>--conf spark.kubernetes.container.image=mykidong/spark:v3.0.0 \<br/>--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \<br/>--conf spark.hadoop.hive.metastore.client.connect.retry.delay=5 \<br/>--conf spark.hadoop.hive.metastore.client.socket.timeout=1800 \<br/>--conf spark.hadoop.hive.metastore.uris=thrift://$HIVE_METASTORE \<br/>--conf spark.hadoop.hive.server2.enable.doAs=false \<br/>--conf spark.hadoop.hive.server2.thrift.http.port=10002 \<br/>--conf spark.hadoop.hive.server2.thrift.port=10016 \<br/>--conf spark.hadoop.hive.server2.transport.mode=binary \<br/>--conf spark.hadoop.metastore.catalog.default=spark \<br/>--conf spark.hadoop.hive.execution.engine=spark \<br/>--conf spark.hadoop.hive.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.hadoop.hive.tez.input.format=io.delta.hive.HiveInputFormat \<br/>--conf spark.sql.warehouse.dir=s3a:/mykidong/apps/spark/warehouse \<br/>--conf spark.hadoop.fs.defaultFS=s3a://mykidong \<br/>--conf spark.hadoop.fs.s3a.access.key=xxx \<br/>--conf spark.hadoop.fs.s3a.secret.key=xxx \<br/>--conf spark.hadoop.fs.s3a.connection.ssl.enabled=true \<br/>--conf spark.hadoop.fs.s3a.endpoint=$ENDPOINT \<br/>--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \<br/>--conf spark.hadoop.fs.s3a.fast.upload=true \<br/>--conf spark.hadoop.fs.s3a.path.style.access=true \<br/>--conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \<br/>--conf spark.executor.instances=3 \<br/>--conf spark.executor.memory=2G \<br/>--conf spark.executor.cores=1 \<br/>--conf spark.driver.memory=1G \<br/>file://$(pwd)/${SPARK_JOB_JAR} \<br/>--master ${MASTER};</span><span id="1c39" class="mu lj iq mq b gy ni mw l mx my"># check pod job status.<br/>pod_name=$(kubectl get po -n ${NAMESPACE} | grep spark-delta-example | awk '{print $1}');<br/>echo "pod name: $pod_name";<br/>status_phase=$(kubectl get po $pod_name -n ${NAMESPACE} -o jsonpath={..status.phase});<br/>echo "status phase: $status_phase";</span><span id="4bf2" class="mu lj iq mq b gy ni mw l mx my">if [[ $status_phase=="Failed" ]]<br/>then<br/>   echo "job $pod_name failed...";<br/>   exit 1;<br/>fi<br/>EOF</span></pre><h2 id="2b09" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">将运行作业shell和spark作业jar上传到S3</h2><p id="3e79" class="pw-post-body-paragraph kd ke iq kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">让我们将所有必要的文件上传到S3，以便在pod中运行spark作业。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="7bf6" class="mu lj iq mq b gy mv mw l mx my"># export s3 endpoint.<br/>export S3_ENDPOINT=<a class="ae kc" href="https://any-s3-endpoint" rel="noopener ugc nofollow" target="_blank">https://any-s3-endpoint</a>;</span><span id="5500" class="mu lj iq mq b gy ni mw l mx my"># upload run-job.sh to s3.<br/>aws s3api --profile=minio --endpoint=$S3_ENDPOINT put-object --bucket mykidong --key test/run-job.sh --body ./run-spark.sh;</span><span id="922c" class="mu lj iq mq b gy ni mw l mx my"># upload spark job jar to s3.<br/>aws s3api --profile=minio --endpoint=$S3_ENDPOINT put-object --bucket mykidong --key test/spark-example-1.0.0-SNAPSHOT-spark-job.jar \<br/>--body &lt;spark-job-path&gt;/spark-example-1.0.0-SNAPSHOT-spark-job.jar;</span></pre><h2 id="ec27" class="mu lj iq bd lk nk nl dn lo nm nn dp ls ko no np lw ks nq nr ma kw ns nt me nu bi translated">创建阿兹卡班流程</h2><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="c912" class="mu lj iq mq b gy mv mw l mx my">---<br/>config:<br/>  failure.emails: <a class="ae kc" href="mailto:mykidong@gmail.com" rel="noopener ugc nofollow" target="_blank">mykidong@gmail.com</a></span><span id="3a98" class="mu lj iq mq b gy ni mw l mx my">nodes:<br/>- name: Start<br/>  type: noop</span><span id="d11b" class="mu lj iq mq b gy ni mw l mx my">- name: RunSparkJob<br/>  type: command<br/>  config:<br/>    command: /bin/sh -c 'cd /opt/azkaban-executor &amp;&amp; java -cp ./pod-runner-1.0.0-SNAPSHOT-fat.jar io.mykidong.kubernetes.PodRunner --conf ./pod-runner.properties --image mykidong/spark-job-runner:v3.0.0 --cmd ./run.sh --args "--job.type=spark" --namespace azkaban --run.job.url mykidong/test/run-job.sh --encryptor.path ./enc'<br/>  dependsOn:<br/>  - Start</span><span id="f639" class="mu lj iq mq b gy ni mw l mx my">- name: End<br/>  type: noop<br/>  dependsOn:<br/>  - RunSparkJob</span></pre><p id="6b21" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上面的命令作业类型所示，将调用阿兹卡班执行程序中嵌入的<code class="fe nf ng nh mq b">PodRunner</code>来创建一个带有几个参数的spark提交作业pod。为了更清楚理解，我们来看看<code class="fe nf ng nh mq b">PodRunner.</code>的类</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="60b9" class="mu lj iq mq b gy mv mw l mx my">package io.mykidong.kubernetes;<br/><br/>import io.mykidong.domain.Kubeconfig;<br/>import io.mykidong.encrypt.KeyEncryptorProcessExecutor;<br/>import io.mykidong.util.*;<br/>import joptsimple.OptionParser;<br/>import joptsimple.OptionSet;<br/>import org.slf4j.Logger;<br/>import org.slf4j.LoggerFactory;<br/><br/>import java.io.ByteArrayInputStream;<br/>import java.io.InputStream;<br/>import java.util.HashMap;<br/>import java.util.Map;<br/>import java.util.Properties;<br/>import java.util.UUID;<br/><br/>public class PodRunner {<br/><br/>    private static Logger LOG = LoggerFactory.getLogger(PodRunner.class);<br/><br/>    public static void main(String[] args) {<br/><br/>        // load log4j.<br/>        Log4jConfigurer.loadLog4j(null);<br/><br/>        // parse arguments.<br/>        OptionParser parser = new OptionParser();<br/>        parser.accepts("conf").withRequiredArg().ofType(String.class);<br/>        parser.accepts("image").withRequiredArg().ofType(String.class);<br/>        parser.accepts("cmd").withRequiredArg().ofType(String.class);<br/>        parser.accepts("args").withRequiredArg().ofType(String.class);<br/>        parser.accepts("namespace").withRequiredArg().ofType(String.class);<br/>        parser.accepts("run.job.url").withRequiredArg().ofType(String.class);<br/>        parser.accepts("encryptor.path").withRequiredArg().ofType(String.class);<br/><br/>        OptionSet options = parser.parse(args);<br/><br/>        String conf = (String) options.valueOf("conf");<br/>        String image = (String) options.valueOf("image");<br/>        String cmd = (String) options.valueOf("cmd");<br/>        String argsList = (String) options.valueOf("args");<br/>        String namespace = (String) options.valueOf("namespace");<br/>        String runJobUrl = (String) options.valueOf("run.job.url");<br/>        String encryptorPath = (String) options.valueOf("encryptor.path");<br/><br/><br/>        // read properties.<br/>        Properties prop = PropertiesUtils.readPropertiesFromFileSystem(conf);<br/>        String encryptedKey = prop.getProperty("encryption.key");<br/>        String encryptedKubeconfig = prop.getProperty("admin.kubeconfig");<br/>        String encryptedS3AccessKey = prop.getProperty("s3.access.key");<br/>        String encryptedS3SecretKey = prop.getProperty("s3.secret.key");<br/>        String encryptedS3Endpoint = prop.getProperty("s3.endpoint");<br/><br/><br/>        // decrypt encrypted encryption key.<br/>        String encryptionKey = KeyEncryptorProcessExecutor.doExec(encryptorPath, encryptedKey, "false");<br/><br/>        // read kubeconfig yaml from encrypted property.<br/>        String adminKubeconfig = EncryptionUtils.decodeBase64AndDecrypt(encryptionKey, encryptedKubeconfig);<br/><br/>        // convert kubeconfig yaml to object.<br/>        Kubeconfig kubeconfig = YamlUtils.readKubeconfigYaml(new ByteArrayInputStream(adminKubeconfig.getBytes()));<br/><br/>        // replace job with params.<br/>        Map&lt;String, String&gt; kv = new HashMap&lt;&gt;();<br/>        String suffix = RandomUtils.getRandomNumber(10000, 4);<br/>        kv.put("suffix", suffix);<br/>        kv.put("namespace", namespace);<br/>        kv.put("image", image);<br/><br/>        cmd = "\"" + cmd + "\"";<br/>        kv.put("cmd", cmd);<br/><br/>        String argsString = "";<br/>        for(String arg : argsList.split("\\s+")) {<br/>            argsString += ((argsString.equals("")) ? "" : ",") + "\"" + arg + "\"";<br/>        }<br/><br/>        // encryption key param.<br/>        argsString += "," + "\"--encryption.key=" + encryptedKey + "\"";<br/><br/>        // s3 access key param.<br/>        argsString += "," + "\"--s3.access.key=" + encryptedS3AccessKey + "\"";<br/><br/>        // s3 secret key param.<br/>        argsString += "," + "\"--s3.secret.key=" + encryptedS3SecretKey + "\"";<br/><br/>        // s3 endpoint param.<br/>        argsString += "," + "\"--s3.endpoint=" + encryptedS3Endpoint + "\"";<br/><br/>        // run job url param.<br/>        argsString += "," + "\"--run.job.url=" + runJobUrl + "\"";<br/><br/>        // encryptor path param.<br/>        argsString += "," + "\"--encryptor.path=" + encryptorPath + "\"";<br/><br/>        // kubeconfig.<br/>        argsString += "," + "\"--admin.kubeconfig=" + encryptedKubeconfig + "\"";<br/><br/><br/>        kv.put("args", argsString);<br/><br/>        String resourceTemplate = FileUtils.fileToString("kubernetes/template/job.yaml", true);<br/>        String jobResource = TemplateUtils.replace(resourceTemplate, kv);<br/>        InputStream jobResourceInputStream = new ByteArrayInputStream(jobResource.getBytes());<br/><br/>        // create pod and watch.<br/>        ResourceController.runPod(kubeconfig, namespace, jobResourceInputStream);<br/>    }<br/>}</span></pre><p id="73c2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nf ng nh mq b">pod-runner.properties</code>中的属性值被读取并传递给<code class="fe nf ng nh mq b">PodRunner</code>将创建的作业框的参数。加密的Kubeconfig Yaml被解密，用于访问kubernetes，由<code class="fe nf ng nh mq b">PodRunner.</code>使用的fabric8 kubernetes客户端创建pod</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="9088" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以扩展azkaban executor中的嵌入式pod runner，不仅可以运行Spark提交作业，还可以运行Python ML作业等。</p></div></div>    
</body>
</html>