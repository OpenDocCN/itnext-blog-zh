<html>
<head>
<title>Apache Spark and Hadoop HDFS: Hello World</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark和Hadoop HDFS: Hello World</h1>
<blockquote>原文：<a href="https://itnext.io/apache-spark-and-hadoop-hdfs-hello-world-ed6fb2077c20?source=collection_archive---------2-----------------------#2019-03-16">https://itnext.io/apache-spark-and-hadoop-hdfs-hello-world-ed6fb2077c20?source=collection_archive---------2-----------------------#2019-03-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/b02c743f613871b32b39c6ac3d3800fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fIXEMu3NrWYcZ4uvq1-N-Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">演职员表:<a class="ae kc" href="https://techcrunch.com/2015/07/12/spark-and-hadoop-are-friends-not-foes/?guccounter=1&amp;guce_referrer_us=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_cs=3LAm_62Zyy12ECBSNt6eFQ" rel="noopener ugc nofollow" target="_blank">techcrunch.com</a></figcaption></figure><p id="5016" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章旨在帮助人们开始他们的大数据之旅，帮助他们创建一个简单的环境来测试Apache Spark和Hadoop HDFS的集成。它不打算描述什么是Apache Spark或Hadoop。</p><h1 id="24e8" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">先决条件</h1><ul class=""><li id="87ca" class="lz ma iq kf b kg mb kk mc ko md ks me kw mf la mg mh mi mj bi translated">Docker环境(本地或远程)。</li><li id="4276" class="lz ma iq kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">本地机器上的一个DNS条目将<code class="fe mp mq mr ms b">hadoop</code>映射到Docker主机的IP地址。</li></ul><h1 id="1bc9" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Hadoop Docker映像</h1><ul class=""><li id="1721" class="lz ma iq kf b kg mb kk mc ko md ks me kw mf la mg mh mi mj bi translated">拉动<code class="fe mp mq mr ms b">teivah/kafka:2.9.2</code> Docker图像:</li></ul><pre class="mt mu mv mw gt mx ms my mz aw na bi"><span id="73b9" class="nb lc iq ms b gy nc nd l ne nf">docker pull teivah/hadoop:2.9.2</span></pre><p id="7892" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该映像启动了一个单节点Hadoop集群。</p><ul class=""><li id="6af5" class="lz ma iq kf b kg kh kk kl ko ng ks nh kw ni la mg mh mi mj bi translated">运行Docker映像:</li></ul><pre class="mt mu mv mw gt mx ms my mz aw na bi"><span id="55ef" class="nb lc iq ms b gy nc nd l ne nf">docker run -ti --hostname hadoop -p 50070:50070 -p 9000:9000 -p 50075:50075 -p 50010:50010 teivah/hadoop:2.9.2</span></pre><p id="15bb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该命令公开了3个端口:50070是HDFS namenode端口，50075是HDFS datanode端口，9000是Spark ( <code class="fe mp mq mr ms b">fs.defaultFS</code>配置)要访问的端口。</p><ul class=""><li id="967b" class="lz ma iq kf b kg kh kk kl ko ng ks nh kw ni la mg mh mi mj bi translated">一旦容器运行，进入web UI:<a class="ae kc" href="http://hadoop:50070/" rel="noopener ugc nofollow" target="_blank">http://Hadoop:50070/</a>然后导航到<em class="nj"> Utilities </em> - &gt; <em class="nj">浏览文件系统</em>。</li><li id="2007" class="lz ma iq kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">在根级别上传文件(您可能希望以后创建层次结构)。</li></ul><h1 id="7cf5" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Apache Spark客户端(Java语言)</h1><ul class=""><li id="e9fe" class="lz ma iq kf b kg mb kk mc ko md ks me kw mf la mg mh mi mj bi translated">Git克隆以下项目:<a class="ae kc" href="https://github.com/teivah/spark-hdfs-helloworld" rel="noopener ugc nofollow" target="_blank">https://github.com/teivah/spark-hdfs-helloworld</a>。</li><li id="0794" class="lz ma iq kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">运行<code class="fe mp mq mr ms b">mvn clean install</code>安装项目并下载依赖项。</li><li id="116b" class="lz ma iq kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">打开<code class="fe mp mq mr ms b">io.teivah.spark.SparkHdfsHelloWorld.java</code>。这个客户端连接到一个HDFS集群(<em class="nj"> hdfs://hadoop:9000/… </em>)并执行一系列查询来计算每个单词在文件中出现的次数。</li><li id="f334" class="lz ma iq kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">将<code class="fe mp mq mr ms b">FILENAME</code> ( <a class="ae kc" href="https://github.com/teivah/spark-hdfs-helloworld/blob/372d3b8a3be9222f3c3ba9c8e71441de6fadb876/src/main/java/io/teivah/spark/SparkHdfsHelloWorld.java#L21" rel="noopener ugc nofollow" target="_blank">第21行</a>)替换为您刚刚上传的文件的名称。</li><li id="827a" class="lz ma iq kf b kg mk kk ml ko mm ks mn kw mo la mg mh mi mj bi translated">执行应用程序。在执行结束时，您应该在<code class="fe mp mq mr ms b">/output</code>文件夹中创建一个代表结果的<code class="fe mp mq mr ms b">part-00000</code>文件。</li></ul><p id="38f7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您现在已经准备好使用Apache Spark和Hadoop HDFS了。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/9a80d93d4e83e996c2959e51f01b0f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*m3zfMamxAABfF5aqjXfzNQ.jpeg"/></div></figure></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="ca0d" class="lb lc iq bd ld le ns lg lh li nt lk ll lm nu lo lp lq nv ls lt lu nw lw lx ly bi translated">进一步阅读</h1><div class="nx ny gp gr nz oa"><a href="https://www.edureka.co/blog/spark-tutorial/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">Spark教程| Apache Spark初学者指南| Edureka</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">Apache Spark是一个我们很高兴通过这个开源集群计算框架开始这一激动人心的旅程…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">www.edureka.co</p></div></div><div class="oj l"><div class="ok l ol om on oj oo jw oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://logz.io/blog/hadoop-vs-spark/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">Hadoop与Spark:势均力敌的比较| Logz.io</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">每年，市场上似乎都有越来越多的分布式系统来管理数据量、种类和…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">logz.io</p></div></div><div class="oj l"><div class="op l ol om on oj oo jw oa"/></div></div></a></div><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oq"><img src="../Images/b6578984dc018c44134738b9e16e76e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ni3QlYv1zjIZfJkxFYTzmA.png"/></div></div></figure><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi or"><img src="../Images/ec9b9710329661db6caf375f69308521.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*8kh8D7Gu7uPHxI0aaqGrZA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">在Twitter上关注我<a class="ae kc" href="https://twitter.com/teivah" rel="noopener ugc nofollow" target="_blank"> @teivah </a></figcaption></figure></div></div>    
</body>
</html>