<html>
<head>
<title>Reinforcement Learning with Multi Arm Bandit (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多臂Bandit强化学习(下)</h1>
<blockquote>原文：<a href="https://itnext.io/reinforcement-learning-with-multi-arm-bandit-part-2-831a43f22a47?source=collection_archive---------2-----------------------#2018-05-06">https://itnext.io/reinforcement-learning-with-multi-arm-bandit-part-2-831a43f22a47?source=collection_archive---------2-----------------------#2018-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="157b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让我们把问题变得更复杂一点。复杂！</h2></div><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="ab gu cl kk"><img src="../Images/fd78801b5f8b795ba8cc11cc4836dbed.png" data-original-src="https://miro.medium.com/v2/format:webp/1*wYi8N9qBc8Ga3cGpSYofmQ.jpeg"/></div></figure><h2 id="d1fa" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">概述</h2><p id="f828" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr kw ls lt lu la lv lw lx le ly lz ma mb ij bi translated">这是最初的<a class="ae mc" rel="noopener ugc nofollow" target="_blank" href="/reinforcement-learning-with-multi-arm-bandit-decf442e02d2">帖子</a>的延续，我强烈建议先浏览一遍，在那里我们理解了多臂强盗的直觉，并试图应用贪婪算法来解决一个典型的问题。但是这个世界没那么简单。有一些因素，引入这些因素，问题完全改变，解决方案必须重新定义。我们将选择我们离开的地方，引入新的问题，展示我们心爱的算法是如何失败的，并尝试建立可能有助于这种情况的直觉。</p><h2 id="25d0" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">非平稳问题</h2><p id="9d62" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr kw ls lt lu la lv lw lx le ly lz ma mb ij bi translated">如果我们回想一下第一篇文章中的问题，我们已经定义了一些具有固定奖励分布的门户，并试图使我们的估计行动值更接近预期行动值或奖励分布。现在，大部分的定义保持不变，但我们将只是调整固定奖励分配的一部分。在最初的问题中，我们定义了一个奖励分布函数，它的值在整个过程中不变，但如果它们变了呢？如果预期动作值不恒定怎么办？就家庭门户游戏而言，如果一个家庭门户正慢慢变成一个海洋门户，或者反之亦然，或者只是波动到足以引起重大混乱。在这种情况下，我们简单的e-greedy算法会起作用吗？好吧，让我们试着找出答案。</p><p id="d396" class="pw-post-body-paragraph lj lk iq ll b lm md jr lo lp me ju lr kw mf lt lu la mg lw lx le mh lz ma mb ij bi translated">首先，在运行原始代码以生成最佳动作选择百分比时，我们的图与预期一样，即使将步长增加到10k，ε值0.01也优于e = 0或1的对比度设置。并且性能是稳定的，未来没有下降的迹象。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mi"><img src="../Images/3fe10ff806eec09d59a4558944e688d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MT9W5NZYuBZFMafLi2zQXw.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk translated">静态问题:e-greedy算法性能；将步数增加到10公里</figcaption></figure><p id="87f1" class="pw-post-body-paragraph lj lk iq ll b lm md jr lo lp me ju lr kw mf lt lu la mg lw lx le mh lz ma mb ij bi translated">现在，我们需要稍微修改一下，把定态问题转化为非定态问题。根据定义，奖励分配可能会发生变化。让我们用均值为0，偏差为0.01的正态分布来定义变化的术语，比如说在每一步之后。所以在每一步之后，我们将根据定义的正态分布计算随机数，并将其添加到之前的预期动作值中。现在这将是新的奖励分配。这很容易做到，只需在原始代码中添加几行代码，然后我们就可以计算出最新的最优动作选择百分比。</p><pre class="kf kg kh ki gt mr ms mt mu aw mv bi"><span id="d175" class="kn ko iq ms b gy mw mx l my mz"># define a function to update the expected_action_value<br/>&gt;&gt;&gt; def update_expected_action_value(expected_action_value):<br/>&gt;&gt;&gt;    expected_action_value += np.random.normal(0, 0.01, arms) <br/>&gt;&gt;&gt;    return(expected_action_value)</span><span id="9f1e" class="kn ko iq ms b gy na mx l my mz"># inside the multi_arm_bandit_problem function add, <br/>&gt;&gt;&gt; estimate_action_value = update_estimate_action_value(estimate_action_value)</span></pre><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nb"><img src="../Images/26e86e76b2ae2da21a0959b3375179b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UDOLBsr9RUkP5qVcmAppXA.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk translated">非平稳问题:初始峰值后e-greedy算法性能下降</figcaption></figure><p id="5186" class="pw-post-body-paragraph lj lk iq ll b lm md jr lo lp me ju lr kw mf lt lu la mg lw lx le mh lz ma mb ij bi translated">相比之下，我们可以说e-greedy算法的性能在一段时间后开始下降。请注意，尽管e=0.01仍显示出良好的结果，但即使是轻微的随机增量(0.01偏差)，性能的急剧下降也是显而易见的，如果变化系数更大呢？事实证明，下降的幅度会更大。要问的问题是，这里的问题是什么？</p><h2 id="e42a" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">报酬分布的估计</h2><p id="df0a" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr kw ls lt lu la lv lw lx le ly lz ma mb ij bi translated">让我们试着回忆一下真实报酬分布的估计函数，大概是这样的，</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/1fc7d85eb74fe3c43c0dbf081a8ca3c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*lMrnCA-ycRthNR83ynJZOA.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk translated">为平稳问题选择的估计公式</figcaption></figure><p id="fa59" class="pw-post-body-paragraph lj lk iq ll b lm md jr lo lp me ju lr kw mf lt lu la mg lw lx le mh lz ma mb ij bi translated">在哪里，</p><ol class=""><li id="fd7f" class="nd ne iq ll b lm md lp me kw nf la ng le nh mb ni nj nk nl bi translated">第一个等式给出了在<code class="fe nm nn no ms b">t</code>的估计奖励值的公式，这是到时间步<code class="fe nm nn no ms b">t-1</code>为止我们收到的所有奖励的简单平均值</li><li id="c2cd" class="nd ne iq ll b lm np lp nq kw nr la ns le nt mb ni nj nk nl bi translated">第二个方程只是写同样东西的一种很好的方式。表示对<code class="fe nm nn no ms b">n+1</code>步骤的估计将是直到步骤<code class="fe nm nn no ms b">n</code>的所有奖励的平均值</li><li id="4ee3" class="nd ne iq ll b lm np lp nq kw nr la ns le nt mb ni nj nk nl bi translated">第三个，是当你扩展第二个方程并加入<code class="fe nm nn no ms b">Qn</code>的公式时得到的，它类似于<code class="fe nm nn no ms b">Qn+1</code>的公式，只是少了一步(用<code class="fe nm nn no ms b">n-1</code>代替<code class="fe nm nn no ms b">n</code>)。这里，<code class="fe nm nn no ms b">Qn+1</code>是对<code class="fe nm nn no ms b">n+1</code>步骤的新估计，<code class="fe nm nn no ms b">Qn</code>是旧估计，即直到步骤<code class="fe nm nn no ms b">n</code>的估计，<code class="fe nm nn no ms b">Rn</code>是对<code class="fe nm nn no ms b">nth</code>步骤的奖励，<code class="fe nm nn no ms b">1/n</code>是我们想要更新新估计的步长。</li></ol><p id="10fe" class="pw-post-body-paragraph lj lk iq ll b lm md jr lo lp me ju lr kw mf lt lu la mg lw lx le mh lz ma mb ij bi translated">为了清楚起见，按照这个公式，如果一个特定的动作被选择了比如说5次，那么这5个奖励(<code class="fe nm nn no ms b">n</code>动作导致<code class="fe nm nn no ms b">n</code>奖励)中的每一个将被除以1/5，然后相加得到估计值，直到步骤5。如果你仔细观察，你会发现我们对所有的奖励都给予了同等的权重，不管它们发生的时间，这意味着我们想说，每一个奖励对我们来说都是同等重要的，因此是同等的权重。这适用于静态问题，但新问题呢？随着奖励分布的变化，最新的奖励不是对真实奖励分布的更好估计吗？那么，难道我们不应该对新的奖励给予更多的重视，对旧的奖励给予更少的重视吗？这个想法绝对值得追求。这将意味着改变奖励权重，这可以通过将平均奖励估计替换为指数最近加权平均来实现。我们可以通过提供一个选项来进一步使这个过程通用化，这个选项可以是新的或旧的奖励应该给予更大的权重，或者是一个随着时间减少权重的中间解决方法。事实证明，这可以很容易地通过用常数代替旧公式中的阶跃函数<code class="fe nm nn no ms b">1/n</code>来实现，比如𝛂.这更新了估计函数，</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c739058b55970ee6138088191370455b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*GOU-Jgx6iMjdCiNzDd18cA.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk translated">为非平稳问题选择的估计公式</figcaption></figure><p id="1f56" class="pw-post-body-paragraph lj lk iq ll b lm md jr lo lp me ju lr kw mf lt lu la mg lw lx le mh lz ma mb ij bi translated">在哪里，</p><ol class=""><li id="fd8c" class="nd ne iq ll b lm md lp me kw nf la ng le nh mb ni nj nk nl bi translated">如果𝛂= 1；<code class="fe nm nn no ms b">Rn</code>也就是说，最近一次奖励的最大权重为1，其余奖励的权重为0。因此，如果您的预期动作值的偏差太大，我们可以使用这个设置。</li><li id="ff48" class="nd ne iq ll b lm np lp nq kw nr la ns le nt mb ni nj nk nl bi translated">如果𝛂= 0；<code class="fe nm nn no ms b">Q1</code>即，最早的奖励估计将具有最大权重1，其余的将具有0。当我们只想考虑初始估计值时，我们可以使用它。</li><li id="0e0c" class="nd ne iq ll b lm np lp nq kw nr la ns le nt mb ni nj nk nl bi translated">如果0 1-𝛂呈指数下降。在这种情况下，最早的奖励权重较小，最新的奖励权重较高。这就是我们想要尝试的。</li></ol><h2 id="aaba" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">非稳态解</h2><p id="0377" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr kw ls lt lu la lv lw lx le ly lz ma mb ij bi translated">让我们通过简单地更新代码，用一个常数值(比如0.1)代替步长，并保持其余参数不变，来正式化这个解决方案。这将实现指数递减权重。稍后，我们将计算最佳行动选择百分比，并对其进行反思。</p><pre class="kf kg kh ki gt mr ms mt mu aw mv bi"><span id="e237" class="kn ko iq ms b gy mw mx l my mz"># update the estimation calculation<br/>&gt;&gt;&gt; estimate_action_value[action] = estimate_action_value[action] + 0.1 * (reward - estimate_action_value[action])</span></pre><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nv"><img src="../Images/03a646d718a3d060ab864d3c7ceb5707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*exj_eqNM2Zi2wXmVwkHGcw.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk translated">非平稳问题:恒定步长导致的e-greedy算法性能</figcaption></figure><p id="1827" class="pw-post-body-paragraph lj lk iq ll b lm md jr lo lp me ju lr kw mf lt lu la mg lw lx le mh lz ma mb ij bi translated">好吧，我们又开始做生意了。我们可以得出结论，</p><ol class=""><li id="39a4" class="nd ne iq ll b lm md lp me kw nf la ng le nh mb ni nj nk nl bi translated">e=0.01的设置优于其竞争对手，并且在一些步骤之后性能收敛到最大值。在这里，我们没有看到性能的任何下降，因为我们修改了评估函数，它考虑了奖励分配的变化性质。</li><li id="b903" class="nd ne iq ll b lm np lp nq kw nr la ns le nt mb ni nj nk nl bi translated">估计函数提供了对环境状态的洞察，基于此我们必须设计函数。对于稳定的问题，我们可以用基本平均来解决，但是对于复杂的变化奖励分配的环境，我们必须应用指数递减的奖励平均。</li><li id="d086" class="nd ne iq ll b lm np lp nq kw nr la ns le nt mb ni nj nk nl bi translated">这提供了一种直觉，在数学上，仅仅通过修改步长的一个参数，我们就能够考虑新环境的复杂行为，这实际上与现实世界的问题非常相似。</li></ol><h2 id="f839" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结论</h2><p id="d347" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr kw ls lt lu la lv lw lx le ly lz ma mb ij bi translated">从这篇文章中学到的是理解稳定和不稳定的环境。通过质疑和考虑环境的所有属性来设计评估函数的艺术。以及有时最小但直观的变化如何导致精度和性能的最大提高。我们仍然有一些问题和改进，可以带来更好的准确性和优化，但以后。</p><p id="1f66" class="pw-post-body-paragraph lj lk iq ll b lm md jr lo lp me ju lr kw mf lt lu la mg lw lx le mh lz ma mb ij bi translated">干杯。</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h2 id="c17b" class="kn ko iq bd kp kq kr dn ks kt ku dp kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">参考</h2><p id="93f9" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr kw ls lt lu la lv lw lx le ly lz ma mb ij bi translated">[1]强化学习——介绍；理查德·萨顿和安德鲁·巴尔托</p></div></div>    
</body>
</html>