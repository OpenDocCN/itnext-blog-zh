<html>
<head>
<title>Kubernetes: load-testing and high-load tuning — problems and solutions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kubernetes:负载测试和高负载调优—问题和解决方案</h1>
<blockquote>原文：<a href="https://itnext.io/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions-244d869a9791?source=collection_archive---------2-----------------------#2020-09-04">https://itnext.io/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions-244d869a9791?source=collection_archive---------2-----------------------#2020-09-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/39bdc7cd93335e1d8eae089f44d3befd.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/format:webp/0*n5qdMz4ipdDaGP7W.png"/></div></figure><p id="43ba" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">实际上，这篇文章是一篇关于使用<code class="fe ks kt ku kv b"><a class="ae kw" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/" rel="noopener ugc nofollow" target="_blank">NodeAffinity</a></code>作为Kubernetes Pod的简短说明:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kx"><img src="../Images/029fd9c26e09ac03c685c86c27fceee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b76iQvzyK-tNjRWJ.png"/></div></div></figure><p id="6e9c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">但是，就像经常发生的那样，在开始写一件事情之后，我面临着另一件事情，然后又是另一件事情，结果——我写了这篇关于Kubernetes负载测试的文章。</p><p id="9c84" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">所以，我已经开始讨论NodeAffinity，但是我想知道Kubernetes <code class="fe ks kt ku kv b"><a class="ae kw" href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler" rel="noopener ugc nofollow" target="_blank">cluster-autoscaler</a></code>将如何工作——它将在新的WorkerNodes创建期间考虑NodeAffinity设置吗？</p><p id="1472" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了检查这一点，我使用<a class="ae kw" href="https://rtfm.co.ua/apache-bench-testiruem-rabotu-veb-servera/" rel="noopener ugc nofollow" target="_blank"> Apache Benchmark </a>进行了一个简单的负载测试，以触发<a class="ae kw" href="https://rtfm.co.ua/kubernetes-horizontalpodautoscaler-obzor-i-primery/" rel="noopener ugc nofollow" target="_blank">Kubernetes HorizontalPodAutoscaler</a>来创建新的pod，这些新的pod必须触发<code class="fe ks kt ku kv b"><a class="ae kw" href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler" rel="noopener ugc nofollow" target="_blank">cluster-autoscaler</a></code>来创建新的AWS EC2实例，这些实例将作为WorkerNodes附加到Kubernetes集群。</p><p id="70b3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">然后，我开始了一个更复杂的负载测试，并面临一个问题，当豆荚停止伸缩。</p><p id="c484" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">然后……我决定在进行负载测试时，测试各种AWS EC2实例类型——T3、M5、C5——可能是个好主意。当然，需要添加这个职位的结果。</p><p id="1f6a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这之后——我们已经开始满负荷测试，并面临一些其他问题，显然我必须写下我是如何解决这些问题的。</p><p id="6d35" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最后，这篇文章是关于Kubernetes负载测试，EC2实例类型，网络和DNS，以及Kubernetes集群中高负载应用的一些其他内容。</p><p id="3a10" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">注</strong> : <code class="fe ks kt ku kv b">kk</code>此处:<code class="fe ks kt ku kv b">alias kk="kubectl" &gt; ~/.bashrc</code></p><ul class=""><li id="21cf" class="lg lh iq jw b jx jy kb kc kf li kj lj kn lk kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#The_Task" rel="noopener ugc nofollow" target="_blank">任务</a></li><li id="a69f" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Choosing_an_EC2_type" rel="noopener ugc nofollow" target="_blank">选择EC2类型</a></li><li id="f3d6" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#EC2_AMD_instances" rel="noopener ugc nofollow" target="_blank"> EC2 AMD实例</a></li><li id="769c" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#EC2_Graviton_instances" rel="noopener ugc nofollow" target="_blank"> EC2引力子实例</a></li><li id="e8e7" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#eksctl_and_Kubernetes_WorkerNode_Groups" rel="noopener ugc nofollow" target="_blank"> eksctl和Kubernetes工人节点组</a></li><li id="dada" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#The_Testing_plan" rel="noopener ugc nofollow" target="_blank">测试计划</a></li><li id="b046" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Kubernetes_NodeAffinity_nodeSelector" rel="noopener ugc nofollow" target="_blank">Kubernetes node affinity&amp;T27】节点选择器</a></li><li id="4d87" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Deployment_update" rel="noopener ugc nofollow" target="_blank">部署更新</a></li><li id="5835" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#nodeSelector_by_a_custom_label" rel="noopener ugc nofollow" target="_blank">通过自定义标签选择节点</a></li><li id="1de1" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#nodeSelector_by_Kuber_label" rel="noopener ugc nofollow" target="_blank">俱吠罗标签的节点选择器</a></li><li id="e865" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Testing" rel="noopener ugc nofollow" target="_blank">测试AWS EC2 t3 vs m5 vs c5 </a></li><li id="28d4" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Kubernetes_NodeAffinity_vs_Kubernetes_ClusterAutoscaler" rel="noopener ugc nofollow" target="_blank">Kubernetes node affinity vs Kubernetes cluster auto scaler</a></li><li id="d87b" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Load_Testing" rel="noopener ugc nofollow" target="_blank">负载测试</a></li><li id="1d47" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated">第一天</li><li id="baf9" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Day_2" rel="noopener ugc nofollow" target="_blank">第二天</a></li><li id="c738" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#net/http_request_canceled_(Client_Timeout_exceeded_while_awaiting_headers)" rel="noopener ugc nofollow" target="_blank"> net/http:请求已取消(客户端。等待标题时超时)</a></li><li id="0821" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#AWS_RDS_%E2%80%93_%E2%80%9CConnection_refused%E2%80%9D" rel="noopener ugc nofollow" target="_blank"> AWS RDS —“连接被拒绝”</a></li><li id="1cbe" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#AWS_RDS_max_connections" rel="noopener ugc nofollow" target="_blank"> AWS RDS最大连接数</a></li><li id="03e7" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Day_3" rel="noopener ugc nofollow" target="_blank">第三天</a></li><li id="189f" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Kubernetes_Liveness_and_Readiness_probes" rel="noopener ugc nofollow" target="_blank"> Kubernetes活性和准备就绪探测器</a></li><li id="0d54" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Kubernetes_PHP_logs_from_Docker" rel="noopener ugc nofollow" target="_blank">Kubernetes:Docker的PHP日志</a></li><li id="8415" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#The_First_Test" rel="noopener ugc nofollow" target="_blank">第一次测试</a></li><li id="9978" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#phpnetworkgetaddresses_getaddrinfo_failed_%D0%B8_DNS" rel="noopener ugc nofollow" target="_blank">PHP _ network _ get addresses:get addrinfo失败и DNS </a></li><li id="705a" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Kubernetes_dnsPolicy" rel="noopener ugc nofollow" target="_blank"> Kubernetes dnsPolicy </a></li><li id="0678" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Running_a_NodeLocal_DNS_in_Kubernetes" rel="noopener ugc nofollow" target="_blank">在Kubernetes中运行node local DNS</a></li><li id="92e4" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Kubernetes_Pod_dnsConfig_nameservers" rel="noopener ugc nofollow" target="_blank">Kubernetes Pod DNS config&amp;&amp;名称服务器</a></li><li id="f459" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#The_Second_Test" rel="noopener ugc nofollow" target="_blank">第二次考验</a></li><li id="7f65" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/#Apache_JMeter_%D0%B8_Grafana" rel="noopener ugc nofollow" target="_blank">阿帕奇JMeter и Grafana </a></li></ul><h1 id="26f4" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">任务</h1><p id="0dd5" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">所以，我们有一个真正热爱CPU的应用。</p><p id="2cfd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">PHP，Laravel。目前，它在数字海洋的50个正在运行的水滴上运行，加上NFS共享，Memcache，Redis和MySQL。</p><p id="78fd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们想要的是将此应用程序迁移到AWS EKS的Kubernetes集群，以节省一些基础架构方面的资金，因为DigitalOcean的当前集群花费我们大约4.000美元/月，而一个AWS EKS集群花费我们大约500-600美元(集群本身，加上4个AWS <em class="mx"> t3.medium </em> EC2实例，用于两个独立的AWS AvailabilityZones中的WorkerNodes，总共8个EC2)。</p><p id="b9e6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">使用DigitalOcean上的这个设置，应用程序停止在12.000个模拟用户上工作(每小时48.000)。</p><p id="5a2d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们希望通过自动缩放功能在我们的AWS EKS上保持多达15，000个用户(60，000/小时，1，440，000/天)。</p><p id="e3b4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">该项目将存在于一个专用的WorkerNodes组中，以避免影响集群中的其他应用程序。为了只在那些worker节点上创建新的pod，我们将使用<code class="fe ks kt ku kv b">NodeAffinity</code>。</p><p id="1976" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">此外，我们将使用三种不同的AWS Ec2实例类型执行负载测试— <strong class="jw ir"> t3 </strong>、<strong class="jw ir"> m5 </strong>、<strong class="jw ir"> c5 </strong>，以选择哪一个更适合我们的应用程序需求，并将执行另一个负载测试来检查HorizontalPodAutoscaler和<code class="fe ks kt ku kv b">cluster-autoscaler</code>将如何工作。</p><h1 id="eee9" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">选择EC2类型</h1><p id="00c2" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">我们用哪一个？</p><ul class=""><li id="1f3f" class="lg lh iq jw b jx jy kb kc kf li kj lj kn lk kr ll lm ln lo bi translated"><strong class="jw ir">т3</strong>？可突发处理器，良好的价格/CPU/内存比，适合大多数需求:<br/> <a class="ae kw" href="https://aws.amazon.com/ec2/instance-types/t3/" rel="noopener ugc nofollow" target="_blank"> T3实例</a>是下一代<a class="ae kw" href="https://aws.amazon.com/ec2/instance-types/#Burstable_Performance_Instances" rel="noopener ugc nofollow" target="_blank">可突发通用实例类型</a>，提供基准级别的CPU性能，能够根据需要随时突发CPU使用。</li><li id="1847" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><strong class="jw ir">м5</strong>？最适合内存消耗型应用—更多RAM，更少CPU: <br/> <a class="ae kw" href="https://aws.amazon.com/ec2/instance-types/m5/" rel="noopener ugc nofollow" target="_blank"> M5实例</a>是由英特尔至强白金8175M处理器驱动的最新一代通用实例。该系列提供了计算、内存和网络资源的平衡，是许多应用程序的良好选择。</li><li id="bb3a" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><strong class="jw ir">с5</strong>？最适合CPU应用—与M5类型相比，CPU内核更多，处理器更好，但内存更少:<br/> <a class="ae kw" href="https://aws.amazon.com/ec2/instance-types/c5/" rel="noopener ugc nofollow" target="_blank"> C5实例</a>针对计算密集型工作负载进行了优化，以较低的单位计算成本提供了经济高效的高性能。</li></ul><p id="5457" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">先说T3a——比常见的T3便宜一点。</p><h2 id="5f6c" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">EC2 AMD实例</h2><p id="f55a" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">AWS为基于AMD的处理器实例(t3a、m5a、c5a)提供几乎相同的CPU/内存/网络，它们的成本稍低，但并非在每个地区都可用，甚至不是在同一AWS地区的所有可用区域都可用。</p><p id="d803" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">例如，在AWS区域<em class="mx"> us-east-2 </em> c5a在<em class="mx"> us-east-2b </em>和<em class="mx">us-east-2c</em>availability zones中可用，但仍不能在<em class="mx"> us-east-2a </em>中使用。由于我现在不想改变我们的自动化(可用性区域是在供应期间选择的，请参见<a class="ae kw" href="https://rtfm.co.ua/en/aws-cloudformation-using-lists-in-parameters/" rel="noopener ugc nofollow" target="_blank"> AWS: CloudFormation —使用参数中的列表</a> ) —那么我们将使用常见的T3类型。</p><h2 id="8b30" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">EC2引力子实例</h2><p id="504f" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">除此之外，AWS <a class="ae kw" href="https://aws.amazon.com/blogs/compute/improving-performance-of-php-for-arm64-and-impact-on-amazon-ec2-m6g-instances/" rel="noopener ugc nofollow" target="_blank">引入了带有<a class="ae kw" href="http://aws.amazon.com/ec2/graviton" rel="noopener ugc nofollow" target="_blank"> AWS Graviton2处理器</a>的</a> <em class="mx"> m6g </em>和<em class="mx"> c6g </em>实例类型，但是要使用它们，您的集群必须满足一些限制，查看文档<a class="ae kw" href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#arm-ami" rel="noopener ugc nofollow" target="_blank">此处&gt; &gt; &gt; </a>。</p><p id="1798" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在，让我们继续用t3、m5和c5实例创建三个WorkerNode组，并检查我们的应用程序在每个实例上的CPU消耗。</p><h1 id="fd90" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><code class="fe ks kt ku kv b">eksctl</code>和Kubernetes WorkerNode组</h1><p id="46e1" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">我们的WorkerNode组的配置文件如下所示:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="c5c8" class="my lv iq kv b gy no np l nq nr">---<br/>apiVersion: eksctl.io/v1alpha5<br/>kind: ClusterConfig<br/><br/>metadata:<br/>  name: "{{ eks_cluster_name }}"<br/>  region: "{{ region }}"<br/>  version: "{{ k8s_version }}"<br/><br/>nodeGroups:<br/><br/>  - name: "eat-test-t3-{{ item }}"<br/>    instanceType: "t3.xlarge"<br/>    privateNetworking: true<br/>    labels: { role: eat-workers }<br/>    volumeSize: 50<br/>    volumeType: gp2<br/>    desiredCapacity: 1<br/>    minSize: 1<br/>    maxSize: 1<br/>    availabilityZones: ["{{ item }}"]<br/>    ssh:<br/>      publicKeyName: "bttrm-eks-nodegroup-{{ region }}"<br/>    iam:<br/>      withAddonPolicies:<br/>        autoScaler: true<br/>        cloudWatch: true<br/>        albIngress: true<br/>        efs: true<br/>    securityGroups:<br/>      withShared: true<br/>      withLocal: true<br/>      attachIDs: [ {{ worker_nodes_add_sg }} ]<br/><br/>  - name: "eat-test-m5-{{ item }}"<br/>    instanceType: "m5.xlarge"<br/>    privateNetworking: true<br/>    labels: { role: eat-workers }<br/>    volumeSize: 50<br/>    volumeType: gp2<br/>    desiredCapacity: 1<br/>    minSize: 1<br/>    maxSize: 1<br/>    availabilityZones: ["{{ item }}"]<br/>    ssh:<br/>      publicKeyName: "bttrm-eks-nodegroup-{{ region }}"<br/>    iam:<br/>      withAddonPolicies:<br/>        autoScaler: true<br/>        cloudWatch: true<br/>        albIngress: true<br/>        efs: true<br/>    securityGroups:<br/>      withShared: true<br/>      withLocal: true<br/>      attachIDs: [ {{ worker_nodes_add_sg }} ]<br/><br/>  - name: "eat-test-c5-{{ item }}"<br/>    instanceType: "c5.xlarge"<br/>    privateNetworking: true<br/>    labels: { role: eat-workers }<br/>    volumeSize: 50<br/>    volumeType: gp2<br/>    desiredCapacity: 1<br/>    minSize: 1<br/>    maxSize: 1<br/>    availabilityZones: ["{{ item }}"]<br/>    ssh:<br/>      publicKeyName: "bttrm-eks-nodegroup-{{ region }}"<br/>    iam:<br/>      withAddonPolicies:<br/>        autoScaler: true<br/>        cloudWatch: true<br/>        albIngress: true<br/>        efs: true<br/>    securityGroups:<br/>      withShared: true<br/>      withLocal: true<br/>      attachIDs: [ {{ worker_nodes_add_sg }} ]</span></pre><p id="a07d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里我们有三个工作节点组，每个都有自己的EC2类型。</p><p id="9030" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">使用Ansible和<code class="fe ks kt ku kv b">eksctl</code>描述部署，参见<a class="ae kw" href="https://rtfm.co.ua/en/aws-elastic-kubernetes-service-a-cluster-creation-automation-part-2-ansible-eksctl/" rel="noopener ugc nofollow" target="_blank"> AWS弹性Kubernetes服务:集群创建自动化，第2部分-ansi ble，eksctl </a>文章，在两个不同的可用性区域中。</p><p id="5428" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">将<code class="fe ks kt ku kv b">minSize</code>和<code class="fe ks kt ku kv b">maxSize</code>设置为<em class="mx"> 1 </em>，这样我们的集群自动缩放器就不会开始缩放它们——在请求测试时，我希望看到一个CPU在一个EC2实例上的负载，并对pod和节点运行<code class="fe ks kt ku kv b">kubectl top</code>。</p><p id="a60e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">之后，我们将选择最适合我们的EC2类型—将删除其他WorkerNode组，并将启用自动缩放。</p><h1 id="498c" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">测试计划</h1><p id="b50c" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">我们将测试什么以及如何测试:</p><ul class=""><li id="e30a" class="lg lh iq jw b jx jy kb kc kf li kj lj kn lk kr ll lm ln lo bi translated">PHP，Laravel，打包成一个Docker映像</li><li id="3f39" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated">所有服务器都有4个CPU内核和16 GB内存(不包括C5–8 GB内存)</li><li id="1fae" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated">在使用<code class="fe ks kt ku kv b"><a class="ae kw" href="https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-resource-requests-and-limits" rel="noopener ugc nofollow" target="_blank">requests</a></code>的应用程序部署中，我们将设置每个WorkerNode只运行一个pod，方法是使用一半的CPU，因此Kubernetes调度程序必须将一个pod放在一个专用的WorkerNode实例上</li><li id="e1bd" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated">通过使用<code class="fe ks kt ku kv b">NodeAffinity</code>,我们将设置只在必要的工作节点上运行我们的pod</li><li id="8fe6" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated">pods和集群自动扩展暂时被禁用</li></ul><p id="02b5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们将创建三个具有不同EC2类型的WorkerNode组，并将应用程序部署到四个Kubernetes名称空间中——一个“默认”,每个实例类型三个。</p><p id="8664" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在每一个这样的名称空间中，里面的应用程序将被配置为使用<code class="fe ks kt ku kv b">NodeAffinity</code>在必要的EC2类型上运行。</p><p id="5bca" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通过这样做，我们将拥有四个带有<a class="ae kw" href="https://rtfm.co.ua/en/aws-elastic-kubernetes-service-running-alb-ingress-controller/" rel="noopener ugc nofollow" target="_blank"> AWS负载平衡器的<code class="fe ks kt ku kv b">Ingress</code>资源，</a>参见<a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-clusterip-vs-nodeport-vs-loadbalancer-services-and-ingress-an-overview-with-examples/" rel="noopener ugc nofollow" target="_blank">Kubernetes:cluster IP vs node port vs负载平衡器、服务和入口–示例概述</a>，我们将拥有四个测试端点。</p><h1 id="9f97" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">kubernetes no affinity &amp; &amp;<code class="fe ks kt ku kv b">nodeSelector</code></h1><p id="e02c" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">文档— <a class="ae kw" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/" rel="noopener ugc nofollow" target="_blank">将pod分配给节点</a>。</p><p id="aec1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了选择Kubernetes必须在哪个WorkerNode上运行pod，我们可以使用两种标签类型——由我们自己创建的标签或由Kubernetes自己分配给worker node的标签。</p><p id="3f9d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在WorkerNodes的配置文件中，我们设置了这样一个标签:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="76b3" class="my lv iq kv b gy no np l nq nr">...<br/>labels: { role: eat-workers }<br/>...</span></pre><p id="09f2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">它将被附加到在这个WorkerNode组中创建的每个EC2。</p><p id="f540" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">更新集群:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi ns"><img src="../Images/425749a12dfba5268aa842631abf337b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YVIglrRZI04ZlRPn.png"/></div></div></figure><p id="74e4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们检查一个实例上的所有标签:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/0f1ba95b11901aa5c254bd825cf1d633.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/0*A9TGKePKKZHTJpws.png"/></div></figure><p id="052c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们检查来自<code class="fe ks kt ku kv b">eksctl</code>的工作节点组:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="d3b7" class="my lv iq kv b gy no np l nq nr">$ eksctl — profile arseniy — cluster bttrm-eks-dev-1 get nodegroups<br/>CLUSTER NODEGROUP CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID<br/>bttrm-eks-dev-1 eat-test-c5-us-east-2a 2020–08–20T09:29:28Z 1 1 1 c5.xlarge ami-0f056ad53eddfda19<br/>bttrm-eks-dev-1 eat-test-c5-us-east-2b 2020–08–20T09:34:54Z 1 1 1 c5.xlarge ami-0f056ad53eddfda19<br/>bttrm-eks-dev-1 eat-test-m5-us-east-2a 2020–08–20T09:29:28Z 1 1 1 m5.xlarge ami-0f056ad53eddfda19<br/>bttrm-eks-dev-1 eat-test-m5-us-east-2b 2020–08–20T09:34:54Z 1 1 1 m5.xlarge ami-0f056ad53eddfda19<br/>bttrm-eks-dev-1 eat-test-t3-us-east-2a 2020–08–20T09:29:27Z 1 1 1 t3.xlarge ami-0f056ad53eddfda19<br/>bttrm-eks-dev-1 eat-test-t3-us-east-2b 2020–08–20T09:34:54Z 1 1 1 t3.xlarge ami-0f056ad53eddfda19</span></pre><p id="ceaa" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们用<code class="fe ks kt ku kv b">-l</code>检查创建的worker nodeес2实例，只选择那些带有我们的自定义标签“<em class="mx"> role: eat-workers </em>”的实例，并按照它们的EC2类型对它们进行排序:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="f3e0" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get node -l role=eat-workers -o=json | jq -r ‘[.items | sort_by(.metadata.labels[“beta.kubernetes.io/instance-type”])[] | {name:.metadata.name, type:.metadata.labels[“beta.kubernetes.io/instance-type”], region:.metadata.labels[“failure-domain.beta.kubernetes.io/zone”]}]’<br/>[<br/>{<br/>“name”: “ip-10–3–47–253.us-east-2.compute.internal”,<br/>“type”: “c5.xlarge”,<br/>“region”: “us-east-2a”<br/>},<br/>{<br/>“name”: “ip-10–3–53–83.us-east-2.compute.internal”,<br/>“type”: “c5.xlarge”,<br/>“region”: “us-east-2b”<br/>},<br/>{<br/>“name”: “ip-10–3–33–222.us-east-2.compute.internal”,<br/>“type”: “m5.xlarge”,<br/>“region”: “us-east-2a”<br/>},<br/>{<br/>“name”: “ip-10–3–61–225.us-east-2.compute.internal”,<br/>“type”: “m5.xlarge”,<br/>“region”: “us-east-2b”<br/>},<br/>{<br/>“name”: “ip-10–3–45–186.us-east-2.compute.internal”,<br/>“type”: “t3.xlarge”,<br/>“region”: “us-east-2a”<br/>},<br/>{<br/>“name”: “ip-10–3–63–119.us-east-2.compute.internal”,<br/>“type”: “t3.xlarge”,<br/>“region”: “us-east-2b”<br/>}<br/>]</span></pre><p id="349a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">点击&gt; &gt;  &gt;查看更多关于<code class="fe ks kt ku kv b">kubectl</code>输出的格式<a class="ae kw" href="https://gist.github.com/so0k/42313dbb3b547a0f51a547bb968696ba" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="f8f4" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">部署更新</h2><h2 id="06a6" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated"><code class="fe ks kt ku kv b">nodeSelector</code>通过定制标签</h2><p id="0b03" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">首先，让我们用<code class="fe ks kt ku kv b">labels: { role: eat-workers }</code>将我们的应用程序部署到所有实例中——Kubernetes必须在6台服务器上创建pods每种EC2类型两个。</p><p id="a8f4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">更新部署，添加带有<em class="mx">角色</em>标签和“<em class="mx">吃人</em>”值的<code class="fe ks kt ku kv b">nodeSelector</code>:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="607f" class="my lv iq kv b gy no np l nq nr">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: {{ .Chart.Name }}<br/>  annotations:<br/>    reloader.stakater.com/auto: "true"<br/>spec:<br/>  replicas: {{ .Values.replicaCount }}<br/>  strategy:<br/>    type: RollingUpdate<br/>  selector:<br/>    matchLabels:<br/>      application: {{ .Chart.Name }}<br/>  template:<br/>    metadata:<br/>      labels:<br/>        application: {{ .Chart.Name }}<br/>        version: {{ .Chart.Version }}-{{ .Chart.AppVersion }}<br/>        managed-by: {{ .Release.Service }}<br/>    spec:<br/>      containers:<br/>      - name: {{ .Chart.Name }}<br/>        image: {{ .Values.image.registry }}/{{ .Values.image.repository }}/{{ .Values.image.name }}:{{ .Values.image.tag }}<br/>        imagePullPolicy: Always<br/>...<br/>        ports:<br/>          - containerPort: {{ .Values.appConfig.port }}<br/>        livenessProbe:<br/>          httpGet:<br/>            path: {{ .Values.appConfig.healthcheckPath }}<br/>            port: {{ .Values.appConfig.port }}<br/>          initialDelaySeconds: 10<br/>        readinessProbe:<br/>          httpGet:<br/>            path: {{ .Values.appConfig.healthcheckPath }}<br/>            port: {{ .Values.appConfig.port }}<br/>          initialDelaySeconds: 10<br/>        resources:<br/>          requests:<br/>            cpu: {{ .Values.resources.requests.cpu | quote }}<br/>            memory: {{ .Values.resources.requests.memory | quote }}<br/>      nodeSelector:<br/>        role: eat-workers<br/>      volumes:<br/>      imagePullSecrets:<br/>        - name: gitlab-secret</span></pre><p id="761f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe ks kt ku kv b">replicaCount</code>设置为6，按实例数。</p><p id="5e5d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">部署它:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="ccd3" class="my lv iq kv b gy no np l nq nr">$ helm secrets upgrade --install --namespace eks-dev-1-eat-backend-ns --set image.tag=179217391 --set appConfig.appEnv=local --set appConfig.appUrl=https://dev-eks.eat.example.com/ --atomic eat-backend . -f secrets.dev.yaml --debug</span></pre><p id="1490" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">检查:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="30b1" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName,TYPE:.spec.nodeSelector</span><span id="35d5" class="my lv iq kv b gy nu np l nq nr">NAME STATUS NODE TYPE<br/>eat-backend-57b7b54d98–7m27q Running ip-10–3–63–119.us-east-2.compute.internal map[role:eat-workers]<br/>eat-backend-57b7b54d98–7tvtk Running ip-10–3–53–83.us-east-2.compute.internal map[role:eat-workers]<br/>eat-backend-57b7b54d98–8kphq Running ip-10–3–47–253.us-east-2.compute.internal map[role:eat-workers]<br/>eat-backend-57b7b54d98-l24wr Running ip-10–3–61–225.us-east-2.compute.internal map[role:eat-workers]<br/>eat-backend-57b7b54d98-ns4nr Running ip-10–3–45–186.us-east-2.compute.internal map[role:eat-workers]<br/>eat-backend-57b7b54d98-sxzk4 Running ip-10–3–33–222.us-east-2.compute.internal map[role:eat-workers]<br/>eat-backend-memcached-0 Running ip-10–3–63–119.us-east-2.compute.internal &lt;none&gt;</span></pre><p id="a3d7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">很好——我们在6个工作节点上有6个吊舱。</p><h2 id="842d" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated"><code class="fe ks kt ku kv b">nodeSelector</code>由俱吠罗标注</h2><p id="9734" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">现在，让我们更新部署以使用由Kubernetes本身设置的标签，例如，我们可以使用<em class="mx">beta.kubernetes.io/instance-type</em>，在这里我们可以设置一个实例类型，我们希望使用它来仅在所选类型的EC2上部署一个pod。</p><p id="43b8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe ks kt ku kv b">replicaCount</code>现在按照相同类型的每个实例设置为2——将有两个pod运行在两个EC2上。</p><p id="c7f0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">删除部署:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="146b" class="my lv iq kv b gy no np l nq nr">$ helm --namespace eks-dev-1-eat-backend-ns uninstall eat-backend<br/>release “eat-backend” uninstalled</span></pre><p id="36fc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">更新清单—添加<em class="mx"> t3 </em>，这样两个条件都可以工作—T5和T6:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="05a5" class="my lv iq kv b gy no np l nq nr">...<br/>      nodeSelector:<br/>        beta.kubernetes.io/instance-type: t3.xlarge<br/>        role: eat-workers<br/>...</span></pre><p id="2439" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们将它们部署到三个新的名称空间，并为t3、m5和c5各添加一个后缀，因此t3组的名称将是“<em class="mx">eks-dev-1-eat—back end-ns-</em><strong class="jw ir"><em class="mx">T3</em></strong>”。</p><p id="01b1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为舵增加<code class="fe ks kt ku kv b">--create-namespace</code>:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="8434" class="my lv iq kv b gy no np l nq nr">$ helm secrets upgrade --install --namespace eks-dev-1-eat-backend-ns-t3 --set image.tag=180029557 --set appConfig.appEnv=local --set appConfig.appUrl=https://t3-dev-eks.eat.example.com/ --atomic eat-backend . -f secrets.dev.yaml --debug --create-namespace</span></pre><p id="b69c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对m5、c5重复同样的操作，并进行检查。</p><p id="b036" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">t3:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="1f2d" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns-t3 get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName,TYPE:.spec.nodeSelector<br/>NAME STATUS NODE TYPE<br/>eat-backend-cc9b8cdbf-tv9h5 Running ip-10–3–45–186.us-east-2.compute.internal map[beta.kubernetes.io/instance-type:t3.xlarge role:eat-workers]<br/>eat-backend-cc9b8cdbf-w7w5w Running ip-10–3–63–119.us-east-2.compute.internal map[beta.kubernetes.io/instance-type:t3.xlarge role:eat-workers]<br/>eat-backend-memcached-0 Running ip-10–3–53–83.us-east-2.compute.internal &lt;none&gt;</span></pre><p id="7b77" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">m5:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="f0d7" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns-m5 get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName,TYPE:.spec.nodeSelector<br/>NAME STATUS NODE TYPE<br/>eat-backend-7dfb56b75c-k8gt6 Running ip-10–3–61–225.us-east-2.compute.internal map[beta.kubernetes.io/instance-type:m5.xlarge role:eat-workers]<br/>eat-backend-7dfb56b75c-wq9n2 Running ip-10–3–33–222.us-east-2.compute.internal map[beta.kubernetes.io/instance-type:m5.xlarge role:eat-workers]<br/>eat-backend-memcached-0 Running ip-10–3–47–253.us-east-2.compute.internal &lt;none&gt;</span></pre><p id="955f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">和c5:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="20df" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns-c5 get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName,TYPE:.spec.nodeSelector<br/>NAME STATUS NODE TYPE<br/>eat-backend-7b6778c5c-9g6st Running ip-10–3–47–253.us-east-2.compute.internal map[beta.kubernetes.io/instance-type:c5.xlarge role:eat-workers]<br/>eat-backend-7b6778c5c-sh5sn Running ip-10–3–53–83.us-east-2.compute.internal map[beta.kubernetes.io/instance-type:c5.xlarge role:eat-workers]<br/>eat-backend-memcached-0 Running ip-10–3–47–58.us-east-2.compute.internal &lt;none&gt;</span></pre><p id="b7fd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">测试的一切都准备好了。</p><h1 id="1f5a" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">测试AWS EC2 t3与m5和c5</h1><p id="133c" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">对所有WorkerNode组运行相同的测试套件，并观察pod的CPU消耗。</p><h2 id="6b8b" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">t3</h2><p id="907c" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">豆荚:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="7d6f" class="my lv iq kv b gy no np l nq nr">$ kk top nod-n eks-dev-1-eat-backend-ns-t3 top pod<br/>NAME CPU(cores) MEMORY(bytes)<br/>eat-backend-79cfc4f9dd-q22rh 1503m 103Mi<br/>eat-backend-79cfc4f9dd-wv5xv 1062m 106Mi<br/>eat-backend-memcached-0 1m 2Mi</span></pre><p id="6272" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">节点:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="3afc" class="my lv iq kv b gy no np l nq nr">$ kk top node -l role=eat-workers,beta.kubernetes.io/instance-type=t3.xlarge<br/>NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%<br/>ip-10–3–45–186.us-east-2.compute.internal 1034m 26% 1125Mi 8%<br/>ip-10–3–63–119.us-east-2.compute.internal 1616m 41% 1080Mi 8%</span></pre><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nv"><img src="../Images/9afc9f097a0413bf4989eb218dee74a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/0*8uzNQDzNEicCiVj2.png"/></div></div></figure><h2 id="bb63" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">M5</h2><p id="0c7b" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">豆荚:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="d0be" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns-m5 top pod<br/>NAME CPU(cores) MEMORY(bytes)<br/>eat-backend-6f5d68778d-484lk 1039m 114Mi<br/>eat-backend-6f5d68778d-lddbw 1207m 105Mi<br/>eat-backend-memcached-0 1m 2Mi</span></pre><p id="d985" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">节点:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="e36e" class="my lv iq kv b gy no np l nq nr">$ kk top node -l role=eat-workers,beta.kubernetes.io/instance-type=m5.xlarge<br/>NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%<br/>ip-10–3–33–222.us-east-2.compute.internal 1550m 39% 1119Mi 8%<br/>ip-10–3–61–225.us-east-2.compute.internal 891m 22% 1087Mi 8%</span></pre><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/70cda7ee960fe2d6e120d7cc725c33f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/0*0aNa-bvu34UU32M3.png"/></div></figure><h2 id="e2b9" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">溴化五烃季胺</h2><p id="4124" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">豆荚:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="eac5" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns-c5 top pod<br/>NAME CPU(cores) MEMORY(bytes)<br/>eat-backend-79b947c74d-mkgm9 941m 103Mi<br/>eat-backend-79b947c74d-x5qjd 905m 107Mi<br/>eat-backend-memcached-0 1m 2Mi</span></pre><p id="8c9d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">节点:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="808f" class="my lv iq kv b gy no np l nq nr">$ kk top node -l role=eat-workers,beta.kubernetes.io/instance-type=c5.xlarge<br/>NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%<br/>ip-10–3–47–253.us-east-2.compute.internal 704m 17% 1114Mi 19%<br/>ip-10–3–53–83.us-east-2.compute.internal 1702m 43% 1122Mi 19%</span></pre><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/cdf6c2f82c48c11639f63163b042290b.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/0*l488tqbbQQxDqiX6.png"/></div></figure><p id="9ec0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其实就这些了。</p><p id="41b3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">结果:</p><ul class=""><li id="cfc2" class="lg lh iq jw b jx jy kb kc kf li kj lj kn lk kr ll lm ln lo bi translated"><strong class="jw ir">T3</strong>:1000–1500 mCPU，385毫秒响应</li><li id="11d8" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><strong class="jw ir">M5</strong>:1000–1200 mCPU，371毫秒响应</li><li id="c8b1" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><strong class="jw ir">C5</strong>:900–1000 mCPU，370毫秒响应</li></ul><p id="490e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">所以，让我们现在使用с5型，因为它们似乎是CPU使用率最高的。</p><h1 id="ab25" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">Kubernetes NodeAffinity与Kubernetes ClusterAutoscaler</h1><p id="fbb6" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">我一直在纠结的一个主要问题是——集群自动伸缩会尊重<code class="fe ks kt ku kv b">NodeAffinity</code>吗？</p><p id="5592" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">展望未来，是的，会的。</p><p id="95ad" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们的<code class="fe ks kt ku kv b">HorizontalPodAutoscaler</code>看起来是这样的:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="b094" class="my lv iq kv b gy no np l nq nr">---         <br/>apiVersion: autoscaling/v2beta2<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/>  name: {{ .Chart.Name }}-hpa<br/>spec:       <br/>  scaleTargetRef:<br/>    apiVersion: apps/v1<br/>    kind: Deployment<br/>    name: {{ .Chart.Name }}<br/>  minReplicas: {{ .Values.hpa.minReplicas }}<br/>  maxReplicas: {{ .Values.hpa.maxReplicas }}<br/>  metrics:    <br/>  - type: Resource<br/>    resource:<br/>      name: cpu<br/>      target: <br/>        type: Utilization<br/>        averageUtilization: {{ .Values.hpa.cpuUtilLimit }}</span></pre><p id="8f20" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe ks kt ku kv b">cpuUtilLimit</code>被设置为30%,所以当PHP-FPM将开始积极使用它的FPM工人时，CPU负载将上升，30%的限制将给我们一些时间来旋转新的pods和EC2实例，而现有的pods将保持现有的连接。</p><p id="2b3a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">更多详情请参见<a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-horizontalpodautoscaler-an-overview-with-examples/" rel="noopener ugc nofollow" target="_blank">Kubernetes:HorizontalPodAutoscaler—示例概述</a>帖子。</p><p id="bd65" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在通过使用Helm模板及其<code class="fe ks kt ku kv b">values.yaml</code>来描述<code class="fe ks kt ku kv b">nodeSelector</code>，查看<a class="ae kw" href="https://rtfm.co.ua/en/helm-kubernetes-package-manager-an-overview-getting-started/" rel="noopener ugc nofollow" target="_blank"> Helm: Kubernetes包管理器-概述，入门</a>:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="7d16" class="my lv iq kv b gy no np l nq nr">...<br/>      nodeSelector:<br/>        beta.kubernetes.io/instance-type: {{ .Values.nodeSelector.instanceType | quote }}<br/>        role: {{ .Values.nodeSelector.role | quote }}<br/>...</span></pre><p id="8826" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">及其<code class="fe ks kt ku kv b">values.yaml</code>:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="6f63" class="my lv iq kv b gy no np l nq nr">...<br/>nodeSelector: <br/>  instanceType: "c5.xlarge"<br/>  role: "eat-workers<br/>...</span></pre><p id="31c9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">重新创建一切，让我们从全负载测试开始。</p><p id="3f82" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在没有任何活动的情况下，接下来是资源消耗:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="eae3" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns top pod<br/>NAME CPU(cores) MEMORY(bytes)<br/>eat-backend-b8b79574–8kjl4 50m 55Mi<br/>eat-backend-b8b79574–8t2pw 39m 55Mi<br/>eat-backend-b8b79574-bq8nw 52m 68Mi<br/>eat-backend-b8b79574-swbvq 40m 55Mi<br/>eat-backend-memcached-0 2m 6Mi</span></pre><p id="7e86" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在4-х <em class="mx"> c5.xlarge </em>服务器上(4个内核，8 GB RAM):</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="bfa1" class="my lv iq kv b gy no np l nq nr">$ kk top node -l role=eat-workers<br/>NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%<br/>ip-10–3–34–151.us-east-2.compute.internal 105m 2% 1033Mi 18%<br/>ip-10–3–39–132.us-east-2.compute.internal 110m 2% 1081Mi 19%<br/>ip-10–3–54–32.us-east-2.compute.internal 166m 4% 1002Mi 17%<br/>ip-10–3–56–98.us-east-2.compute.internal 106m 2% 1010Mi 17%</span></pre><p id="44be" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">前面已经提到过的<code class="fe ks kt ku kv b">HorizontalPodAutoscaler</code>，на 30% CPU的请求限制:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="cde2" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>eat-backend-hpa Deployment/eat-backend 1%/30% 4 40 4 6m27s</span></pre><h1 id="b49b" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">负载测试</h1><h2 id="ae65" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">第一天</h2><p id="0b3e" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">简而言之，这是整个测试的第一天，总共花了三天时间。</p><p id="1a18" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">该测试是在四个<em class="mx"> t3a.medium </em>实例中执行的，每个WorkerNode有相同的1个pod，并启用了HPA和集群自动缩放。</p><p id="f26e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一切都很顺利，直到我们达到了8，000个同时用户——查看响应时间:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi ny"><img src="../Images/b9d843e6169ea4cd79579cdebd5e746f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9a4xqfuCmlNo4wOp.png"/></div></div></figure><p id="11af" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">豆荚停止膨胀:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nz"><img src="../Images/d075b3106d55cbaecc99fea751fd50e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zVTLc1ePl3_bzpBs.png"/></div></div></figure><p id="b1cc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因为他们停止产生超过30%的CPU负载。</p><p id="cf0d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我的第一个假设是正确的:PHP-FPM被配置为最多有5个FPM工人的<code class="fe ks kt ku kv b">OnDemand</code>(参见<a class="ae kw" href="https://rtfm.co.ua/php-fpm-process-manager-dynamic-vs-ondemand-vs-static/" rel="noopener ugc nofollow" target="_blank"> PHP-FPM:流程管理器——动态vs按需vs静态</a>，<em class="mx"> Rus </em>)。</p><p id="b2c9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因此，FPM启动了5个工作线程，这些工作线程无法根据部署请求在CPU内核上产生超过30%的负载，HPA停止了对它们的扩展。</p><p id="16d5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">第二天，我们将它改为最多有50名工作人员的<code class="fe ks kt ku kv b">Dynamic</code>(第三天改为<code class="fe ks kt ku kv b">Static</code>，以避免花费时间创建新流程),之后，他们开始一直产生CPU负载，因此HPA开始扩展我们的pod。</p><p id="06b1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">虽然还有另一种解决方案，比如只为HPA添加一个条件，例如通过负载平衡器连接，稍后我们将这样做(参见<a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-a-clusters-monitoring-with-the-prometheus-operator/" rel="noopener ugc nofollow" target="_blank"> Kubernetes:使用Prometheus操作符的集群监控</a>)。</p><h2 id="dd41" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">第二天</h2><p id="907d" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">继续由JMeter使用昨天(和明天)相同的测试套件进行测试。</p><p id="7503" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从一个用户开始，增加到15000个并发用户。</p><p id="a53b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">数字海洋上的当前基础设施最多可处理12，000个用户，但在EKS自动气象站上，我们希望能够保持多达15，000个用户。</p><p id="4a49" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们走吧:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oa"><img src="../Images/8fbb6f77086b19fe3481c360e17ee87c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*irrk8Q-ww2AeOlCf.png"/></div></div></figure><p id="f466" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在3300用户上，pod开始扩展:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="22f6" class="my lv iq kv b gy no np l nq nr">…<br/>0s Normal SuccessfulRescale HorizontalPodAutoscaler New size: 5; reason: cpu resource utilization (percentage of request) above target<br/>0s Normal ScalingReplicaSet Deployment Scaled up replica set eat-backend-b8b79574 to 5<br/>0s Normal SuccessfulCreate ReplicaSet Created pod: eat-backend-b8b79574-l68vq<br/>0s Warning FailedScheduling Pod 0/12 nodes are available: 12 Insufficient cpu, 8 node(s) didn’t match node selector.<br/>0s Warning FailedScheduling Pod 0/12 nodes are available: 12 Insufficient cpu, 8 node(s) didn’t match node selector.<br/>0s Normal TriggeredScaleUp Pod pod triggered scale-up: [{eksctl-bttrm-eks-dev-1-nodegroup-eat-us-east-2b-NodeGroup-1N0QUROWQ8K2Q 2-&gt;3 (max: 20)}]<br/>…</span></pre><p id="f6b3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">以及新的EC2节点:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="d3d3" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns top pod<br/>NAME CPU(cores) MEMORY(bytes)<br/>eat-backend-b8b79574–8kjl4 968m 85Mi<br/>eat-backend-b8b79574–8t2pw 1386m 85Mi<br/>eat-backend-b8b79574-bq8nw 737m 71Mi<br/>eat-backend-b8b79574-l68vq 0m 0Mi<br/>eat-backend-b8b79574-swbvq 573m 71Mi<br/>eat-backend-memcached-0 20m 15Mi</span><span id="5b7d" class="my lv iq kv b gy nu np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>eat-backend-hpa Deployment/eat-backend 36%/30% 4 40 5 37m</span><span id="fd1b" class="my lv iq kv b gy nu np l nq nr">$ kk top node -l role=eat-workers<br/>NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%<br/>ip-10–3–34–151.us-east-2.compute.internal 662m 16% 1051Mi 18%<br/>ip-10–3–39–132.us-east-2.compute.internal 811m 20% 1095Mi 19%<br/>ip-10–3–53–136.us-east-2.compute.internal 2023m 51% 567Mi 9%<br/>ip-10–3–54–32.us-east-2.compute.internal 1115m 28% 1032Mi 18%<br/>ip-10–3–56–98.us-east-2.compute.internal 1485m 37% 1040Mi 18%</span></pre><p id="2a51" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">5500 —目前一切正常:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oa"><img src="../Images/f49b51951164af63be7d34c299b62951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qnvITpgpY9uDoYJV.png"/></div></div></figure><h2 id="9ea7" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">net/http:请求已取消(客户端。等待标题时超时)</h2><p id="cc88" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">在7.000–8.000上，我们遇到了问题—pod开始无法通过"<strong class="jw ir"> <em class="mx">客户端进行活性和就绪性检查。等待标题时超时</em> </strong>"错误:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="6abe" class="my lv iq kv b gy no np l nq nr">0s Warning Unhealthy Pod Liveness probe failed: Get <a class="ae kw" href="http://10.3.38.7:80/:" rel="noopener ugc nofollow" target="_blank">http://10.3.38.7:80/:</a> net/http: request canceled (Client.Timeout exceeded while awaiting headers)<br/>1s Warning Unhealthy Pod Readiness probe failed: Get <a class="ae kw" href="http://10.3.44.96:80/:" rel="noopener ugc nofollow" target="_blank">http://10.3.44.96:80/:</a> net/http: request canceled (Client.Timeout exceeded while awaiting headers)<br/>0s Normal MODIFY Ingress rule 1 modified with conditions [{ Field: “path-pattern”, Values: [“/*”] }]<br/>0s Warning Unhealthy Pod Liveness probe failed: Get <a class="ae kw" href="http://10.3.44.34:80/:" rel="noopener ugc nofollow" target="_blank">http://10.3.44.34:80/:</a> net/http: request canceled (Client.Timeout exceeded while awaiting headers)</span></pre><p id="4720" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">随着用户越来越多，情况只会越来越糟— 10.000:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oa"><img src="../Images/8f86fa29a34fb4e82ed2ae18ae66675d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*X6b6ZYQi61RWdS6I.png"/></div></div></figure><p id="f108" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Pods几乎总是开始失败，最糟糕的是我们甚至没有来自应用程序的日志——它继续写入容器内部的日志文件，我们直到第三天才修复它。</p><p id="4bcd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">负载是这样的:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="586d" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>eat-backend-hpa Deployment/eat-backend 60%/30% 4 40 15 63m</span><span id="a1be" class="my lv iq kv b gy nu np l nq nr">$ kk top node -l role=eat-workers<br/>NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%<br/>ip-10–3–33–155.us-east-2.compute.internal 88m 2% 951Mi 16%<br/>ip-10–3–34–151.us-east-2.compute.internal 1642m 41% 1196Mi 20%<br/>ip-10–3–39–128.us-east-2.compute.internal 67m 1% 946Mi 16%<br/>ip-10–3–39–132.us-east-2.compute.internal 73m 1% 1029Mi 18%<br/>ip-10–3–43–76.us-east-2.compute.internal 185m 4% 1008Mi 17%<br/>ip-10–3–47–243.us-east-2.compute.internal 71m 1% 959Mi 16%<br/>ip-10–3–47–61.us-east-2.compute.internal 69m 1% 945Mi 16%<br/>ip-10–3–53–124.us-east-2.compute.internal 61m 1% 955Mi 16%<br/>ip-10–3–53–136.us-east-2.compute.internal 75m 1% 946Mi 16%<br/>ip-10–3–53–143.us-east-2.compute.internal 1262m 32% 1110Mi 19%<br/>ip-10–3–54–32.us-east-2.compute.internal 117m 2% 985Mi 17%<br/>ip-10–3–55–140.us-east-2.compute.internal 992m 25% 931Mi 16%<br/>ip-10–3–55–208.us-east-2.compute.internal 76m 1% 942Mi 16%<br/>ip-10–3–56–98.us-east-2.compute.internal 1578m 40% 1152Mi 20%<br/>ip-10–3–59–239.us-east-2.compute.internal 1661m 42% 1175Mi 20%</span><span id="5758" class="my lv iq kv b gy nu np l nq nr">$ kk -n eks-dev-1-eat-backend-ns top pod<br/>NAME CPU(cores) MEMORY(bytes)<br/>eat-backend-b8b79574–5d6zl 0m 0Mi<br/>eat-backend-b8b79574–7n7pq 986m 184Mi<br/>eat-backend-b8b79574–8t2pw 709m 135Mi<br/>eat — backend-b8b79574-bq8nw 0m 0Mi<br/>eat-backend-b8b79574-ds68n 0m 0Mi<br/>eat-backend-b8b79574-f4qcm 0m 0Mi<br/>eat-backend-b8b79574-f6wfj 0m 0Mi<br/>eat-backend-b8b79574-g7jm7 842m 165Mi<br/>eat-backend-b8b79574-ggrdg 0m 0Mi<br/>eat-backend-b8b79574-hjcnh 0m 0Mi<br/>eat-backend-b8b79574-l68vq 0m 0Mi<br/>eat-backend-b8b79574-mlpqs 0m 0Mi<br/>eat-backend-b8b79574-nkwjc 2882m 103Mi<br/>eat-backend-b8b79574-swbvq 2091m 180Mi<br/>eat-backend-memcached-0 31m 54Mi</span></pre><p id="2f9a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">豆荚无限重启:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="0c26" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get pod<br/>NAME READY STATUS RESTARTS AGE<br/>eat-backend-b8b79574–5d6zl 0/1 CrashLoopBackOff 6 17m<br/>eat-backend-b8b79574–7n7pq 1/1 Running 5 9m13s<br/>eat-backend-b8b79574–8kjl4 0/1 CrashLoopBackOff 7 64m<br/>eat-backend-b8b79574–8t2pw 0/1 CrashLoopBackOff 6 64m<br/>eat-backend-b8b79574-bq8nw 1/1 Running 6 64m<br/>eat-backend-b8b79574-ds68n 0/1 CrashLoopBackOff 7 17m<br/>eat-backend-b8b79574-f4qcm 1/1 Running 6 9m13s<br/>eat-backend-b8b79574-f6wfj 0/1 Running 6 9m13s<br/>eat-backend-b8b79574-g7jm7 0/1 CrashLoopBackOff 5 25m<br/>eat-backend-b8b79574-ggrdg 1/1 Running 6 9m13s<br/>eat-backend-b8b79574-hjcnh 0/1 CrashLoopBackOff 6 25m<br/>eat-backend-b8b79574-l68vq 1/1 Running 7 29m<br/>eat-backend-b8b79574-mlpqs 0/1 CrashLoopBackOff 6 21m<br/>eat-backend-b8b79574-nkwjc 0/1 CrashLoopBackOff 5 9m13s<br/>eat-backend-b8b79574-swbvq 0/1 CrashLoopBackOff 6 64m<br/>eat-backend-memcached-0 1/1 Running 0 64m</span></pre><p id="231d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在12.000–13.000个用户之后，我们只有一个pod还活着:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="0c4f" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns top pod<br/>NAME CPU(cores) MEMORY(bytes)<br/>eat-backend-b8b79574–7n7pq 0m 0Mi<br/>eat-backend-b8b79574–8kjl4 0m 0Mi<br/>eat-backend-b8b79574–8t2pw 0m 0Mi<br/>eat — backend-b8b79574-bq8nw 0m 0Mi<br/>eat-backend-b8b79574-ds68n 0m 0Mi<br/>eat-backend-b8b79574-f4qcm 0m 0Mi<br/>eat-backend-b8b79574-f6wfj 0m 0Mi<br/>eat-backend-b8b79574-g7jm7 0m 0Mi<br/>eat-backend-b8b79574-ggrdg 0m 0Mi<br/>eat-backend-b8b79574-hjcnh 0m 0Mi<br/>eat-backend-b8b79574-l68vq 0m 0Mi<br/>eat-backend-b8b79574-mlpqs 0m 0Mi<br/>eat-backend-b8b79574-nkwjc 3269m 129Mi<br/>eat-backend-b8b79574-swbvq 0m 0Mi<br/>eat-backend-memcached-0 23m 61Mi</span><span id="69f2" class="my lv iq kv b gy nu np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get pod<br/>NAME READY STATUS RESTARTS AGE<br/>eat-backend-b8b79574–5d6zl 1/1 Running 7 20m<br/>eat-backend-b8b79574–7n7pq 0/1 CrashLoopBackOff 6 12m<br/>eat-backend-b8b79574–8kjl4 0/1 CrashLoopBackOff 7 67m<br/>eat-backend-b8b79574–8t2pw 0/1 CrashLoopBackOff 7 67m<br/>eat-backend-b8b79574-bq8nw 0/1 CrashLoopBackOff 6 67m<br/>eat-backend-b8b79574-ds68n 0/1 CrashLoopBackOff 8 20m<br/>eat-backend-b8b79574-f4qcm 0/1 CrashLoopBackOff 6 12m<br/>eat-backend-b8b79574-f6wfj 0/1 CrashLoopBackOff 6 12m<br/>eat-backend-b8b79574-g7jm7 0/1 CrashLoopBackOff 6 28m<br/>eat-backend-b8b79574-ggrdg 0/1 Running 7 12m<br/>eat-backend-b8b79574-hjcnh 0/1 CrashLoopBackOff 7 28m<br/>eat-backend-b8b79574-l68vq 0/1 CrashLoopBackOff 7 32m<br/>eat-backend-b8b79574-mlpqs 0/1 CrashLoopBackOff 7 24m<br/>eat-backend-b8b79574-nkwjc 1/1 Running 7 12m<br/>eat-backend-b8b79574-swbvq 0/1 CrashLoopBackOff 7 67m<br/>eat-backend-memcached-0 1/1 Running 0 67m</span></pre><p id="28cf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">直到这时，我才回忆起容器中的日志文件，并检查了它们——发现我们的数据库服务器开始拒绝连接:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="19f1" class="my lv iq kv b gy no np l nq nr">bash-4.4# cat ./new-eat-backend/storage/logs/laravel-2020–08–20.log<br/>[2020–08–20 16:53:25] production.ERROR: SQLSTATE[HY000] [2002] Connection refused {“exception”:”[object] (Doctrine\\DBAL\\Driver\\PDOException(code: 2002): SQLSTATE[HY000] [2002] Connection refused at /var/www/new-eat-backend/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/PDOConnection.php:31, PDOException(code: 2002): SQLSTATE[HY000] [2002] Connection refused at /var/www/new-eat-backend/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/PDOConnection.php:27)</span></pre><h2 id="2993" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">AWS RDS —“连接被拒绝”</h2><p id="1c0d" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">对于数据库，我们使用RDS Aurora MySQL和它自己的Slaves自动伸缩。</p><p id="9877" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里的问题是，首先，测试是在具有小型数据库实例的开发环境中执行的— <em class="mx"> db.t2.medium </em>具有4 GB RAM，其次，来自应用程序的所有请求都被发送到主数据库实例，而Aurora的从数据库实例根本没有被使用。主服务器每秒处理大约155个请求。</p><p id="2999" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">实际上，Aurora RDS的主要好处之一就是主/从划分——所有修改数据的请求(<code class="fe ks kt ku kv b">UPDATE</code>、<code class="fe ks kt ku kv b">CREATE</code>等)都必须发送给主设备，而所有的<code class="fe ks kt ku kv b">SELECT</code>都必须发送给从设备。</p><p id="b689" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在此期间，从属服务器可以通过自己的自动扩展策略进行扩展:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c1a3b280059e833a8e757d370e71d03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/0*Ps3tBrxpfqVvgcEb.png"/></div></figure><p id="1b84" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">顺便说一下，我们在这里的做法是错误的——我们最好通过连接数而不是CPU来扩展从服务器。以后会改的。</p><h2 id="fb89" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">AWS RDS最大连接数</h2><p id="b5c8" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">实际上，根据<a class="ae kw" href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html" rel="noopener ugc nofollow" target="_blank">文档</a>——T3 . medium的连接限制必须是同时90个连接，而我们在50–60个连接后被拒绝:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oc"><img src="../Images/874ff13ca5b4c5e559390592999b83d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*u8vQrxQPbSFHnbtr.png"/></div></div></figure><p id="79ef" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我当时与AWS架构进行了交谈，并询问他们关于“文档中的90个连接”的问题，但他们无法帮助我们回答类似“<em class="mx">可能是</em> <strong class="jw ir"> <em class="mx">到</em> </strong> <em class="mx"> 90 </em>？”</p><p id="a43f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">总的来说，在测试之后，我们得到了这样一张图片:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi od"><img src="../Images/2325b1d973fea9bb8bf928448ece505c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tCFDSH0ojKKBbRz6.png"/></div></div></figure><p id="c141" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">52%失败了，这显然非常糟糕:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oe"><img src="../Images/190759add617068fac9ffd20e9ada881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5_uiCytK2FneLcbt.png"/></div></div></figure><p id="21fc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">但对我来说，这里最重要的是集群本身、它的控制平面和网络都像预期的那样工作。</p><p id="85bf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">数据库问题将在第三天得到解决——将升级实例类型并将应用程序配置为开始与Aurora Slaves一起工作。</p><h2 id="1a76" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">第三天</h2><p id="cbde" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">嗯——最有趣的一天:-)</p><p id="439c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，开发人员修复了Aurora Slaves，因此应用程序现在将使用它们。</p><p id="8865" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">顺便说一下，我昨天和AWS团队谈过，他们告诉我关于RDS代理服务的事情——需要检查一下，看起来很有希望。</p><p id="4be5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">此外，需要检查OpCache设置，因为它可以减少CPU的使用，参见<a class="ae kw" href="https://rtfm.co.ua/php-keshirovanie-php-skriptov-nastrojka-i-tyuning-opcache/" rel="noopener ugc nofollow" target="_blank">PHP:кешированиеPHP-скииптов—настиоккаитюнингop cache</a>(俄语)。</p><p id="682a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">当开发人员进行更改时——让我们看看我们的Kubernetes活跃度和就绪性探针。</p><h2 id="24ac" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">Kubernetes活跃度和就绪性探测</h2><p id="782d" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">发现了几个有趣的帖子——<a class="ae kw" href="https://blog.colinbreck.com/kubernetes-liveness-and-readiness-probes-how-to-avoid-shooting-yourself-in-the-foot/" rel="noopener ugc nofollow" target="_blank">Kubernetes活跃度和准备度调查:如何避免搬起石头砸自己的脚</a> и <a class="ae kw" href="https://dev.to/pgoodjohn/liveness-and-readiness-probes-with-laravel-5d65" rel="noopener ugc nofollow" target="_blank">拉勒维尔</a>的活跃度和准备度调查。</p><p id="7ddd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们的开发人员已经添加了两个新的端点:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="3677" class="my lv iq kv b gy no np l nq nr">...<br/>$router-&gt;get('healthz', 'HealthController@phpCheck');<br/>$router-&gt;get('readiness', 'HealthController@dbReadCheck');<br/>...</span></pre><p id="3e7f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">而<code class="fe ks kt ku kv b">HealthController</code>是下一个:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="d271" class="my lv iq kv b gy no np l nq nr">&lt;?php<br/><br/>namespace App\Http\Controllers;<br/><br/>class HealthController extends Controller<br/>{<br/>    public function phpCheck()<br/>    {<br/>        return response('ok');<br/>    }<br/><br/>    public function dbReadCheck()<br/>    {<br/>        try {<br/>            $rows = \DB::select('SELECT 1 AS ok');<br/>            if ($rows &amp;&amp; $rows[0]-&gt;ok == 1) {<br/>                return response('ok');<br/>            }<br/>        } catch (\Throwable $err) {<br/>            // ignore<br/>        }<br/>        return response('err', 500);<br/>    }<br/>}</span></pre><p id="4544" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">到了<code class="fe ks kt ku kv b">/healthz</code> URI，我们将检查pod本身是否已经启动，PHP是否正在工作。</p><p id="e92c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通过<code class="fe ks kt ku kv b">/readiness</code> -将检查应用程序是否已启动并准备好接受连接:</p><ul class=""><li id="b21c" class="lg lh iq jw b jx jy kb kc kf li kj lj kn lk kr ll lm ln lo bi translated"><code class="fe ks kt ku kv b">livenessProbe</code>:如果失败——Kubernetes将重启pod</li><li id="212e" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><code class="fe ks kt ku kv b">initialDelaySeconds</code> : <em class="mx">应该比容器</em>的最大初始化时间长——需要多少时间？让我们把它设置为5秒钟</li><li id="ed5a" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><code class="fe ks kt ku kv b">failureThreshold</code>:三次尝试，如果都失败pod将重新启动</li><li id="31f2" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated">我记得默认值是15秒——随它去吧</li><li id="ff75" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><code class="fe ks kt ku kv b">readinessProbe</code>:定义应用程序何时准备好服务请求。如果该检查失败，Kubernetes将关闭该pod的负载平衡/服务</li><li id="e999" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><code class="fe ks kt ku kv b">initialDelaySeconds</code>:让我们用5秒钟的时间启动PHP并连接到数据库</li><li id="fa03" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><code class="fe ks kt ku kv b">periodSeconds</code>:由于我们预计数据库连接会出现问题，所以我们将其设置为5秒</li><li id="ba99" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><code class="fe ks kt ku kv b">failureThreshold</code>:也是三个，至于<code class="fe ks kt ku kv b">livenessProbe</code></li><li id="c47f" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><code class="fe ks kt ku kv b">successThreshold</code>:在多少次成功尝试后，认为pod已准备好接受流量——让我们将其设置为1</li><li id="2b58" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><code class="fe ks kt ku kv b">timeoutSeconds</code>:默认为1，我们使用它</li></ul><p id="c551" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">参见<a class="ae kw" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes" rel="noopener ugc nofollow" target="_blank">配置探头</a>。</p><p id="e378" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">更新部署中的探测器:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="b9a3" class="my lv iq kv b gy no np l nq nr">...<br/>        livenessProbe:<br/>          httpGet:<br/>            path: {{ .Values.appConfig.healthcheckPath }}<br/>            port: {{ .Values.appConfig.port }}<br/>          initialDelaySeconds: 5<br/>          failureThreshold: 3<br/>          periodSeconds: 15<br/>        readinessProbe:<br/>          httpGet: <br/>            path: {{ .Values.appConfig.readycheckPath }}<br/>            port: {{ .Values.appConfig.port }}<br/>          initialDelaySeconds: 5<br/>          periodSeconds: 5<br/>          failureThreshold: 3<br/>          successThreshold: 1<br/>          timeoutSeconds: 1<br/>...</span></pre><p id="4344" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">后来将它移到了<code class="fe ks kt ku kv b">values.yaml</code>。</p><p id="4ea3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">并为从属数据库服务器添加一个新变量:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="d1cd" class="my lv iq kv b gy no np l nq nr">...<br/>        - name: DB_WRITE_HOST<br/>          value: {{ .Values.appConfig.db.writeHost }}              <br/>        - name: DB_READ_HOST<br/>          value: {{ .Values.appConfig.db.readHost }}<br/>...</span></pre><h2 id="1c29" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">Kubernetes:来自Docker的PHP日志</h2><p id="5a5c" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">啊，还有圆木！。</p><p id="eab0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">开发者允许日志被发送到<code class="fe ks kt ku kv b">/dev/stderr</code>而不是写入文件，Docker守护进程必须获取它们并发送到Kubernetes——但是在<code class="fe ks kt ku kv b">kubectl logs</code>中，我们只能看到来自NGINX的消息。</p><p id="ef16" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">去检查一下<a class="ae kw" href="https://rtfm.co.ua/en/linux-php-fpm-docker-stdout-and-stderr-no-an-applications-error-logs/" rel="noopener ugc nofollow" target="_blank"> Linux: PHP-FPM、Docker、STDOUT和STDERR——没有一个应用程序的错误日志</a>，回忆一下它是如何工作的，然后去检查描述符。</p><p id="b42a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在pod中找到一个主PHP进程PID:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="231f" class="my lv iq kv b gy no np l nq nr">bash-4.4# ps aux |grep php-fpm | grep master<br/>root 9 0.0 0.2 171220 20784 ? S 12:00 0:00 php-fpm: master process (/etc/php/7.1/php-fpm.conf)</span></pre><p id="356b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">并检查其描述符:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="24d8" class="my lv iq kv b gy no np l nq nr">bash-4.4# ls -l /proc/9/fd/2<br/>l-wx — — — 1 root root 64 Aug 21 12:04 /proc/9/fd/2 -&gt; /var/log/php/7.1/php-fpm.log</span><span id="cb62" class="my lv iq kv b gy nu np l nq nr">bash-4.4# ls -l /proc/9/fd/1<br/>lrwx — — — 1 root root 64 Aug 21 12:04 /proc/9/fd/1 -&gt; /dev/null</span></pre><p id="e1cc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="mx"> fd/2 </em>，它是进程的<code class="fe ks kt ku kv b">stderr</code>，它被映射到<code class="fe ks kt ku kv b">/var/log/php/7.1/php-fpm.log</code>而不是<code class="fe ks kt ku kv b">/dev/stderr</code>——这就是为什么我们在<code class="fe ks kt ku kv b">kubectl logs</code>中看不到任何东西。</p><p id="85b6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在<code class="fe ks kt ku kv b">/etc/php/7.1</code>目录中递归地grep "<code class="fe ks kt ku kv b">/var/log/php/7.1/php-fpm.log</code>"字符串，并找到默认有<code class="fe ks kt ku kv b">error_log = /var/log/php/7.1/php-fpm.log</code>的<code class="fe ks kt ku kv b">php-fpm.conf</code>。将它固定在<code class="fe ks kt ku kv b">/dev/stderr</code>上——这就完成了</p><p id="ebef" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">再次运行测试！</p><p id="0687" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从1个用户到15000个用户，持续30分钟。</p><h2 id="7c55" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">第一次测试</h2><p id="50dc" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">3300名用户—都很好:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/9af1ab058aab6c5ff63ea2d633e5e32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/0*EWV8l3PFe3sZmwto.png"/></div></figure><p id="7c01" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">豆荚:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="205a" class="my lv iq kv b gy no np l nq nr">kk -n eks-dev-1-eat-backend-ns top pod<br/>NAME CPU(cores) MEMORY(bytes)<br/>eat-backend-867b59c4dc-742vf 856m 325Mi<br/>eat-backend-867b59c4dc-bj74b 623m 316Mi<br/>eat-backend-867b59c4dc-cq5gd 891m 319Mi<br/>eat-backend-867b59c4dc-mm2ll 600m 310Mi<br/>eat-ackend-867b59c4dc-x8b8d 679m 313Mi<br/>eat-backend-memcached-0 19m 68Mi</span></pre><p id="088f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">HPA:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="e121" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>eat-backend-hpa Deployment/eat-backend 30%/30% 4 40 5 20h</span></pre><p id="ab7f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在7.000用户上我们得到了新的错误——“<strong class="jw ir"><em class="mx">PHP _ network _ get addresses:get addrinfo failed</em></strong>”——我的老“朋友”，在AWS上面对了几次:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="b7f2" class="my lv iq kv b gy no np l nq nr">[2020–08–21 14:14:59] local.ERROR: SQLSTATE[HY000] [2002] php_network_getaddresses: getaddrinfo failed: Try again (SQL: insert into `order_logs` (`order_id`, `action`, `data`, `updated_at`, `created_at`) values (175951, nav, “Result page: ok”, 2020–08–21 14:14:54, 2020–08–21 14:14:54)) {“exception”:”[object] (Illuminate\\Database\\QueryException(code: 2002): SQLSTATE[HY000] [2002] php_network_getaddresses: getaddrinfo failed: Try again (SQL: insert into `order_logs` (`order_id`, `action`, `data`, `updated_at`, `created_at`) values (175951, nav, \”Result page: ok\”, 2020–08–21 14:14:54, 2020–08–21 14:14:54))</span></pre><p id="7fef" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">简而言之，AWS中的“<strong class="jw ir">PHP _ network _ get addresses:get addrinfo failed</strong>”错误有三个原因(至少我知道是这样):</p><ul class=""><li id="1e4b" class="lg lh iq jw b jx jy kb kc kf li kj lj kn lk kr ll lm ln lo bi translated">AWS EC2的网络接口上每秒数据包太多，参见<a class="ae kw" href="https://www.bluematador.com/blog/ec2-packets-per-second-guaranteed-throughput-vs-best-effort" rel="noopener ugc nofollow" target="_blank"> EC2每秒数据包:保证吞吐量与最佳效果</a></li><li id="fa24" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated">网络吞吐量耗尽—参见<a class="ae kw" href="https://cloudonaut.io/ec2-network-performance-cheat-sheet/" rel="noopener ugc nofollow" target="_blank"> EC2网络性能备忘单</a></li><li id="6d7a" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated">许多DNS查询被发送到AWS VPC DNS-它的限制是1024/秒，见<a class="ae kw" href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-limits" rel="noopener ugc nofollow" target="_blank"> DNS配额</a></li></ul><p id="033d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们将在本帖的稍后部分讨论当前案例的原因。</p><p id="8cbf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在9.000+pod上开始重新启动:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="dc38" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get pod<br/>NAME READY STATUS RESTARTS AGE<br/>eat-backend-867b59c4dc-2m7fd 0/1 Running 2 4m17s<br/>eat-backend-867b59c4dc-742vf 0/1 CrashLoopBackOff 5 68m<br/>eat-backend-867b59c4dc-bj74b 1/1 Running 5 68m<br/>…<br/>eat-backend-867b59c4dc-w24pz 0/1 CrashLoopBackOff 5 19m<br/>eat-backend-867b59c4dc-x8b8d 0/1 CrashLoopBackOff 5 68m<br/>eat-backend-memcached-0 1/1 Running 0 21h</span></pre><p id="00e8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因为他们弯腰回复活跃度和准备度检查:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="d2c7" class="my lv iq kv b gy no np l nq nr">0s Warning Unhealthy Pod Readiness probe failed: Get <a class="ae kw" href="http://10.3.62.195:80/readiness:" rel="noopener ugc nofollow" target="_blank">http://10.3.62.195:80/readiness:</a> net/http: request canceled (Client.Timeout exceeded while awaiting headers)<br/>0s Warning Unhealthy Pod Liveness probe failed: Get <a class="ae kw" href="http://10.3.56.206:80/healthz:" rel="noopener ugc nofollow" target="_blank">http://10.3.56.206:80/healthz:</a> net/http: request canceled (Client.Timeout exceeded while awaiting headers)</span></pre><p id="6329" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在10.000之后，我们的数据库服务器开始拒绝连接:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="a244" class="my lv iq kv b gy no np l nq nr">[2020–08–21 13:05:11] production.ERROR: SQLSTATE[HY000] [2002] Connection refused {“exception”:”[object] (Doctrine\\DBAL\\Driver\\PDOException(code: 2002): SQLSTATE[HY000] [2002] Connection refused at /var/www/new-eat-backend/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/PDOConnection.php:31, PDOException(code: 2002): SQLSTATE[HY000] [2002] Connection refused</span></pre><h2 id="7536" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">PHP _ network _ get addresses:get addrinfo失败и DNS</h2><p id="6c3a" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">那么，这次我们发现了哪些问题:</p><ul class=""><li id="eeed" class="lg lh iq jw b jx jy kb kc kf li kj lj kn lk kr ll lm ln lo bi translated"><strong class="jw ir">错误:SQLSTATE[HY000] [2002]连接被拒绝</strong></li><li id="cc3c" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><strong class="jw ir">PHP _ network _ get addresses:get addrinfo失败</strong></li></ul><p id="5708" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">“<strong class="jw ir"> <em class="mx">错误:SQLSTATE[HY000] [2002]连接被拒绝</em> </strong>”是一个已知问题，我们知道如何处理它——我将把RDS实例从<em class="mx"> t3.medium </em>更新为<em class="mx"> r5.large </em>，但是DNS问题呢？</p><p id="549d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因为从上面提到的原因——网络接口上每秒的数据包数、网络链路吞吐量和AWS VPC DNS——来看，最可行的似乎是DNS服务:每次当我们的应用程序想要连接到数据库服务器时，它都会进行DNS查询以确定DB-server的IP，加上所有其他DNS记录，它们合起来可以满足每秒1024个请求的限制。</p><p id="33ca" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">顺便看看<a class="ae kw" href="https://rtfm.co.ua/en/grafana-loki-the-logqls-prometheus-like-counters-aggregation-functions-and-dnsmasqs-requests-graphs/" rel="noopener ugc nofollow" target="_blank">Grafana:Loki——LogQL的类似普罗米修斯的计数器、聚合函数和dnsmasq的请求图</a>帖子。</p><p id="210c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在让我们检查一下我们的pod的DNS设置:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="bd1b" class="my lv iq kv b gy no np l nq nr">bash-4.4# cat /etc/resolv.conf<br/>nameserver 172.20.0.10<br/>search eks-dev-1-eat-backend-ns.svc.cluster.local svc.cluster.local cluster.local us-east-2.compute.internal<br/>options ndots:5</span></pre><p id="2d1c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="mx">名称服务器172.20.0.10 </em> —必须是我们的<code class="fe ks kt ku kv b">kube-dns</code>:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="bcbc" class="my lv iq kv b gy no np l nq nr">bash-4.4# nslookup 172.20.0.10<br/>10.0.20.172.in-addr.arpa name = kube-dns.kube-system.svc.cluster.local.</span></pre><p id="a910" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">是的，它是。</p><p id="7377" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">顺便说一下，它在slogs中告诉我们它不能连接到API服务器:</p><blockquote class="og oh oi"><p id="110a" class="ju jv mx jw b jx jy jz ka kb kc kd ke oj kg kh ki ok kk kl km ol ko kp kq kr ij bi translated">e 0805 21:32:40.283128 1 reflector . go:283]pkg/mod/k8s . io/client-go @ v 0 . 0 . 0–2019 0620085101–78d 2 af 792 Bab/tools/cache/reflector . go:98:无法观看*v1。命名空间:Get<a class="ae kw" href="https://172.20.0.1:443/api/v1/namespaces?resourceVersion=23502628&amp;timeout=9m40s&amp;timeoutSeconds=580&amp;watch=true:" rel="noopener ugc nofollow" target="_blank">https://172 . 20 . 0 . 1:443/API/v1/namespaces？resource version = 23502628&amp;time out = 9m 40s&amp;time out seconds = 580&amp;watch = true:</a>拨tcp 172.20.0.1:443: connect:连接被拒绝</p></blockquote><p id="a4ff" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">那么，我们能做些什么来防止过度使用AWS VPC域名系统呢？</p><ul class=""><li id="5fae" class="lg lh iq jw b jx jy kb kc kf li kj lj kn lk kr ll lm ln lo bi translated">旋起<code class="fe ks kt ku kv b">dnsmasq</code>？对于Kubernetes来说，这似乎有点奇怪，首先——因为Kubernetes已经有了自己的DNS，其次——我肯定我们不是第一个面临这个问题的人，我怀疑他们通过运行一个带有<code class="fe ks kt ku kv b">dnsmasq</code>的额外容器解决了这个问题(仍然检查<a class="ae kw" href="https://rtfm.co.ua/en/dnsmasq-aws-temporary-failure-in-name-resolution-logs-debug-and-dnsmasq-cache-size/" rel="noopener ugc nofollow" target="_blank">dnsmasq:AWS-“名称解析中的临时故障”，日志、调试和dnsmasq缓存大小</a>)</li><li id="5efb" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated">另一个解决方案可以使用来自cloud flare(<em class="mx">1.1.1.1</em>)或Google(<em class="mx">8.8.8.8)</em>)的DNS，这样我们将完全停止使用VPC的DNS，但会增加DNS响应时间</li></ul><p id="8b83" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">Kubernetes</strong>T2】</p><p id="e47c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">好了，让我们看看DNS在Kubernetes中一般是如何配置的:</p><blockquote class="og oh oi"><p id="124f" class="ju jv mx jw b jx jy jz ka kb kc kd ke oj kg kh ki ok kk kl km ol ko kp kq kr ij bi translated"><strong class="jw ir">注意:</strong>您可以使用pod规范中的<strong class="jw ir"> dnsPolicy </strong>字段管理您的pod的DNS配置。如果未填充该字段，则默认使用<strong class="jw ir"> ClusterFirst </strong>和<a class="ae kw" href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy" rel="noopener ugc nofollow" target="_blank"> DNS策略</a>。</p></blockquote><p id="4fff" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因此，默认情况下，pod设置了<code class="fe ks kt ku kv b">ClusterFirst</code>,它:</p><blockquote class="og oh oi"><p id="9210" class="ju jv mx jw b jx jy jz ka kb kc kd ke oj kg kh ki ok kk kl km ol ko kp kq kr ij bi translated">任何与配置的群集域后缀不匹配的DNS查询，如“<code class="fe ks kt ku kv b">www.kubernetes.io</code>”，都会被转发到从该节点继承的上游名称服务器。</p></blockquote><p id="597b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">显然，默认情况下，AWS EC2将完全使用AWS VPC DNS。</p><p id="ca4a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">另请参见— <a class="ae kw" href="https://aws.amazon.com/premiumsupport/knowledge-center/eks-dns-failure/" rel="noopener ugc nofollow" target="_blank">如何使用亚马逊EKS解决DNS故障？</a></p><p id="45c4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">可以使用ClusterAutoScaler设置配置节点DNS:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="c7ba" class="my lv iq kv b gy no np l nq nr">$ kk -n kube-system get pod cluster-autoscaler-5dddc9c9b-fstft -o yaml<br/>…<br/>spec:<br/>containers:<br/>- command:<br/>- ./cluster-autoscaler<br/>- — v=4<br/>- — stderrthreshold=info<br/>- — cloud-provider=aws<br/>- — skip-nodes-with-local-storage=false<br/>- — expander=least-waste<br/>- — node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/bttrm-eks-dev-1<br/>- — balance-similar-node-groups<br/>- — skip-nodes-with-system-pods=false<br/>…</span></pre><p id="5459" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">但是在我们的例子中，这里什么都没有改变，一切都保留了默认设置。</p><p id="33ed" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在Kubernetes中运行节点本地DNS</p><p id="82fa" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">但是关于<code class="fe ks kt ku kv b">dnsmasq</code>的想法是正确的，但是对于Kubernetes，有一个<a class="ae kw" href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/" rel="noopener ugc nofollow" target="_blank"> NodeLocal DNS </a>解决方案，它与<code class="fe ks kt ku kv b">dnsmasq</code>是完全相同的缓存服务，但是它将使用<code class="fe ks kt ku kv b">kube-dns</code>来获取记录，然后<code class="fe ks kt ku kv b">kube-dns</code>将转到VPC DNS。</p><p id="f8b4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们需要什么来运行它:</p><ul class=""><li id="372b" class="lg lh iq jw b jx jy kb kc kf li kj lj kn lk kr ll lm ln lo bi translated"><em class="mx"> kubedns </em>:将由<code class="fe ks kt ku kv b">kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}</code>命令控制</li><li id="a678" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><em class="mx">域</em>:是我们的<code class="fe ks kt ku kv b">&lt;cluster-domain&gt;</code>，<em class="mx"> cluster.local </em></li><li id="db25" class="lg lh iq jw b jx lp kb lq kf lr kj ls kn lt kr ll lm ln lo bi translated"><em class="mx">本地dns </em> : <code class="fe ks kt ku kv b">&lt;node-local-address&gt;</code>，地址，本地dns缓存将被访问，让我们使用<em class="mx"> 169.254.20.10 </em></li></ul><p id="1f75" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">获取<code class="fe ks kt ku kv b">kube-dns</code>的服务IP:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="8c3b" class="my lv iq kv b gy no np l nq nr">$ kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}<br/>172.20.0.10</span></pre><p id="1274" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">另请参见<a class="ae kw" href="https://www.vladionescu.me/posts/eks-dns.html" rel="noopener ugc nofollow" target="_blank">修复EKS DNS </a>。</p><p id="f35e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">下载<code class="fe ks kt ku kv b"><a class="ae kw" href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml" rel="noopener ugc nofollow" target="_blank">nodelocaldns.yaml</a></code>文件:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="5f59" class="my lv iq kv b gy no np l nq nr">$ wget <a class="ae kw" href="https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml</a></span></pre><p id="4903" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">用<code class="fe ks kt ku kv b">sed</code>更新它，并设置我们上面确定的数据:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="b2d2" class="my lv iq kv b gy no np l nq nr">$ sed -i “s/__PILLAR__LOCAL__DNS__/169.254.20.10/g; s/__PILLAR__DNS__DOMAIN__/cluster.local/g; s/__PILLAR__DNS__SERVER__/172.20.0.10/g” nodelocaldns.yaml</span></pre><p id="0d58" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">检查清单的内容——它将做什么——这里将创建一个Kubernetes <code class="fe ks kt ku kv b">DaemonSet</code>,它将使用每个Kubernetes WorkerNode上的NodeLocal DNS启动pods:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="10d5" class="my lv iq kv b gy no np l nq nr">...<br/>---<br/>apiVersion: apps/v1<br/>kind: DaemonSet<br/>metadata:<br/>  name: node-local-dns<br/>...</span></pre><p id="1df6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">及其<code class="fe ks kt ku kv b">ConfigMap</code>:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="95b6" class="my lv iq kv b gy no np l nq nr">...<br/>---<br/>apiVersion: v1<br/>kind: ConfigMap<br/>metadata:<br/>  name: node-local-dns<br/>  namespace: kube-system<br/>  labels:<br/>    addonmanager.kubernetes.io/mode: Reconcile<br/>data:<br/>  Corefile: |<br/>    cluster.local:53 {<br/>        errors<br/>        cache {<br/>                success 9984 30<br/>                denial 9984 5<br/>        }<br/>        reload<br/>        loop<br/>        bind 169.254.20.10 172.20.0.10<br/>        forward . __PILLAR__CLUSTER__DNS__ {<br/>                force_tcp<br/>        }<br/>        prometheus :9253<br/>        health 169.254.20.10:8080<br/>        }<br/><br/>...</span></pre><p id="0709" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">部署它:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="3d68" class="my lv iq kv b gy no np l nq nr">$ kubectl apply -f nodelocaldns.yaml<br/>serviceaccount/node-local-dns created<br/>service/kube-dns-upstream created<br/>configmap/node-local-dns created<br/>daemonset.apps/node-local-dns created</span></pre><p id="826c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">检查舱:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="00fd" class="my lv iq kv b gy no np l nq nr">$ kk -n kube-system get pod | grep local-dns<br/>node-local-dns-7cndv 1/1 Running 0 33s<br/>node-local-dns-7hrlc 1/1 Running 0 33s<br/>node-local-dns-c5bhm 1/1 Running 0 33s</span></pre><p id="53f2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其服务:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="a15f" class="my lv iq kv b gy no np l nq nr">$ kk -n kube-system get svc | grep dns<br/>kube-dns ClusterIP 172.20.0.10 &lt;none&gt; 53/UDP,53/TCP 88d<br/>kube-dns-upstream ClusterIP 172.20.245.211 &lt;none&gt; 53/UDP,53/TCP 107s</span></pre><p id="4f94" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="mx">kube-DNS-upstream cluster IP 172 . 20 . 245 . 21</em>，但是从我们的pod内部，它必须可以被我们在<code class="fe ks kt ku kv b">localdns</code>中设置的<em class="mx"> 169.254.20.10 </em> IP访问。</p><p id="c406" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这有用吗？从pod进行检查:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="2747" class="my lv iq kv b gy no np l nq nr">bash-4.4# dig @169.254.20.10 ya.ru +short<br/>87.250.250.242</span></pre><p id="8db3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">是的，有效，很好。</p><p id="a3ba" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">下一件事是重新配置我们的pod，这样它们将使用<em class="mx"> 169.254.20.10 </em>而不是<code class="fe ks kt ku kv b">kube-dns</code>服务。</p><p id="e708" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在<code class="fe ks kt ku kv b">eksctl</code>配置文件中，这可以通过<code class="fe ks kt ku kv b">clusterDNS</code>完成:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="9315" class="my lv iq kv b gy no np l nq nr">...<br/>nodeGroups:<br/>- name: mygroup<br/>    clusterDNS: 169.254.20.10<br/>...</span></pre><p id="9fff" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">但是接下来您需要更新(实际上是重新创建)您现有的WorkerNode组。</p><p id="248e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">kubernetes Pod<code class="fe ks kt ku kv b">dnsConfig</code>&amp;&amp;<code class="fe ks kt ku kv b">nameservers</code></p><p id="e933" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">要应用更改而不创建WorkerNode组，我们可以通过添加<code class="fe ks kt ku kv b">dnsConfig</code>和<code class="fe ks kt ku kv b">nameservers</code>在<code class="fe ks kt ku kv b">Deployment</code>中指定必要的DNS设置:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="fbbf" class="my lv iq kv b gy no np l nq nr">...<br/>        resources:<br/>          requests:<br/>            cpu: 2500m<br/>            memory: 500m<br/>        terminationMessagePath: /dev/termination-log<br/>        terminationMessagePolicy: File<br/>      dnsConfig:<br/>        nameservers:<br/>        - 169.254.20.10<br/>      dnsPolicy: None<br/>      imagePullSecrets:<br/>      - name: gitlab-secret<br/>      nodeSelector:<br/>        beta.kubernetes.io/instance-type: c5.xlarge<br/>        role: eat-workers<br/>...</span></pre><p id="9749" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">部署，检查:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="1047" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns exec -ti eat-backend-f7b49b4b7–4jtk5 cat /etc/resolv.conf<br/>nameserver 169.254.20.10</span></pre><p id="6fc6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">好吧…</p><p id="7e7e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">有用吗？</p><p id="b15a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们用吊舱中的<code class="fe ks kt ku kv b">dig</code>检查一下:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="93f0" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns exec -ti eat-backend-f7b49b4b7–4jtk5 dig ya.ru +short<br/>87.250.250.242</span></pre><p id="bb87" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">是的，都很好。</p><p id="daa2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在我们可以进行第二项测试。</p><p id="9d2e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">第一次测试的结果如下:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi om"><img src="../Images/222d61b2cc844673485c6d31acee009f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CBm4M2lg2jXLwIQN.png"/></div></div></figure><p id="1adb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">当我们在8.000个用户后遇到错误时。</p><h2 id="b1c1" class="my lv iq bd lw mz na dn ma nb nc dp me kf nd ne mi kj nf ng mm kn nh ni mq nj bi translated">第二个测试</h2><p id="da47" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">8500 —目前一切正常:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi on"><img src="../Images/a3c0238d60c1076011a806b69abcb0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*58h2Mqt2kJvgWOx7.png"/></div></div></figure><p id="548a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在之前的测试中，我们在7.000之后开始出现错误，大约有150-200个错误，目前只有5个错误。</p><p id="a08f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">pod状态:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="0395" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get pod<br/>NAME READY STATUS RESTARTS AGE<br/>eat-backend-5d8984656–2ftd6 1/1 Running 0 17m<br/>eat-backend-5d8984656–45xvk 1/1 Running 0 9m11s<br/>eat-backend-5d8984656–6v6zr 1/1 Running 0 5m10s<br/>…<br/>eat-backend-5d8984656-th2h6 1/1 Running 0 37m<br/>eat-backend-memcached-0 1/1 Running 0 24h</span></pre><p id="7552" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi">НРА:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="35fa" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>eat-backend-hpa Deployment/eat-backend 32%/30% 4 40 13 24h</span></pre><p id="16bf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">10.000 —仍然有效:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi on"><img src="../Images/3b3af501c5ca2b558daa37c4ae77c01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*G4iu1MDr1OYvv7Ah.png"/></div></div></figure><p id="a7f5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi">НРА:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="6d2d" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>eat-backend-hpa Deployment/eat-backend 30%/30% 4 40 15 24h</span></pre><p id="9c27" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">豆荚:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="9cd4" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get pod<br/>NAME READY STATUS RESTARTS AGE<br/>eat-backend-5d8984656–2ftd6 1/1 Running 0 28m<br/>eat — backend-5d8984656–45xvk 1/1 Running 0 20m<br/>eat-backend-5d8984656–6v6zr 1/1 Running 0 16m<br/>…<br/>eat-backend-5d8984656-th2h6 1/1 Running 0 48m<br/>eat-backend-5d8984656-z2tpp 1/1 Running 0 3m51s<br/>eat-backend-memcached-0 1/1 Running 0 24h</span></pre><p id="f5d9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">连接到数据库服务器:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oo"><img src="../Images/7361f8e41949acf49344551846c7ffe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-qzeXKE61xo1Wztp.png"/></div></div></figure><p id="d6fc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">节点:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="8d04" class="my lv iq kv b gy no np l nq nr">$ kk top node -l role=eat-workers<br/>NAME CPU(cores) CPU% MEMORY(bytes) MEMORY%<br/>ip-10–3–39–145.us-east-2.compute.internal 743m 18% 1418Mi 24%<br/>ip-10–3–44–14.us-east-2.compute.internal 822m 20% 1327Mi 23%<br/>…<br/>ip-10–3–62–143.us-east-2.compute.internal 652m 16% 1259Mi 21%<br/>ip-10–3–63–96.us-east-2.compute.internal 664m 16% 1266Mi 22%<br/>ip-10–3–63–186.us-east-2.compute.internal &lt;unknown&gt; &lt;unknown&gt; &lt;unknown&gt; &lt;unknown&gt;<br/>ip-10–3–58–180.us-east-2.compute.internal &lt;unknown&gt; &lt;unknown&gt; &lt;unknown&gt; &lt;unknown&gt;<br/>…<br/>ip-10–3–51–254.us-east-2.compute.internal &lt;unknown&gt; &lt;unknown&gt; &lt;unknown&gt; &lt;unknown&gt;</span></pre><p id="8dcb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">自动缩放仍然有效，一切正常:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi op"><img src="../Images/aacfd838c4535166739fdb70db248606.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7jjEo1cvYqV4wI9E.png"/></div></div></figure><p id="465a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在17:45，有一个响应时间上升和几个错误——但随后一切正常。</p><p id="26c6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">没有pod重新启动:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="ab1a" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get pod<br/>NAME READY STATUS RESTARTS AGE<br/>eat-backend-5d8984656–2ftd6 1/1 Running 0 44m<br/>eat-backend-5d8984656–45xvk 1/1 Running 0 36m<br/>eat-backend-5d8984656–47vp9 1/1 Running 0 6m49s<br/>eat-backend-5d8984656–6v6zr 1/1 Running 0 32m<br/>eat-backend-5d8984656–78tq9 1/1 Running 0 2m45s<br/>…<br/>eat-backend-5d8984656-th2h6 1/1 Running 0 64m<br/>eat-backend-5d8984656-vbzhr 1/1 Running 0 6m49s<br/>eat-backend-5d8984656-xzv6n 1/1 Running 0 6m49s<br/>eat-backend-5d8984656-z2tpp 1/1 Running 0 20m<br/>eat-backend-5d8984656-zfrb7 1/1 Running 0 16m<br/>eat-backend-memcached-0 1/1 Running 0 24h</span></pre><p id="cc83" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">30个豆荚被放大:</p><pre class="ky kz la lb gt nk kv nl nm aw nn bi"><span id="26a0" class="my lv iq kv b gy no np l nq nr">$ kk -n eks-dev-1-eat-backend-ns get hpa<br/>NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE<br/>eat-backend-hpa Deployment/eat-backend 1%/30% 4 40 30 24h</span></pre><p id="88e3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">0%误差:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oa"><img src="../Images/22e11d1f5bf14a3be34a4ee2620ab369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yFGrud9hCCjadmCB.png"/></div></div></figure><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi op"><img src="../Images/bf0e3470d4a1b2851a4a2eb6bf721e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Peu3CkXt-c3BPHRv.png"/></div></div></figure><h1 id="0617" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">Apache JMeter и Grafana</h1><p id="69f9" class="pw-post-body-paragraph ju jv iq jw b jx ms jz ka kb mt kd ke kf mu kh ki kj mv kl km kn mw kp kq kr ij bi translated">最后，我第一次看到这样的解决方案，它看起来真的很好——QA团队让他们的JMeterз将测试结果发送到InfluxDB，然后Grafana用它来绘制图表:</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oa"><img src="../Images/681a6880acbab4c36c56a950c57ea538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bNhGjlwR7tZ8BVp0.png"/></div></div></figure><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oa"><img src="../Images/d934d2488c37549813b1928af7564323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lAw3VfXf2raFhyhN.png"/></div></div></figure></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><p id="8cf0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="mx">最初发布于</em> <a class="ae kw" href="https://rtfm.co.ua/en/kubernetes-load-testing-and-high-load-tuning-problems-and-solutions/" rel="noopener ugc nofollow" target="_blank"> <em class="mx"> RTFM: Linux、DevOps、系统管理</em> </a> <em class="mx">。</em></p></div></div>    
</body>
</html>