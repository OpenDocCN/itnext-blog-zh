<html>
<head>
<title>Enabling NVIDIA GPUs on K3s for CUDA workloads</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在K3s上为CUDA工作负载启用NVIDIA GPUs</h1>
<blockquote>原文：<a href="https://itnext.io/enabling-nvidia-gpus-on-k3s-for-cuda-workloads-a11b96f967b0?source=collection_archive---------0-----------------------#2021-08-22">https://itnext.io/enabling-nvidia-gpus-on-k3s-for-cuda-workloads-a11b96f967b0?source=collection_archive---------0-----------------------#2021-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="761d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你是一个机器学习或加密爱好者，并且在家里用几台连接GPU的机器构建了一个迷你数据中心，那么你可能会将宝贵的时间花在管理机器上的工作负载上。当您决定重新格式化您的一台机器时，您如何重新配置一切并部署您的工作负载，您如何将一个工作负载从一台机器转移到另一台机器，您如何确保您的设置是可重复的？难道您不想在自己的家庭实验室中部署Kubernetes集群，集中管理您的所有工作负载，而不必过多担心如何设置您从易贝购买的新的戴尔R510吗？</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/bb98edd5ac20d7cdee832edd8e06e119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_5nO4-vMtk2tUym1R9bMcg.jpeg"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">图片来源:programmerwiki.com</figcaption></figure><p id="6196" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">K3S允许任何人在几分钟内创建Kubernetes集群，从而使Kubernetes民主化。即使在单节点设置中，您也可以使用K3S轻松设置Kubernetes集群，并改善您管理工作负载的方式。稍后，您可以将相同的工作负载迁移到云或更大的本地Kubernetes集群，只需进行最少的更改。不用说，如果你想提高你的Kubernetes技能，这也是一个很好的学习机会。</p><p id="54a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自然，本文的目的不是告诉Kubernetes有多棒，而是如何针对需要GPU的工作负载增强它。我最近的挑战是在我的K3S集群上运行CUDA工作负载，以进一步提高我的机器学习技能。可以在K3S上运行使用NVIDIA GPUs处理CUDA工作负载的Kubernetes Pods，除了几个步骤之外，它不需要任何东西。但是，我注意到没有一套清晰的说明来显示这些步骤，需要哪些组件，如何启用它们，等等。因此，我已经决定写这篇文章，概述了所需的步骤一个接一个。</p><p id="712f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从鸟瞰的角度来看，所需的步骤首先是在K3s节点上安装NVIDIA驱动程序。请注意，在节点本身上，我们只需要设备驱动程序，而不一定需要CUDA本身。NVIDIA发布了支持CUDA的容器，你可以用它来构建你的应用程序容器。这也意味着在同一台机器上，您可以轻松地运行不同CUDA版本的不同容器。第二步是安装<a class="ae lc" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html" rel="noopener ugc nofollow" target="_blank"> NVIDIA容器工具包</a>，它有助于将K3S节点的GPU资源暴露给运行在其上的容器。第三步是告诉K3S使用这个工具包在容器上启用GPU。在这些步骤之后，其他的一切都是标准的Kubernetes的东西，这就是将用于Kubernetes的NVIDIA设备插件安装到您的集群，以使Kubernetes知道GPU资源，并使它们对Pods可用。</p><p id="d734" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，我们假设您已经有一个正在运行的K3S集群，并且您有一些安装了NVIDIA GPUs的节点。另一个假设是你的节点运行Ubuntu 20.04和K3s v1.21.3+k3s1，但我相信同样的说明也适用于其他发行版和大多数其他最新的K3s版本。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="360f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们进入步骤:</p><h1 id="1d4e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤1:安装NVIDIA驱动程序</h1><p id="9746" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">我们旅程的第一步是在我们的节点机器上安装NVIDIA驱动程序。我们可以首先使用apt搜索可用的驱动程序:</p><pre class="kn ko kp kq gt mn mo mp mq aw mr bi"><span id="c540" class="ms ll iq mo b gy mt mu l mv mw">$ apt search nvidia-driver</span></pre><p id="491c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在撰写本文时，最新的可用驱动程序版本是470，所以让我们继续安装这个版本:</p><pre class="kn ko kp kq gt mn mo mp mq aw mr bi"><span id="2184" class="ms ll iq mo b gy mt mu l mv mw">$ sudo apt install nvidia-headless-470-server</span></pre><p id="ca40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">需要注意的重要一点是我们选择安装“无头”和“服务器”驱动程序。许多资源建议安装的标准NVIDIA驱动程序附带了X11的包袱。这意味着一旦您安装了这些标准驱动程序，您也可以在您的主机上启用GUI。对于Kubernetes节点，您很可能不希望这样，尽管这不会影响您的Kubernetes集群的工作方式。另一方面，驱动程序包的无头服务器版本只安装设备驱动程序，我们更喜欢安装它们。</p><h1 id="ac09" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">第二步:<em class="mx">安装NVIDIA容器工具包</em></h1><p id="6d21" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated"><a class="ae lc" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html" rel="noopener ugc nofollow" target="_blank"> NVIDIA容器工具包</a>帮助我们构建和运行GPU加速容器。换句话说，它使我们能够将GPU暴露给在我们的节点上运行的容器。</p><p id="c965" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">容器工具包的文档很清楚。需要注意的一点是只安装工具包的containerd版本。K3S根本不使用Docker，因为Kubernetes已经弃用了Docker，它只使用containerd来管理容器。安装Docker支持不会影响集群的工作方式，因为它也会隐式安装containerd支持，但是因为我们避免在我们的瘦Kubernetes节点上安装不必要的包，所以我们直接安装containerd。</p><p id="acea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文档是这里的<a class="ae lc" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html" rel="noopener ugc nofollow" target="_blank"/>。如果您想直接跳到安装，首先安装存储库:</p><pre class="kn ko kp kq gt mn mo mp mq aw mr bi"><span id="b9dc" class="ms ll iq mo b gy mt mu l mv mw">$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \<br/> &amp;&amp; curl -s -L <a class="ae lc" href="https://nvidia.github.io/nvidia-docker/gpgkey" rel="noopener ugc nofollow" target="_blank">https://nvidia.github.io/nvidia-docker/gpgkey</a> | sudo apt-key add — \<br/> &amp;&amp; curl -s -L <a class="ae lc" href="https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list" rel="noopener ugc nofollow" target="_blank">https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list</a> | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span></pre><p id="3d3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并安装nvidia-container-runtime:</p><pre class="kn ko kp kq gt mn mo mp mq aw mr bi"><span id="c897" class="ms ll iq mo b gy mt mu l mv mw">$ sudo apt-get update \<br/> &amp;&amp; sudo apt-get install -y nvidia-container-runtime</span></pre><p id="3d3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，您可以运行一个测试容器来确保您的GPU向容器公开:</p><pre class="kn ko kp kq gt mn mo mp mq aw mr bi"><span id="5ebc" class="ms ll iq mo b gy mt mu l mv mw">$ sudo ctr image pull docker.io/nvidia/cuda:11.0-base<br/>$ sudo ctr run --rm --gpus 0 -t docker.io/nvidia/cuda:11.0-base cuda-11.0-base nvidia-smi</span></pre><p id="7261" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您应该会看到<a class="ae lc" href="https://developer.nvidia.com/nvidia-system-management-interface" rel="noopener ugc nofollow" target="_blank"> nvidia-smi </a>输出，但是这次是在一个容器中运行的！</p><h1 id="07fa" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤3:配置K3S使用nvidia-container-runtime</h1><p id="6aee" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">我们现在应该告诉K3S在我们节点的containerd上使用nvidia-container-runtime(这是containerd的一种插件)。</p><p id="bc05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们K3D的朋友为此制作了一个<a class="ae lc" href="https://k3d.io/usage/guides/cuda/#configure-containerd" rel="noopener ugc nofollow" target="_blank">实用指南</a>。我们在该指南中唯一感兴趣的部分是“配置容器”部分。他们共享的模板是配置containerd使用nvidia-container-runtime插件，以及一些额外的样板设置。要将模板安装到我们节点上，我们只需运行以下命令:</p><pre class="kn ko kp kq gt mn mo mp mq aw mr bi"><span id="2bed" class="ms ll iq mo b gy mt mu l mv mw">$ sudo wget <a class="ae lc" href="https://k3d.io/v4.4.8/usage/guides/cuda/config.toml.tmpl" rel="noopener ugc nofollow" target="_blank">https://k3d.io/v4.4.8/usage/guides/cuda/config.toml.tmpl</a> -O /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl</span></pre><h1 id="acd2" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤4:为Kubernetes安装NVIDIA设备插件</h1><p id="467a" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">用于Kubernetes的NVIDIA设备插件是一个DaemonSet，它扫描每个节点上的GPU，并将它们作为GPU资源暴露给我们的Kubernetes节点。</p><p id="b2f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你遵循设备插件的文档，也有一个舵图可以安装它。在K3S上，我们有一个简单的舵控制器，允许我们在我们的集群上安装舵图表。让我们利用它并部署这张舵轮图:</p><pre class="kn ko kp kq gt mn mo mp mq aw mr bi"><span id="5b59" class="ms ll iq mo b gy mt mu l mv mw">$ cat &lt;&lt;EOF | kubectl apply -f -<br/>apiVersion: helm.cattle.io/v1<br/>kind: HelmChart<br/>metadata:<br/>  name: nvidia-device-plugin<br/>  namespace: kube-system<br/>spec:<br/>  chart: nvidia-device-plugin<br/>  repo: <a class="ae lc" href="https://nvidia.github.io/k8s-device-plugin" rel="noopener ugc nofollow" target="_blank">https://nvidia.github.io/k8s-device-plugin</a><br/>EOF</span></pre><p id="149f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当然，您也可以通过直接应用清单或使用“helm install”安装图表来安装设备插件<a class="ae lc" href="https://github.com/NVIDIA/k8s-device-plugin#enabling-gpu-support-in-kubernetes" rel="noopener ugc nofollow" target="_blank">。这真的取决于你的口味。</a></p><h1 id="3f88" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">步骤5:在支持CUDA的Pod上测试一切</h1><p id="ee5d" class="pw-post-body-paragraph jn jo iq jp b jq mi js jt ju mj jw jx jy mk ka kb kc ml ke kf kg mm ki kj kk ij bi translated">最后，我们可以通过创建一个使用<a class="ae lc" href="https://hub.docker.com/r/nvidia/cuda" rel="noopener ugc nofollow" target="_blank"> CUDA Docker映像</a>并请求GPU资源的Pod来测试一切:</p><pre class="kn ko kp kq gt mn mo mp mq aw mr bi"><span id="c6de" class="ms ll iq mo b gy mt mu l mv mw">$ cat &lt;&lt;EOF | kubectl create -f -<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: gpu<br/>spec:<br/>  restartPolicy: Never<br/>  containers:<br/>    - name: gpu<br/>      image: "nvidia/cuda:11.4.1-base-ubuntu20.04"<br/>      command: [ "/bin/bash", "-c", "--" ]<br/>      args: [ "while true; do sleep 30; done;" ]<br/>      resources:<br/>        limits:<br/>          nvidia.com/gpu: 1<br/>EOF</span></pre><p id="3c71" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，让我们在Pod上运行nvidia-smi:</p><pre class="kn ko kp kq gt mn mo mp mq aw mr bi"><span id="5e1a" class="ms ll iq mo b gy mt mu l mv mw">$ kubectl exec -it gpu -- nvidia-smi<br/>Sun Aug 22 10:02:05 2021<br/>+-----------------------------------------------------------------------------+<br/>| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |<br/>|-------------------------------+----------------------+----------------------+<br/>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br/>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br/>|                               |                      |               MIG M. |<br/>|===============================+======================+======================|<br/>|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |<br/>| 33%   40C    P8    10W / 180W |      0MiB /  8117MiB |      0%      Default |<br/>|                               |                      |                  N/A |<br/>+-------------------------------+----------------------+----------------------+</span><span id="9c93" class="ms ll iq mo b gy my mu l mv mw">+-----------------------------------------------------------------------------+<br/>| Processes:                                                                  |<br/>|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |<br/>|        ID   ID                                                   Usage      |<br/>|=============================================================================|<br/>|  No running processes found                                                 |<br/>+-----------------------------------------------------------------------------+</span></pre><p id="9796" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">恭喜你！</p><p id="a1e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，您有了一个具有可用GPU资源的Kubernetes集群。这与Kubernetes文档定义的接口完全相同。这意味着您为新集群设计的GPU工作负载可以完全移植到任何其他拥有GPU资源的Kubernetes集群。</p><p id="5cde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你觉得这篇文章有用吗，或者有其他意见吗？请随时让我知道。</p></div></div>    
</body>
</html>