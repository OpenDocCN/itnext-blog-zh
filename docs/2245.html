<html>
<head>
<title>Kubeflow on GPU Enabled AWS-EKS Cluster</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPU上的Kubeflow支持AWS-EKS集群</h1>
<blockquote>原文：<a href="https://itnext.io/kubeflow-on-gpu-enabled-aws-eks-cluster-7c5d88c09d9b?source=collection_archive---------3-----------------------#2019-04-23">https://itnext.io/kubeflow-on-gpu-enabled-aws-eks-cluster-7c5d88c09d9b?source=collection_archive---------3-----------------------#2019-04-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8d61b0c5d22d7fbe42ed7b06625883d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wCsxFaCQRxKKwL4CsW1XVA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">在AWS-EKS的强大GPU支持实例上运行Kubeflow</figcaption></figure><div class=""/><p id="6710" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">自从Google将Kubernetes创建为一个开源容器编排工具以来，它已经以它可能从未想象过的方式蓬勃发展。随着这个项目越来越受欢迎，我们看到了许多辅助程序的发展。Kubeflow旨在将机器学习引入Kubernetes容器。该项目的目标不是重新创建其他服务，而是提供一种简单的方法来为不同的基础设施部署ML的最佳开源系统。</p><p id="8572" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">GPU已经从仅仅是一个图形芯片发展成为深度学习和机器学习的核心组件。CPU是为更一般的计算工作负载而设计的。相比之下，GPU的灵活性较差，但是GPU被设计为并行计算相同的指令，GPU中的这种并行架构非常适合矢量和矩阵运算。机器学习是一个对计算要求很高的领域，GPU的选择将从根本上决定用户的学习体验。在没有GPU的情况下，这可能看起来像是几个月的等待实验完成，或者运行一天或更长时间的实验，却看到所选的参数被关闭，模型出现偏差。</p><p id="2fce" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">随着越来越多的HPC应用、AI驱动的应用以及GPU在公共云中的广泛可用性，开源Kubernetes需要能够感知GPU。借助NVIDIA GPUs上的Kubernetes，软件开发人员和DevOps工程师可以大规模无缝地构建和部署GPU加速的深度学习训练或推理应用程序到异构GPU集群。Kubernetes中的GPU支持由NVIDIA设备插件提供，该插件将主机上的GPU暴露给容器空间。</p><p id="a96a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">NVIDIA和其他公司为AWS上基于GPU的加速计算实例提供了ami。这使得开发人员能够在AWS EC2实例上线性扩展他们的模型训练性能，从而加速预处理和消除数据传输瓶颈，并快速提高他们的机器学习模型的质量。Nvidia的CUDA是一个并行计算平台和编程模型，由NVIDIA开发，用于图形处理单元(GPU)上的通用计算。有了CUDA，开发人员就可以利用GPU的强大功能，大幅提高计算应用的速度。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/1c42cc90782b663e74655730b3473115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MJGM6nZictl4_Eyo"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">AWS上可用的GPU实例</figcaption></figure><h1 id="ea60" class="lf lg jf bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">EKS上支持GPU的Kubernetes节点</h1><p id="d5df" class="pw-post-body-paragraph kc kd jf ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">AWS提供GPU支持的EC2实例，可用于四个AWS地区的EKS。P3实例由多达八个<a class="ae mi" href="https://www.nvidia.com/en-us/data-center/tesla-v100/" rel="noopener ugc nofollow" target="_blank">NVIDIA Tesla V100</a>GPU提供支持，旨在处理计算密集型机器学习、深度学习、计算流体动力学、计算金融、地震分析、分子建模和基因组工作负载。</p><p id="4999" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">用户可以在为工作节点创建EKS集群时选择特定的风格。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/ae09a9f623235351bd07ac94123edfc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6dR37eyZRqL9QdLI"/></div></div></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/064acbaf8081b1f777295e9c5589a765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zjrqcWGCbR3A6fAF"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">将GPU节点指定为EKS的Kubernetes节点</figcaption></figure><p id="cae8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于上述集群，Kubernetes节点将配备4个特斯拉V100-SXM2 GPU，如下所示:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/857669e448059a0a5da919705ad50d66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/0*fjHA7ZngZgWyQQy9"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">GPUS —英伟达</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/da3aace69a72bf78b6677d329d28692d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Rby-_BS53BFJMbRV"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">GPUS —英伟达</figcaption></figure><h1 id="1429" class="lf lg jf bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">用于Kubernetes的NVIDIA设备插件</h1><p id="43c6" class="pw-post-body-paragraph kc kd jf ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">用于Kubernetes的NVIDIA <a class="ae mi" href="https://github.com/NVIDIA/k8s-device-plugin" rel="noopener ugc nofollow" target="_blank">设备插件</a>是一款Daemonset，允许您自动:</p><ul class=""><li id="e140" class="mk ml jf ke b kf kg kj kk kn mm kr mn kv mo kz mp mq mr ms bi translated">公开集群的每个节点上的GPU数量</li><li id="3081" class="mk ml jf ke b kf mt kj mu kn mv kr mw kv mx kz mp mq mr ms bi translated">跟踪GPU的运行状况</li><li id="bb6c" class="mk ml jf ke b kf mt kj mu kn mv kr mw kv mx kz mp mq mr ms bi translated">在Kubernetes集群中运行启用GPU的容器。</li></ul><p id="8d9f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该插件将在以下节点上配置为默认运行时:</p><pre class="lb lc ld le gt my mz na nb aw nc bi"><span id="807b" class="nd lg jf mz b gy ne nf l ng nh">{<br/>    "default-runtime": "nvidia",<br/>    "runtimes": {<br/>        "nvidia": {<br/>            "path": "/usr/bin/nvidia-container-runtime",<br/>            "runtimeArgs": []<br/>        }<br/>    }<br/>}</span></pre><p id="c54d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">NVIDIA GPUs现在可以通过使用资源名称nvidia.com/gpu:的容器级资源需求来使用</p><pre class="lb lc ld le gt my mz na nb aw nc bi"><span id="6e1d" class="nd lg jf mz b gy ne nf l ng nh">apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: gpu-pod<br/>spec:<br/>  containers:<br/>    - name: cuda-container<br/>      image: nvidia/cuda:9.0-devel<br/>      resources:<br/>        limits:<br/>          nvidia.com/gpu: 2 # requesting 2 GPUs<br/>    - name: digits-container<br/>      image: nvidia/digits:6.0<br/>      resources:<br/>        limits:<br/>          nvidia.com/gpu: 2 # requesting 2 GPUs</span></pre><p id="3405" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该插件在Kubernetes集群上作为daemonset运行，如下所示:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/c40f58977adcb818b17c0320d3a25cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y7FIVEFGX0qlduNa"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Kubernetes上的英伟达设备插件</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/49a0bcf4873829e6ed5382c800d5e124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UzRlTLBEVp8GUZse"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">英伟达插件在Kubelet注册</figcaption></figure><p id="d2e4" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">安装设备插件后，Kubernetes节点现在可以将NVIDIA GPU视为常规资源，如下所示:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/38f2e7d54aef884aefd0ccc9b38a7fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jMzQ2fYM1lbdMf97"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">库柏内斯节点上的GPU可见性</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/598a92b39e89d1e2c1c9bf39bccf6b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sPk0Y1edOEhtkaSm"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">库柏内斯节点上的GPU可见性</figcaption></figure><h1 id="3a83" class="lf lg jf bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">库贝弗洛谈EKS</h1><p id="e54d" class="pw-post-body-paragraph kc kd jf ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">虽然可以使用CPU实例运行机器学习工作负载，但GPU实例具有数千个CUDA核心，这显著提高了深层神经网络的训练和处理大型数据集的性能。Kubeflow可以部署在EKS，消耗基于高性能GPU的EC2实例。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5a461e3eef6dd3e18043c9fbef0b2ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/0*S8wSsyJ4ZCUFwlgK"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Kubernetes上的Kubeflow堆栈</figcaption></figure><pre class="lb lc ld le gt my mz na nb aw nc bi"><span id="5f26" class="nd lg jf mz b gy ne nf l ng nh">* tf-hub-0: JupyterHub web application that spawns and manages Jupyter notebooks.<br/>* tf-job-operator, tf-job-dashboard: Runs and monitors TensorFlow jobs in Kubeflow.<br/>* ambassador: Ambassador API Gateway that routes services for Kubeflow.<br/>* centraldashboard: Kubeflow central dashboard UI.<br/>centraldashboard: Kubeflow central dashboard UI.</span></pre><h1 id="8937" class="lf lg jf bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">消费GPU库贝弗洛</h1><p id="0624" class="pw-post-body-paragraph kc kd jf ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">如上所述，作为Kubeflow的一部分的任何pod可以使用资源标志“nvidia.com/gpu”来消耗可用资源。库贝弗洛在集群中增加了一些资源来协助完成各种任务，包括培训和服务模特，以及运行<a class="ae mi" href="http://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>，用户可以创建和共享包含实时代码、公式、可视化效果和叙事文本的文档。</p><p id="b2c4" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae mi" href="https://jupyterhub.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> JupyterHub </a>允许用户管理对多个单用户Jupyter笔记本电脑的身份验证访问。JupyterHub将单用户笔记本的推出委托给了称为“spawners”的可插入组件。JupyterHub有一个名为kubespawner的子项目，由该社区维护，该项目使用户能够调配由Kubernetes pods支持的单用户Jupyter笔记本，这些笔记本本身就是Kubernetes pods。kubeform _ spawner扩展了kubespawner，使用户能够拥有指定cpu、内存、gpu和所需图像的表单。Spawners配置允许用户选择Jupyter Server要使用的GPU数量。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/c0209e03964fe43636b83dc44f57da8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NV77DH4Ig7oiOwyc"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">设置资源限制GPU的数量</figcaption></figure><p id="6e65" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">例如，上面的Spawner配置为使用3个GPU作为resource_limit。这将在Kubernetes上创建一个pod，其中指定的资源限制将该值替换为“nvidia.com/gpu:3”，并且特定的JupyterHub pod在创建后可以看到3个gpu，如下所示:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/f5f291c34b5cd89ccda3d31ea3bc6e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1ODm1hfPIW4lr7EG"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">使用GPU的Tensorflow应用程序</figcaption></figure><p id="c544" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如上所示，特定应用程序可以利用三个GPU，如spawner选项中配置的那样。</p><p id="0997" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以上游Kubeflow — GitHub问题摘要为例:使用GitHub问题公共数据集的自动摘要生成器。这个具体的例子包括多个步骤，如获取数据、预处理数据集、执行TensorFlow NLP模型的训练等。使用上面在Kubernetes上创建的JupyterHub应用程序可以加载和执行相同的内容。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/120ff7247479977e62c21decce25fb0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8ZcZi3XCy9oK-qzQ"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">JupyterHub —应用程序目录</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/4801ea64488d354314ef821cf0932af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*URLcFovIIBZuMQZB"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">步骤—应用</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/aeede6db7164546dc40d78a23a88a299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vwaKAeOfSB47dUO1"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">处理数据</figcaption></figure><h1 id="ae21" class="lf lg jf bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">使用GPU进行培训</h1><p id="940b" class="pw-post-body-paragraph kc kd jf ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">在下面的示例中，用户在Spawner选项中将1个GPU配置为资源限制的JupyterHub应用程序。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/a2f1c1f7378e21f944806a19031dec56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CJKgQMdn5l3fkl8_"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">配置资源限制</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/ea199e190673d26395a0eb068bdaa7d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8xEuxO4Ya2pNLDAL"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Kubernetes上的笔记本应用程序</figcaption></figure><p id="44fc" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这使得应用程序只能从应用程序端看到节点上四个GPU中的一个。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/f59f47b8ca0b2e3bb67e54f351aebf48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pjUA9xXtSCxTTDyl"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">基于资源配置的应用GPU使用</figcaption></figure><p id="ae55" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用上面的配置运行培训:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/cf6e0a4463c8e959bd9ac57396ca48b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BPnyAzbFRoqP6_LY"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">培养</figcaption></figure><p id="b05b" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如上所述，train_model利用单个GPU进行训练，在4个可用GPU中的一个GPU上，利用率达到峰值，如下所示:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi la"><img src="../Images/53f6db79348f9fb331917f25dd353617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C21g1_7Up6epbeSv"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">使用GPU的训练模型</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/819c42498da9859eb47a0cac57f8a260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xNycDeTtJ_cN20HR"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">运行训练时的GPU统计</figcaption></figure><p id="b327" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">GPU实例拥有数千个CUDA核心，在上述海量数据的训练中显著提升了性能。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/ea47dc41a26b2fd9140e0d1b6962b124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IAcoP6yuKnPCrIwt"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">培训模式</figcaption></figure><p id="db68" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">同样，用户可以根据训练数据集的要求和大小来选择GPU的数量。下面显示了一个使用3个GPU的示例:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/e4e955173778cd34b207c0bd034c3661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ij4qOhnHiEUgemny"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">资源限制设置为3个GPU</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/b44904a193badc7375572fcc0754bd7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wPwATOHk6pYsl2uN"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">使用3个GPU的Kubernetes上的笔记本应用程序</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/20574d0c418b30ed85c00001b0ada891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2pArsoZ5_4dSTk6U"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">为应用程序配置了三个GPU</figcaption></figure><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/25deace4ffded4af6ea729d4548b89ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CxbZlUh8tqDyLN7f"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">GPU统计</figcaption></figure><p id="4ca4" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如上所述，该过程利用三个GPU，但并不均匀地分布在GPU上，因为并行编程或多GPU构造必须通过代码库来启用，以便以并行方式正确利用所有GPU。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/f4aab762474933731c979c95836225fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l5tPxPQ9F5eRACwz"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">使用GPU的训练模型</figcaption></figure><p id="5680" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Nvidia的CUDA(由NVIDIA创建的并行计算平台和应用程序编程接口模型)构成了多个模型，使用户能够使用多个GPU来利用像上面的AWS EC@ instance这样的支持多GPU的系统的真正力量。比如说。cuda()接受一个设备id，用户可以在同一个training_model中基于每个任务分配GPU。</p><pre class="lb lc ld le gt my mz na nb aw nc bi"><span id="108b" class="nd lg jf mz b gy ne nf l ng nh">model1 = nn.DataParallel(model1).cuda(device=0)<br/>model1_feat = model1(input_image)</span><span id="12fa" class="nd lg jf mz b gy nx nf l ng nh">model2 = nn.DataParallel(model2).cuda(device=1)<br/>model2_feat = model2(model1_feat, input_feat)</span></pre><p id="76fa" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这样，用户可以充分利用系统中的GPU，如下图所示，其中培训任务跨多个GPU:</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1e696dcfb9fd9a1a5eadb97f6220933f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/0*7CITBU9RlzBU03kb"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">使用CUDA跨GPU进行培训</figcaption></figure></div><div class="ab cl nz oa hu ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="ij ik il im in"><p id="0964" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">像Kubeflow这样的工具包确实加强了这样一个梦想，即运行人工智能任务并为其服务不仅仅局限于少数组织，而是每个人都可以轻松访问。目前，人工智能和深度学习在全球范围内创造了巨大的市场机会，旨在改善我们的生活和我们周围的世界。为深度学习任务选择正确类型的硬件是一个广泛讨论的话题。随着越来越多的公共云提供商，如AWS、GCE和Azure，使用户能够轻松地采购这些基础设施，从财务和时间角度来看都有可观的收益。</p></div></div>    
</body>
</html>