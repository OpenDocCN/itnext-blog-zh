<html>
<head>
<title>Learning Data Science — Predict Adult Income with Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习数据科学—用决策树预测成人收入</h1>
<blockquote>原文：<a href="https://itnext.io/learning-data-science-predict-adult-income-with-decision-tree-ae8dd57a76cc?source=collection_archive---------2-----------------------#2019-03-17">https://itnext.io/learning-data-science-predict-adult-income-with-decision-tree-ae8dd57a76cc?source=collection_archive---------2-----------------------#2019-03-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/a446ed34158bb2ac324d3a6e1ede1016.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*Ij4lOqZzLLODoSXIbkDB8g.jpeg"/></div></figure><p id="50c9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae ks" href="https://en.wikipedia.org/wiki/Decision_tree" rel="noopener ugc nofollow" target="_blank">决策树</a>是一种决策支持工具，使用决策及其可能结果的树状模型，包括偶然事件结果、资源成本和效用。这是显示只包含条件控制语句的算法的一种方式。</p><p id="3796" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">决策树是机器学习中一个非常简单而强大的工具。让我们来看一个来自<a class="ae ks" href="https://becominghuman.ai/understanding-decision-trees-43032111380f" rel="noopener ugc nofollow" target="_blank">成为人类的例子:</a></p><figure class="kt ku kv kw gt jr gh gi paragraph-image"><div class="ab gu cl kx"><img src="../Images/121f313716b15922c3e17c48f3ab317c.png" data-original-src="https://miro.medium.com/v2/format:webp/0*Yclq0kqMAwCQcIV_.jpg"/></div></figure><p id="2af5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">该树由以下组件组成:</p><ul class=""><li id="7a6f" class="ky kz iq jw b jx jy kb kc kf la kj lb kn lc kr ld le lf lg bi translated">问题/条件是节点。</li><li id="4796" class="ky kz iq jw b jx lh kb li kf lj kj lk kn ll kr ld le lf lg bi translated">是/否选项表示边。</li><li id="6839" class="ky kz iq jw b jx lh kb li kf lj kj lk kn ll kr ld le lf lg bi translated">终端动作是树的叶子。</li></ul><p id="0745" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这篇文章中，我将使用这种有监督的机器学习方法——决策树分类——来预测一个成年人的收入是否大于50K/年。</p><p id="6101" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">获取数据</strong></p><p id="2c5c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">成人普查数据来自加州大学欧文分校。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="bf55" class="lr ls iq ln b gy lt lu l lv lw">Listing of attributes: <br/><br/>&gt;50K, &lt;=50K. <br/><br/>age: continuous. <br/>workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. <br/>fnlwgt: continuous. <br/>education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. <br/>education-num: continuous. <br/>marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. <br/>occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. <br/>relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. <br/>race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. <br/>sex: Female, Male. <br/>capital-gain: continuous. <br/>capital-loss: continuous. <br/>hours-per-week: continuous. <br/>native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;Tobago, Peru, Hong, Holand-Netherlands.</span></pre><p id="ae66" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">安装依赖关系</strong></p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="85da" class="lr ls iq ln b gy lt lu l lv lw">import pandas as pd<br/>import numpy as np<br/>from sklearn import tree<br/>import graphviz<br/>from sklearn.model_selection import cross_val_score</span></pre><p id="b0fd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">决策树功能来自sklearn。</p><p id="99e2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">数据准备</strong></p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="57ce" class="lr ls iq ln b gy lt lu l lv lw"><em class="lx"># Load dataset </em><br/>df = pd.read_csv('adult.csv', sep=',')<br/>len(df) # 32561</span></pre><p id="03f5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">许多行包含问号“？”。我会开始移除它们。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="7727" class="lr ls iq ln b gy lt lu l lv lw"># Remove invalid data from table<br/>df = df[(df.astype(str) != ' ?').all(axis=1)]<br/>len(df) # 30162</span></pre><p id="9571" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">大约有2399行被删除。</p><p id="3f42" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">接下来，出于预测的目的，我将把收入列改为二进制，有几列对我的预测方法没有真正的贡献。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="dc00" class="lr ls iq ln b gy lt lu l lv lw"><em class="lx"># Create a new income_bi column<br/></em>df['income_bi'] = df.apply(lambda row: 1 if '&gt;50K'in row['income'] else 0, axis=1)</span><span id="e690" class="lr ls iq ln b gy ly lu l lv lw"><em class="lx"># Remove redundant columns</em><br/>df = df.drop(['income','fnlwgt','capital-gain','capital-loss','native-country'], axis=1)</span></pre><p id="62fc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了建立预测模型，我们还需要将分类值转换为数值。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="fc31" class="lr ls iq ln b gy lt lu l lv lw"># Use one-hot encoding on categorial columns<br/>df = pd.get_dummies(df, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex'])</span></pre><figure class="kt ku kv kw gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/3800087205437aa1f453d0324cf5fbf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*1ge-4Y9op0Oqp5q3G3NVGA.png"/></div></figure><p id="4435" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">构建决策树模型</strong></p><p id="461e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们大约有30，000行，我将按80/20划分训练集和测试集。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="df38" class="lr ls iq ln b gy lt lu l lv lw"><em class="lx"># shuffle rows</em><br/>df = df.sample(frac=1)</span><span id="9b4e" class="lr ls iq ln b gy ly lu l lv lw"># split training and testing data<br/>d_train = df[:25000]<br/>d_test = df[25000:]</span><span id="d0c0" class="lr ls iq ln b gy ly lu l lv lw">d_train_att = d_train.drop(['income_bi'], axis=1)<br/>d_train_gt50 = d_train['income_bi']</span><span id="d1d0" class="lr ls iq ln b gy ly lu l lv lw">d_test_att = d_test.drop(['income_bi'], axis=1)<br/>d_test_gt50 = d_test['income_bi']</span><span id="104e" class="lr ls iq ln b gy ly lu l lv lw">d_att = df.drop(['income_bi'], axis=1)<br/>d_gt50 = df['income_bi']</span><span id="5313" class="lr ls iq ln b gy ly lu l lv lw"><em class="lx"># number of income &gt; 50K in whole dataset:</em><br/>print("Income &gt;50K: %d out of %d (%.2f%%)" % (np.sum(d_gt50), len(d_gt50), 100*float(np.sum(d_gt50)) / len(d_gt50)))</span><span id="0bf7" class="lr ls iq ln b gy ly lu l lv lw"># Income &gt;50K: 7508 out of 30162 (24.89%)</span></pre><p id="1e8b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们大约有24.89%的人年薪超过5万英镑。</p><p id="c141" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在，我们开始训练模型。我会在这篇文章的最后向你解释我为什么选择max_depth=7。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="4af3" class="lr ls iq ln b gy lt lu l lv lw"><em class="lx"># Fit a decision tree</em><br/>t = tree.DecisionTreeClassifier(criterion='entropy', max_depth=7)<br/>t = t.fit(d_train_att, d_train_gt50)</span></pre><p id="ae70" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果你想可视化决策树，你可以使用<a class="ae ks" href="https://graphviz.org/" rel="noopener ugc nofollow" target="_blank"> graphviz </a>工具。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="ac09" class="lr ls iq ln b gy lt lu l lv lw"><em class="lx"># Visualize tree</em><br/>dot_data = tree.export_graphviz(t, out_file=None, label='all', impurity=False, proportion=True, <br/>                               feature_names=list(d_train_att), class_names=['lt50K', 'gt50K'],<br/>                               filled=True, rounded=True)<br/>graph = graphviz.Source(dot_data)<br/>graph</span></pre><p id="be4d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在我们建立模型后，我们可以检验它的准确性。结果显示~82%是非常好的。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="c5e0" class="lr ls iq ln b gy lt lu l lv lw">t.score(d_test_att, d_test_gt50)<br/># 0.820030995738086</span></pre><p id="980e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们可以进一步通过<a class="ae ks" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html" rel="noopener ugc nofollow" target="_blank">交叉验证</a>来评估分数。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="e993" class="lr ls iq ln b gy lt lu l lv lw">scores = cross_val_score(t, d_att, d_gt50, cv=5)</span><span id="4b57" class="lr ls iq ln b gy ly lu l lv lw"><em class="lx"># Show avarage score and +/- two standard deviations away (covering 95% or scores)</em><br/>print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std()*2))</span><span id="44a1" class="lr ls iq ln b gy ly lu l lv lw"># Accuracy: 0.83 (+/- 0.00)</span></pre><p id="3960" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">开始预测</strong></p><p id="29e6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们可以通过在修改后保存数据框的第一行来准备预测模板。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="e4f6" class="lr ls iq ln b gy lt lu l lv lw"><em class="lx"># Create a sample csv for prediction</em><br/>df.iloc[[0]].to_csv('prediction.csv', sep=',', encoding='utf-8', index=False)</span></pre><p id="0dba" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在我们有了一个CSV文件，其中包含了开始预测所需的数据。您可以修改行值以获得合适的用户配置文件。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="be32" class="lr ls iq ln b gy lt lu l lv lw"><em class="lx"># Prepare user profile</em><br/>sample_df = pd.read_csv('prediction.csv', sep=',')<br/>sample_df = sample_df.drop(['income_bi'], axis=1)</span><span id="3c92" class="lr ls iq ln b gy ly lu l lv lw"><em class="lx"># Start predicting </em><br/>predict_value = sample_df.iloc[0]<br/>y_predict = t.predict([predict_value.tolist()])<br/>y_predict[0] #0</span></pre><p id="67cb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于我测试的用户，他们的工资不到50K。</p><p id="e46e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">如何为决策树选择正确的深度</strong></p><p id="c0bc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">正确的方法是测试几个深度，以便为您的模型找到正确的最大深度。</p><pre class="kt ku kv kw gt lm ln lo lp aw lq bi"><span id="3483" class="lr ls iq ln b gy lt lu l lv lw">for max_depth in range(1, 20):<br/>    t = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)<br/>    scores = cross_val_score(t, d_att, d_gt50, cv=5)<br/>    print("Max depth: %d, Accuracy: %0.2f (+/- %0.2f)" % (max_depth, scores.mean(), scores.std()*2))</span><span id="1461" class="lr ls iq ln b gy ly lu l lv lw"><em class="lx"># Results</em><br/>Max depth: 1, Accuracy: 0.75 (+/- 0.00)<br/>Max depth: 2, Accuracy: 0.82 (+/- 0.01)<br/>Max depth: 3, Accuracy: 0.81 (+/- 0.01)<br/>Max depth: 4, Accuracy: 0.82 (+/- 0.01)<br/>Max depth: 5, Accuracy: 0.82 (+/- 0.01)<br/>Max depth: 6, Accuracy: 0.82 (+/- 0.01)<br/>Max depth: 7, Accuracy: 0.83 (+/- 0.00)<br/>Max depth: 8, Accuracy: 0.83 (+/- 0.00)<br/>Max depth: 9, Accuracy: 0.83 (+/- 0.01)<br/>Max depth: 10, Accuracy: 0.82 (+/- 0.01)<br/>Max depth: 11, Accuracy: 0.82 (+/- 0.01)<br/>Max depth: 12, Accuracy: 0.82 (+/- 0.01)<br/>Max depth: 13, Accuracy: 0.82 (+/- 0.01)<br/>Max depth: 14, Accuracy: 0.81 (+/- 0.01)<br/>Max depth: 15, Accuracy: 0.81 (+/- 0.01)<br/>Max depth: 16, Accuracy: 0.81 (+/- 0.01)<br/>Max depth: 17, Accuracy: 0.80 (+/- 0.01)<br/>Max depth: 18, Accuracy: 0.80 (+/- 0.01)<br/>Max depth: 19, Accuracy: 0.80 (+/- 0.00)</span></pre><p id="59f7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如您所见，7–9的最大深度产生了最好的结果— 83%。这就是为什么我选择7作为训练模型的max_depth。</p><p id="7198" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这篇文章中，我们学习了如何建立一个决策树模型，并根据他们的特征对成年人的工资进行分类/预测——你可以从我的github 中查看这个项目的笔记本<a class="ae ks" href="https://github.com/dalenguyen/python-for-data-science/blob/master/week-9-final-project/income-prediction/income-prediction.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>