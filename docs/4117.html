<html>
<head>
<title>Handling Data Skew in Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Apache Spark中处理数据不对称</h1>
<blockquote>原文：<a href="https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8?source=collection_archive---------0-----------------------#2020-04-30">https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8?source=collection_archive---------0-----------------------#2020-04-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/08f230249e313ab9fe9476df5e31a557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SICi8EJBHIpWzeQvBb1Jog.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">均匀分布与偏斜分布</figcaption></figure><h1 id="51f4" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">介绍</h1><p id="a64a" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">并行计算系统中一个众所周知的问题是<strong class="lc ir">数据偏斜</strong>。通常，在Apache Spark中，数据偏斜是由像join、groupBy和orderBy这样改变数据分区的转换引起的。例如，连接一个不均匀分布在集群中的键，导致一些分区非常大，并且不允许Spark并行处理数据。因为这是一个众所周知的问题，所以有很多可用的解决方案。在本文中，我将分享我在Apache Spark中处理数据偏斜的经验。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><blockquote class="mf mg mh"><p id="2df4" class="la lb mi lc b ld mj lf lg lh mk lj lk ml mm ln lo mn mo lr ls mp mq lv lw lx ij bi translated">您可以在该资源库中找到所有相关的源代码示例:</p></blockquote><div class="mr ms gp gr mt mu"><a href="https://github.com/dimastatz/video-streaming-analytics" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">GitHub-dimastatz/视频流-分析</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">考虑一场像NBA比赛这样的现场比赛。这个视频是由安装在篮球场上的摄像机拍摄的，它让…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">github.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni jw mu"/></div></div></a></div><blockquote class="mf mg mh"><p id="b6a2" class="la lb mi lc b ld mj lf lg lh mk lj lk ml mm ln lo mn mo lr ls mp mq lv lw lx ij bi translated">使用data-process/processing/flumenz SBT项目并按照readme.md运行示例。如果你喜欢，给回购你的星星。</p></blockquote></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="0aaf" class="kc kd iq bd ke kf nj kh ki kj nk kl km kn nl kp kq kr nm kt ku kv nn kx ky kz bi translated">投入</h1><p id="9237" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在这个具体的例子中，我们将讨论CDN访问日志的每小时批处理。不同的cdn产生不同格式和大小的日志文件。为了简化处理，我们正在运行一个预处理器任务，创建大小相等的大约30MB的parquet格式的文件。所有文件都保存在AWS S3。批量大小从50GB到1.5TB不等，具体取决于cdn上的流量。对于本文中的所有示例，我使用的事件批处理包含3570个文件(356945717行)，总大小为107.4GB。</p><h1 id="84a2" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">目标</h1><p id="6f01" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我们的数据管道中的一个步骤是数据丰富。这个想法是用不同类型的元数据充实每个日志行，比如地理数据、用户代理、所有者元数据等。例如，如果日志行有<strong class="lc ir">的owner_id </strong>字段，OwnerMetadata有<strong class="lc ir">的owner_id，owner_name </strong>，我们将向日志行添加一个<strong class="lc ir"> owner_name </strong>列。我猜数据丰富对于许多数据管道来说是很常见的。代码可能是这样的</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="985b" class="nx kd iq nt b gy ny nz l oa ob">val df = spark.read.parquet(“s3://…”)<br/>val geoDataDf = spark.read.parquet(“s3://…”)<br/>val userAgentDf = spark.read.parquet(“s3://…”)<br/>val ownerMetadataDf = spark.read.parquet(“s3://…”)</span><span id="1e02" class="nx kd iq nt b gy oc nz l oa ob">df<br/> .join(geoDataDf, exprGeo, “left”)<br/> .join(userAgentDf, exprUserAgent, “left”)<br/> .join(ownerMetadataDf, exprOwnerMetadata, “left”)<br/> .write<br/> .parquet(“s3://…”)</span></pre><h1 id="42f6" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">连接类型</h1><p id="1b27" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">Apache Spark有3种不同的连接类型:广播连接、排序合并连接和混排连接。从Apache Spark 2.3开始，排序合并和广播连接是最常用的，因此我将重点介绍这两种。你可以在这里和这里找到更多关于洗牌加入<a class="ae od" href="https://databricks.com/session/optimizing-apache-spark-sql-joins" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><p id="5d86" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated"><strong class="lc ir">排序合并</strong>算法的关键思想是首先通过连接键对关系进行排序，这样交错的线性扫描将同时遇到这些集合，然后合并，这样每个执行器将持有一组特定的连接键。可以想象，这种策略可能非常昂贵:节点需要使用网络来共享数据。<br/>在<strong class="lc ir">广播加入</strong>中，Spark向每个执行者发送一份完整的查找表副本。显然，在这种方法中，每个执行器在执行连接操作时都是自给自足的。该网络仅用于广播。</p><h1 id="b8b5" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">排序合并联接</h1><p id="8a11" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">因此，我们有一个通过读取3570 AWS S3对象创建的事件数据帧，总大小为107GB。创建的数据框有356945717行。我们对有1100000行的元数据表执行连接操作，以便用附加信息丰富事件。运行作业并查看Spark UI后，我们可以看到下图:</p><p id="c93d" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">作业开始12分钟后:</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/7eda7e8b7e98a34e6e6636db3cd11fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tozSsZjHfUjLujByWfrdEg.png"/></div></div></figure><p id="dbe4" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">作业开始21分钟后:</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/b86741f3a0fdac37e660c9407b79a306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z35CYJKnwrKcFgA7szeVSA.png"/></div></div></figure><p id="e851" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">最后，需要1.2小时才能完成</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/6cc56d3b2bb2e35f8034741f59918b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJ5bcyBGA-xF5CRQpPX7Qw.png"/></div></div></figure><p id="e65e" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">显然，我们可以看到一个长时间运行的任务有大量的无序读取。在仔细观察这个长时间运行的任务后，我们可以看到它处理了几乎50%的输入(参见Shuffle Read Records专栏)。</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oh"><img src="../Images/30e2d5c21240e0a5b0c98491ebf16d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xjOXMvVQzkikqMqXknRHxw.png"/></div></div></figure><p id="969d" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">问题是为什么给定作业的处理没有适当地并行化。在Apache Spark中，排序合并连接将所有具有相同连接键的记录发送到同一个分区，看起来几乎50%的输入行在连接列中具有相同的值。通过在我的输入文件上运行下面的查询，我可以看到确实是这样，我们有一个典型的数据偏斜问题。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="8664" class="nx kd iq nt b gy ny nz l oa ob">spark.read.parquet(“s3://…”)<br/> .select(“o_id”)<br/> .groupBy(“o_id”)<br/> .agg(count(“o_id”).as(“count_o_id”))<br/> .sort(desc(“count_o_id”))</span></pre><h1 id="4ba5" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">广播加入</h1><p id="a0b9" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">显然，广播连接看起来是解决数据偏斜问题的好方法。我们有一个几乎完美地分布在Apache Spark集群中的事件数据框架。我们的元数据数据框比事件数据框小得多。直觉上，我们希望避免移动大小高达1.5TB的事件数据帧，而是将元数据表的整个副本发送给每个Spark执行器。因此，我将代码改为</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="0b71" class="nx kd iq nt b gy ny nz l oa ob">val df = spark.read.parquet(“s3://...”)<br/>val geoDataDf = spark.read.parquet(“s3://...”)<br/>val userAgentDf = spark.read.parquet(“s3://...”)<br/>val ownerMetadataDf = spark.read.parquet(“s3://...”)</span><span id="fd41" class="nx kd iq nt b gy oc nz l oa ob">df<br/> .join(<strong class="nt ir">broadcast</strong>(geoDataDf), exprGeo, “left”)<br/> .join(<strong class="nt ir">broadcast</strong>(userAgentDf), exprUserAgent, “left”)<br/> .join(<strong class="nt ir">broadcast</strong>(ownerMetadataDf), exprOwnerMetadata, “left”)<br/> .write<br/> .parquet(“s3://...”)</span></pre><p id="d701" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">在这里，我使用broadcast关键字作为Apache Spark的提示，来广播join操作的右侧。请注意，当其中一个数据帧小于Spark . SQL . autobroadcastjointhreshold .<a class="ae od" href="https://spark.apache.org/docs/latest/sql-performance-tuning.html" rel="noopener ugc nofollow" target="_blank">的值时，Apache Spark会自动将连接转换为广播连接，更多信息请参见</a> Apache Spark文档。现在让我们运行代码，并使用Spark UI分析运行的作业。</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/d9a059eebb9f6e7141ddbcda3734a084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1SnCuoJQZOy-PBWlvdJIgA.png"/></div></div></figure><p id="1d4a" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">这项工作只用了5.2分钟就完成了，与之前使用Sort Merge join运行相比，这是一个巨大的改进。让我们更深入地研究数据分布统计</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/dd363f3cb62ecea06f4c0eb4ce496230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5J2O1yvoyUMuLSZ_41S-3g.png"/></div></div></figure><p id="5b03" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">正如您在所附的任务指标表中看到的，广播连接工作得非常好。我们从均匀分布的数据开始，进行广播不会改变分区。您可以看到，在第25、中间和第75个百分位数中，数据保持均匀分布，并且没有发生洗牌。性能非常好，107GB的parquet文件的数据丰富在5.2分钟内完成，包括写入IO。将tun时间从1.2小时提高到5.2分钟，令人印象深刻。</p><p id="724b" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">唯一的问题是，这种方法是否也适用于500GB、1TB和2TB的输入数据。广播数据帧的大小有什么限制？不幸的是，广播连接存在以下问题，并且不能总是用作数据偏斜的解决方案:</p><ol class=""><li id="fdad" class="ok ol iq lc b ld mj lh mk ll om lp on lt oo lx op oq or os bi translated">可扩展性。广播连接对广播表的大小有一个硬编码限制— 8GB的表大小或512000000行，这使得该解决方案不可伸缩。这里看源代码<a class="ae od" href="https://github.com/apache/spark/blob/79c66894296840cc4a5bf6c8718ecfd2b08bcca8/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala#L104" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="8e8b" class="ok ol iq lc b ld ot lh ou ll ov lp ow lt ox lx op oq or os bi translated">高内存占用。广播的数据应该完全适合驱动程序和执行器的内存。例如，当一个作业运行100个执行器并且广播的数据帧是1GB时，使用广播连接的价格是额外的100 GB RAM。另外，如果你的广播表有增加的趋势，你会经常看到下面的异常，你需要经常调整Spark Executor和Driver的内存大小。</li></ol><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="e867" class="nx kd iq nt b gy ny nz l oa ob">java.lang.OutOfMemoryError: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:122)</span></pre><p id="3813" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">3.性能。显然，广播操作是昂贵的，因为所有执行者都需要接收表的副本。因此，当输入大小很大，并且一个作业需要很多执行者时，广播时间会增加，并且可能会高于加入成本本身。</p><h1 id="cf68" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">关键加盐</h1><p id="d57f" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">还有另一种众所周知的技术来克服这个问题—密钥加盐。这个想法是发明一个新的密钥，保证数据的均匀分布。下面，你可以看到可能的“密钥加盐”实现。这里，我们向大数据框添加一个具有均匀分布值的列。然后我们向小数据框添加一列，最后我们“分解”小数据框，这意味着我们为每个“旧id”和[0，salt]范围内的每个n创建新行。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="12f9" class="nx kd iq nt b gy ny nz l oa ob">def saltedJoin(df: DataFrame, buildDf: DataFrame, joinExpression: Column, joinType: String, salt: Int): DataFrame = {</span><span id="a3ce" class="nx kd iq nt b gy oc nz l oa ob"><strong class="nt ir">import</strong> org.apache.spark.sql.functions._<br/> <strong class="nt ir">val</strong> tmpDf = buildDf.withColumn(“slt_range”, array(Range(0, salt).toList.map(lit): _*))<br/> <br/> <strong class="nt ir">val</strong> tableDf = tmpDf.withColumn(“slt_ratio_s”, <strong class="nt ir">explode</strong>(tmpDf(“slt_range”))).drop(“slt_range”)</span><span id="2f2f" class="nx kd iq nt b gy oc nz l oa ob"><strong class="nt ir">val</strong> streamDf = df.withColumn(“slt_ratio”, <strong class="nt ir">monotonically_increasing_id</strong> % salt)<br/> <br/> <strong class="nt ir">val</strong> saltedExpr = streamDf(“slt_ratio”) === tableDf(“slt_ratio_s”) <br/>&amp;&amp; joinExpression</span><span id="708e" class="nx kd iq nt b gy oc nz l oa ob">streamDf.join(tableDf, saltedExpr,     joinType).drop(“slt_ratio_s”).drop(“slt_ratio”)</span><span id="afeb" class="nx kd iq nt b gy oc nz l oa ob">}</span></pre><p id="0463" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">现在我们可以通过使用新的和旧的连接列来连接。通过利用scala <strong class="lc ir">隐式</strong> <a class="ae od" href="https://medium.com/@dima.statz_89242/scala-introduction-for-java-developers-ffd53ad4f10d" rel="noopener">类</a>，我们可以如下重写我们的浓缩流程</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="1fc2" class="nx kd iq nt b gy ny nz l oa ob">val df = spark.read.parquet(“s3://...”)<br/>val geoDataDf = spark.read.parquet(“s3://...”)<br/>val userAgentDf = spark.read.parquet(“s3://...”)<br/>val ownerMetadataDf = spark.read.parquet(“s3://...”)</span><span id="cca1" class="nx kd iq nt b gy oc nz l oa ob">df<br/> .saltedJoin(geoDataDf, exprGeo, “left”, 200)<br/> .saltedJoin(userAgentDf, exprUserAgent, “left”, 200)<br/> .saltedJoin(ownerMetadataDf, exprOwnerMetadata, “left”, 200)<br/> .write<br/> .parquet(“s3://...”)</span></pre><p id="87e8" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">现在我们可以运行并查看结果</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oy"><img src="../Images/db832ac171d91941c547c899a5bc16c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fKc3lLG8VwZE5YiIhEQN2Q.png"/></div></div></figure><p id="c488" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">与之前使用Sort Merge join的运行相比，我们可以看到从1.2小时到26分钟的显著改进。这并不令人印象深刻，虽然在我们看到广播加入行动，5.2分钟做完全相同的工作。让我们看看数据是如何分布的</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/cd7912621fe6b2bd97050d68aded385d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rp1lCubj1sK50sgyLg_oyg.png"/></div></div></figure><p id="9fda" class="pw-post-body-paragraph la lb iq lc b ld mj lf lg lh mk lj lk ll mm ln lo lp mo lr ls lt mq lv lw lx ij bi translated">分布在中间值周围相当对称。大概可以通过增加salt值和<strong class="lc ir">spark . SQL . shuffle . partitions .</strong>来改善</p><h1 id="d35f" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">摘要</h1><p id="22fc" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">数据不对称的处理取决于许多参数，如数据量、数据种类、集群配置和处理目标。因此，没有简单的方法可以做到这一点。你必须检查你的数据，代码，并决定什么最适合你。但是这里有一些事实可以帮助你做决定。</p><ul class=""><li id="782a" class="ok ol iq lc b ld mj lh mk ll om lp on lt oo lx pa oq or os bi translated"><strong class="lc ir">可扩展性- </strong>广播加入解决方案不可扩展。它对广播表的大小有硬编码限制。带键的排序合并连接可以缩放到任何大小。</li><li id="8613" class="ok ol iq lc b ld ot lh ou ll ov lp ow lt ox lx pa oq or os bi translated"><strong class="lc ir">可靠性- </strong>当输入大小变化较大时，广播连接不可靠。每次当输入大小改变时，都需要仔细调整执行器和驱动程序设置，否则当输入大小增加时，您将会看到许多OOM异常，当输入大小减少时，您将会非常浪费集群资源。带有键加盐的排序合并连接更可靠。</li><li id="f813" class="ok ol iq lc b ld ot lh ou ll ov lp ow lt ox lx pa oq or os bi translated"><strong class="lc ir">性能- </strong>如果配置得当，广播连接将优于排序合并连接。其主要原因是，它只移动连接的一端，通常是较小的一端，因此网络操作较少。排序合并连接，在连接之前对两个数据框进行排序。</li><li id="a39b" class="ok ol iq lc b ld ot lh ou ll ov lp ow lt ox lx pa oq or os bi translated"><strong class="lc ir">可维护性- </strong>广播加入更容易维护，因为不需要额外的代码。只需在Apache Spark的join操作中添加一个“broadcast”提示，就万事大吉了。</li></ul></div></div>    
</body>
</html>