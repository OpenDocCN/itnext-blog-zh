<html>
<head>
<title>Introduction to Natural Language Processing: NLP Tools For Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理简介:Python的NLP工具</h1>
<blockquote>原文：<a href="https://itnext.io/introduction-to-natural-language-processing-nlp-tools-for-python-cf39af3cfc64?source=collection_archive---------2-----------------------#2021-05-01">https://itnext.io/introduction-to-natural-language-processing-nlp-tools-for-python-cf39af3cfc64?source=collection_archive---------2-----------------------#2021-05-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/717ded4c113918081310e228b3392993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WV1DcFAGKZ2xt2uj"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">由<a class="ae kf" href="https://unsplash.com/@barnimages?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">谷仓图片</a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="ee4b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="5939" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">关于<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">【NLP】</strong></a><strong class="lg iu">(自然语言处理)</strong><a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/nlp-named-entity-recognition-ner-with-spacy-and-python-dabaf843cab2">以前的</a>文章里我已经<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/nlp-named-entity-recognition-ner-with-spacy-and-python-dabaf843cab2"> <strong class="lg iu">讲过了。在这一篇中，我的目标是<strong class="lg iu">总结</strong>并给与<strong class="lg iu"> Python </strong>一起工作的<strong class="lg iu"> NLP </strong>工程师可用的工具一个快速概述。</strong></a></p><p id="111b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我将从什么是NLP  以及它在AI  <strong class="lg iu">生态系统</strong>中的位置开始一个非常简短的概述。然后，我将谈论Python开发者可用的一些工具，我将提供一些基本的<strong class="lg iu">示例</strong>。在本文中，我将只关注<strong class="lg iu">文本</strong>数据，而不是音频或视频处理。</p><h1 id="02f5" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">什么是NLP？</h1><p id="5555" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">简单来说，NLP是<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">机器学习</strong> </a>专注于从<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">自然语言</strong> </a>中提取洞察的一个领域。你的目标是<strong class="lg iu">让计算机理解我们自己的语言</strong>。</p><p id="28f6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">自然语言处理的一些实际例子有语音识别、翻译、情感分析、主题建模、词汇分析、实体提取等等。</p><p id="bbf9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">使用所有这些工具和算法，你可以从自然语言中提取结构化数据，这些数据可以被计算机处理。此外，<strong class="lg iu"> NLP </strong>任务的输出通常是机器学习算法，该算法将使用这些原始数据来进行<strong class="lg iu">预测</strong>。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/7fd7edb0ba1161a1b62efb4e7f72d7a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xLRsbQ02J7sQpNNy"/></div></div></figure><p id="e527" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">通过<strong class="lg iu">将许多算法结合在一起</strong>，您可以提取有用的数据，这些数据可用于广泛的场景，例如:</p><ul class=""><li id="1018" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">欺诈检测</li><li id="2266" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://aylien.com/solutions/risk-intelligence" rel="noopener ugc nofollow" target="_blank">风险情报</a></li><li id="ce50" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">电子邮件分类</li><li id="4552" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://pythonspot.com/python-sentiment-analysis/" rel="noopener ugc nofollow" target="_blank">情绪分析</a></li></ul><h1 id="4185" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">常见NLP任务</h1><p id="73e6" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你大概听说过<strong class="lg iu">花在<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">机器学习</strong> </a>的时间80% </strong>是<strong class="lg iu">数据准备</strong>:数据清洗、<a class="ae kf" href="https://en.wikipedia.org/wiki/Data_wrangling" rel="noopener ugc nofollow" target="_blank">数据角力</a>，特征工程等。对于NLP来说尤其如此，因为我们的主要目标是将文本转换成计算机可以使用的数字。</p><h2 id="280f" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">数据准备</h2><p id="e21c" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这些任务包括初始准备，如加载文本数据，删除停用词，小写字母，删除页眉和页脚或任何其他无用的文本；还有更多。</p><p id="f078" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">熊猫</strong> </a>是这个探索阶段的必备工具，因为它允许你轻松地<strong class="lg iu">实验</strong>数据，并与其他团队成员和利益相关者分享你的见解和成果。</p><p id="e68b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">记号化</strong> </a>是另一个重要的任务，有很多记号化器可用，它的配置将取决于你要实现的目标。这个想法是将文本分割成单词，这些单词以后可以被量化并表示为数字。除了将文本拆分成单词之外，您可能还想将它们拆分成元组。<strong class="lg iu">句子分割</strong>是一个类似的过程，但目标是将文本分割成完整的句子。</p><p id="b3a2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">词法分析</strong>是自然语言处理中的一项重要任务，目标是将单词还原为其基本形式。有两种方法可以做到这一点:</p><ul class=""><li id="2463" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a><strong class="lg iu"/>词干化是将单词还原为基本形式的过程(<em class="nm">如</em>)，“close”将是“closed”、“closing”、“close”、“closer”等的词根。).我这样做是基于规则，而不是字典。换句话说，结果可能是也可能不是一个真正的单词，它更像是一个前缀。</li><li id="7a64" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">引理化</strong> </a> <strong class="lg iu"> </strong>的任务是仅移除屈折词尾，并返回单词的基本字典形式，也称为<strong class="lg iu">引理</strong>。换句话说，它从字典中产生一个真实的单词。</li></ul><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/31d3ce24f679c4f0e3833441e28f74ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*s8traf5FiOXIg6cL"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">词干化和词汇化的区别</figcaption></figure><p id="4b6d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">注意，根据你想要达到的目标<strong class="lg iu">，每种方法都有自己的优点和缺点</strong>。如果你不在乎单词的意思，而只想对文本进行分类，那么词干提取会更快更容易。</p><p id="e0ee" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一旦你已经阅读了文本数据，删除不必要的内容，删除停用词，小写的文本，标记的文本和减少单词的词条；您已经准备好应用NLP算法。请注意，根据您的任务和您计划使用的算法，数据准备会有所不同，一些任务(如词条化或小写字母)可能不需要和/或不推荐使用文本。</p><h2 id="0a4e" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">特征提取和文本分类</h2><p id="7295" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在机器学习中，给你一组列或<strong class="lg iu">特征</strong>，你用它们来预测一些结果。NLP的棘手之处在于能够从非结构化的文本中提取有意义的特征。这就是<strong class="lg iu">特征提取</strong>的目标；完成这些之后，您可以应用其他ML算法进行文本分类。</p><p id="a3d2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在NLP中，你还有<a class="ae kf" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">有监督的</a>和<a class="ae kf" href="https://en.wikipedia.org/wiki/Unsupervised_learning" rel="noopener ugc nofollow" target="_blank">无监督的</a>算法；简而言之，<strong class="lg iu">监督的</strong>算法需要<strong class="lg iu">标记的数据</strong>而非监督的不需要。我们将在本文后面讨论标记数据，但为了简单起见；让我们用一个简单的例子来解释这个过程:你想要检测SPAN邮件的图像。您已经标记了<a class="ae kf" href="https://www.kaggle.com/balaka18/email-spam-classification-dataset-csv" rel="noopener ugc nofollow" target="_blank">数据</a>，其中包含邮件正文以及邮件是否是span。</p><p id="19e5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在机器学习中，你通常会有一个列的列表(<strong class="lg iu">特征</strong>)和一个<strong class="lg iu">目标</strong> (Span/NotSpan)。例如，如果您试图预测一所房子的价格，您将拥有诸如卧室数量、大小、到学校的距离等列。这些都是可以转换成数字的数字或类别；并且可以容易地输入到机器学习算法中。但是我们如何用自然语言做到这一点呢？</p><p id="ee35" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> NLP特征提取</strong>算法用于将单词转换成包含足够信息的数字表示，以便将其输入到统计模型中。为此，我们将文本转换成一种称为<strong class="lg iu">特征向量</strong>的数字表示。<a class="ae kf" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">袋词模型</strong> </a>是实现这一点的简单方法。在这种情况下，我们只是将每个单词转换成一个数字，即该单词在语料库中出现的次数。例如，如果我们有:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="a917" class="na kh it np b gy nt nu l nv nw">'All my cats in a row',<br/>'When my cat sits down, she looks like a Furby toy!',</span></pre><p id="431a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这将被转换为:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="a02f" class="na kh it np b gy nt nu l nv nw">{'all': 0, 'cat': 1, 'cats': 2, 'down': 3, 'furby': 4, 'in': 5, 'like': 6, 'looks': 7, 'my': 8, 'row': 9, 'she': 10, 'sits': 11, 'toy': 12, 'when': 13 }</span></pre><p id="e226" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在我们有数字了！每个单词将是高维向量中的一列，该向量可以被馈送给<a class="ae kf" href="https://en.wikipedia.org/wiki/Neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>或任何其他算法，并用于分类/回归。我们还可以<a class="ae kf" href="https://iq.opengenus.org/normalization-in-detail/" rel="noopener ugc nofollow" target="_blank">归一化</a>这些值，以便为某些算法获得更好的结果。</p><p id="a4ee" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在我们的<em class="nm"> Span </em>示例中，机器算法可以“学习”如果某个词(如“buy ”)的数量很大，那么它将被Span。为了做到这一点，我们真的需要删除经常出现的停用词，并将词减少到其引理，这样它们就不会被计算在内，而是单独的特征。<strong class="lg iu">这就是数据准备如此重要的原因。</strong></p><p id="8b0a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">然而，词频是非常基本的，大多数时候，你会对出现在你的输入文本中但不在一般语料库中的词感兴趣。此时<a class="ae kf" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> TF-IDF </strong> </a>前来救援，定义如下:</p><blockquote class="nx ny nz"><p id="b9c4" class="le lf nm lg b lh mc lj lk ll md ln lo oa me lr ls ob mf lv lw oc mg lz ma mb im bi translated">TF-IDF中的高权重通过该术语在整个文档集合中的高术语频率(在给定文档中)和低文档频率来实现。</p></blockquote><p id="bf48" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它结合了两种算法:</p><p id="7325" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">词频</strong></p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="c10b" class="na kh it np b gy nt nu l nv nw"><strong class="np iu">TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)</strong></span></pre><p id="9ea4" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">逆文档频率</strong></p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="e1dd" class="na kh it np b gy nt nu l nv nw"><strong class="np iu">IDF(t) = log_e(Total number of documents / Number of documents with term t in it)</strong></span></pre><p id="bcab" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">将文本转换为矢量的另一种技术是<a class="ae kf" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">单词嵌入</strong> </a> <strong class="lg iu"> </strong>，它将单词<strong class="lg iu"> </strong>转换为n维矢量。像“汽车”和“车辆”这样的相关单词将映射到相似的n维向量，而像“狗”和“箭头”这样的单词在向量空间中将是遥远的。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi od"><img src="../Images/195931ecc0e08971fbed84a19fa68942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ls0YrGVjU2lwTM1b.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">自然语言处理中两种主要特征工程技术综述</figcaption></figure><p id="ca25" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">总之，<strong class="lg iu">我们使用NLP中的特征提取从文本中提取特征，</strong>因此它们可以被馈送到监督机器学习模型中用于<strong class="lg iu">文本分类</strong>。使用这些技术的一些例子是跨度检测或<a class="ae kf" href="https://en.wikipedia.org/wiki/Sentiment_analysis" rel="noopener ugc nofollow" target="_blank">情感分析</a>。稍后，我们将看到一些简化文本分类过程的库。</p><h2 id="c0c6" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">词性标注</h2><p id="c332" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">词性标注</strong> </a> ( <strong class="lg iu"> POS </strong>)是将句子中的词标注为名词、形容词、动词等的过程。这些算法自动用正确的标签标记文本数据的内容，其他算法可以使用输出来检测主题、相似性等。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a7adc35f4f3090883eb6827ae9411572.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/0*T7yST3osJjNvb-pq"/></div></figure><p id="6876" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Support_vector_machine" rel="noopener ugc nofollow" target="_blank"> SVM </a>、<a class="ae kf" href="https://en.wikipedia.org/wiki/Maximum_entropy_classifier" rel="noopener ugc nofollow" target="_blank">最大熵分类器</a>、<a class="ae kf" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm" rel="noopener ugc nofollow" target="_blank">最近邻法</a>等机器学习算法可以用于<strong class="lg iu"> POS </strong>，大部分可以达到95%以上的准确率。</p><h2 id="706c" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">命名实体识别(NER)</h2><p id="97de" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Named-entity_recognition" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>是一种<strong class="lg iu">监督</strong>技术，用于将<a class="ae kf" href="https://en.wikipedia.org/wiki/Unstructured_data" rel="noopener ugc nofollow" target="_blank">非结构化文本</a>中提到的实体分类成预定义的类别，如人名、组织、地点、<a class="ae kf" href="https://en.wikipedia.org/wiki/Medical_classification" rel="noopener ugc nofollow" target="_blank">医疗代码</a>、时间表达式、数量、货币值、百分比等。目标是<strong class="lg iu">检测你的文本中的类别，这样你就可以提取关于文本内容的见解</strong>或者它是否在谈论某个主题。这对于<strong class="lg iu">风险检测</strong>、分类、欺诈检测等非常有用。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/7582047184199f5304df33639935e0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7wXfu45SENan5ofl"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">NER在行动中检测人员，日期，组织…</figcaption></figure><p id="61ce" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这篇<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/nlp-named-entity-recognition-ner-with-spacy-and-python-dabaf843cab2"> <strong class="lg iu">文章</strong> </a>中，我给出了一个完整的<strong class="lg iu"> NER </strong>的端到端例子。</p><h2 id="a7f6" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">主题建模</h2><p id="1f5d" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">主题建模</strong> </a>是一种<strong class="lg iu">无监督</strong>方法，用于发现文档集合中出现的抽象“主题”。它可以用于文本挖掘或发现隐藏的语义结构。主题建模技术产生的“主题”是一组相似的单词。</p><p id="ee50" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> LDA </strong> </a>算法常用于主题建模。像其他非监督算法一样，你需要预先选择你想要提取多少主题。</p><h1 id="c3c1" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">NLP工具</h1><p id="41e9" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">到目前为止，我们似乎可以获取原始文本数据，加载它，清理它，标记它，最后执行特征工程，将文本转换为数字，以便它可以用于实体提取或分类。现在，我们将回顾Python中可用的一些工具，<strong class="lg iu">我们将从更通用/低级的工具开始，然后转向更专业和更高级别的库和工具。</strong></p><h2 id="302d" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">sci kit-学习</h2><p id="ce5e" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>是一个著名的<strong class="lg iu">Python通用ML库</strong>。它广泛用于各种各样的机器学习任务，如<a class="ae kf" href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning" rel="noopener ugc nofollow" target="_blank">分类</a>、<a class="ae kf" href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning" rel="noopener ugc nofollow" target="_blank">回归</a>或<a class="ae kf" href="https://scikit-learn.org/stable/modules/clustering.html#clustering" rel="noopener ugc nofollow" target="_blank">聚类</a>。还可以通过连接多个模型来构建管道。</p><p id="3d42" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为了<strong class="lg iu">安装</strong>和<a class="ae kf" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>运行:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="ceeb" class="na kh it np b gy nt nu l nv nw">pip install -U scikit-learn</span></pre><p id="b234" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>附带了许多<a class="ae kf" href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> NLP特性</strong> </a>包括分词器、词袋、词频、分类器等等。例如，要处理文本、标记化、删除停用词并使用词袋构建特征向量，我们可以使用<code class="fe og oh oi np b"><a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">CountVectorizer</strong></a></code>来一气呵成地完成所有这些工作:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="0455" class="na kh it np b gy nt nu l nv nw"><strong class="np iu">from</strong> <strong class="np iu">sklearn.feature_extraction.text</strong> <strong class="np iu">import</strong> CountVectorizer<br/>count_vect = CountVectorizer()<br/>X_train_counts = count_vect.fit_transform(email.data)</span></pre><p id="3db6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">结果是一个带有文本数据数字表示的向量，可以用作分类器的输入。<code class="fe og oh oi np b">email.data</code>包含邮件正文。</p><p id="3719" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它还带有<code class="fe og oh oi np b"><a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">TfidfTransformer</strong></a></code> <strong class="lg iu"> </strong>，使用<strong class="lg iu"> tf-idf </strong>算法实现相同的方法:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="3d45" class="na kh it np b gy nt nu l nv nw"><strong class="np iu">from</strong> <strong class="np iu">sklearn.feature_extraction.text</strong> <strong class="np iu">import</strong> TfidfTransformer<br/>X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)</span></pre><p id="0a1f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们可以很容易地训练一个模型来进行预测。首先，我们训练一个模型，例如<strong class="lg iu">多项式</strong></p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="3473" class="na kh it np b gy nt nu l nv nw"><strong class="np iu">from</strong> <strong class="np iu">sklearn.naive_bayes</strong> <strong class="np iu">import</strong> MultinomialNB<br/>clf = MultinomialNB().fit(X_train_tfidf, email_span.target)</span></pre><p id="7d2a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">目标列包含电子邮件是否是Span。上面的代码训练我们的模型。现在我们可以预测:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="f446" class="na kh it np b gy nt nu l nv nw">emails_test = ['Buy this', 'OpenGL on the GPU is fast']<br/>X_new_counts = count_vect.transform(emails_test)<br/>X_new_tfidf = tfidf_transformer.transform(X_new_counts)<br/>predicted = clf.predict(X_new_tfidf)</span></pre><p id="8236" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Scikit-learn是一个非常著名且易于使用的库，是你工具箱中的必备工具。我推荐这个库，因为你已经用它来完成其他的ML任务，并且你想增加一些NLP功能。然而，还有其他更专业的自然语言处理库。</p><h2 id="38f0" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">NLTK</h2><p id="2c4e" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> NLTK </strong> </a>是著名的NLP库。它不仅仅是一个库，它是一个平台，不仅提供库，还提供简单易用的接口来连接超过50个语料库和词汇资源。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oj"><img src="../Images/5aeab33085542d080f5afb82335fd002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6XBf5OGGujaeppP9"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">NLTK功能</figcaption></figure><p id="99e2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它拥有用于分类、标记化、词干化、标记、解析和语义推理的文本处理库，用于工业级自然语言处理库的包装器，以及一个活跃的论坛。</p><p id="7553" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">要安装NLTK运行:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="15b8" class="na kh it np b gy nt nu l nv nw">pip install --user -U nltk</span></pre><p id="8c49" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">建议您也下载一些语料库来运行:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="aa2f" class="na kh it np b gy nt nu l nv nw"><strong class="np iu">import</strong> <strong class="np iu">nltk</strong><br/>nltk.download()</span></pre><p id="e892" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">应该会打开一个新窗口，显示NLTK下载程序。您可以选择并下载许多易于导入的数据集。</p><p id="3e31" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">得益于大量的<strong class="lg iu">语料库数据</strong>，NLTK在<strong class="lg iu">学术界</strong>中非常<strong class="lg iu">受欢迎</strong>，因为你可以很容易地迭代并使用容易访问的数据集处理你的模型。它还提供了许多工具和算法，例如，要显示一个词汇树，你可以简单地运行:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="4a61" class="na kh it np b gy nt nu l nv nw"><strong class="np iu">from</strong> <strong class="np iu">nltk.corpus</strong> <strong class="np iu">import</strong> treebank<br/>t = treebank.parsed_sents('wsj_0001.mrg')[0]<br/>t.draw()</span></pre><p id="8d72" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它还支持词干/词汇化和<a class="ae kf" href="https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/" rel="noopener ugc nofollow" target="_blank">词性标注</a>。查看<a class="ae kf" href="https://likegeeks.com/nlp-tutorial-using-python-nltk/" rel="noopener ugc nofollow" target="_blank">这篇</a> <a class="ae kf" href="https://likegeeks.com/nlp-tutorial-using-python-nltk/" rel="noopener ugc nofollow" target="_blank">文章</a>以了解有关NLTK功能的更多信息。我向那些想学习和尝试自然语言处理的人推荐这个库，但是它不是我生产就绪自然语言处理的首选。</p><h2 id="975c" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">空间</h2><p id="2a6d" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">T15】SpacyT17】是我最喜欢的<strong class="lg iu">NLP</strong>T20】库。它是一个开源的库，致力于使执行任何NLP任务变得非常容易。不像</a><a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_Language_Toolkit" rel="noopener ugc nofollow" target="_blank"> NLTK </a>用在学术上；<strong class="lg iu"> Spacy是为现实世界的使用</strong>而构建的，这得益于其出色的<strong class="lg iu">性能</strong>，大量的<strong class="lg iu">优化</strong>和<strong class="lg iu">易于使用的</strong>API。</p><p id="db92" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它支持许多自然的<a class="ae kf" href="https://spacy.io/usage/models#languages" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">语言</strong> </a>开箱即用，它提供许多<a class="ae kf" href="https://spacy.io/usage/models#download" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">训练好的管道</strong> </a>随时可以用作你的起点。它提供了一个很棒的<a class="ae kf" href="https://spacy.io/usage/linguistic-features#tokenization" rel="noopener ugc nofollow" target="_blank">标记器</a>和许多<a class="ae kf" href="https://spacy.io/usage/linguistic-features" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">语言特性</strong> </a>，比如<a class="ae kf" href="https://spacy.io/usage/linguistic-features#pos-tagging" rel="noopener ugc nofollow" target="_blank">词性标注</a>、词汇化、<a class="ae kf" href="https://spacy.io/usage/linguistic-features#entity-linking" rel="noopener ugc nofollow" target="_blank">实体链接</a>、依存解析等等！</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/9ba2b4dbf1befb1d731eb13b07826589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45iFFsx-fduZwOKIdyHiMg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">空间特征:<a class="ae kf" href="https://spacy.io/usage/facts-figures" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage/facts-figures</a></figcaption></figure><p id="33c6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> SpaCy </strong>支持<a class="ae kf" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">深度学习</strong> </a> <strong class="lg iu"> </strong>工作流，你可以在其中训练和部署神经网络模型，或者将由流行的<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>库训练的外部模型，如<a class="ae kf" href="https://en.wikipedia.org/wiki/TensorFlow" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>或<a class="ae kf" href="https://en.wikipedia.org/wiki/PyTorch" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>插入到你的管道中。</p><p id="53ad" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我真正喜欢SpaCy的是它的入门如此简单！</p><p id="74a2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">安装空间</strong></p><p id="714e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">有关基于您的平台的详细<a class="ae kf" href="https://spacy.io/usage" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">安装说明</strong> </a>，请参考文档。使用画中画时，只需:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="ab3b" class="na kh it np b gy nt nu l nv nw">pip install -U pip setuptools wheel<br/>pip install -U spacy<br/>python -m spacy download en_core_web_sm</span></pre><p id="578c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">由于<strong class="lg iu">预训练管道</strong>是使用大型文本语料库训练的，所以很容易开始。首先，你需要导入Spacy，然后你可以加载一个预先训练好的模型，有<a class="ae kf" href="https://spacy.io/models/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">不同尺寸</strong> </a> <strong class="lg iu"> </strong>可供选择；然后你可以创建你的<strong class="lg iu"> <em class="nm"> nlp </em> </strong>对象包含你的原始数据:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="26bd" class="na kh it np b gy nt nu l nv nw">import spacy</span><span id="426e" class="na kh it np b gy ol nu l nv nw">nlp = spacy.load("en_core_web_sm")<br/>doc = nlp("Apple is looking at buying U.K. startup for $1 billion")</span></pre><p id="3ac1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> SpaCy </strong>为您处理所有的数据准备和标记化，这是在您调用<strong class="lg iu"> <em class="nm"> nlp </em> </strong>对象时完成和计算的，该对象接受一个文本字符串，并返回一个包含标记化文本、POS注释、命名实体等等的已处理的<code class="fe og oh oi np b">Doc</code>。即使对一个<code class="fe og oh oi np b">Doc</code>进行了处理(拆分成单个单词并进行注释),它仍然保存着<strong class="lg iu">原始文本的所有信息。</strong></p><p id="e6b2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们可以很容易地遍历令牌:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="3761" class="na kh it np b gy nt nu l nv nw">for token in doc:<br/>    print(token.text)</span></pre><p id="bbb5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这将打印令牌列表。</p><p id="7cbc" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">位置标记</strong></p><p id="2d10" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> POS tagging </strong> </a>也是为你计算的:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="e5c7" class="na kh it np b gy nt nu l nv nw">for token in doc:<br/>    print(token.text, token.pos_, token.dep_)</span></pre><p id="45ba" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这将打印:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="cf5f" class="na kh it np b gy nt nu l nv nw">Apple PROPN nsubj<br/>is AUX aux<br/>looking VERB ROOT<br/>at ADP prep<br/>buying VERB pcomp<br/>U.K. PROPN dobj<br/>startup NOUN advcl<br/>for ADP prep<br/>$ SYM quantmod<br/>1 NUM compound<br/>billion NUM pobj</span></pre><p id="bb75" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">词向量相似度</strong></p><p id="5cde" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">相似度是通过比较“<a class="ae kf" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>”词语嵌入量来确定的。可以使用像<a class="ae kf" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>这样的算法生成单词向量。大多数SpaCy管道包都带有内置的单词向量，使它们可以作为<code class="fe og oh oi np b"><a class="ae kf" href="https://spacy.io/api/token#vector" rel="noopener ugc nofollow" target="_blank">Token.vector</a></code>属性使用。例如给出下面的句子:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="b73e" class="na kh it np b gy nt nu l nv nw">tokens = nlp("dog cat banana afskfsd")</span><span id="ae31" class="na kh it np b gy ol nu l nv nw">for token in tokens:<br/>    print(token.text, token.has_vector, token.vector_norm)</span></pre><p id="5224" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们得到这样的输出:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="5047" class="na kh it np b gy nt nu l nv nw">dog True 7.0336733 False<br/>cat True 6.6808186 False<br/>banana True 6.700014 False<br/>afskfsd False 0.0 True</span></pre><p id="ad24" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">预测相似性对于建立推荐系统或者标记重复是有用的。</p><p id="8a10" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">命名实体识别(NER) </strong></p><p id="cbd5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">使用SpaCy提取实体非常简单:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="769b" class="na kh it np b gy nt nu l nv nw">nlp = spacy.load("en_core_web_sm")<br/>doc = nlp("Apple is looking at buying U.K. startup for $1 billion")</span><span id="137e" class="na kh it np b gy ol nu l nv nw">for ent in doc.ents:<br/>    print(ent.text, ent.start_char, ent.end_char, ent.label_)</span></pre><p id="32f2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这将输出:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="89ad" class="na kh it np b gy nt nu l nv nw">Apple 0 5 ORG<br/>U.K. 27 31 GPE<br/>$1 billion 44 54 MONEY</span></pre><p id="1cad" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如您所见，SpaCy提供了一个非常简单的API和<strong class="lg iu">强大的预训练管道</strong>，允许开发人员轻松构建生产就绪的NLP解决方案。</p><p id="4ee1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">但是我如何训练模型来检测其他实体呢？我在这篇文章<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/nlp-named-entity-recognition-ner-with-spacy-and-python-dabaf843cab2"><strong class="lg iu"/></a><strong class="lg iu"/>中解释了这个过程，但是基本上你需要做的就是<strong class="lg iu">使用一些将在下一节介绍的工具给数据</strong>加标签，<strong class="lg iu">训练模型</strong>，最后像使用预训练模型一样使用。这是代码:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="c047" class="na kh it np b gy nt nu l nv nw">import spacy<br/>import random<br/>import json</span><span id="8d7a" class="na kh it np b gy ol nu l nv nw">nlp = spacy.blank("en")<br/>ner = nlp.create_pipe("ner")<br/>nlp.add_pipe(ner)<br/>ner.add_label("OIL")</span><span id="824f" class="na kh it np b gy ol nu l nv nw"># Start the training<br/>nlp.begin_training()</span><span id="6df1" class="na kh it np b gy ol nu l nv nw"># Loop for 40 iterations<br/>for itn in range(40):<br/>    # Shuffle the training data<br/>    random.shuffle(TRAINING_DATA)<br/>    losses = {}</span><span id="85d7" class="na kh it np b gy ol nu l nv nw">    # Batch the examples and iterate over them<br/>    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):<br/>        texts = [text for text, entities in batch]<br/>        annotations = [entities for text, entities in batch]</span><span id="3506" class="na kh it np b gy ol nu l nv nw">        # Update the model<br/>        nlp.update(texts, annotations, losses=losses, drop=0.3)<br/>    print(losses)</span></pre><p id="1948" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这个例子中，我们使用<em class="nm"> TRAINING_DATA </em>对象作为输入，该对象包含带有训练数据的句子，其中标注了与<em class="nm"> OIL </em>相关的单词。然后，我们使用深度学习算法(神经网络)通过几次迭代来训练模型。现在我们可以将模型保存到磁盘:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="52a5" class="na kh it np b gy nt nu l nv nw">nlp.to_disk("oil.model")</span></pre><p id="0f4e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">并像其他模型一样加载它。我们可以像使用预训练模型一样使用它:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="4b75" class="na kh it np b gy nt nu l nv nw">for ent in doc.ents:<br/>    print(ent.text, ent.start_char, ent.end_char, ent.label_)</span></pre><p id="05a3" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">或者使用<a class="ae kf" href="https://explosion.ai/demos/displacy" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">显示</strong> </a>库获得更好的可视化效果:</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/874ad7c5e48e0b00f0fe6b94eeefb998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_lWEUlFgXiFqEHYs.png"/></div></div></figure><p id="8d33" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">模式匹配</strong></p><p id="f80e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> Spacy </strong>还提供了<strong class="lg iu">匹配器</strong>，可以很容易地用来查找特定的子字符串、数字等。我们还可以根据词性标签设置规则。</p><p id="6308" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是代码:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="941b" class="na kh it np b gy nt nu l nv nw">import spacy</span><span id="647f" class="na kh it np b gy ol nu l nv nw"># Import the Matcher<br/>from spacy.matcher import Matcher</span><span id="8e25" class="na kh it np b gy ol nu l nv nw">nlp = spacy.load("en_core_web_sm")<br/>doc = nlp(example)</span><span id="720e" class="na kh it np b gy ol nu l nv nw"># Initialize the Matcher with the shared vocabulary<br/>matcher = Matcher(nlp.vocab)</span><span id="4a5a" class="na kh it np b gy ol nu l nv nw"># Add the pattern to the matcher<br/>matcher.add("OIL_PATTERN", None, [{"LOWER": "oil"}], [{"LOWER": "petroleum"}])</span><span id="2bd7" class="na kh it np b gy ol nu l nv nw"># Use the matcher on the doc<br/>matches = matcher(doc)<br/>print("Matches:", [doc[start:end].text for match_id, start, end in matches])</span></pre><p id="b8eb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">你应该会看到印在笔记本上的火柴。</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="08e0" class="na kh it np b gy nt nu l nv nw">Matches: ['petroleum', 'oil']</span></pre><h2 id="5e57" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">根西姆</h2><p id="7186" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">根思</strong> </a> <strong class="lg iu"> m </strong>是一个强大的专注于<a class="ae kf" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">主题</strong> <strong class="lg iu">造型</strong> </a>的库。它提供了许多算法，如<a class="ae kf" href="https://towardsdatascience.com/nlp-with-lda-latent-dirichlet-allocation-and-text-clustering-to-improve-classification-97688c23d98" rel="noopener" target="_blank"> LDA </a>记忆，它是一个<strong class="lg iu">无监督</strong>算法。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi on"><img src="../Images/5a4aca0c739ea35ebae345410ebcf7f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*0ibksziKCdbrPZaK8TSSWQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">https://radimrehurek.com/gensim/的例子</figcaption></figure><p id="8753" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><code class="fe og oh oi np b">gensim</code>的核心概念是:</p><ul class=""><li id="e59f" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-document" rel="noopener ugc nofollow" target="_blank">文件</a>:输入文本。</li><li id="1713" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-corpus" rel="noopener ugc nofollow" target="_blank">文集</a>:文件的集合。</li><li id="8921" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-vector" rel="noopener ugc nofollow" target="_blank"> Vector </a>:文档的数学上的方便表示。</li><li id="c24c" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-model" rel="noopener ugc nofollow" target="_blank">模型</a>:将向量从一种表示转换成另一种表示的算法。</li></ul><p id="d690" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是我们已经看过的核心概念。为了使用<strong class="lg iu"> LDA </strong>进行主题建模，首先我们需要从数据中创建一个<strong class="lg iu">字典</strong>，然后将其转换为<a class="ae kf" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词袋</a>模型:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="78ac" class="na kh it np b gy nt nu l nv nw">from gensim import corpora</span><span id="59f9" class="na kh it np b gy ol nu l nv nw">dictionary = corpora.Dictionary(input_text)<br/>corpus = [dictionary.doc2bow(text) for text in input_text]</span></pre><p id="4285" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">然后我们可以使用LDA:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="d37b" class="na kh it np b gy nt nu l nv nw">import gensim<br/>NUM_TOPICS = 2 # how many topic we want to extract</span><span id="6a30" class="na kh it np b gy ol nu l nv nw">ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=12)</span><span id="c12e" class="na kh it np b gy ol nu l nv nw">topics = ldamodel.print_topics(num_words=5)<br/>for topic in topics:<br/>    print(topic)</span></pre><p id="9e99" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">正如我们前面提到的，我们需要事先选择要提取多少主题，然后我们可以使用前面创建的语料库和字典来创建模型。代码将打印两个主题，每个主题有5个示例单词。</p><p id="a34e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们可以使用<a class="ae kf" href="https://pypi.python.org/pypi/pyLDAvis/2.1.1" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> pyLDAvis </strong> </a>这个神奇的库来可视化结果:</p><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="520a" class="na kh it np b gy nt nu l nv nw">import pyLDAvis.gensim</span><span id="795f" class="na kh it np b gy ol nu l nv nw">lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)</span><span id="ff70" class="na kh it np b gy ol nu l nv nw">pyLDAvis.display(lda_display)</span></pre><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/b32729f91dbab18b45a07f8304293b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dFuba-btYZnqFQmEg4eOoQ.png"/></div></div></figure><p id="8890" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">检查一下这个<a class="ae kf" href="https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb#topic=0&amp;lambda=1&amp;term=" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">笔记本</strong> </a>上的<a class="ae kf" href="https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb#topic=0&amp;lambda=1&amp;term=" rel="noopener ugc nofollow" target="_blank">演示</a><a class="ae kf" href="https://pypi.python.org/pypi/pyLDAvis/2.1.1" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> pyLDAvis </strong> </a>功能。</p><h1 id="3689" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">数据标记</h1><p id="e305" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="lg iu">对于文本分类或NER等监督算法，您需要标记您的文本数据。你将需要一些工具来帮助你完成这项任务。<em class="nm">让我们回顾一下这些工具……</em></strong></p><h2 id="fb30" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">多卡诺</h2><p id="1a9e" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://doccano.herokuapp.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Doccano </strong> </a>是一款基于web、<strong class="lg iu">开源的</strong>注释工具。Doccano使您能够让它自托管，这提供了更多的控制以及根据您的需要修改代码的能力。它支持不同的团队，并且非常容易使用。你可以试试这里的 一个<strong class="lg iu">演示</strong> <a class="ae kf" href="https://doccano.herokuapp.com/demo/named-entity-recognition/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">。</strong></a></p><p id="0bc1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">查看我之前的<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/nlp-named-entity-recognition-ner-with-spacy-and-python-dabaf843cab2"> <strong class="lg iu">文章</strong> </a>了解更多关于Doccano的细节。你可以按照<a class="ae kf" href="https://doccano.github.io/doccano/tutorial/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Doccano </strong> </a>的说明来安装这个开源工具。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi op"><img src="../Images/c15cb7d7e5c7f5524d7eec14bb425aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9XKG2jbpyXliiWgc.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">用Doccano标记数据</figcaption></figure><h2 id="95e1" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">奇迹ˌ奇事</h2><p id="cdb2" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://prodi.gy/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">神童</strong> </a>由<a class="ae kf" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> SpaCy </strong> </a>背后的同一个团队打造。它是一个现代的注释工具，用于为机器学习模型创建训练和评估数据。</p><p id="e3e6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它由<a class="ae kf" href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">主动学习</strong> </a>提供动力，这意味着它提供<strong class="lg iu">半自动化</strong>。您可以从标记几个样本开始，主动学习模型将尝试为您学习和标记其余的数据集，因此您只能指出样本是否正确。此外，它会根据信息增益建议最佳样本，因此您不会在不会改进模型预测的样本上浪费时间。可以在这里  <strong class="lg iu">查看一个现场<strong class="lg iu"> demo </strong> <a class="ae kf" href="https://prodi.gy/demo" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">。</strong></a></strong></p><p id="6818" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">使用Prodigy，您可以在快速和<strong class="lg iu">迭代过程中标记和训练模型</strong>消除大量手工工作。它合并了标记和训练过程，因此专家可以用有用和有意义的方式标记数据，而不是外包标记过程，浪费大量时间来标记不必要的文本样本。这样你可以很快尝试新想法。</p><p id="80f1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Prodigy可以用来标记命名实体为<strong class="lg iu"> NER </strong>，文本为分类，甚至图像，视频和声音！。它附带了许多"<a class="ae kf" href="https://prodi.gy/docs/recipes" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">配方</strong> </a>"这些都是执行某个任务的工作流程，查看这个<a class="ae kf" href="https://prodi.gy/prodigy_flowchart_ner-36f76cffd9cb4ef653a21ee78659d366.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">流程图</strong> </a>来检查它们。</p><p id="297d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">简单地说过程如下:</strong></p><ul class=""><li id="bc42" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">首先，您需要获得您的示例文本数据JSONL，其中每行包含一个条目，这是一个<a class="ae kf" href="https://raw.githubusercontent.com/explosion/prodigy-recipes/master/example-datasets/news_headlines.jsonl" rel="noopener ugc nofollow" target="_blank">示例</a>。</li><li id="fba5" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">下一步是开始手动注释，例如对于NER，您将在命令行中运行:</li></ul><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="1c2c" class="na kh it np b gy nt nu l nv nw">prodigy ner.manual ner_news_headlines blank:en ./news_headlines.jsonl --label PERSON,ORG,PRODUCT,LOCATION</span></pre><ul class=""><li id="a007" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">这将启动<code class="fe og oh oi np b">ner.manual</code> Prodigy recipe以便您可以开始贴标，这将打开一个web服务器，其Web UI位于:<a class="ae kf" href="http://localhost:8080" rel="noopener ugc nofollow" target="_blank"><em class="nm">http://localhost:8080</em></a></li><li id="d398" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">现在，您可以开始添加注释了:</li></ul><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/0073e83139c307a0432006fa5f97a0d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EBuij7m8sjUTMUHQJ5G3zg.png"/></div></div></figure><ul class=""><li id="9a94" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">在注释了一些示例后，您可以训练模型:</li></ul><pre class="mi mj mk ml gt no np nq nr aw ns bi"><span id="5dd8" class="na kh it np b gy nt nu l nv nw">prodigy train ner ner_news_headlines en_vectors_web_lg</span></pre><ul class=""><li id="b81d" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">第一个参数定义了要训练的组件:在本例中是<code class="fe og oh oi np b">ner</code>。您还需要传入数据集的名称、要从中训练的注释以及要开始的基本模型。在这个例子中，最后一个参数是使用一个大的<code class="fe og oh oi np b"><a class="ae kf" href="https://spacy.io/models/en#en_vectors_web_lg" rel="noopener ugc nofollow" target="_blank">en_vectors_web_lg</a></code> <a class="ae kf" href="https://spacy.io/models/en#en_vectors_web_lg" rel="noopener ugc nofollow" target="_blank">模型</a>作为基础模型。向量将在训练中用作特征，这可以大大提高准确性。如果你还没有安装矢量包，你可以通过<code class="fe og oh oi np b">spacy download en_vectors_web_lg</code>下载</li><li id="5f2c" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">根据结果指标，如<strong class="lg iu">准确度或</strong> <a class="ae kf" href="https://en.wikipedia.org/wiki/F-score" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> f值</strong> </a>，你可以决定下一步该怎么做。Prodigy有许多<a class="ae kf" href="https://prodi.gy/prodigy_flowchart_ner-36f76cffd9cb4ef653a21ee78659d366.pdf" rel="noopener ugc nofollow" target="_blank">方法</a>来帮助你，一些可以用来检测标记更多的数据是否会改善你的模型，其他的会尝试为你标记，所以你只需要接受或拒绝，其他的会从你的数据集中检测最有意义的例子，等等。查看此<a class="ae kf" href="https://prodi.gy/prodigy_flowchart_ner-36f76cffd9cb4ef653a21ee78659d366.pdf" rel="noopener ugc nofollow" target="_blank">图表</a>了解更多详情。</li></ul><p id="6cbe" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如你所见，Prodigy<strong class="lg iu">非常强大</strong>并且<strong class="lg iu">与SpaCy </strong>有很好的兼容性。这两个工具允许您为您的组织实现<strong class="lg iu">完整的端到端NLP解决方案</strong>。注意<strong class="lg iu"> Prodigy不是开源的</strong>，你需要获得一个许可，这样你就可以下载并在你的环境中安装它。</p><h2 id="d135" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">其他工具</h2><ul class=""><li id="2cdf" class="mm mn it lg b lh li ll lm lp or lt os lx ot mb mr ms mt mu bi translated"><a class="ae kf" href="https://www.tagtog.net/" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">TagTog</strong></a><strong class="lg iu">:</strong>基于web的文本标注工具，无需安装。它提供云产品。支持团队的组注释。它还具有机器学习能力:从以前的注释中学习，并自动生成类似的注释。</li><li id="7dc5" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://brat.nlplab.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">乳臭未干</strong> </a> <strong class="lg iu">:开源</strong>免费注释工具。</li></ul><h1 id="bb47" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><p id="27fd" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们可以尝试总结NLP，说<strong class="lg iu">它结合了一套工具和技术，将复杂的自然语言转换成机器可读的数据。</strong></p><p id="5763" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">特征工程</strong>和提取是<strong class="lg iu">自然语言处理</strong>中的关键步骤。像<strong class="lg iu"> TF-IDF </strong>和单词嵌入这样的技术被用来将单词转换成数字，这些数字可以被统计模型用于文本分类。<strong class="lg iu">命名实体提取(NER) </strong>是一个非常有用的算法，允许我们从文本数据中提取类别和其他见解。文本分类和NER都是<strong class="lg iu">监督的</strong>算法，你需要提供标签数据。对于文本分类，您将在目标列旁边提供文本字符串，例如Span/NoSpan。对于NER，您需要使用Prodigy等工具来标记标签。</p><p id="8202" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">主题建模</strong>可用于聚集文章内的相关主题，例如创建实时<a class="ae kf" href="https://aylien.com/blog/getting-started-with-real-time-topic-clustering" rel="noopener ugc nofollow" target="_blank">新闻内容主题集群</a>。这是一个<strong class="lg iu">无监督</strong>算法。</p><p id="5d79" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如你所见，像任何机器学习任务一样，<strong class="lg iu">大部分工作都是准备和标记数据</strong>，这一部分不仅耗时，而且<strong class="lg iu">至关重要</strong>。对于自然语言处理来说，正确标记数据并对其进行适当的预处理是获得好结果的最重要的因素。</p><p id="9b4c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">许多组织花费大量资源建立复杂的模型来改善结果，而不是关注数据。好的<strong class="lg iu">数据准备和特征工程</strong>可以比任何ML算法更快更好的提升模型性能。</p><p id="b596" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果你喜欢这篇文章，记得鼓掌，并关注我的更多更新！</p></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><p id="d96d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我希望你喜欢这篇文章。欢迎发表评论或分享这篇文章。跟随<a class="ae kf" href="https://twitter.com/JavierRamosRod" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"><em class="nm">me</em></strong></a><strong class="lg iu"><em class="nm"/></strong><em class="nm">进行未来岗位。</em></p></div></div>    
</body>
</html>