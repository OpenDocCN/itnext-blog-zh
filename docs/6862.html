<html>
<head>
<title>Vector Processing on CPUs and GPUs Compared</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CPU和GPU上的向量处理比较</h1>
<blockquote>原文：<a href="https://itnext.io/vector-processing-on-cpus-and-gpus-compared-b1fab24343e6?source=collection_archive---------0-----------------------#2022-03-26">https://itnext.io/vector-processing-on-cpus-and-gpus-compared-b1fab24343e6?source=collection_archive---------0-----------------------#2022-03-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b18e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">SIMD、CUDA、SSE、MMX、SVE2和RVV，这些并行处理的方法有何不同？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/10892ed490e0a6278b072d367693526d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1cWhK3QlIxZi714-KgsGg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">Nvidia Hopper架构</figcaption></figure><p id="1121" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">现代CPU和GPU都可以并行处理大量数据，那么它们到底有什么不同呢？这个问题变得越来越重要，因为我们看到Arm处理器添加了可扩展的矢量扩展，英特尔和AMD将AVX添加到x86微处理器架构中，而RISC-V最近正式推出了RISC-V矢量扩展。</p><p id="6c71" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">苹果正在为他们的M1片上系统(SoC)添加神经引擎，我们最近看到英伟达发布了他们新的Hopper H100 GPU架构。所有这些系统的共同点是，它们旨在通过利用数据并行性来提高性能。这些系统的核心是SIMD系统，单指令多数据。这与如下图所示的单指令单数据(SISD)的经典计算机体系结构形成对比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/f87866ada1a49989866456c4a23f742f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*VineM-3sao5qn76JYeWhMQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">单指令多数据和单指令单数据处理的区别。</figcaption></figure><p id="75ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你在这里看到的指令有加、乘、减、移和除。数据是我们正在执行这些操作的数字。</p><p id="1147" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个故事比计划的要长得多，所以我将给出一些要点和每个主要部分的简短描述:</p><ul class=""><li id="0197" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><strong class="la iu">简单RISC微处理器的操作</strong> —解释简单RISC处理器如何执行指令，与SIMD指令的执行方式进行对比。</li><li id="bf75" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><strong class="la iu">SIMD指令如何工作</strong>——看看一些已知的SIMD指令集的代码示例，并尝试通过一些图表来理解它们是如何工作的。</li><li id="b7f3" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><strong class="la iu">从压缩SIMD到向量处理</strong> —如何在旧的Cray计算机以及现代指令集扩展(如ARM的SVE2和RISC-V的V扩展)中进行向量处理。</li><li id="8d54" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><strong class="la iu">在多个内核之间划分任务</strong> —如果您想要构建数百个支持SIMD的内核，该怎么办？您如何管理这么多内核上的工作负载？</li><li id="d173" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><strong class="la iu">单指令多线程——SIMT</strong>——英伟达等公司的现代显卡和人工智能加速器如何解决在具有类似SIMD功能的大量内核之间分配任务的问题。</li><li id="27f7" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><strong class="la iu">组织和管理经线中的线程</strong> —解释GPU编程中的线程概念以及它与普通CPU线程的不同之处。</li><li id="5612" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><strong class="la iu">GPU硬件需要这么复杂吗？</strong> —对源自图形硬件的矢量处理和基于SIMT的处理的复杂性差异的比较和思考。</li></ul><h1 id="2c4c" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">简单RISC微处理器的操作</h1><p id="e2b2" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">微处理器由不同的功能单元组成，用灰色方框表示。每个灰框以某种方式处理数据。当数据被处理时，它必须通过数据总线(蓝色箭头)或地址总线(绿色箭头)传送到另一个功能单元。您可以将彩色箭头想象为发送电信号的铜迹线或电线，将灰色方框想象为协同工作以完成特定任务的一组晶体管。</p><p id="1bab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">澄清这一点只是让你思考的一种方式。由硅片制成的微芯片当然不像你在印刷电路板上看到的那样有铜迹线。</p><p id="0fc2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">CPU的核心是算术逻辑单元(ALU ),它就像微处理器的计算器。它从寄存器中读取加减或移位的数字。在一个典型的RISC处理器中，你有32个寄存器，每个寄存器可以保存一个数字。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/6fd0bbf8a3b260294923b32919487293.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Hk5-BFEokBZpSrX91_bhow.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">一种简单RISC微处理器(CPU)的体系结构</figcaption></figure><p id="77cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了准确地决定ALU执行什么算术运算，必须通过控制线(红色)来指示该做什么，控制线是用于开启或关闭不同功能的电信号。<em class="np">解码器</em>单元负责切换这些控制线。解码器通过从<em class="np">指令寄存器</em>中读取指令，计算出开启或关闭哪条控制线。我们通过从存储程序的内存中一个接一个地读取指令来将指令存入指令寄存器。</p><p id="920a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了程序计数器如何告诉内存要读取什么地址，以及该地址的数据如何发送到指令寄存器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/57f1ef19d66f0cc4cf580906d3c53054.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Zli67-aMk_l4WTugpuwdCg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">如何从CPU的内存中取出指令。</figcaption></figure><p id="4fd0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这给了你一些关于CPU如何正常运行的背景知识。它将数据混洗，目的是向ALU提供数据。然后，结果被发送回寄存器，接下来另一条指令可能会将其存储到内存位置。</p><p id="c17b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个简单的RISC-V汇编代码程序，显示了类似的东西。正如你所看到的，每条指令都是非常基本的。<code class="fe nq nr ns nt b">LI</code> (Load Immediate)将一个数载入寄存器。这就是它所做的一切。<code class="fe nq nr ns nt b">ADD</code>指示ALU将两个数相加。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="0484" class="ny mt it nt b gy nz oa l ob oc"># RISC-V Assembly code for adding numbers 42 and 12<br/><br/>LI  x2, 42      # store 42 in register x2<br/>LI  x3, 12      # store 12 in register x3<br/>ADD x4, x2, x3  # add x2 and x3, result in x4<br/>SW  x4, 90(x0)  # store result at memory location 90 + x0</span></pre><p id="1410" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一步是更好地理解我们如何以SIMD的方式从添加单对数字转移到添加多对数字。</p><h1 id="ada6" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">SIMD指令如何工作</h1><p id="909f" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">在一个简单的处理器中，寄存器和ALU的结构如下图所示。一些寄存器<code class="fe nq nr ns nt b">r1</code>和<code class="fe nq nr ns nt b">r2</code>用作输入，结果存储在另一个寄存器<code class="fe nq nr ns nt b">r2</code>中。当然，可以使用任何寄存器。根据建筑的不同，它们可能被命名为<code class="fe nq nr ns nt b">x0</code>、<code class="fe nq nr ns nt b">x1</code>、...，<code class="fe nq nr ns nt b">x31</code>或者它们可以是<code class="fe nq nr ns nt b">r0</code>，<code class="fe nq nr ns nt b">r1</code>，...，<code class="fe nq nr ns nt b">r15</code>如32位ARM架构上的情况。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/0f6e005c75d81e10d2e5609bf402dcb4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*icNKZRPM1q911oT7GpPKrg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">以SISD方式使用算术逻辑单元。</figcaption></figure><p id="a416" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了支持SIMD指令，我们在CPU中增加了更多的alu，并将寄存器分割成多个元素。因此，我们可以将一个32位寄存器拆分成两个16位元素，然后将这两个16位元素馈送给一个独立的alu。现在，我们突然能够将每个时钟周期执行的算术运算数量增加一倍。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/a4134269f1341fdd9f577a50dc06dc85.png" data-original-src="https://miro.medium.com/v2/format:webp/1*m6OId2hKGBKthKRPoEL6Og.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">使用多个alu来实现数据的SIMD处理。</figcaption></figure><p id="952a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们不需要局限于两个alu，我们可以添加更多。如果我们有四个alu，我们可以并行处理四个数字对。与ALU结合的每个元素对被称为SIMD通道。有了两个通道，我们可以处理两对数字。有了八个通道，我们可以并行处理八个数字。</p><p id="32eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以并行处理多少个数字受到通用寄存器或向量寄存器的位长度的限制。在某些CPU上，您可以对常规通用寄存器执行SIMD操作。在其他情况下，您使用特殊寄存器进行SIMD操作。</p><p id="89ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们以RISC-V为例，因为它提供了一个相当简单的指令集。我们将在RISC-V P扩展中使用<code class="fe nq nr ns nt b">ADD16</code>和<code class="fe nq nr ns nt b">ADD8</code>指令。</p><p id="577b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nq nr ns nt b">LW</code>(加载字)指令将在32位RISC-V处理器(RV32IP)上加载32位值。我们可以将这个值视为两个16位的值，并分别将它们相加。这就是<code class="fe nq nr ns nt b">ADD16</code>的作用。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="b351" class="ny mt it nt b gy nz oa l ob oc"># RISC-V Assembly: Add two 16-bit values.<br/><br/>LW    x2, 12(x0)   # x2 ← memory[x0 + 12]<br/>LW    x3, 16(x0)   # x3 ← memory[x0 + 16]<br/>ADD16 x4, x2, x3   <br/>SW    x4, 20(x0)   # x4 → memory[x0 + 20]</span></pre><p id="cb29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者，我们可以使用<code class="fe nq nr ns nt b">ADD8</code>,它会将我们从地址12和地址16加载的32位值视为四个8位值。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="46e0" class="ny mt it nt b gy nz oa l ob oc"># RISC-V Add four 8-bit values.<br/><br/>ADD8  x4, x2, x3</span></pre><p id="a10c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们使用Arm处理器，即使指令的语法略有不同，逻辑也会非常相似。下面是一个使用Arm的新SIMD指令和16个8位值的例子。注意，Arm使用向每个向量寄存器添加后缀的惯例(<code class="fe nq nr ns nt b">r0</code>，<code class="fe nq nr ns nt b">r1</code>，...<code class="fe nq nr ns nt b">r31</code>)来表示元素的大小和数量。因此，<code class="fe nq nr ns nt b">.16B</code>后缀意味着16个元素，<code class="fe nq nr ns nt b">B</code>意味着字节大小的元素。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="2b5e" class="ny mt it nt b gy nz oa l ob oc">; ARM Neon Add sixteen 8-bit values with (128-bit vector regs) <br/><br/>LDR v0, [x4]    ; v0 ← memory[x4]<br/>LDR v1, [x6]    ; v1 ← memory[x6]<br/><br/>ADD v4.16B, v0.16B, v1.16B <br/>STR v4, [x8]    ; v4 → memory[x8]</span></pre><p id="d0b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们想要添加32位值，那么这些值被称为单字值或简称为<code class="fe nq nr ns nt b">S</code>。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="eb04" class="ny mt it nt b gy nz oa l ob oc">; ARM Neon adding four 32-bit values<br/>ADD v4.4S, v0.4S, v1.4S</span></pre><p id="ab2f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">英特尔x86 SSE指令更简单一些，因为它们最初只支持32位元素值。他们添加了名为<code class="fe nq nr ns nt b">xmm0</code>、<code class="fe nq nr ns nt b">xmm1</code>的新寄存器，...<code class="fe nq nr ns nt b">xmm7</code>均为128位。这赋予了每条指令处理四个32位值的能力。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="0b83" class="ny mt it nt b gy nz oa l ob oc">; x86 SSE adding four 32-bit values<br/><br/>MOVAPS xmm0, [12]  ; xmm0 ← memory[12]<br/>MOVAPS xmm1, [28]  ; xmm1 ← memory[28]<br/>ADDPS  xmm0, xmm1<br/>MOVAPS [44], xmm0  ; memory[44] ← xmm0</span></pre><p id="80a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">SIMD处理可以以许多不同的方式发生。我们刚才看到的叫做压缩SIMD指令。但是向量SIMD指令也存在。</p><p id="0dad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用压缩SIMD指令，我们也可以清楚地知道要运算的元素的大小和个数。注意Arm Neon packed-SIMD指令是如何要求你给寄存器加上一个像<code class="fe nq nr ns nt b">.16B</code>或<code class="fe nq nr ns nt b">.4S</code>这样的后缀的。这些足以准确说明我们处理了多少个元素，每个元素有多宽。</p><p id="ed73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">RISC-V P扩展在这里稍微灵活一些，因为元素的数量实际上是由CPU是32位还是64位决定的。在32位RISC-V处理器上，<code class="fe nq nr ns nt b">ADD16</code>指令每个输入寄存器使用两个16位数，而对于64位处理器，每个输入寄存器使用四个16位数。</p><p id="1c44" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">底线是，所有这些压缩SIMD指令集都受到这样一个事实的限制，即它们严重限制了扩展元素大小的能力以及在不添加更多指令的情况下可以处理的元素数量。下面的代码编码了两条完全不同的指令。如果你想支持另一个后缀，你需要添加更多的指令到指令集中。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="913e" class="ny mt it nt b gy nz oa l ob oc">; ARM Neon<br/>ADD v4.16B, v0.16B, v1.16B <br/>ADD v4.4S, v0.4S, v1.4S</span></pre><p id="21e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一要求是包装SIMD指令的主要限制。相反，我们想要的是更高层次的抽象，它允许硬件设计者增加更多的处理能力，而不必每次都改变硬件的接口。</p><h1 id="11da" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">从压缩SIMD到矢量处理</h1><p id="c9bf" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">我们希望能够添加更多的SIMD通道和更大的向量寄存器，但在遵循打包SIMD方法时，如果不添加新指令，我们就无法做到这一点。</p><p id="8057" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">早在70年代和80年代，克雷超级计算机使用的这个问题的早期解决方案是定义矢量SIMD指令。有了这些指令，向量寄存器被认为是无类型的。向量指令没有说我们有多少元素和它们的大小。</p><p id="e9de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是RISC-V矢量扩展(RVV)和Arm可伸缩矢量扩展(SVE)使用的策略。对于RVV se，使用一个名为<code class="fe nq nr ns nt b">VSETVLI</code>的指令来配置元素的大小和数量。我们每次执行一个SIMD运算，比如<code class="fe nq nr ns nt b">VADD.VV</code>(带有两个向量寄存器参数的向量加法)，用我们想要处理的元素数填充一个寄存器。</p><p id="5ff2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，这条指令告诉CPU配置为处理16位元素。<code class="fe nq nr ns nt b">x2</code>包含了我们想要处理的多少元素。然而，我们的SIMD硬件可能没有足够大的寄存器来处理这么多的16位元素，这就是为什么每次我们调用向量SIMD指令时，指令都会在<code class="fe nq nr ns nt b">x1</code>中返回我们能够处理的元素的实际数量。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="c1b6" class="ny mt it nt b gy nz oa l ob oc">VSETVLI  x1, x2, e16</span></pre><p id="b06f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上，我们必须在加载和存储时指定元素的大小，因为它会影响位的排序。因此，我们发出一个<code class="fe nq nr ns nt b">VLE16.V</code>来加载<code class="fe nq nr ns nt b">x1</code>个16位值。<code class="fe nq nr ns nt b">VLSE16.V</code>用于存储<code class="fe nq nr ns nt b">x1</code>个16位值。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="401a" class="ny mt it nt b gy nz oa l ob oc"># RISC-V Vector processing (adding two vectors)<br/><br/>VSETVLI  x1, x2, e16 # Use x2 no. 16-bit elements<br/>VLE16.V  v0, (x4)    # Load x1 no. elments into v0<br/>VLE16.V  v1, (x5)    # Load x1 no. elments into v1<br/>VADD.VV  v3, v0, v1  # v3 ← v0 + v1<br/>VLSE16.V v3, (x6)    # v3 → memory[x6]</span></pre><p id="e780" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用矢量SIMD指令，我们可以从指令集中抽象出多少条SIMD通道。下图显示了矢量处理的工作原理。每个寄存器有16个元素，但只有两个SIMD通道。这不是问题，因为向量处理器会简单地遍历所有元素，直到完成。在压缩SIMD中，我们会在一个时钟周期内处理两对数字。使用向量SIMD，我们花费四个CPU时钟周期来处理八对数字。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/179b59f47eea1b163c128913c71d4650.png" data-original-src="https://miro.medium.com/v2/format:webp/1*hw3qmlgscNFtz1KAUkESYg.png"/></div></figure><p id="b010" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们有四个SIMD通道(四个alu ),我们可以在两个时钟周期内处理八对数字。这种方法的美妙之处在于，你可以在不同的CPU上运行完全相同的代码，这些CPU有不同数量的SIMD通道。</p><p id="677d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，你可以拥有一个只有一个通道的廉价微控制器，或者一个复杂的高端CPU，用于64个SIMD通道的科学计算。两者都能够运行相同的代码。唯一的区别是高端CPU能够更快地完成。</p><h1 id="55bb" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">跨多个内核划分任务</h1><p id="efe8" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">我们实际上还没有完成SIMD计算的抽象。我们可以走得更远。为了获得最高的性能，我们希望能够并行执行尽可能多的工作，但是我们并不总是需要对大量的元素执行完全相同的操作。这就是拥有多个CPU内核的意义所在。还因为有许多非向量代码您可能希望与向量处理并行执行。</p><p id="5272" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这件事有不同的处理方式。现代CPU内核非常大，因为它包含许多晶体管来实现许多功能，这允许它并行执行许多指令。这就是超标量处理器所做的。下图可能会给你一个想法。请注意，与最初的简单图不同，这张图底部有多个指令解码器。我们创建了微操作，而不是一个解码后的指令立即切换各种控制线来控制不同的单元，如ALU。</p><p id="67f3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">CPU包含各种聪明的东西，可以计算出这些微操作中的哪些操作是相互独立的，因此操作可以并行进行。也许它发现有一种乘法不依赖于被解码的加法指令。在这种情况下，乘法器和ALU可以并行工作。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/566dcecfd8387f99525afd6ecf5314ea.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6hkmuCvILENN5eQnu4WJYA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">超标量CPU有许多解码器，可以并行运行许多指令。</figcaption></figure><p id="2269" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更奇妙的是，超标量CPU可以乱序执行指令(OoO)，这意味着它想出一种方法来重新排序它们，以便更容易地并行运行。后来它不得不再次命令返回结果。</p><p id="db4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好吧，好吧，我知道，你想知道这和SIMD指令有什么关系。公平地说，这与SIMD没有直接关系。这只是为了让你理解为什么现代CPU要装这么多晶体管。他们做了很多需要很多晶体管的聪明的事情。</p><p id="3702" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一方面，如果你的处理器内核应该主要执行大量的SIMD指令，你就不需要那些花哨的东西。这只是一个沉重的负担。事实上，如果你放弃超标量OoO能力，花哨的分支预测器和所有这些好东西，你会得到更小的处理器内核。事实上，有序的面向SIMD的核心可以做得非常小。</p><p id="19ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设你有200亿个晶体管可以玩。用这样的晶体管预算，你也许可以制造16-32个厚内核。但是如果你制造简单的向量处理核心，你可以制造一千个向量处理核心！</p><p id="43a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，您有一千个内核，每个内核都有512位向量寄存器，允许您在每个时钟周期处理32个16位值(512/16 = 32)。</p><p id="fb5c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也许你有一百万个元素需要处理。你需要一些更高层次的抽象来划分工作，并把它推到你的一千个向量处理核心中的每一个。假设内存不是瓶颈，所有这些内核每个CPU周期总共可以处理32 000个元素(32*1000)。</p><p id="5bd8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我刚才所描述的本质上是世界语技术公司正在做的基于RISC-V的人工智能加速卡。他们制作了一个片上系统(SoC ),称为ET-SOC-1，它有四个胖超标量通用内核，称为ET-Maxion。此外，他们还有1088个称为ET-Minion的微型矢量处理器核心。现在后者也是通用CPU，但它们缺少所有花哨的超标量OoO东西，这使它们能够快速运行常规程序。相反，它们针对矢量处理进行了优化(矢量SIMD指令)。</p><p id="ab03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我来总结一下我们到目前为止学到的东西:</p><ol class=""><li id="e7ee" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt od mk ml mm bi translated">我们研究了具有标量运算的常规哑RISC处理器。</li><li id="37b0" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt od mk ml mm bi translated">然后我们想:嘿，让我们假设一个寄存器是一个数字阵列，让我们通过添加更多的alu和乘法器来并行计算这些数字。</li><li id="c902" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt od mk ml mm bi translated">利用这些多个alu和向量寄存器的第一个简单方法是定义压缩SIMD指令。</li><li id="fe88" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt od mk ml mm bi translated">打包time不会随着时间的推移而扩展。每次硬件设计人员想要增加更多SIMD通道时，都需要新的指令。向量-SIMD救援。</li><li id="b413" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt od mk ml mm bi translated">单核与矢量SIMD是不够的。我们需要更多内核。现在我们有了一个新问题:我们如何组织和划分工作负载？</li></ol><p id="aaf7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在世界语技术出现之前，Nvidia等显卡制造商一直在解决同样的问题。显卡不仅仅是一个处理很长向量寄存器的单核。就像ET-SOC-1一样，它们包含大量针对运行SIMD指令而优化的小内核。</p><p id="8af3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是现在事情变得复杂多了。</p><h1 id="6faf" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">单指令多线程— SIMT</h1><p id="e412" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">对于图形硬件，事情变得复杂得多。当我开始写这个故事时，我的意图是将图形硬件解释为SIMD处理，更高级的东西在上面。</p><p id="3958" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，这种描述并不公平。显卡制造商称之为SIMT的东西——单指令多线程——与SIMD截然不同，它应该有自己的缩写。</p><p id="1a67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不是谈论SIMD巷图形家伙谈论线程。起初我认为这是对众所周知的术语的滥用。通常一个线程封装一个执行线程。它需要存储一个程序计数器，这个计数器表示一个特定线程在程序中的位置。</p><p id="830b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你有一个可以恢复线程的调度程序。然后，当线程暂停时，它将从存储器中弹出该线程的程序计数器，以在程序中的最后一个位置恢复执行。</p><p id="3bed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">线程在普通软件中是很实用的，因为你可以让代码读写磁盘或网络，这在等待响应时会被卡住。使用线程，你可以暂停一个正在等待数据的线程，并恢复另一个线程去做有用的事情。</p><p id="f7a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这听起来不太像SIMD巷，对不对？那么，当讨论GPU中的并行处理时，这个术语是如何溜进来的呢？这是因为GPU核上的SIMD莱恩斯实际上更像是一个线程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/98a6c2263320f867cf7ad671c77b8c72.png" data-original-src="https://miro.medium.com/v2/format:webp/1*0b7lN8L-Irn_ifX9civiAA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">单指令多线程</figcaption></figure><p id="1021" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这不仅仅是一个通道作为自己的计算单元，如ALU，而且它们还有自己的<em class="np">程序计数器</em> (PC)和自己的<em class="np">加载存储单元</em> (LSU)。什么？所以你们每个SIMD巷都能像正常的CPU核心一样完全独立地运行指令？</p><p id="bd3e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不完全是。在Nvidia GPU上，你基本上有32个SIMD通道。一条指令可以同时在32对数字上执行。有趣的是，条件指令仍然是允许的。假设条件是12个线程(通道)的<code class="fe nq nr ns nt b">true</code>和20个线程的<code class="fe nq nr ns nt b">false</code>。这意味着这两组线程需要运行不同的代码。每一个都更新了他们的程序计数器，但是我们只在同一时间用相同的程序计数器运行线程。</p><p id="94c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，前12个线程可以运行一条<code class="fe nq nr ns nt b">ADD</code>指令。也许他们运行另一个指令，一个<code class="fe nq nr ns nt b">SHIFT</code>(乘以2的因子)指令。每当程序计数器(PC)为12个线程中的每一个更新时。</p><p id="3571" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在某个时候，你可能会得到一条从内存中读取数据的指令。这让事情发生了翻天覆地的变化。通过打包SIMD和向量SIMD，我们负责在SIMD引擎开始工作之前从内存中填充向量寄存器。SIMT的情况有所不同:每个“通道”都可以从内存中提取数据。每个通道执行一个从内存加载指令，但是寄存器可能指向不同的内存地址。</p><p id="8b2b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">内存读取通常很慢，因此此时12个线程可能会暂停。此时，我们运行具有相同程序计数器位置的20个其他踏板。也许他们执行的是<code class="fe nq nr ns nt b">SUB</code>(减法)指令。不运行的线程被屏蔽。</p><p id="b160" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">屏蔽是可以用压缩SIMD和向量SIMD(向量处理)实现的，但是早期的SIMD指令集不支持。它基本上允许您在进行特定计算时禁用某些元素(通道)。</p><p id="44d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们总结一下要点:</p><ul class=""><li id="c87d" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated">GPU核心中的每个线程都有自己的程序计数器，指向它们共享程序(内核)中的不同位置。</li><li id="44af" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">一次只能执行一条指令。然而，该指令在具有相同程序计数器的所有线程中重复。</li><li id="19b9" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">每条指令最多在32个并行线程上执行。</li></ul><h2 id="92f0" class="ny mt it bd mu oe of dn my og oh dp nc lh oi oj ne ll ok ol ng lp om on ni oo bi translated">使用LSU的并行存储器访问</h2><p id="4c42" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">每个线程(SIMD巷)在运行时得到自己的LSU。一种计算内存地址的LSU。这意味着每个线程都可以对内存执行读写指令。</p><p id="9c4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">显然，并行执行加载操作的多个线程<em class="np">不能同时</em>从多个内存位置读取，因为内存是共享资源。相反，CPU和GPU都是成块读取内存的。这些块倾向于64或32字节。因此，LSU地址被分组。许多LSU内存地址将属于同一个32字节块。我们一次读一大块。这意味着每个时钟周期只有一些活动线程获得它们请求的数据。</p><p id="220e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我试图在下图中说明这一点。为了简化起见，我从该图中删除了指令流、程序计数器和指令解码器，以关注存储器和寄存器之间的交互。基本上，该图显示了可以并行执行两个GPU线程的两个通道。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/064c8e18155bbd9f789c923818777285.png" data-original-src="https://miro.medium.com/v2/format:webp/1*64dNvhcZByK3tDYlLNEqQw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">GPU中的每个SIMD通道如何访问内存</figcaption></figure><p id="fa29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于获取每条SIMD车道的数据可能需要多个周期，显卡制造商试图通过安排新任务来优化系统。这让我们想到了下一个概念:<em class="np">扭曲</em>和扭曲调度器。</p><h1 id="a495" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">组织和管理经线中的线</h1><p id="1077" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">对于Nvidia speak，我们称之为对称多处理器(SM)的GPU核心。这些GPU核心中的每一个都可以同时在32个线程上执行一条指令。你可以认为每个GPU核心都有32个通道的SIMD引擎。</p><p id="012d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种描述是一种简化，因为每个内核可以有1到4个SIMD引擎，每个引擎在32个并行线程上执行一个操作。</p><p id="21ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们刚刚讨论了内存访问如何需要多个周期。因此，为了更有效地利用GPU核心，我们在线程之间切换。这就像常规的多线程一样。如果一个线程在常规CPU上等待输入数据时停滞不前，您可以切换到另一个线程。</p><p id="674d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样的事情发生在GPU上，除了每个线程不能彼此独立地运行指令，它们被捆绑成所谓的<em class="np">经线</em>(显然在编织术语中，经线是一束线程)。</p><p id="4e9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">GPU核心(SMs)可以存储许多扭曲的状态，并在每次另一个扭曲停止时安排一个新的扭曲。这允许更有效地利用GPU核心的SIMD引擎中的处理能力。在等待输入数据时，另一个经线可以被唤醒并继续处理，而不是闲置。</p><p id="d59d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些扭曲之间的切换非常快，不像CPU中的线程之间的切换。我的理解是，你可以在多个扭曲之间快速切换，并且每个扭曲只执行一条指令，而不会产生额外的开销。</p><p id="c1a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图试图说明这种调度如何在具有两个warp调度器的GPU核心中工作。你有四个不同的经线，它们有自己的指令序列需要处理。请注意每个经线的指令是如何按顺序执行的，但其他经线可能会交错执行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/632decf5077b8191528156610a1cf70c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*xbARsSQK1WbIc0dRd54bvw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">扭曲是如何在GPU核心中调度的</figcaption></figure><p id="f89c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你对细节更感兴趣，你可以阅读Nvidia Fermin微架构的白皮书。</p><h1 id="c07b" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">将工作组织到线程块中</h1><p id="959e" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">我们已经研究了在SIMT体系结构中如何执行指令的最底层，但没有研究如何分割比如说一百万个元素并分块处理它们。为了解释这一点，我们将看一些矩阵和向量数学相关的代码。</p><h2 id="cb3a" class="ny mt it bd mu oe of dn my og oh dp nc lh oi oj ne ll ok ol ng lp om on ni oo bi translated">关于线性代数和SIMD代码例子的题外话</h2><p id="2afe" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">围绕矩阵和向量的数学称为线性代数。它包含关于矩阵乘法，矩阵求逆和许多其他事情的信息。因为这在所有科学和工程中是如此重要，几十年来我们已经有了一个名为<a class="ae op" href="http://www.netlib.org/blas/#_reference_blas_version_3_10_0" rel="noopener ugc nofollow" target="_blank"> BLAS </a>和LAPACK的Fortran库，其中包含了丰富的线性代数函数集合。这是黄金标准。</p><p id="bb1c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为线性代数涉及矩阵和向量，所以它是任何基于SIMD处理的系统的流行目标。因此，无论是查看RISC-V向量扩展示例，还是Nvidia CUDA或OpenCL示例代码，您都会看到提到一些命名神秘的函数，如<a class="ae op" href="http://www.netlib.org/lapack/explore-html/df/d28/group__single__blas__level1_gad2a52de0e32a6fc111931ece9b39726c.html" rel="noopener ugc nofollow" target="_blank"> SAXPY </a>和<a class="ae op" href="http://www.netlib.org/lapack/explore-html/db/dc9/group__single__blas__level3_gafe51bacb54592ff5de056acabd83c260.html" rel="noopener ugc nofollow" target="_blank"> SGEMM </a>。</p><p id="cbb2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些函数在所有与SIMD相关的代码示例中都很常见，所以我们将在这里使用它们来解释在基于SIMT的代码中，线程是如何组织成线程块的。SAXPY计算:</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="b843" class="ny mt it nt b gy nz oa l ob oc">Y = αX + Y    # saxpy</span></pre><p id="0d23" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<code class="fe nq nr ns nt b">X</code>和<code class="fe nq nr ns nt b">Y</code>是矩阵(或向量)<code class="fe nq nr ns nt b">a</code>是标量。每个BLAS函数都是做这些简单的操作。实例的SGEMM执行以下操作:</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="8ce3" class="ny mt it nt b gy nz oa l ob oc">C = aA⋅B + βC   # sgemm</span></pre><p id="41bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在线性代数中，矩阵用大写字母，标量用小写字母是很常见的。在我的编程示例中，我将使用<code class="fe nq nr ns nt b">xs</code>和<code class="fe nq nr ns nt b">ys</code>来表示元素数组。因此<code class="fe nq nr ns nt b">x</code>是<code class="fe nq nr ns nt b">xs</code>中的一个元素，而<code class="fe nq nr ns nt b">y</code>是<code class="fe nq nr ns nt b">ys</code>中的一个元素。</p><h2 id="85c0" class="ny mt it bd mu oe of dn my og oh dp nc lh oi oj ne ll ok ol ng lp om on ni oo bi translated">实现SAXPY</h2><p id="3166" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">通常情况下，您会像这样实现SAXPY，以便在CPU上运行。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="ba26" class="ny mt it nt b gy nz oa l ob oc">// Single-Precision α ⋅ X Plus Y,<br/>void<br/>saxpy(int n, float a, float *xs, float *ys) {<br/>    for (int i=0; i&lt;n; i++)<br/>        ys[i] = a * xs[i] + ys[i];<br/>}</span></pre><p id="4632" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了让它在GPU上运行，我们定义了一个函数，这个函数通常被称为内核。内核通常表示应用于大量元素的代码片段。此语法基于CUDA (Nvidia)。</p><p id="1dd0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，for循环已经消失了。相反，代码在SIMT处理器中的一个SIMD通道上执行。这段代码将在GPU上运行，而不是在CPU上运行，因此内核代码将由图形驱动程序编译并推送到GPU。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="0403" class="ny mt it nt b gy nz oa l ob oc">_global_void <br/>gpu_saxpy(int n, float a, float *xs, float *ys) {<br/>    int i = blockIdx.x * blockDim.x + threadIdx.x;<br/>    if (i &lt; n) <br/>        y[i] = a*xs[i] + ys[i]; <br/>}</span></pre><p id="3117" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">阅读更多:<a class="ae op" href="https://people.cs.pitt.edu/~melhem/courses/xx45p/cuda_examples.pdf" rel="noopener ugc nofollow" target="_blank"> CUDA示例代码。</a></p><p id="12d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">显卡本身不会这样做，所以你需要在CPU上运行一些代码来设置一切。我们得到了两个包含我们想要处理的<code class="fe nq nr ns nt b">n</code>元素的数组<code class="fe nq nr ns nt b">xs</code>和<code class="fe nq nr ns nt b">ys</code>。为了并行完成这项工作，我们希望将这项任务分成多个块。在OpenGL中，他们恰当地将其命名为工作组。Nvidia称之为线程块。每个GPU核可以处理一个线程块。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="5f68" class="ny mt it nt b gy nz oa l ob oc">// Invoke parallel SAXPY kernel (256 threads per block)<br/><br/>int blocksize = 256<br/>int nblocks = (n + blocksize-1) / blocksize;<br/>saxpy_parallel&lt;&lt;&lt;nblocks, blocksize&gt;&gt;&gt;(n, 2.0, xs, ys);</span></pre><p id="5253" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为我们经常处理矩阵数据，所以我们开发了一个系统，可以很容易地找出一个线程被分配处理矩阵的哪一行和哪一列。这被称为线程索引。将线程调度到经线的GPU机器并不关心线程索引，而是与线程ID相关。线程ID是唯一标识特定线程的东西。</p><p id="eecb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我在一个矩阵上工作，并且想知道我的内核代码正在处理什么行和列，那么我可以问<code class="fe nq nr ns nt b">threadId.x</code>和<code class="fe nq nr ns nt b">threadIdx.y</code>的值是什么。这些是映射到每个GPU核心(SM)中的寄存器的全局变量。如果我处理一个立方体的数据，我可能也会对<code class="fe nq nr ns nt b">threadIdx.z</code>感兴趣。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/ac5213aed9fd98874a41915bf973e3de.png" data-original-src="https://miro.medium.com/v2/format:webp/1*2tN8_5worg5K61q2ksWVlQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">将线程组织成不同维度的块</figcaption></figure><p id="40f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了利用更多的GPU核心，我们将线程聚集成线程块。硬件设置使得每个GPU核可以并行处理一个线程块。</p><p id="ed7c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里有一个重要的观察点:一个GPU核心不会同时处理每一个线程。记住它只能同时处理1-4根经纱，每根经纱是32根。因此，一个GPU内核最多可以同时处理128个(4 × 32)线程。</p><h2 id="749e" class="ny mt it bd mu oe of dn my og oh dp nc lh oi oj ne ll ok ol ng lp om on ni oo bi translated">为什么分配给GPU核心的线程比它能够并行处理的线程多？</h2><p id="81f7" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">如果您无法处理所有线程，那么为什么不为每个内核分配更少的线程，并将它们分布在更多的内核上呢？</p><p id="9fdb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在你要记住我们所说的因为等待内存而导致扭曲停止。各种事情都可能发生，这意味着warp中的当前指令无法执行。</p><p id="519a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了能够有效地利用硬件资源，我们希望有其他的变形来代替它们。因此，即使您的内核只能并行处理64个线程，您仍然应该分配更多的线程来保持SIMD引擎忙碌。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/a9003cfb1cd23f125f4cf4b08f2eec20.png" data-original-src="https://miro.medium.com/v2/format:webp/1*nhPOvBVIveoD8MiAWB2VGw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">同一块上的线程可以共享内存。</figcaption></figure><p id="9858" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二个要点是块中的线程可以共享内存。请记住，每个GPU核心都有自己的本地内存。这意味着在同一个GPU核上执行的加载/存储指令能够访问同一个内存。</p><p id="30da" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，如果线程需要彼此共享结果，它们需要在同一个线程块上。</p><h2 id="a84c" class="ny mt it bd mu oe of dn my og oh dp nc lh oi oj ne ll ok ol ng lp om on ni oo bi translated">数据块大小并不总是与您的阵列匹配</h2><p id="841c" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">假设你想处理两个有80个元素的数组。如果使用一个64元素的块，就不能处理整个数组。如果使用两个64元素的块，则得到128个元素，这太多了。这就是为什么<code class="fe nq nr ns nt b">saxpy</code>实现有这些行:</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="55a4" class="ny mt it nt b gy nz oa l ob oc">if (i &lt; n) <br/>        y[i] = a*xs[i] + ys[i];</span></pre><p id="b715" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请记住，内核的调用是基于您设置的线程块配置，而不是基于您的数组实际拥有的元素数量。</p><p id="ab05" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们必须确保不会对每个线程索引进行计算。有些会出界。</p><h2 id="99ca" class="ny mt it bd mu oe of dn my og oh dp nc lh oi oj ne ll ok ol ng lp om on ni oo bi translated">线是如何映射到经线的？</h2><p id="796a" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">当处理1D数据时，你的线程ID和你的<code class="fe nq nr ns nt b">threadIdx.x</code>将是相同的。ID为0，1，...31将形成一根经纱，而ID为32，33，...63将形成下一个曲速。</p><p id="0bcb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这意味着当我们执行if语句<code class="fe nq nr ns nt b">if (i &lt; n)</code>时，它将发生在一个warp中，其中某些线程的条件为真，而其他线程的条件为假。</p><p id="3df8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当运行以下代码的指令时，条件不为真的线程将被简单地屏蔽(程序计数器不更新):</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="7bbb" class="ny mt it nt b gy nz oa l ob oc">y[i] = a*xs[i] + ys[i];</span></pre><p id="d85d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用遮罩是当今所有矢量处理中的常见策略。你会发现这两个向量SIMD和包装SIMD指令。RVV、SVE和AVX都使用口罩。这是为了解决一个常见的问题，即由SIMD引擎处理的元素块很少会整齐地划分您正在处理的整个数据数组。你通常会得到一个尾巴。</p><p id="e01c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">早期的压缩SIMD指令不支持掩码，因此必须用常规标量指令处理向量的尾端，使得尾端的处理相当慢。</p><h1 id="267f" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">GPU硬件需要这么复杂吗？</h1><p id="8502" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">所有旨在将大量工作负载调度到不同内核的硬件都会带来一定的复杂性。然而，我认为这仍然是一个公平的问题。</p><p id="9643" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从历史的角度来看，SIMT硬件的概念是有意义的。图形管道通常将3D模型中的顶点输入到我们称之为顶点着色器的内核中。稍后，流水线将为顶点之间形成的表面生成片段(具有深度的像素)。这些片段中的每一个都将运行一个片段着色器(内核)。</p><p id="b5d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个人都想做大量复杂的事情。你会想要做一个顶点坐标的矩阵乘法来做投影变换，移动场景中的相机和许多其他事情。对于片段，你需要查找纹理数据。</p><p id="171c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于每个“SIMD通道”来说，这是一个非常复杂的东西，因此传统的SIMD思维必须大大增强，以至于我们得到了SIMT，它开始越来越像小CPU内核本身。也许这就是为什么Nvidia称它们为核心，即使它们有点像SIMD车道。</p><p id="ba27" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于许多机器学习(ML)和许多科学任务来说，这种方法不一定是关于数据处理的最符合逻辑的思考方式。即使回到20世纪80年代，编译器也可以这样处理代码:</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="6e91" class="ny mt it nt b gy nz oa l ob oc">void<br/>saxpy(int n, float a, float *xs, float *ys) {<br/>    for (int i=0; i&lt;n; i++)<br/>        ys[i] = a * xs[i] + ys[i];<br/>}</span></pre><p id="95b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并将其转化为矢量化汇编代码。我们可以看看现代的RISC-V矢量扩展变体。当然，这是一个集合，所以更容易记账，也更难跟踪。然而从概念上来说，这比GPU卡所做的要简单得多。</p><p id="0db4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些代码所做的就是计算出我们的向量处理硬件可以处理多少个元素，然后将这些元素加载到向量寄存器中。然后我们调用完成整个计算的汇编代码指令。</p><p id="46e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每一次迭代，我们都获取另一个块，并加载它进行处理。</p><pre class="kj kk kl km gt nu nt nv nw aw nx bi"><span id="055c" class="ny mt it nt b gy nz oa l ob oc">saxpy:<br/>    # t0 ← min(a0, (VLEN × 8) / 32)<br/>    vsetvli t0, a0, e32, m8, ta, ma<br/>   <br/>    vle32.v v0, (a1)    # v0 ← mem[a1:a1+t0]<br/><br/>    sub a0, a0, t0      # calc xs elements left to read<br/>    slli t0, t0, 2      # t0 &lt;&lt; 2 (each element is 4 bytes)   <br/>    add a1, a1, t0      # point a1 to next xs batch to read<br/><br/>    vle32.v v8, (a2)    # v8 ← mem[a2:a2+t0]<br/>    vfmacc.vf v8, fa0, v0  # v8 ← fa0 × v0 + v8<br/>   <br/>    vse32.v v8, (a2)    # store batch of results at a2<br/>    add a2, a2, t0      # point a2 to next ys batch to read<br/>    bnez a0, saxpy      # goto saxpy if a0 ≠ 0<br/>    ret</span></pre><p id="f99e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这也许也是为什么我们开始看到各种各样的供应商挑战科学计算和机器学习的大型显卡的霸权。你不需要复杂的硬件来处理大量这类数据。</p><p id="b7e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以机器学习和深度神经网络为例。你所做的大部分事情都是大矩阵的乘法和加法。这个问题域可以用一个比CUDA更简单的界面来表达。</p><p id="b0b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么为什么3D显卡及其后代会统治这个市场这么久呢？我不是这方面的专家，所以这将是一个猜测:像Nvidia这样的公司已经在制造强大的显卡方面建立了能力和规模。调整他们的核心技术以拓展新市场是有意义的。</p><p id="26a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">CUDA建立在图形硬件工作特性的基础上。对于英伟达和其他公司来说，放弃一个已经主导市场的平台没有太大意义。</p><p id="4dd5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似的例子可能是x86 CPU市场。我们都知道x86指令集不再是最佳的。这是一个完全过时的指令集(is a)。尽管如此，它仍是主导当前市场的ISA。对英特尔和AMD来说，更有意义的是通过添加新的强大指令来调整现有的设计，在内部添加类似RISC的引擎，并添加各种其他昂贵的技巧来阻止竞争，而不是创建一个干净的设计。</p><p id="55fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当90年代的高级RISC芯片挑战英特尔时，他们被击败了，因为凭借着较高的产量和利润，英特尔能够在他们的问题上砸钱，并且比RISC公司花得更多。</p><p id="ec73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我怀疑类似的策略是什么给了我们作为人工智能加速器的显卡。它们可能不是一个理想的平台，但Nvidia等公司通过加入大量新的特殊功能(如张量核心)来保持这些卡的竞争力，从而使它们留在了游戏中。</p><p id="ffcd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但最终你可能会达到一个转折点，就像我们今天看到的x86与ARM的对比。最终，遗产会赶上你，更干净设计的好处会胜出。</p><h1 id="2a5f" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">总结和结束语</h1><p id="bc19" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">如果你读到最后:恭喜你。这个故事有了自己的生命。我非常希望我能写一个更简洁的版本。但正如布莱士·帕斯卡所说:“很抱歉信写得这么长，我没有时间写一封更短的。”</p><p id="385c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我决定推出这个版本，而不是花时间写一个更短的版本。无论如何，我想总结一下这是怎么回事。</p><p id="7a49" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现代高性能计算都与某种并行性有关。要么我们使用超标量CPU内核来实现指令级并行，要么我们通过创建多个内核来实现任务并行。每个内核可以运行一个硬件线程，执行不同的任务。</p><p id="b27b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们可以利用数据并行性，这是本文的重点。即处理同一操作可以同时应用于多个元素的情况。它是科学计算、线性代数、计算机图形学、机器学习和许多其他领域中出现的东西。</p><p id="77af" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我试着从最简单的方法开始，让你理解:</p><ul class=""><li id="17ca" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated">压缩SIMD指令——特定的指令，它说:“将寄存器v0乘以v1，其中两者都有8个32位元素。”</li><li id="dc5c" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">向量SIMD指令——我们抽象地向上移动，隐藏我们有多少条SIMD车道。硬件架构师可以在不改变指令集的情况下增强CPU。</li><li id="8a74" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">单指令多线程(SIMT) —意味着每个SIMD通道现在都在执行一大堆指令。它们甚至可以读取内存，跟踪它们在程序中的位置(程序计数器寄存器)等等。</li></ul><p id="9201" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有了GPU，我们获得了一个全新的抽象层次。您永远不会看到生成的汇编代码。你把高级代码发送给一个驱动程序，驱动程序对它进行编译。硬件的功能已经基本上被抽象化了。</p><p id="61ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，这种抽象可能无法完美地工作，原因很简单，GPU核心不是通用处理器。常规的C代码可以被编译成矢量化代码。你不能用GPU代码做到这一点。你必须通过指定诸如线程块的数量和大小之类的东西来明确你实际上是针对GPU的。</p><h1 id="a2a8" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">相关故事</h1><p id="639f" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">我写的关于硬件的故事，可能会感兴趣。</p><ul class=""><li id="db97" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><a class="ae op" href="https://medium.com/swlh/risc-v-vector-instructions-vs-arm-and-x86-simd-8c9b17963a31" rel="noopener"> RISC-V矢量指令vs ARM和x86 SIMD </a> —重点比较压缩SIMD和矢量SIMD指令以及它们存在的原因。</li><li id="3eb9" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">ARMv9:有什么大不了的？ —详细介绍添加到ARMv9架构中的SVE2矢量SIMD指令集。</li><li id="c41f" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><a class="ae op" href="https://erik-engheim.medium.com/the-genius-of-risc-v-microprocessors-b19d735abaa6" rel="noopener">RISC-V微处理器的天才</a>——与SIMD无关，而是关于RISC-V内核中的指令压缩和宏操作融合。</li><li id="0e0c" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><a class="ae op" href="https://medium.com/swlh/how-does-a-microprocessor-work-e06d196efd8f" rel="noopener">现代微处理器是如何工作的</a>——意为初学者介绍微处理器是如何工作的。我们来看一个简化的RISC处理器。它对十进制数而不是二进制数进行运算，以简化解释。</li></ul><h1 id="9be6" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">资源</h1><ul class=""><li id="9295" class="me mf it la b lb nk le nl lh oq ll or lp os lt mj mk ml mm bi translated"><a class="ae op" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html" rel="noopener ugc nofollow" target="_blank"> CUDA C++编程指南</a> —伟大的Nvidia指南，通过良好的矩阵乘法示例详细介绍线程和模块。</li><li id="4cdb" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><a class="ae op" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#instruction-set-ref" rel="noopener ugc nofollow" target="_blank"> CUDA二进制实用程序</a> —通过查看CUDA的汇编器/反汇编器来解释GPU汇编代码。</li><li id="e479" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><a class="ae op" href="https://anteru.net/blog/2018/more-compute-shaders/" rel="noopener ugc nofollow" target="_blank">更多计算着色器</a> —解释着色器汇编指令如何工作。</li><li id="8dc8" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><a class="ae op" href="https://en.wikipedia.org/wiki/ARB_assembly_language" rel="noopener ugc nofollow" target="_blank"> ARB汇编语言</a> —有点类似汇编的低级着色语言。旧的OpenGL的东西。</li></ul></div></div>    
</body>
</html>