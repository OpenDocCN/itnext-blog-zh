<html>
<head>
<title>Oracle Cloud IO test using a distributed storage for Docker (Storidge)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Docker (Storidge)分布式存储的Oracle云IO测试</h1>
<blockquote>原文：<a href="https://itnext.io/oracle-cloud-io-test-using-a-distributed-storage-for-docker-storidge-d41c6555f63f?source=collection_archive---------2-----------------------#2019-05-11">https://itnext.io/oracle-cloud-io-test-using-a-distributed-storage-for-docker-storidge-d41c6555f63f?source=collection_archive---------2-----------------------#2019-05-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="d6b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当迁移到云时，第一个问题(忘记成本)是我如何预测我的IO性能？这是因为当您创建计算实例时，没有人知道在幕后使用了哪个硬件。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/c5d263b6c8f4bc5dcbfa73a999dd0707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LlhrD4cUICHbhBCwAvPp2A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">Oracle云控制台</figcaption></figure><p id="6049" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">查看来自CPU的信息很容易知道处理器的性能，例如使用shape <strong class="js iu"> <em class="le"> VM创建的实例。标准。e 2.1</em>T3将有两个核心类型:</strong></p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="0c03" class="lk ll it lg b gy lm ln l lo lp">ubuntu@node1:~$ cat /proc/cpuinfo |grep "model name"<br/>model name : AMD EPYC 7551 32-Core Processor<br/>model name : AMD EPYC 7551 32-Core Processor<br/>ubuntu@node1:~$ cat /proc/cpuinfo |grep "cpu MHz"<br/>cpu MHz  : 1996.243<br/>cpu MHz  : 1996.243</span></pre><p id="5c51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是磁盘呢，引导还是附带<strong class="js iu">T5【iSCSI】T6</strong>卷，没人知道什么是没有硬件信息的<strong class="js iu">T9】半虚拟化T11】磁盘，请看:</strong></p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="974d" class="lk ll it lg b gy lm ln l lo lp">root@node1:/srv/nfs4# hdparm -i /dev/sda</span><span id="ccc6" class="lk ll it lg b gy lq ln l lo lp">/dev/sda:<br/>SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00<br/> HDIO_GET_IDENTITY failed: Invalid argument<br/>root@node1:/srv/nfs4# hdparm -i /dev/sdb</span><span id="e922" class="lk ll it lg b gy lq ln l lo lp">/dev/sdb:<br/>SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00<br/> HDIO_GET_IDENTITY failed: Invalid argument</span></pre><p id="43a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">好了，有了以上信息，我决定测试Oracle云基础设施的性能，在两个可能的场景中使用Docker Swarm集群，<strong class="js iu"><em class="le">【NFS】</em></strong>共享存储和分布式使用<strong class="js iu"><em class="le">CIO</em></strong>from<a class="ae lr" href="https://storidge.com/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="le">Storidge</em></strong></a>，为我的Docker实例存储持久数据的两个选项。</p><p id="aa20" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了进行测试，我使用了一个云实例，类似于我在上一篇文章<a class="ae lr" href="https://medium.com/@marcelo.ochoa/deploy-docker-swarm-at-oracle-cloud-with-oracle-linux-7-168746f6d9f4" rel="noopener">中使用的在Oracle Cloud上部署Docker Swarm和Oracle Linux 7 </a>，但在这种情况下，使用Ubuntu来方便安装<strong class="js iu"> <em class="le"> Storidge </em> </strong>。在这种情况下，使用4个节点进行分布式存储，一个节点作为<strong class="js iu"><em class="le"/></strong>NFS服务器，在所有使用高IO块存储/延迟的情况下，可能会使用<strong class="js iu"> <em class="le"> SSD </em> </strong>。</p><h1 id="ad32" class="ls ll it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">使用dd的第一个原始测试</h1><p id="e0fd" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">一个非常简单的IO测试是用dd做的，我们可以检查引导分区和块存储分区(ext4格式化的)，这里的引导磁盘:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="baf0" class="lk ll it lg b gy lm ln l lo lp">root@de866e:/tmp# dd if=/dev/zero of=test4.img  bs=1G count=1 oflag=dsync<br/>1+0 records in<br/>1+0 records out<br/>1073741824 bytes (1.1 GB) copied, 13.9244 s, <strong class="lg iu"><em class="le">77.1 MB/s</em></strong><br/>root@de866e:/tmp# dd if=test4.img of=/dev/null oflag=dsync<br/>2097152+0 records in<br/>2097152+0 records out<br/>1073741824 bytes (1.1 GB) copied, 2.06966 s, <strong class="lg iu"><em class="le">519 MB/s</em></strong></span></pre><p id="8365" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里是存储/延迟磁盘上的数据磁盘:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="0354" class="lk ll it lg b gy lm ln l lo lp">root@de866e:/srv/nfs4# dd if=/dev/zero of=test4.img  bs=1G count=1 oflag=dsync<br/>1+0 records in<br/>1+0 records out<br/>1073741824 bytes (1.1 GB) copied, 1.70058 s, <strong class="lg iu"><em class="le">631 MB/s</em></strong><br/>root@de866e:/srv/nfs4# dd if=test4.img of=/dev/null oflag=dsync<br/>2097152+0 records in<br/>2097152+0 records out<br/>1073741824 bytes (1.1 GB) copied, 2.06904 s, <strong class="lg iu"><em class="le">519 MB/s</em></strong></span></pre><p id="ba4e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意不同之处，引导磁盘似乎针对读取性能进行了优化(519 MB/s，与其他磁盘相当),但写入性能较低(77 MB/s ),存储/延迟具有更好的写入性能(631 MB/s ), DD命令中的<strong class="js iu"> <em class="le"> dsync </em> </strong>标志可避免内核缓存，并意味着立即同步到磁盘。</p><p id="727e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但对于常规服务器来说，这意味着什么？这里是与使用英特尔i7 CPU和SATA SSD磁盘(最大传输容量为6 GB)的示例服务器的比较，首次测试使用非SSD/非RAID SATA磁盘:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="f1da" class="lk ll it lg b gy lm ln l lo lp">root@vmsvr10-slab:/tmp# dd if=/dev/zero of=test4.img  bs=1G count=1 oflag=dsync<br/>1+0 records in<br/>1+0 records out<br/>1073741824 bytes (1,1 GB) copied, 6,99693 s, <strong class="lg iu"><em class="le">153 MB/s</em></strong><br/>root@vmsvr10-slab:/tmp# dd if=test4.img of=/dev/null oflag=dsync<br/>2097152+0 records in<br/>2097152+0 records out<br/>1073741824 bytes (1,1 GB) copied, 1,29753 s, <strong class="lg iu"><em class="le">828 MB/s</em></strong></span></pre><p id="fc61" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二个测试在RAID0模式下使用两个SSD (KINGSTON 480.1 GB)磁盘:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="1325" class="lk ll it lg b gy lm ln l lo lp">root@vmsvr10-slab:/home/VMs# dd if=/dev/zero of=test4.img  bs=1G count=1 oflag=dsync<br/>1+0 records in<br/>1+0 records out<br/>1073741824 bytes (1,1 GB) copied, 2,02704 s, <strong class="lg iu"><em class="le">530 MB/s</em></strong><br/>root@vmsvr10-slab:/home/VMs# dd if=test4.img of=/dev/null oflag=dsync<br/>2097152+0 records in<br/>2097152+0 records out<br/>1073741824 bytes (1,1 GB) copied, 1,29408 s, <strong class="lg iu"><em class="le">830 MB/s</em></strong></span></pre><p id="ff62" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过使用<strong class="js iu"> <em class="le"> SSD </em> </strong>和<strong class="js iu"> <em class="le"> RAID0 </em> </strong>配置，我们拥有与常规硬件类似的写入性能，但这是一个本地磁盘(裸机服务器)，不受网络延迟的影响，在云环境中，您有一个虚拟机，而网络延迟导致的磁盘通常位于其他隔间中。这对于您的云规划部署来说是一个好消息，因为无论您的块存储位于何处，您都拥有比常规裸机服务器更好的写入性能。</p><h1 id="cc36" class="ls ll it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">NFS与本地存储测试</h1><p id="fe88" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">正如我在上面所说的，这个想法是使用集群的一个节点作为存储服务器，将<strong class="js iu"> <em class="le">存储/延迟</em> </strong>优化的磁盘目录导出为<strong class="js iu"> <em class="le"> NFS V4 </em> </strong>导出并将其作为Docker实例中的一个卷进行挂载，为了进行这个测试，我使用了打包为Docker映像的Oracle Orion工具，正如我在上一篇文章<a class="ae lr" href="https://medium.com/@marcelo.ochoa/estimating-your-io-throughput-at-your-cloud-deployment-626a644d6e98" rel="noopener">中描述的那样，在您的云部署中估计IO吞吐量</a>。首先，使用本地装载测试一台客户机，作为以后比较远程装载性能的参考:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="3c8b" class="lk ll it lg b gy lm ln l lo lp">root@a37709:/srv/nfs4# docker run -ti --rm -v local:/home --name test-local oracle/orion-official:12.2.0.1 /usr/lib/oracle/12.2/client64/bin/orion -run simple -testname firsttest -hugenotneeded<br/>ORION: ORacle IO Numbers -- Version 12.2.0.1.0<br/>firsttest_20180913_0034<br/>Calibration will take approximately 9 minutes.<br/>Using a large value for -cache_size may take longer.</span><span id="991c" class="lk ll it lg b gy lq ln l lo lp">Maximum Large <strong class="lg iu"><em class="le">MBPS=867.73</em></strong> @ Small=0 and Large=2</span><span id="5c72" class="lk ll it lg b gy lq ln l lo lp">Maximum Small <strong class="lg iu"><em class="le">IOPS=12679</em></strong> @ Small=5 and Large=0<br/>Small Read Latency: avg=392.499 us, min=198.667 us, max=38147.308 us, std dev=425.804 us @ Small=5 and Large=0</span><span id="7a52" class="lk ll it lg b gy lq ln l lo lp">Minimum Small Latency=392.499 usecs @ Small=5 and Large=0<br/>Small Read Latency: avg=392.499 us, min=198.667 us, max=38147.308 us, std dev=425.804 us @ Small=5 and Large=0</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/84ce54eb0c465b1b51c0fd1e946b2335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*MdDQbCvp2IoRLmPljdo3wg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">小读取延迟直方图(小=5，大=0)</figcaption></figure><p id="0d56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从本地存储装载Docker卷时，我们的读取性能为<strong class="js iu"><em class="le"/></strong>867 MBPs，大约<strong class="js iu"><em class="le"/></strong>12679 IOPs，主要是IO请求，范围为256-512 us。如直方图所示。</p><p id="9aa7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在从远程节点测试一个<strong class="js iu"> <em class="le"> NFS </em> </strong>挂载:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="e8a9" class="lk ll it lg b gy lm ln l lo lp">root@a4f1ee:~# docker volume create --driver local --opt type=nfs --opt o=addr=192.168.0.6,rw,intr,hard,timeo=600,wsize=32768,rsize=32768,tcp --opt device=:/srv/nfs4 nfs_test<br/>root@a37709:~# docker run -ti --rm -v nfs_test:/home --name test-high oracle/orion-official:12.2.0.1 /usr/lib/oracle/12.2/client64/bin/orion -run simple -testname firsttest -hugenotneeded<br/>ORION: ORacle IO Numbers -- Version 12.2.0.1.0<br/>firsttest_20181004_1102<br/>Calibration will take approximately 16 minutes.<br/>Using a large value for -cache_size may take longer.</span><span id="7979" class="lk ll it lg b gy lq ln l lo lp">Maximum Large <strong class="lg iu"><em class="le">MBPS=427.37</em></strong> @ Small=0 and Large=3</span><span id="e275" class="lk ll it lg b gy lq ln l lo lp">Maximum Small <strong class="lg iu"><em class="le">IOPS=13185</em></strong> @ Small=10 and Large=0<br/>Small Read Latency: avg=756.598 us, min=288.397 us, max=14976.572 us, std dev=511.523 us @ Small=10 and Large=0</span><span id="c461" class="lk ll it lg b gy lq ln l lo lp">Minimum Small Latency=480.544 usecs @ Small=3 and Large=0<br/>Small Read Latency: avg=480.544 us, min=255.911 us, max=22302.739 us, std dev=367.564 us @ Small=3 and Large=0</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/9ee6eb128966b24539c7f1a5642ccab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*eUqkMFqJSQ1ygJFJzmKVzg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">小读取延迟直方图(小=3，大=0)</figcaption></figure><p id="640f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过读取延迟直方图和<strong class="js iu"> <em class="le"> IOPs </em> </strong>显示的类似值，读取性能降低至<strong class="js iu"> <em class="le"> 427 MBPs </em> </strong>，肯定是由于<strong class="js iu"> <em class="le"> NFS </em> </strong>协议栈的开销导致读取性能降低。</p><h1 id="9480" class="ls ll it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">两个客户并行的NFS</h1><p id="2634" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">显然，我们推断，当两个客户端试图访问同一个NFS存储服务器时，会在服务器的网络端口或磁盘管道中产生瓶颈，结果如下:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="fa37" class="lk ll it lg b gy lm ln l lo lp">root@d54224:~# docker volume create --driver local --opt type=nfs --opt o=addr=192.168.0.6,rw,intr,hard,timeo=600,wsize=32768,rsize=32768,tcp --opt device=:/srv/nfs4 nfs_test<br/>root@a4f1ee:~# docker volume create --driver local --opt type=nfs --opt o=addr=192.168.0.6,rw,intr,hard,timeo=600,wsize=32768,rsize=32768,tcp --opt device=:/srv/nfs4 nfs_test</span><span id="2430" class="lk ll it lg b gy lq ln l lo lp">docker run -ti --rm -v nfs_test:/home --name test-high oracle/orion-official:12.2.0.1 /usr/lib/oracle/12.2/client64/bin/orion -run simple -testname firsttest1 -hugenotneeded<br/>Maximum Large <strong class="lg iu"><em class="le">MBPS=330.20</em></strong> @ Small=0 and Large=2<br/>Maximum Small <strong class="lg iu"><em class="le">IOPS=8181</em></strong> @ Small=5 and Large=0</span><span id="f803" class="lk ll it lg b gy lq ln l lo lp">docker run -ti --rm -v nfs_test:/home --name test-high oracle/orion-official:12.2.0.1 /usr/lib/oracle/12.2/client64/bin/orion -run simple -testname firsttest2 -hugenotneeded<br/>Maximum Large <strong class="lg iu"><em class="le">MBPS=328.63</em></strong> @ Small=0 and Large=2<br/>Maximum Small <strong class="lg iu"><em class="le">IOPS=8618</em></strong> @ Small=5 and Large=0</span></pre><p id="c20f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里是延迟直方图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mw"><img src="../Images/ea30ddd44fb2d307fe7c9a148b828860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9H67Kuojzj4ZrNeDBGpV0A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">读取延迟直方图客户端1/客户端2</figcaption></figure><p id="7b7e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">毫不奇怪，<strong class="js iu"> <em class="le"> MBPs </em> </strong>之和(330.20+328.63=658.83)高于一个客户端但不高于本地存储，<strong class="js iu"><em class="le">IOPs</em></strong>(8181+8618 = 16799)与之类似，但这里的总IO吞吐量高于本地存储。</p><h1 id="76c4" class="ls ll it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">使用Storidge CIO的分布式存储</h1><p id="1c98" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated"><a class="ae lr" href="https://storidge.com/" rel="noopener ugc nofollow" target="_blank"> Storidge </a>分布式存储是实现Docker容器持久存储的一个很好的选择，遵循指南<a class="ae lr" href="https://guide.storidge.com/getting_started/install.html" rel="noopener ugc nofollow" target="_blank">安装cio </a>您可以安装四个节点，但在开始使用该指南之前，您必须用linux-image-aws替换instance的内核，抱歉，Oracle的伙计们，但Storidge的团队没有在安装程序选择中包括该内核，请执行以下操作:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="e78a" class="lk ll it lg b gy lm ln l lo lp">root@node1:~# apt-get install linux-image-aws<br/>...<br/>root@node1:~# apt-get remove linux-image-*oracle<br/>...<br/>root@node1:~# update-grub;reboot</span></pre><p id="1987" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">或者，您可以使用Centos 7映像，无需任何更改，只要我使用配置文件<a class="ae lr" href="https://guide.storidge.com/getting_started/why_profiles.html" rel="noopener ugc nofollow" target="_blank"> HighIO </a>对Storidge集群进行启动和运行测试:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="3759" class="lk ll it lg b gy lm ln l lo lp">root@a37709:~# cio cat SUPERIO <br/>---<br/>capacity: 20<br/>directory: /cio/volumes<br/>iops:<br/>  min: 1000<br/>  max: 15000<br/>level: 2<br/>local: no<br/>provision: thin<br/>type: ssd<br/>service:<br/>  compression: no<br/>  dedupe: no<br/>  encryption:<br/>    enabled: no<br/>  replication:<br/>    enabled: no<br/>    destination: none<br/>    interval: 120<br/>    type: synchronous<br/>  snapshot:<br/>    enabled: no<br/>    interval: 60<br/>    max: 10</span></pre><p id="094d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意iops(最小值/最大值)和级别参数，这意味着实际上对IOPs和一个复制副本没有限制。</p><h1 id="7241" class="ls ll it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">一个客户的存储</h1><p id="1daa" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">让我们测试存储在我们的集群范围存储中的一个客户端卷:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="3114" class="lk ll it lg b gy lm ln l lo lp">root@a37709:~# docker volume create --driver cio --name highio1 --opt profile=SUPERIO<br/>root@a37709:~# docker run -ti --rm -v highio1:/home --name test-high oracle/orion-official:12.2.0.1 /usr/lib/oracle/12.2/client64/bin/orion -run simple -testname firsttest -hugenotneeded<br/>ORION: ORacle IO Numbers -- Version 12.2.0.1.0<br/>firsttest_20181004_1236<br/>Calibration will take approximately 9 minutes.<br/>Using a large value for -cache_size may take longer.</span><span id="123a" class="lk ll it lg b gy lq ln l lo lp">Maximum Large <strong class="lg iu"><em class="le">MBPS=297.31</em></strong> @ Small=0 and Large=1</span><span id="0653" class="lk ll it lg b gy lq ln l lo lp">Maximum Small <strong class="lg iu"><em class="le">IOPS=11863</em></strong> @ Small=5 and Large=0<br/>Small Read Latency: avg=419.660 us, min=185.907 us, max=15414.395 us, std dev=448.024 us @ Small=5 and Large=0</span><span id="e441" class="lk ll it lg b gy lq ln l lo lp">Minimum Small Latency=419.660 usecs @ Small=5 and Large=0<br/>Small Read Latency: avg=419.660 us, min=185.907 us, max=15414.395 us, std dev=448.024 us @ Small=5 and Large=0</span></pre><p id="c1fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">读取吞吐量大幅下降<strong class="js iu"> <em class="le"> 297.31 MBPs </em> </strong>相比<strong class="js iu"><em class="le">【NFS】</em></strong>的427.37 MBPs快了45%。就<strong class="js iu"> <em class="le">而言，IOPS </em> </strong>只相差10%。就延迟而言，这是直方图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/afdb0cfd1cacff7de0102603f47de656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*76QML70UDLhKGcT13jxOcQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">小读取延迟直方图</figcaption></figure><p id="73b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看起来相似，大部分响应时间在256-512 us范围内，但比其他产品更稳定。</p><h1 id="5061" class="ls ll it bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">有三个客户的仓库</h1><p id="74fb" class="pw-post-body-paragraph jq jr it js b jt mp jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj mt kl km kn im bi translated">让我们与更多的客户端一起摇滚，因为是分布式存储:</p><pre class="kp kq kr ks gt lf lg lh li aw lj bi"><span id="caed" class="lk ll it lg b gy lm ln l lo lp">root@a37709:~# docker run -ti --rm -v highio1:/home --name test-high oracle/orion-official:12.2.0.1 /usr/lib/oracle/12.2/client64/bin/orion -run simple -testname firsttest -hugenotneeded<br/>Maximum Large <strong class="lg iu"><em class="le">MBPS=296.15</em></strong> @ Small=0 and Large=1<br/>Maximum Small <strong class="lg iu"><em class="le">IOPS=11508</em></strong> @ Small=5 and Large=0</span><span id="72b4" class="lk ll it lg b gy lq ln l lo lp">root@d54224:~# docker run -ti --rm -v highio2:/home --name test-high oracle/orion-official:12.2.0.1 /usr/lib/oracle/12.2/client64/bin/orion -run simple -testname firsttest -hugenotneeded<br/>Maximum Large <strong class="lg iu"><em class="le">MBPS=314.14</em></strong> @ Small=0 and Large=2<br/>Maximum Small <strong class="lg iu"><em class="le">IOPS=9795</em></strong> @ Small=5 and Large=0</span><span id="e86d" class="lk ll it lg b gy lq ln l lo lp">root@b8ef30:~# docker run -ti --rm -v highio3:/home --name test-high oracle/orion-official:12.2.0.1 /usr/lib/oracle/12.2/client64/bin/orion -run simple -testname firsttest -hugenotneeded<br/>Maximum Large <strong class="lg iu"><em class="le">MBPS=292.21</em></strong> @ Small=0 and Large=1<br/>Maximum Small <strong class="lg iu"><em class="le">IOPS=11061</em></strong> @ Small=5 and Large=0</span></pre><p id="b389" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在情况不同了，我的聚合吞吐量为<strong class="js iu"> <em class="le"> 902，5</em></strong><strong class="js iu"><em class="le">MPBPs</em></strong>(296，15+314，14+292，21)，比有一个客户端的NFS多一倍，也比本地存储高(867.73)，为什么呢？因为Storidge是一种分布式实施，然后您可以聚合本地IO带宽和网络端口的总和。类似的还有IOPS的<strong class="js iu"><em class="le">32364</em></strong>！！！而本地存储只有12679个。因此，很明显，为您的持久Docker群服务使用分布式存储解决方案是一个不错的选择。这里是延迟直方图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mx"><img src="../Images/b0abd996f3be2ceb6a10cbdc5202c831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNvCu8FoAKAxJKkHxt-WyA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">三客户端延迟直方图</figcaption></figure><p id="5eb8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下一篇关于<a class="ae lr" href="https://www.beegfs.io/" rel="noopener ugc nofollow" target="_blank">bee GFS . io</a>Oracle Cloud分布式存储的文章。</p></div></div>    
</body>
</html>