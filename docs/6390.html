<html>
<head>
<title>Highly Available NFS cluster in Kubernetes, a cloud vendor independent storage solution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kubernetes中的高可用性NFS集群，这是一个独立于云供应商的存储解决方案</h1>
<blockquote>原文：<a href="https://itnext.io/highly-available-nfs-cluster-in-kubernetes-a-cloud-vendor-independent-storage-solution-f9a314cfdfcc?source=collection_archive---------0-----------------------#2021-11-05">https://itnext.io/highly-available-nfs-cluster-in-kubernetes-a-cloud-vendor-independent-storage-solution-f9a314cfdfcc?source=collection_archive---------0-----------------------#2021-11-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="c015" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我在为Kubernetes寻找一个<a class="ae ko" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes" rel="noopener ugc nofollow" target="_blank">存储解决方案</a>时，我认为我可能需要随时迁移到不同的云供应商，我发现大多数存储解决方案都是特定于供应商的。</p><h1 id="13e6" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">鲁克-Ceph</h1><p id="0c5e" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">在过去的几年里，我一直使用<a class="ae ko" href="https://rook.io/" rel="noopener ugc nofollow" target="_blank"> Rook-Ceph </a>解决方案。它抽象了云/本地提供的底层存储，并提供了rbd/cephfs卷(等等)。它非常容易使用，非常稳定。我的一个最老的集群已经运行了3年，没有出现任何问题。<a class="ae ko" href="https://github.com/rook/rook" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae ko" href="https://slack.rook.io/" rel="noopener ugc nofollow" target="_blank"> Slack </a>中的Rook社区非常有帮助。</p><p id="b0e8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除了与供应商无关之外，我还在Rook Slack channel上发现了使用Rook-Ceph优于云提供的存储的优势。即使独立于供应商不是目标，使用Rook-Ceph也是有价值的。报价:</p><blockquote class="ls lt lu"><p id="bf21" class="jq jr lv js b jt ju jv jw jx jy jz ka lw kc kd ke lx kg kh ki ly kk kl km kn im bi translated">相同的存储解决方案(如果在本地和云上安装rook ceph)，允许混合云设置，更具可配置性，读写很多，无需使用NFS</p><p id="8528" class="jq jr lv js b jt ju jv jw jx jy jz ka lw kc kd ke lx kg kh ki ly kk kl km kn im bi translated">克服每个节点的卷数限制。ceph的容量没有限制。</p><p id="5934" class="jq jr lv js b jt ju jv jw jx jy jz ka lw kc kd ke lx kg kh ki ly kk kl km kn im bi translated">如果您有许多小型PV，它们将会从云提供商那里获得较低的性能。使用Ceph，您可以用具有更好性能特征的大卷来支持它</p><p id="61df" class="jq jr lv js b jt ju jv jw jx jy jz ka lw kc kd ke lx kg kh ki ly kk kl km kn im bi translated">Ceph不受AZs的限制</p><p id="5956" class="jq jr lv js b jt ju jv jw jx jy jz ka lw kc kd ke lx kg kh ki ly kk kl km kn im bi translated">使用高级ceph功能，如集群之间的复制(rbd/cephfs镜像、rgw多站点)</p></blockquote><h1 id="266f" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">文件存储服务</h1><p id="1fc8" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我一直在Kubernetes集群中使用Rook-Ceph解决方案，例如，在一个k8s集群中，有存储节点并向同一个集群提供存储。在某些情况下，这可能是一个问题。</p><ul class=""><li id="ff9d" class="lz ma it js b jt ju jx jy kb mb kf mc kj md kn me mf mg mh bi translated">由于Rook-Ceph存储只能由同一集群中的工作负载使用，独立服务(不在任何集群中)必须使用云供应商的存储。</li><li id="ed90" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn me mf mg mh bi translated">当创建新的k8s集群来替换旧的集群时，数据迁移是一件痛苦的事情。我发现的最好的方法是从备份中恢复，但是必须仔细计划。</li><li id="b7b2" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn me mf mg mh bi translated">当使用多个Kubernetes策略来隔离不同的服务时，每个集群都需要自己的存储节点。如果集群的总规模很小，有时这是一种资源浪费。</li><li id="1d10" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn me mf mg mh bi translated">总是需要进行备份，这需要群集外存储。</li></ul><p id="377f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，我在寻找一种升级，为多个Kubernetes集群和/或集群外的服务提供存储。通过查看<a class="ae ko" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes" rel="noopener ugc nofollow" target="_blank">这个列表</a>，nfs是一个厂商中立的协议，由Kubernetes通过<a class="ae ko" href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner" rel="noopener ugc nofollow" target="_blank"> NFS provisioner </a>很好地支持，也由大多数类似Linux的系统广泛支持。看看是否可以扩展Rook-Ceph解决方案来满足需求是一个简单的想法。</p><p id="d247" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在得出我的解决方案之前，我四处搜索，看看是否已经有了什么。一个是“<a class="ae ko" href="https://www.adaltas.com/en/2020/04/16/expose-ceph-from-rook-kubernetes/" rel="noopener ugc nofollow" target="_blank">在Kubernetes </a>外部暴露一个基于Rook的Ceph集群”，通过主机网络对外暴露Rook创建的Ceph集群。必须做大量额外的工作来保护网络。另一个是GlusterFS，但似乎K8S 上的<a class="ae ko" href="https://github.com/gluster/gluster-kubernetes" rel="noopener ugc nofollow" target="_blank"> GlusterFS项目已经被搁置了。可以通过基于虚拟机的方式来实现，比如“</a><a class="ae ko" href="https://microdevsys.com/wp/glusterfs-configuration-and-setup-w-nfs-ganesha-for-an-ha-nfs-cluster/" rel="noopener ugc nofollow" target="_blank"> GlusterFS:配置和设置一个高可用性NFS集群的NFS-甘尼萨”。</a></p><h1 id="c06f" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">NFS集群</h1><p id="0d19" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">经过一些实验，这是解决方案，如下图所示。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mn"><img src="../Images/b2ef82bbb82f9d8ab08beba237aed531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_F9Kl6ACCtHP_2UXLQN79g.png"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated">系统结构</figcaption></figure><p id="ec41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有两个Haproxy实例共享一个虚拟IP地址，作为端口32049上的NFS集群的入口点。Haproxy还充当高可用性集群的apiserver负载平衡器。3个<a class="ae ko" href="https://github.com/traefik/traefik" rel="noopener ugc nofollow" target="_blank"> Traefik </a>实例是入口控制器集群，将节点端口32049暴露给haproxy。<a class="ae ko" href="https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/#kind-ingressroutetcp" rel="noopener ugc nofollow" target="_blank"> IngressRouteTCP </a>用于将流量路由到3个NFS甘尼萨服务，由鲁克Ceph <a class="ae ko" href="https://rook.github.io/docs/rook/v1.7/ceph-nfs-crd.html" rel="noopener ugc nofollow" target="_blank"> NFS CRD </a>提供。在NFS甘尼萨后面是由Rook-Ceph提供的Ceph文件系统。底层存储可以是由云/本地提供的满足Ceph <a class="ae ko" href="https://rook.github.io/docs/rook/v1.7/pre-reqs.html#ceph-prerequisites" rel="noopener ugc nofollow" target="_blank">要求</a>的任何东西。</p><p id="d6a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当客户端通过Haproxy的虚拟IP地址挂载时，这个NFS集群是高度可用的。如果其中一个Haproxy实例已关闭，或者其中一个Traefik实例已关闭，或者其中一个Ganesha实例已关闭，或者Ceph群集已降级但仍在运行，则NFS装载仍在运行。</p><p id="8436" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">github链接中共享了集群的一些配置。</p><pre class="mo mp mq mr gt nd ne nf ng aw nh bi"><span id="90da" class="ni kq it ne b gy nj nk l nl nm"><a class="ae ko" href="https://gist.github.com/liejuntao001/88ec87dff82ab0cc1f741a2fa4ee35ce" rel="noopener ugc nofollow" target="_blank">Github Link</a></span></pre><h1 id="62c1" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">表演</h1><p id="3e02" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">在我的设置中，NFS集群的设计目的是提供一个可扩展、高可用性、低成本的标准HDD层存储，因为我使用的底层存储有500 IOPS的限制。因此，我无法测试这种架构的性能极限。</p><p id="a8d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与Ceph文件系统的性能相比，3个增加的层，例如NFS甘尼萨、入口控制器和Haproxy，将影响最终NFS文件系统的性能。与本地网络相比，公共云网络中的网络延迟要高得多，因此我认为每个额外的网络跳增加的网络延迟也会影响性能。</p><p id="7ed5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我做了一个非常粗略的4k随机读写性能测试，以比较体系结构中4个点的平均iops。</p><p id="2cb1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">测试命令</p><pre class="mo mp mq mr gt nd ne nf ng aw nh bi"><span id="5db3" class="ni kq it ne b gy nj nk l nl nm">fio --randrepeat=1 --ioengine=libaio --direct=1 --numjobs=2 --nrfiles=4 --gtod_reduce=1 --name=test --filename=random_read_write.fio --bs=4k --iodepth=64 --size=1G --readwrite=randrw --rwmixread=75 --ramp_time=30 --runtime=30 --time_based --group_reporting --stonewall</span></pre><p id="855e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">测试结果示例</p><pre class="mo mp mq mr gt nd ne nf ng aw nh bi"><span id="d9e0" class="ni kq it ne b gy nj nk l nl nm">fio-3.19</span><span id="d3d8" class="ni kq it ne b gy nn nk l nl nm">read: IOPS=1098, BW=4406KiB/s (4511kB/s)(131MiB/30434msec)</span><span id="fb15" class="ni kq it ne b gy nn nk l nl nm">bw (  KiB/s): min=  753, max= 6360, per=100.00%, avg=4462.34, stdev=528.48, samples=118</span><span id="bb14" class="ni kq it ne b gy nn nk l nl nm">iops        : min=  187, max= 1590, <strong class="ne iu">avg=1115.19</strong>, stdev=132.19, samples=118</span><span id="c48a" class="ni kq it ne b gy nn nk l nl nm">write: IOPS=365, BW=1468KiB/s (1503kB/s)(43.6MiB/30434msec); 0 zone resets</span><span id="ea1c" class="ni kq it ne b gy nn nk l nl nm">bw (  KiB/s): min=   82, max= 2088, per=100.00%, avg=1481.05, stdev=189.45, samples=118</span><span id="9cf4" class="ni kq it ne b gy nn nk l nl nm">iops        : min=   20, max=  522, <strong class="ne iu">avg=369.92</strong>, stdev=47.47, samples=118</span></pre><p id="beed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本例中，我将“读取iops平均值1115”和“写入iops平均值369”相加，得出总iops为1484。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mn"><img src="../Images/b777262393f8e7d868245ed863532899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L1c2YYn4lf9kdUMY42rQ2A.png"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated">性能和安全性</figcaption></figure><p id="24c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如图所示，4个测试点是:</p><ol class=""><li id="777b" class="lz ma it js b jt ju jx jy kb mb kf mc kj md kn no mf mg mh bi translated">将Ceph文件系统直接挂载到集群中，作为基线</li><li id="115a" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn no mf mg mh bi translated">为每个nfs svc在集群中装载NFS</li><li id="1c3b" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn no mf mg mh bi translated">对于每个Traefik实例，从入口控制器挂载NFS</li><li id="4324" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn no mf mg mh bi translated">从哈普洛克西登上NFS</li></ol><p id="3d8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Ceph群集有3个4CPU/8G RAM节点，每个节点有2个500IOPS磁盘。SSD磁盘用于OSD的元数据。我收集的数据如下。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi np"><img src="../Images/1a99f65649fd598a5c4506c5d018df62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*KhmVVGpRApf4ZAIVxMYo2A.png"/></div></figure><p id="0a06" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我的设置中，通过将Ceph文件系统转换为HA NFS集群，性能损失超过了50%。我不明白为什么测试点2比测试点3慢。</p><h1 id="5b92" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">安全性</h1><p id="eb64" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">安全有许多方面，这里的特殊部分是保护NFS港，以避免意外的访问。当然，所有的服务都将在私有IP地址上运行。</p><ol class=""><li id="11d7" class="lz ma it js b jt ju jx jy kb mb kf mc kj md kn no mf mg mh bi translated">安全点a .使用iptables来保护2个Haproxy实例。因为它们不在K8s集群中，所以很容易只允许已识别的IP。</li><li id="7448" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn no mf mg mh bi translated">安全点b。3个入口控制器为Haproxy暴露端口32049。在Kubernetes中，如果同一个数据中心中有其他邻居主机可以访问私有IP，那么保护通过NodePort暴露的端口并不容易。我还没有掌握使用<a class="ae ko" href="https://docs.projectcalico.org/reference/resources/globalnetworkpolicy" rel="noopener ugc nofollow" target="_blank">Calico GlobalNetworkPolicy</a>的方法，所以这里我使用Traefik的IPWhiteList中间件，只允许识别的IP。为了实现这一点，Haproxy和Traefik都启用了<a class="ae ko" href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt" rel="noopener ugc nofollow" target="_blank">代理协议</a>来传递客户端的IP地址。</li><li id="9c79" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn no mf mg mh bi translated">从NFS甘尼萨的角度来看，所有连接都来自入口控制器的集群内IP地址。而且我没有发现它支持代理协议，所以NFS不可能知道实际客户的IP地址。所有基于IP地址的权限控制都无法使用。</li></ol><h1 id="1538" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">供选择的</h1><p id="8bee" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">还有另一个<a class="ae ko" href="https://github.com/rook/nfs" rel="noopener ugc nofollow" target="_blank"> Rook-NFS </a>项目，它将一个文件系统(K8s中的任何PV)转换成一个NFS集群。因此，另一种结构是在入口控制器后面使用Rook-NFS，并使用Rook-Ceph中的Ceph文件系统作为后端存储。据我测试，它工作正常，我觉得它更像是云原生的，但目前鲁克-NFS还处于alpha阶段，缺乏一些重要的产品特性。</p><p id="855b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢阅读。</p><blockquote class="ls lt lu"><p id="7b6d" class="jq jr lv js b jt ju jv jw jx jy jz ka lw kc kd ke lx kg kh ki ly kk kl km kn im bi translated">2021年11月16日更新</p></blockquote><p id="54f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">NFS集群由Ceph文件系统提供支持。我们需要为存储的数据制定一个备份计划。这可以从NFS客户端完成，也可以从服务器端集中完成。我开发了一种备份Ceph文件系统的方法，在这里共享<a class="ae ko" href="https://liejuntao001.medium.com/file-system-backup-for-ceph-in-kubernetes-6c299c860ab3" rel="noopener"/>。</p></div></div>    
</body>
</html>