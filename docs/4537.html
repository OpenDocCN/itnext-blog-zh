<html>
<head>
<title>Enrich your Ceph Object Storage Data Lake by leveraging Kafka as the Data Source</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过利用Kafka作为数据源来丰富您的Ceph对象存储数据湖</h1>
<blockquote>原文：<a href="https://itnext.io/enrich-your-ceph-object-storage-data-lake-by-leveraging-kafka-as-the-data-source-e9a4d305abcf?source=collection_archive---------4-----------------------#2020-07-20">https://itnext.io/enrich-your-ceph-object-storage-data-lake-by-leveraging-kafka-as-the-data-source-e9a4d305abcf?source=collection_archive---------4-----------------------#2020-07-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="54fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">了解如何使用Secor将Kafka消息移动到Ceph S3对象存储中</em></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/eae126a47a40a47c29d86c02d43ab43b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OS66HyMmsBPZw83R.png"/></div></div></figure><blockquote class="ky kz la"><p id="0f2f" class="jn jo kl jp b jq jr js jt ju jv jw jx lb jz ka kb lc kd ke kf ld kh ki kj kk ij bi translated">如果你的业务是你的太阳系，那么你的数据就是太阳，它既有引力又有质量，一切都围绕着它转，它必须永远存在——我自己</p></blockquote><h1 id="71f1" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">介绍</h1><p id="d609" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">Kafka是最受欢迎的消息系统之一，用于实时数据流，收集大数据，或进行实时分析或两者兼而有之。Kafka用于将数据流导入数据湖、应用程序和实时流分析系统。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mh"><img src="../Images/313250f966b671cc4bd047cf3f1c8334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O6uma1aZtjH_Gr5r.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">Apache Kafka用例</figcaption></figure><h1 id="a2a9" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Kafka:数据湖的主要来源</h1><p id="01ac" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">Kafka是一把瑞士军刀，用于构建可扩展的容错架构。虽然您可以在各种用例中使用Kafka，但这篇文章将详细介绍如何利用Kafka作为您的数据湖的主要来源。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mm"><img src="../Images/3da2057c033097f97a16cc9ec5751353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AOggFrkP4i1e0cba.png"/></div></div></figure><p id="b18e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面显示的高级架构很简单，您关心的长期存储数据，无论是与业务决策相关、分析相关还是与合规性相关，或者只是您不想删除这些数据，您都可以选择将Kafka中的数据转储到符合S3标准的共享对象存储中(如Ceph)。S3和S3A接口无处不在，几乎所有您喜欢的工具都支持这些协议。你永远不会觉得自己被锁在里面。</p><p id="fe60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于绿地部署，如果您要在公共云上构建数据基础架构，可以使用AWS、GCP、Azure和Oracle Cloud。您拥有来自各个提供商的对象存储服务，您可以利用它来构建这个体系结构。</p><p id="a71c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于内部绿地/棕地部署，在为您的架构选择组件时，您需要非常谨慎。如果你的业务是你的太阳系，那么你的数据就是太阳，它既有引力又有质量。一切都围绕着它，它必须永生。</p><p id="f8c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">公共云以较低的成本为您带来灵活性。有了Snow Ball / Snow Mobile这样的服务，把自己多年积累的数据转移到公有云对象存储服务上并不是超级困难的事情。最被忽视的想法是“<em class="kl">如何从公共云服务</em>把你的数据带回家”。在云计算巨头发布的任何产品和服务公告中，您是否听说过他们中的任何一家推出了一项服务，可以帮助您以合理的成本将存储在他们的任何服务中的数据大规模移动到您的内部位置？</p><blockquote class="ky kz la"><p id="f9f1" class="jn jo kl jp b jq jr js jt ju jv jw jx lb jz ka kb lc kd ke kf ld kh ki kj kk ij bi translated">公共云==锁定专有技术</p></blockquote><p id="5f1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您已经战略性地选择将您的计算保留在本地，那么构建您自己的本地对象存储服务作为您的共享数据存储库是完全有意义的。开源数据存储解决方案，如Red Hat Ceph Storage/Red Hat open shift Container Storage，不仅帮助您获得对数据的完全控制，还为您面向未来的数据服务提供了一个经济高效且可扩展的选项。你在这种情况下考虑HDFS的日子已经一去不复返了。HDFS在过去十年表现出色，你应该投资于符合行业标准、面向未来的本地云解决方案，如Ceph S3对象存储。</p><h1 id="860d" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">将数据从Kafka移动到Ceph S3对象存储</h1><p id="de0e" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated">有多种方法可以将数据从Kafka主题转移到Ceph S3对象存储，例如使用开源工具，如<a class="ae mn" href="https://github.com/pinterest/secor" rel="noopener ugc nofollow" target="_blank"> Secor(最初来自Pinterest ) </a>、Apache-Camel🐪S3连接器(<a class="ae mn" href="https://medium.com/@karansingh010/exporting-data-from-apache-kafka-red-hat-amq-streams-topics-to-s3-using-apache-camel-connector-66a56af490e" rel="noopener">见我的另一个博客</a>)或汇合的卡夫卡S3连接器。</p><p id="405c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我们将深入探讨如何在OpenShift上部署Secor，并将其配置为将Kafka主题消息移动到Red Hat OpenShift容器存储S3对象存储服务。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mo"><img src="../Images/8a1bf3f56a6f4665f14c107d074b821c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1A4QnY_3G5cly_YO.png"/></div></div></figure><h2 id="e9cd" class="mp lf iq bd lg mq mr dn lk ms mt dp lo jy mu mv ls kc mw mx lw kg my mz ma na bi translated">先决条件</h2><ul class=""><li id="cdfc" class="nb nc iq jp b jq mc ju md jy nd kc ne kg nf kk ng nh ni nj bi translated">运行RedHat OpenShift 4集群</li><li id="946e" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">运行Red Hat OpenShift容器存储4</li></ul><h2 id="bc4c" class="mp lf iq bd lg mq mr dn lk ms mt dp lo jy mu mv ls kc mw mx lw kg my mz ma na bi translated">履行</h2><p id="fc28" class="pw-post-body-paragraph jn jo iq jp b jq mc js jt ju md jw jx jy me ka kb kc mf ke kf kg mg ki kj kk ij bi translated"><strong class="jp ir">第1部分:在OpenShift上设置Kafka集群</strong></p><ul class=""><li id="c7ff" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">验证您的OpenShift集群</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="0473" class="mp lf iq nt b gy nx ny l nz oa">oc get nodes<br/>oc get sc</span></pre><ul class=""><li id="a879" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">在OpenShift上部署Kafka集群</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="f0a4" class="mp lf iq nt b gy nx ny l nz oa">oc new-project kafka-to-s3 </span><span id="89de" class="mp lf iq nt b gy ob ny l nz oa">wget<a class="ae mn" href="https://github.com/confluentinc/cp-helm-charts/releases/download/v5.5.0/cp-helm-charts-0.5.0.tgz" rel="noopener ugc nofollow" target="_blank">https://github.com/confluentinc/cp-helm-charts/releases/download/v5.5.0/cp-helm-charts-0.5.0.tgz</a></span><span id="56e0" class="mp lf iq nt b gy ob ny l nz oa">tar -xvf cp-helm-charts-0.5.0.tgz</span></pre><ul class=""><li id="f54c" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">编辑Kafka头盔图表值文件<code class="fe oc od oe nt b">vim cp-helm-charts/values.yaml</code>，为Zookeeper和Kafka舱添加持久存储。运行<code class="fe oc od oe nt b">oc get sc</code>获取您的OCP块存储类的名称</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/178b7d83484fc17391cc3c58acac04ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/0*-RNwsQVbmxzh6OAX.png"/></div></figure><ul class=""><li id="cdc4" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">禁用目前不相关的其他服务</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/fbfea41bcfe64f9c73fc140e37fb0141.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/0*a2n9TjoqQhUXy8Yq.png"/></div></figure><ul class=""><li id="eb73" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">这个Kafka helm图表使用需要root权限的docker映像，调整OCP安全上下文约束来启用它</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="550b" class="mp lf iq nt b gy nx ny l nz oa">oc adm policy add-scc-to-user anyuid -z default</span></pre><ul class=""><li id="f343" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">安装卡夫卡</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="c358" class="mp lf iq nt b gy nx ny l nz oa">helm install cp-helm-charts --generate-name -n kafka-to-s3<br/>oc get all -n kafka-to-s3</span></pre><p id="21a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第2部分:启用Ceph S3对象存储服务</strong></p><ul class=""><li id="77f3" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">使用<code class="fe oc od oe nt b">oc project openshift-storage</code>切换到<code class="fe oc od oe nt b">openshift-storage</code>项目</li><li id="9f48" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">用以下内容创建一个CR文件<code class="fe oc od oe nt b">ceph_rgw_object_store.yaml</code></li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="d215" class="mp lf iq nt b gy nx ny l nz oa">apiVersion: v1<br/>items:<br/>- apiVersion: ceph.rook.io/v1<br/>  kind: CephObjectStore<br/>  metadata:<br/>    generation: 1<br/>    name: s3a<br/>    namespace: openshift-storage<br/>  spec:<br/>    dataPool:<br/>      crushRoot: ""<br/>      deviceClass: ""<br/>      erasureCoded:<br/>        algorithm: ""<br/>        codingChunks: 0<br/>        dataChunks: 0<br/>      failureDomain: zone<br/>      replicated:<br/>        size: 3<br/>    gateway:<br/>      allNodes: false<br/>      instances: 1<br/>      placement:<br/>        nodeAffinity:<br/>          requiredDuringSchedulingIgnoredDuringExecution:<br/>            nodeSelectorTerms:<br/>            - matchExpressions:<br/>              - key: cluster.ocs.openshift.io/openshift-storage<br/>                operator: Exists<br/>        podAntiAffinity:<br/>          preferredDuringSchedulingIgnoredDuringExecution:<br/>          - podAffinityTerm:<br/>              labelSelector:<br/>                matchExpressions:<br/>                - key: app<br/>                  operator: In<br/>                  values:<br/>                  - rook-ceph-rgw<br/>              topologyKey: kubernetes.io/hostname<br/>            weight: 100<br/>        tolerations:<br/>        - effect: NoSchedule<br/>          key: node.ocs.openshift.io/storage<br/>          operator: Equal<br/>          value: "true"<br/>      port: 80<br/>      resources:<br/>        limits:<br/>          cpu: "1"<br/>          memory: 2Gi<br/>        requests:<br/>          cpu: "1"<br/>          memory: 1Gi<br/>      securePort: 0<br/>      sslCertificateRef: ""<br/>    metadataPool:<br/>      crushRoot: ""<br/>      deviceClass: ""<br/>      erasureCoded:<br/>        algorithm: ""<br/>        codingChunks: 0<br/>        dataChunks: 0<br/>      failureDomain: zone<br/>      replicated:<br/>        size: 3<br/>kind: List<br/>metadata:<br/>  resourceVersion: ""<br/>  selfLink: ""</span></pre><ul class=""><li id="6d5d" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">应用CR <code class="fe oc od oe nt b">oc create -f ceph_rgw_object_store.yaml -n openshift-storage</code></li><li id="bfd6" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">这将触发Ceph RGW吊舱的创建，使用<code class="fe oc od oe nt b">oc get po | grep -i rgw</code>进行验证</li><li id="cfe4" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">通过应用以下CR文件创建对象存储用户</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="28bc" class="mp lf iq nt b gy nx ny l nz oa">apiVersion: ceph.rook.io/v1<br/>kind: CephObjectStoreUser<br/>metadata:<br/>  name: s3user1<br/>  namespace: openshift-storage<br/>spec:<br/>  store: s3a<br/>  displayName: "s3 user1"</span></pre><p id="ceb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您在没有有效SSL证书的OpenShift环境中尝试这样做，那么您必须生成Let's Encrypt证书，并将这些证书应用到OpenShift集群。SSL证书的缺失会给Secor联系不安全的S3端点带来问题。<a class="ae mn" href="https://medium.com/@karansingh010/lets-automate-let-s-encrypt-tls-certs-for-openshift-4-211d6c081875" rel="noopener">使用我的另一个博客</a>来实现使用Let's Encrypt的SSL证书。</p><p id="09ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第3部分:验证对OpenShift容器存储对象服务的访问</strong></p><ul class=""><li id="dfbb" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">出口<code class="fe oc od oe nt b">AWS_ACCESS_KEY</code> <code class="fe oc od oe nt b">AWS_SECRET_KEY</code> <code class="fe oc od oe nt b">RGW_ENDPOINT</code></li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="27fd" class="mp lf iq nt b gy nx ny l nz oa">export AWS_ACCESS_KEY_ID=`oc get secret -n openshift-storage rook-ceph-object-user-s3a-s3user1 -o 'jsonpath={.data.AccessKey}' | base64 --decode;echo`</span><span id="2e8e" class="mp lf iq nt b gy ob ny l nz oa">export AWS_SECRET_ACCESS_KEY=`oc get -n openshift-storage secret rook-ceph-object-user-s3a-s3user1 -o 'jsonpath={.data.SecretKey}' | base64 --decode;echo`</span><span id="dcf0" class="mp lf iq nt b gy ob ny l nz oa">export RGW_ENDPOINT=$(oc get route -n openshift-storage --selector='app=rook-ceph-rgw' -o 'jsonpath={.items..spec.host}')</span></pre><ul class=""><li id="1d8c" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">尝试使用s3cmd命令行工具列出和创建存储桶</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="bc01" class="mp lf iq nt b gy nx ny l nz oa">s3cmd --access_key=$AWS_ACCESS_KEY_ID --secret_key=$AWS_SECRET_ACCESS_KEY --ssl  --host=$RGW_ENDPOINT  --host-bucket="$RGW_ENDPOINT/%(bucket)" ls</span><span id="482a" class="mp lf iq nt b gy ob ny l nz oa">s3cmd --access_key=$AWS_ACCESS_KEY_ID --secret_key=$AWS_SECRET_ACCESS_KEY --ssl  --host=$RGW_ENDPOINT  --host-bucket="$RGW_ENDPOINT/%(bucket)" mb s3://kafka-to-ceph-s3</span><span id="ba08" class="mp lf iq nt b gy ob ny l nz oa">s3cmd --access_key=$AWS_ACCESS_KEY_ID --secret_key=$AWS_SECRET_ACCESS_KEY --ssl  --host=$RGW_ENDPOINT  --host-bucket="$RGW_ENDPOINT/%(bucket)" ls</span></pre><p id="0f89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第四部分:与卡夫卡集群的联系和互动</strong></p><ul class=""><li id="e69f" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">使用这个YAML创建一个Kafka客户端</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="bfa3" class="mp lf iq nt b gy nx ny l nz oa">oc project kafka-to-s3vim kafka-client.yamlapiVersion: v1</span><span id="b56e" class="mp lf iq nt b gy ob ny l nz oa">kind: Pod<br/>metadata:<br/>  name: kafka-client<br/>spec:<br/>  containers:<br/>  - name: kafka-client<br/>    image: confluentinc/cp-kafka:5.5.0<br/>    command:<br/>      - sh<br/>      - -c<br/>      - "exec tail -f /dev/null"oc create -f kafka-client.yaml</span><span id="d173" class="mp lf iq nt b gy ob ny l nz oa">oc get po</span></pre><ul class=""><li id="7642" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">抓取Kafka集群端点</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="a664" class="mp lf iq nt b gy nx ny l nz oa">oc get svc -n kafka-to-s3 --selector=app=cp-kafka | grep -v None</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oh"><img src="../Images/1117616293894b468edde8bac0e1fb8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XmJEBCyPEy1lXeqX.png"/></div></div></figure><ul class=""><li id="56b5" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">登录Kafka客户端窗格，使用<code class="fe oc od oe nt b">kafka-console-producer</code>生成一些消息</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="a015" class="mp lf iq nt b gy nx ny l nz oa">oc project kafka-to-s3<br/>oc rsh kafka-client</span><span id="0ad8" class="mp lf iq nt b gy ob ny l nz oa">## Replace kafka endpoint with your environment</span><span id="2b7b" class="mp lf iq nt b gy ob ny l nz oa">kafka-console-producer --broker-list cp-helm-charts-1595009069-cp-kafka:9092 --topic my-topic<br/>&gt;</span></pre><ul class=""><li id="db54" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">在<code class="fe oc od oe nt b">&gt;</code>提示符下生成几条随机信息，按<code class="fe oc od oe nt b">ctrl+c</code>退出</li><li id="e7b6" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">使用<code class="fe oc od oe nt b">kafka-console-consumer</code>重新连接并列出您的消息，然后按<code class="fe oc od oe nt b">ctrl+c</code>退出。</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="222a" class="mp lf iq nt b gy nx ny l nz oa">kafka-console-consumer --bootstrap-server cp-helm-charts-1595009069-cp-kafka:9092 --topic my-topic --from-beginning</span></pre><ul class=""><li id="a850" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">如果您能够查看您的消息，那么到目前为止一切都很顺利</li><li id="e08a" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">生成几条关于卡夫卡主题的信息</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="6d14" class="mp lf iq nt b gy nx ny l nz oa">cat /etc/sysctl.conf | kafka-console-producer --broker-list cp-helm-charts-1595009069-cp-kafka:9092 --topic my-topic</span></pre><p id="6b15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第5部分:部署Secor服务</strong></p><p id="3973" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如本文引言部分所解释的，我们将使用开源部门项目(由Pinterest开发)向Ceph S3对象存储服务发送Kafka主题消息。让我们在OpenShift容器平台上部署Secor应用程序</p><ul class=""><li id="45ef" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">创建一个名为<code class="fe oc od oe nt b">secor-kafka.yaml</code>的YAML文件。在应用之前，您需要根据您的环境更改一些变量</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="b780" class="mp lf iq nt b gy nx ny l nz oa">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  labels:<br/>    app: secor<br/>  name: secor<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      app: secor<br/>  strategy: {}<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: secor<br/>    spec:<br/>      containers:<br/>      - image: karansingh/secor-0.29-hadoop-2.9.2:latest<br/>        name: secor-0-29-hadoop-2-9-2<br/>        env:<br/>        - name: ZOOKEEPER_PATH<br/>          value: "/"<br/>        - name: ZOOKEEPER_QUORUM<br/>          value: "cp-helm-charts-1595009069-cp-zookeeper:2181"<br/>        - name: KAFKA_SEED_BROKER_HOST<br/>          value: "cp-helm-charts-1595009069-cp-kafka"<br/>        - name: KAFKA_SEED_BROKER_PORT<br/>          value: "9092"<br/>        - name: AWS_ACCESS_KEY<br/>          value: "YOUR_ACCESS_KEY"<br/>        - name: AWS_SECRET_KEY<br/>          value: "YOUR_SECRET_KEY"<br/>        - name: AWS_ENDPOINT<br/>          value: "rook-ceph-rgw-s3a-https-aws-lb-openshift-storage.apps.presto.ceph-s3.com:443"<br/>        - name: AWS_PATH_STYLE_ACCESS<br/>          value: "true"<br/>        - name: SECOR_S3_BUCKET<br/>          value: "kafka-to-ceph-s3"<br/>        - name: SECOR_GROUP<br/>          value: "raw_logs"<br/>        - name: SECOR_S3_PATH<br/>          value: "kafka-messages"<br/>        - name: KAFKA_OFFSETS_STORAGE<br/>          value: "zookeeper"<br/>        - name: SECOR_MAX_FILE_SECONDS<br/>          value: "10"<br/>        - name: SECOR_MAX_FILE_BYTES<br/>          value: "10000"<br/>        - name: SECOR_UPLOAD_MANAGER<br/>          value: "com.pinterest.secor.uploader.S3UploadManager"<br/>        - name: SECOR_MESSAGE_PARSER<br/>          value: "com.pinterest.secor.parser.OffsetMessageParser"<br/>        - name: DEBUG<br/>          value: "True"<br/>        - name: SECOR_KAFKA_TOPIC_FILTER<br/>          value: "my-topic"<br/>        resources: {}<br/>status: {}</span></pre><ul class=""><li id="8fc5" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">至少，你需要改变变量的值，比如<code class="fe oc od oe nt b">ZOOKEEPER_QUORUM</code> <code class="fe oc od oe nt b">KAFKA_SEED_BROKER_HOST</code> <code class="fe oc od oe nt b">AWS_ACCESS_KEY</code> <code class="fe oc od oe nt b">AWS_SECRET_KEY</code> <code class="fe oc od oe nt b">AWS_ENDPOINT</code></li><li id="4277" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">以下命令应该会给您提供正确的值</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="f458" class="mp lf iq nt b gy nx ny l nz oa">oc get svc -n kafka-to-s3  | grep -v None</span><span id="c5d0" class="mp lf iq nt b gy ob ny l nz oa">export AWS_ACCESS_KEY_ID=`oc get secret -n openshift-storage rook-ceph-object-user-s3a-s3user1 -o 'jsonpath={.data.AccessKey}' | base64 --decode;echo`</span><span id="c012" class="mp lf iq nt b gy ob ny l nz oa">export AWS_SECRET_ACCESS_KEY=`oc get -n openshift-storage secret rook-ceph-object-user-s3a-s3user1 -o 'jsonpath={.data.SecretKey}' | base64 --decode;echo`</span><span id="2950" class="mp lf iq nt b gy ob ny l nz oa">export RGW_ENDPOINT=$(oc get route -n openshift-storage --selector='app=rook-ceph-rgw' -o 'jsonpath={.items..spec.host}')</span><span id="2e4d" class="mp lf iq nt b gy ob ny l nz oa">echo $AWS_ACCESS_KEY_ID<br/>echo $AWS_SECRET_ACCESS_KEY<br/>echo $RGW_ENDPOINT</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oi"><img src="../Images/103bb1e2c2efec3f747cb42b0afc8812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*G45x_6_xw48Uxq-h.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">卡夫卡和动物园管理员端点</figcaption></figure><ul class=""><li id="4843" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">一旦有了正确的值，更新<code class="fe oc od oe nt b">secor-kafka.yaml</code>并部署扇区服务</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="3e50" class="mp lf iq nt b gy nx ny l nz oa">oc create -f secor-kafka.yaml<br/>oc get all -n kafka-to-s3</span></pre><p id="3e1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第6部分:验证Secor移动数据到Ceph S3 </strong></p><p id="3a15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于我们在Kafka主题中已经有了名为<code class="fe oc od oe nt b">my-topic</code>的消息，只要您部署secor服务，它就会检测到主题中有一些未被移动的Kafka消息，它会立即处理这些消息，并将它们移动到选定的Ceph S3桶中。</p><ul class=""><li id="4c83" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">要看到神奇之处，例如secor如何将Kafka主题消息移动到Ceph S3桶，您需要跟踪secor日志</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="d2a5" class="mp lf iq nt b gy nx ny l nz oa"># On one terminal tail secor logs</span><span id="0378" class="mp lf iq nt b gy ob ny l nz oa">oc logs -f &lt;secor_pod_name&gt;</span></pre><ul class=""><li id="14f7" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">在日志中，您会看到如下条目，这验证了Secor正在向Ceph S3 bucket发送Kafka消息</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="76be" class="mp lf iq nt b gy nx ny l nz oa">2020-07-20 13:56:15,225 [Thread-3] (com.pinterest.secor.uploader.S3UploadManager) INFO  uploading file /mnt/secor_data/message_logs/partition/1_12/my-topic/offset=0/1_0_00000000000000000018 to s3://kafka-to-ceph-s3/raw_logs/my-topic/offset=0/1_0_00000000000000000018 with no encryption</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oj"><img src="../Images/a587933b3394d03b69fd19b2df77af0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IKupXspa7ZFSgTZF.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">Secor向Ceph S3铲斗发送信息</figcaption></figure><ul class=""><li id="36e0" class="nb nc iq jp b jq jr ju jv jy np kc nq kg nr kk ng nh ni nj bi translated">列出Ceph S3存储桶，以验证Ceph对象存储服务正在从Kafka接收数据</li></ul><pre class="kn ko kp kq gt ns nt nu nv aw nw bi"><span id="658f" class="mp lf iq nt b gy nx ny l nz oa">s3cmd --access_key=$AWS_ACCESS_KEY_ID --secret_key=$AWS_SECRET_ACCESS_KEY --ssl  --host=$RGW_ENDPOINT  --host-bucket="$RGW_ENDPOINT/%(bucket)" ls s3://kafka-to-ceph-s3/raw_logs/my-topic/offset=0/</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ok"><img src="../Images/845ece528200b7ef71c0f192b02ef19f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oVXxZzzx-OcH69-M.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">列出Ceph S3桶以验证Kafka消息的输入</figcaption></figure><h1 id="98d0" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">摘要</h1><ul class=""><li id="3997" class="nb nc iq jp b jq mc ju md jy nd kc ne kg nf kk ng nh ni nj bi translated">Red Hat OpenShift容器存储对象服务使您能够为您的业务开发一个数据湖库。</li><li id="5346" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">开源项目<strong class="jp ir"> Secor </strong>提供了一种将你的Kafka数据转移到S3对象存储器的可靠方法</li><li id="39f6" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">Ceph是一个符合S3标准的可扩展对象存储开源解决方案，与S3 it部门一起还支持S3A协议，这是消费对象存储兼容数据湖解决方案的行业标准方式。</li><li id="65df" class="nb nc iq jp b jq nk ju nl jy nm kc nn kg no kk ng nh ni nj bi translated">一旦数据被接收到Ceph数据湖中，就可以使用您选择的引擎对其进行处理，使用您选择的工具对其进行可视化。</li></ul></div></div>    
</body>
</html>