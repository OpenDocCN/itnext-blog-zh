<html>
<head>
<title>Getting Started with Spark Structured Streaming and Kafka on AWS using Amazon MSK and Amazon EMR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用亚马逊MSK和亚马逊EMR在AWS上开始使用Spark结构化流和Kafka</h1>
<blockquote>原文：<a href="https://itnext.io/getting-started-with-spark-structured-streaming-and-kafka-on-aws-using-amazon-msk-and-amazon-emr-91b1f2ef0162?source=collection_archive---------0-----------------------#2021-09-09">https://itnext.io/getting-started-with-spark-structured-streaming-and-kafka-on-aws-using-amazon-msk-and-amazon-emr-91b1f2ef0162?source=collection_archive---------0-----------------------#2021-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ad42" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用批处理查询和Spark结构化流，使用Apache Kafka探索Apache Spark</h2></div><p id="115f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结构化流是一个基于Spark SQL引擎的可扩展和容错的流处理引擎。使用结构化流，您可以像在静态数据上表达批处理计算一样表达您的流计算。在本帖中，我们将学习如何在Apache Kafka中使用Apache Spark和Spark结构化流。具体来说，我们将利用亚马逊EMR(<em class="le">fka Amazon Elastic MapReduce</em>)上的结构化流，以及针对Apache Kafka(亚马逊MSK)的亚马逊托管流。我们将使用批处理和流查询从Kafka消费和发布到Kafka。对于这篇文章，Spark作业将使用PySpark用Python编写。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/3ec2e1fe91ad3bebf6d390d5d4d21322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8bK_UDwA6HOn6FUCJcuROg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">本文演示的高级AWS架构</figcaption></figure><h2 id="d377" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">阿帕奇火花</h2><p id="89e9" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">根据<a class="ae mt" href="https://spark.apache.org/docs/latest/index.html" rel="noopener ugc nofollow" target="_blank">文档</a>，Apache Spark是一个用于大规模数据处理的统一分析引擎。它提供了Java、Scala、Python (PySpark)和R中的高级API，以及一个支持通用执行图的优化引擎。此外，Spark支持一组丰富的高级工具，包括用于SQL和结构化数据处理的Spark SQL，用于机器学习的MLlib，用于图形处理的GraphX，以及用于增量计算和流处理的结构化流。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mu"><img src="../Images/be5e9807273fac1cecdc1c04ef268a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxG7RAvda8tWiHYmTaVk3Q.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">据<a class="ae mt" href="https://trends.google.com/trends/explore?geo=US&amp;q=%2Fm%2F0ndhxqz,PySpark,%2Fm%2F0g55vt5,%2Fg%2F11bywl5x_j" rel="noopener ugc nofollow" target="_blank">谷歌趋势</a>称，Apache Spark和PySpark对Apache Hive，随着时间的推移，兴趣迅速增长</figcaption></figure><h2 id="6b59" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">火花结构化流</h2><p id="c196" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">根据<a class="ae mt" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">文档</a>，Spark结构化流是一个基于Spark SQL引擎的可扩展和容错的流处理引擎。您可以像表达静态数据上的批处理计算一样表达您的流计算。Spark SQL引擎将不断递增地运行它，并随着流数据的不断到达更新最终结果。简而言之，结构化流提供了快速、可伸缩、容错、端到端、恰好一次的流处理，而无需用户对流进行推理。</p><h2 id="d3ac" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">亚马逊电子病历</h2><p id="f839" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">根据<a class="ae mt" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank">文档</a>，Amazon EMR(<em class="le">fka Amazon Elastic MapReduce</em>是一个基于云的大数据平台，使用<a class="ae mt" href="https://aws.amazon.com/emr/features/spark/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>、<a class="ae mt" href="https://hadoop.apache.org/" rel="noopener ugc nofollow" target="_blank"> Hadoop </a>、<a class="ae mt" href="https://aws.amazon.com/emr/features/hive/" rel="noopener ugc nofollow" target="_blank"> Hive </a>、<a class="ae mt" href="https://aws.amazon.com/emr/features/hbase/" rel="noopener ugc nofollow" target="_blank"> HBase </a>、<a class="ae mt" href="https://aws.amazon.com/blogs/big-data/use-apache-flink-on-amazon-emr/" rel="noopener ugc nofollow" target="_blank"> Flink </a>、<a class="ae mt" href="https://aws.amazon.com/emr/features/hudi/" rel="noopener ugc nofollow" target="_blank">胡迪</a>、<a class="ae mt" href="https://aws.amazon.com/emr/features/presto/" rel="noopener ugc nofollow" target="_blank"> Presto </a>等开源工具处理海量数据。Amazon EMR是一项完全托管的AWS服务，通过自动执行配置容量和调整集群等耗时的任务，可以轻松设置、操作和扩展您的大数据环境。</p><p id="f5cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自2020年12月以来，亚马逊EMR的一个部署选项，<a class="ae mt" href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks.html" rel="noopener ugc nofollow" target="_blank">EKS的亚马逊EMR</a>，允许你在<a class="ae mt" href="https://aws.amazon.com/eks/" rel="noopener ugc nofollow" target="_blank">亚马逊弹性Kubernetes服务</a>(亚马逊EKS)上运行亚马逊EMR。借助EKS部署选项，您可以专注于运行分析工作负载，而Amazon EMR on EKS为开源应用构建、配置和管理容器。</p><p id="1772" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不熟悉Amazon EMR for Spark，特别是PySpark，我推荐一个由两部分组成的早期系列文章，<a class="ae mt" href="https://medium.com/swlh/running-pyspark-applications-on-amazon-emr-e536b7a865ca" rel="noopener">在Amazon EMR上运行PySpark应用程序:在Amazon Elastic MapReduce上与PySpark交互的方法</a>。</p><div class="mv mw gp gr mx my"><a href="https://medium.com/swlh/running-pyspark-applications-on-amazon-emr-e536b7a865ca" rel="noopener follow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">在Amazon EMR上运行PySpark应用程序</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">Amazon Elastic MapReduce上与PySpark交互的方法</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">medium.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm lp my"/></div></div></a></div><h2 id="7451" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">阿帕奇卡夫卡</h2><p id="076b" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">根据<a class="ae mt" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank">文档</a>，Apache Kafka是一个开源的分布式事件流平台，被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。</p><h2 id="c05c" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">亚马逊MSK</h2><p id="5cce" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Apache Kafka集群在生产环境中的设置、扩展和管理极具挑战性。根据<a class="ae mt" href="https://aws.amazon.com/msk/" rel="noopener ugc nofollow" target="_blank">文档</a>的说法，亚马逊MSK是一个完全托管的AWS服务，让你可以轻松构建和运行使用<a class="ae mt" href="https://aws.amazon.com/streaming-data/what-is-kafka/" rel="noopener ugc nofollow" target="_blank"> Apache Kafka </a>处理流媒体数据的应用程序。有了亚马逊MSK，您可以使用原生Apache Kafka APIs来填充数据湖，将更改传入和传出数据库，并支持机器学习和分析应用程序。</p><h1 id="e408" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">先决条件</h1><p id="6f9d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这篇文章将主要关注在Amazon EMR上配置和运行Apache Spark作业。为了跟进，您需要在AWS上部署和配置以下资源:</p><ol class=""><li id="afee" class="ny nz it kk b kl km ko kp kr oa kv ob kz oc ld od oe of og bi translated">亚马逊S3桶(保存星火资源和输出)；</li><li id="f8dd" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">亚马逊MSK集群(使用<a class="ae mt" href="https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html#how-to-use-iam-access-control" rel="noopener ugc nofollow" target="_blank"> IAM访问控制</a>)；</li><li id="a5ff" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">安装了Kafka APIs并且能够连接到亚马逊MSK的亚马逊EKS容器或EC2实例；</li><li id="cf9a" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">亚马逊EKS集群或EC2和亚马逊MSK集群之间的连接；</li><li id="266c" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">确保亚马逊MSK配置有<code class="fe om on oo op b">auto.create.topics.enable=true</code>；该设置默认为<code class="fe om on oo op b">false</code>；</li></ol><p id="c376" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上面的架构图所示，演示在同一个AWS帐户和AWS区域<code class="fe om on oo op b">us-east-1</code>内使用了三个独立的VPC，分别用于Amazon EMR、Amazon MSK和Amazon EKS。使用<a class="ae mt" href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html" rel="noopener ugc nofollow" target="_blank"> VPC对等</a>连接三个VPC。确保您在亚马逊EMR、亚马逊MSK和亚马逊EKS安全组中公开了正确的入口端口和相应的CIDR范围。为了获得额外的安全性和成本节约，使用一个<a class="ae mt" href="https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html" rel="noopener ugc nofollow" target="_blank"> VPC端点</a>用于亚马逊EMR和亚马逊S3之间的私人通信。</p><h1 id="8914" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">源代码</h1><p id="6875" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这篇文章和亚马逊MSK系列的前两篇文章的所有源代码，包括这里演示的Python/PySpark脚本，都是开源的，位于<a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上。</p><div class="mv mw gp gr mx my"><a href="https://github.com/garystafford/kafka-connect-msk-demo" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">GitHub-garystafter/Kafka-connect-MSK-demo:</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">对于这篇文章，在AWS - GitHub上使用变更数据捕获(CDC)、Apache Kafka和Kubernetes来补充数据湖…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">github.com</p></div></div><div class="nh l"><div class="oq l nj nk nl nh nm lp my"/></div></div></a></div><h2 id="354e" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark脚本</h2><p id="0db6" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">根据Apache Spark <a class="ae mt" href="http://spark.apache.org/docs/latest/api/python/" rel="noopener ugc nofollow" target="_blank">文档</a>，PySpark是Python中Apache Spark的一个接口。它允许您使用Python API编写Spark应用程序。PySpark支持Spark的大部分功能，如Spark SQL、DataFrame、Streaming、MLlib(机器学习)和Spark Core。</p><p id="6655" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章涵盖了九个Python/PySpark脚本:</p><ol class=""><li id="e206" class="ny nz it kk b kl km ko kp kr oa kv ob kz oc ld od oe of og bi translated">初始销售数据发布到卡夫卡<br/>T3 01 _ seed _ sales _ Kafka . py</li><li id="d76c" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">Kafka <br/>的批量查询<a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/02_batch_read_kafka.py" rel="noopener ugc nofollow" target="_blank"> 02_batch_read_kafka.py </a></li><li id="8bd8" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">使用分组聚合对Kafka进行流式查询<br/><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/03_streaming_read_kafka_console.py" rel="noopener ugc nofollow" target="_blank">03 _ streaming _ read _ Kafka _ console . py</a></li><li id="586f" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">使用滑动事件时间窗口的流式查询<br/><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/04_incremental_sales_kafka.py" rel="noopener ugc nofollow" target="_blank">04 _ streaming _ read _ Kafka _ console _ window . py</a></li><li id="5f8a" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">增量销售数据发布到Kafka<br/><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/05_streaming_read_kafka_console_window.py" rel="noopener ugc nofollow" target="_blank">05 _ incremental _ sales _ Kafka . py</a></li><li id="5d8c" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">使用分组聚合从/向Kafka进行流式查询<br/><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/06_streaming_read_kafka_kafka.py" rel="noopener ugc nofollow" target="_blank">06 _ streaming _ read _ Kafka _ Kafka . py</a></li><li id="5488" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">Kafka <br/> <a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/07_batch_read_kafka.py" rel="noopener ugc nofollow" target="_blank">中流式查询结果的批量查询07_batch_read_kafka.py </a></li><li id="60bc" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">使用静态连接和滑动窗口的流式查询<br/><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/08_streaming_read_kafka_join_window.py" rel="noopener ugc nofollow" target="_blank">08 _ streaming _ read _ Kafka _ join _ window . py</a></li><li id="996e" class="ny nz it kk b kl oh ko oi kr oj kv ok kz ol ld od oe of og bi translated">使用静态连接和分组聚合的流式查询<br/><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/09_streaming_read_kafka_join.py" rel="noopener ugc nofollow" target="_blank">09 _ streaming _ read _ Kafka _ join . py</a></li></ol><h1 id="c2b0" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">亚马逊MSK认证和授权</h1><p id="3c28" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">亚马逊MSK提供多种<a class="ae mt" href="https://docs.aws.amazon.com/msk/latest/developerguide/kafka_apis_iam.html" rel="noopener ugc nofollow" target="_blank">认证和授权方法</a>来与Apache Kafka APIs交互。在本文中，PySpark脚本使用特定于<a class="ae mt" href="https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html#how-to-use-iam-access-control" rel="noopener ugc nofollow" target="_blank"> IAM访问控制</a>的Kafka连接属性。您可以使用IAM对客户端进行身份验证，并允许或拒绝Apache Kafka操作。或者，您可以使用TLS或SASL/SCRAM来验证客户端，并使用Apache Kafka ACLs来允许或拒绝操作。在最近的一篇文章中，我演示了SASL/SCRAM和Kafka ACLs在亚马逊MSK上的使用:</p><div class="mv mw gp gr mx my"><a rel="noopener  ugc nofollow" target="_blank" href="/securely-decoupling-applications-on-amazon-eks-using-kafka-with-sasl-scram-48c340e1ffe9"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">使用带有SASL/SCRAM的Kafka安全地解耦亚马逊EKS上的应用</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">Gary Stafford的“使用亚马逊MSK与IRSA、SASL/SCRAM和数据加密安全地分离亚马逊EKS上基于Go的微服务”</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">itnext.io</p></div></div><div class="nh l"><div class="or l nj nk nl nh nm lp my"/></div></div></a></div><h1 id="9b2b" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">语言选择</h1><p id="5886" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">根据最新的Spark 3.1.2 <a class="ae mt" href="https://spark.apache.org/docs/latest/index.html" rel="noopener ugc nofollow" target="_blank">文档</a>，Spark运行在Java 8/11、Scala 2.12、Python 3.6+和R 3.5+上。Spark文档包含用所有四种语言编写的代码示例，并在GitHub上为<a class="ae mt" href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples" rel="noopener ugc nofollow" target="_blank"> Scala </a>、<a class="ae mt" href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples" rel="noopener ugc nofollow" target="_blank"> Java </a>、<a class="ae mt" href="https://github.com/apache/spark/tree/master/examples/src/main/python" rel="noopener ugc nofollow" target="_blank"> Python </a>和<a class="ae mt" href="https://github.com/apache/spark/tree/master/examples/src/main/r" rel="noopener ugc nofollow" target="_blank"> R </a>提供示例代码。Spark是用Scala写的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mu"><img src="../Images/22a829d5408c521a67b01c16f1b0a1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q8dUOOp8QNZFN1f8KNVS_A.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">根据谷歌趋势<a class="ae mt" href="https://trends.google.com/trends/explore?geo=US&amp;q=%2Fm%2F05z1_,%2Fm%2F091hdj,%2Fm%2F07sbkfb,%2Fm%2F0212jm" rel="noopener ugc nofollow" target="_blank">和</a>，随着时间的推移，激发对语言的兴趣</figcaption></figure><p id="9a3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于为Spark选择最佳语言的帖子和行业观点不计其数。不偏袒任何一方，我选择了我最常用的数据分析语言，Python使用PySpark。与Scala相比，这两种语言展示了一些<a class="ae mt" href="https://www.educba.com/python-vs-scala/" rel="noopener ugc nofollow" target="_blank">显著差异</a>:编译与解释、静态类型与动态类型、基于JVM与不基于JVM、Scala对并发和真正多线程的支持、Scala 10倍的原始性能与Python的易用性、更大的社区和相对成熟度。</p><h1 id="b7c4" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">准备</h1><h2 id="dc10" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">亚马逊S3</h2><p id="ba5f" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">我们将从收集和复制必要的文件到你的亚马逊S3桶开始。bucket将作为Amazon EMR引导脚本、Spark所需的其他JAR文件、PySpark脚本、CSV格式数据文件以及Spark作业的最终输出的位置。</p><p id="694d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将要运行的Spark作业需要一小组额外的JAR文件。从<a class="ae mt" href="https://mvnrepository.com/artifact/org.apache.spark" rel="noopener ugc nofollow" target="_blank"> Maven Central </a>和GitHub下载jar，并将它们放在<code class="fe om on oo op b"><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/tree/main/pyspark/emr_jars" rel="noopener ugc nofollow" target="_blank">emr_jars</a></code>项目目录中。这些jar将包括<a class="ae mt" href="https://github.com/aws/aws-msk-iam-auth" rel="noopener ugc nofollow" target="_blank"> AWS MSK IAM Auth </a>、<a class="ae mt" href="https://aws.amazon.com/blogs/developer/java-sdk-bundle/" rel="noopener ugc nofollow" target="_blank"> AWS SDK </a>、<a class="ae mt" href="https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients" rel="noopener ugc nofollow" target="_blank"> Kafka客户端</a>、<a class="ae mt" href="https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10" rel="noopener ugc nofollow" target="_blank">用于Kafka的Spark SQL</a>、<a class="ae mt" href="https://mvnrepository.com/artifact/org.apache.spark/spark-streaming" rel="noopener ugc nofollow" target="_blank"> Spark流</a>以及其他依赖项。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="a120" class="lv lw it op b gy ow ox l oy oz">cd ./pyspark/emr_jars/</span><span id="7917" class="lv lw it op b gy pa ox l oy oz">wget https://github.com/aws/aws-msk-iam-auth/releases/download/1.1.0/aws-msk-iam-auth-1.1.0-all.jar</span><span id="6cd6" class="lv lw it op b gy pa ox l oy oz">wget https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.17.28/bundle-2.17.28.jar</span><span id="c6f3" class="lv lw it op b gy pa ox l oy oz">wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.0/commons-pool2-2.11.0.jar</span><span id="6058" class="lv lw it op b gy pa ox l oy oz">wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.0/kafka-clients-2.8.0.jar</span><span id="23bf" class="lv lw it op b gy pa ox l oy oz">wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.1.2/spark-sql-kafka-0-10_2.12-3.1.2.jar</span><span id="75a3" class="lv lw it op b gy pa ox l oy oz">wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming_2.12/3.1.2/spark-streaming_2.12-3.1.2.jar</span><span id="7fb2" class="lv lw it op b gy pa ox l oy oz">wget https://repo1.maven.org/maven2/org/apache/spark/spark-tags_2.12/3.1.2/spark-tags_2.12-3.1.2.jar</span><span id="f51d" class="lv lw it op b gy pa ox l oy oz">wget https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.1.2/spark-token-provider-kafka-0-10_2.12-3.1.2.jar</span></pre><p id="cbc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，更新<code class="fe om on oo op b">SPARK_BUCKET</code>环境变量，然后使用AWS <code class="fe om on oo op b">s3</code> API将JARs和所有必要的项目文件从您的GitHub项目存储库副本上传到您的亚马逊S3 bucket。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="8c6a" class="lv lw it op b gy ow ox l oy oz">cd ./pyspark/</span><span id="4650" class="lv lw it op b gy pa ox l oy oz"><strong class="op iu">export SPARK_BUCKET="&lt;your-bucket-111222333444-us-east-1&gt;"<br/></strong><br/>aws s3 cp emr_jars/ "s3://${SPARK_BUCKET}/jars/" --recursive</span><span id="1ba0" class="lv lw it op b gy pa ox l oy oz">aws s3 cp pyspark_scripts/ "s3://${SPARK_BUCKET}/spark/" --recursive</span><span id="65b0" class="lv lw it op b gy pa ox l oy oz">aws s3 cp emr_bootstrap/ "s3://${SPARK_BUCKET}/spark/" --recursive</span><span id="4891" class="lv lw it op b gy pa ox l oy oz">aws s3 cp data/ "s3://${SPARK_BUCKET}/spark/" --recursive</span></pre><h2 id="271f" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">亚马逊电子病历</h2><p id="f5ef" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">GitHub项目资源库包括一个样本AWS CloudFormation <a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/cloudformation/stack.yml" rel="noopener ugc nofollow" target="_blank">模板</a>和一个相关联的JSON格式CloudFormation <a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/cloudformation/dev.json" rel="noopener ugc nofollow" target="_blank">参数文件</a>。模板<code class="fe om on oo op b"><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/cloudformation/stack.yml" rel="noopener ugc nofollow" target="_blank">stack.yml</a></code>接受几个<a class="ae mt" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html" rel="noopener ugc nofollow" target="_blank">参数</a>。为了与您的环境相匹配，您需要更新参数值，例如SSK密钥、子网和S3存储桶。该模板将构建一个最小规模的Amazon EMR集群，在现有的VPC中有一个主节点和两个核心节点。该模板可以很容易地修改，以满足您的要求和预算。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="cb51" class="lv lw it op b gy ow ox l oy oz">aws cloudformation deploy \<br/>    --stack-name spark-kafka-demo-dev \<br/>    --template-file ./cloudformation/stack.yml \<br/>    --parameter-overrides file://cloudformation/dev.json \<br/>    --capabilities CAPABILITY_NAMED_IAM</span></pre><p id="045d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">无论您是否决定使用CloudFormation模板，EMR模板中的两个基本Spark配置项是要安装的应用程序列表和引导脚本部署。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="c080" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面，我们看到EMR bootstrap shell脚本<a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/emr_bootstrap/bootstrap_actions.sh" rel="noopener ugc nofollow" target="_blank"> bootstrap_actions.sh </a>，部署并在集群的节点上执行。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="ae19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该脚本执行了几项任务，包括部署我们之前复制到亚马逊S3的额外JAR文件。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pd"><img src="../Images/38a8613d0bf75941e3c0af31cd55fc85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O5jMDyGCu6kkcyyvK9VCeg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">Amazon EMR集群引导操作选项卡</figcaption></figure><h2 id="5714" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">AWS系统管理器参数存储</h2><p id="8f99" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">本演示中的PySpark脚本将从AWS系统管理器(AWS SSM)参数存储中获取两个参数。它们包括亚马逊MSK bootstrap brokers和包含Spark资产的亚马逊S3 bucket。使用参数存储可以确保没有敏感的或特定于环境的配置被硬编码到PySpark脚本中。修改并执行<code class="fe om on oo op b"><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/ssm_params.sh" rel="noopener ugc nofollow" target="_blank">ssm_params.sh</a></code>脚本来创建两个AWS SSM参数存储参数。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="113a" class="lv lw it op b gy ow ox l oy oz">aws ssm put-parameter \<br/>  --name /kafka_spark_demo/kafka_servers \<br/>  --type String \<br/><strong class="op iu">  --value "&lt;b-1.your-brokers.kafka.us-east-1.amazonaws.com:9098,b-2...&gt;" \<br/></strong>  --description "Amazon MSK Kafka broker list" \<br/>  --overwrite<br/><br/>aws ssm put-parameter \<br/>  --name /kafka_spark_demo/kafka_demo_bucket \<br/>  --type String \<br/><strong class="op iu">  --value "&lt;your-bucket-111222333444-us-east-1&gt;" \<br/></strong>  --description "Amazon S3 bucket" \<br/>  --overwrite</span></pre><h1 id="9135" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">亚马逊电子病历的Spark提交选项</h1><p id="6c2d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Amazon EMR提供了多个选项来运行Spark作业。PySpark脚本的推荐方法是从EMR控制台或AWS CLI使用<a class="ae mt" href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-work-with-steps.html" rel="noopener ugc nofollow" target="_blank"> Amazon EMR步骤</a>向安装在EMR集群上的Spark提交工作。在控制台和CLI中，您可以使用Spark应用程序步骤来实现这一点，该步骤代表您运行<code class="fe om on oo op b">spark-submit</code>脚本。通过API，您可以使用一个步骤通过<code class="fe om on oo op b">command-runner.jar</code>调用<code class="fe om on oo op b">spark-submit</code>。或者，您可以SSH到EMR集群的主节点并运行<code class="fe om on oo op b">spark-submit</code>。我们将使用这两种技术来运行PySpark作业。</p><h1 id="4929" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">从Spark安全访问亚马逊MSK</h1><p id="ba7d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">本文中演示的每个PySpark脚本都使用一个通用模式，通过IAM身份验证从Amazon EMR访问Amazon MSK。无论是生成还是消费来自Kafka的消息，都使用相同的安全相关选项来配置Spark ( <em class="le">从第10行开始，在</em>下面)。每个选项背后的细节在<a class="ae mt" href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html" rel="noopener ugc nofollow" target="_blank"> Spark结构化流+ Kafka集成指南</a>的<a class="ae mt" href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#security" rel="noopener ugc nofollow" target="_blank">安全性</a>部分和亚马逊MSK IAM访问控制文档的<a class="ae mt" href="https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html" rel="noopener ugc nofollow" target="_blank">配置IAM访问控制客户端</a>部分中有所概述。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">用IAM阅读卡夫卡作品的PySpark配置</figcaption></figure><h1 id="99bf" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">数据来源和分析目标</h1><p id="6a4c" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在这篇文章中，我们将继续使用PostgreSQL示例<a class="ae mt" href="https://www.postgresql.org/ftp/projects/pgFoundry/dbsamples/pagila/pagila/" rel="noopener ugc nofollow" target="_blank"> Pagila数据库</a>中的数据。该数据库包含模拟电影租赁数据。数据集相当小，这使得它不太适合“大数据”用例，但也足够小，可以快速安装并最大限度地降低数据存储和分析查询成本。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">post中使用的电影租赁销售数据示例</figcaption></figure><p id="dd72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据<a class="ae mt" href="https://www.mastersindatascience.org/learning/what-is-data-analytics/" rel="noopener ugc nofollow" target="_blank">mastersindatascience.org</a>的说法，数据分析是“<em class="le">…分析原始数据以发现趋势和回答问题的过程… </em>”使用Spark，我们可以批量或近实时地分析电影租赁销售数据，使用结构化流来回答不同的问题。例如，使用静态数据的批处理计算，我们可以回答这个问题，<em class="le">与欧洲其他国家相比，法国目前的总销售额如何？</em>或者，<em class="le">八月份印度的总销售额是多少？</em>使用流计算，我们可以回答这样的问题，<em class="le">在当前四小时的营销推广期间，美国的销售量是多少？</em>或者，<em class="le">随着奥运会在黄金时段播出，对北美的销售是否开始放缓？</em></p><blockquote class="pe pf pg"><p id="c875" class="ki kj le kk b kl km ju kn ko kp jx kq ph ks kt ku pi kw kx ky pj la lb lc ld im bi translated">数据分析—分析原始数据以发现趋势和回答问题的过程。(<a class="ae mt" href="https://www.mastersindatascience.org/learning/what-is-data-analytics/" rel="noopener ugc nofollow" target="_blank">mastersindatascience.org</a>)</p></blockquote><h1 id="526c" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">批量查询</h1><p id="98e1" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在探索使用Spark结构化流进行流计算这一更高级的主题之前，让我们首先使用一个简单的批处理查询和批处理计算来消费来自Kafka主题的消息，执行一个基本的聚合，并将输出写入控制台和亚马逊S3。</p><h2 id="9234" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark工作1:初始销售数据</h2><p id="bfb2" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Kafka支持<a class="ae mt" href="https://developers.google.com/protocol-buffers/" rel="noopener ugc nofollow" target="_blank">协议缓冲区</a>、<a class="ae mt" href="https://json-schema.org/" rel="noopener ugc nofollow" target="_blank"> JSON模式</a>和<a class="ae mt" href="http://avro.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Avro </a>。然而，为了在第一篇文章中保持简单，我们将使用JSON。我们将用第一批250条JSON格式的消息来播种一个新的Kafka主题。这第一批消息代表先前的在线电影租赁销售交易记录。我们将使用这些销售交易进行批处理和流式查询。</p><p id="954c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark脚本<code class="fe om on oo op b">01_seed_sales_kafka.py</code>和种子数据文件<code class="fe om on oo op b"><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/data/sales_seed.csv" rel="noopener ugc nofollow" target="_blank">sales_seed.csv</a></code>都是由Spark从亚马逊S3读取的，运行在亚马逊EMR上。使用前面创建的参数从AWS SSM参数存储中提取亚马逊S3存储桶名称的位置和亚马逊MSK的经纪人列表值。存储销售数据的Kafka主题<code class="fe om on oo op b">pagila.sales.spark.streaming</code>，是脚本第一次运行时自动创建的。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="a92b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更新两个环境变量，然后使用AWS CLI和<code class="fe om on oo op b">emr</code> API提交您的第一个Spark作业作为Amazon EMR步骤:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pk"><img src="../Images/cf964724b0d0ab178f4ddac9e8aae771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dgj26P9dRVsTnuwXMHclqQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">成功地向Amazon EMR集群添加了一个步骤(Spark作业)</figcaption></figure><p id="bbe1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从Amazon EMR控制台，我们应该观察到Spark作业已经在大约30-90秒内成功完成。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pl"><img src="../Images/b40a18e747fb510a4cd3b97ab3c8e740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D916ZSciXR4_vw-SSeDb1w.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">Amazon EMR步骤(Spark作业)成功完成</figcaption></figure><p id="9499" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Kafka <a class="ae mt" href="https://kafka.apache.org/documentation/#consumerapi" rel="noopener ugc nofollow" target="_blank">消费者API </a>允许应用程序从Kafka集群中的主题读取数据流。使用Kafka消费者API，从运行在Amazon EKS上的Kubernetes容器或EC2实例中，我们可以观察到新的Kafka主题已经成功创建，并且消息(<em class="le">初始销售数据</em>)已经发布到新的Kafka主题。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="da76" class="lv lw it op b gy ow ox l oy oz">export BBROKERS="b-1.your-cluster.kafka.us-east-1.amazonaws.com:9098,b-2.your-cluster.kafka.us-east-1.amazonaws.com:9098, ..."</span><span id="77c3" class="lv lw it op b gy pa ox l oy oz">bin/kafka-console-consumer.sh \<br/>  --topic pagila.sales.spark.streaming \<br/>  --from-beginning \<br/>  --property print.key=true \<br/>  --property print.value=true \<br/>  --property print.offset=true \<br/>  --property print.partition=true \<br/>  --property print.headers=true \<br/>  --property print.timestamp=true \<br/>  --bootstrap-server $BBROKERS \<br/>  --consumer.config config/client-iam.properties</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pk"><img src="../Images/40ce4c0b702b2b9c4808ccc025660bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZZykUVO_s_HAoRfDSIJoA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">Kafka主题中作为消息的初始销售数据</figcaption></figure><h2 id="5d3e" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark Job 2:批量查询亚马逊MSK话题</h2><p id="c661" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">PySpark脚本<code class="fe om on oo op b">02_batch_read_kafka.py</code>对Kafka主题中最初的250条消息执行批量查询。运行时，PySpark脚本解析JSON格式的消息，然后按国家汇总总销售额和订单数，最后按总销售额排序。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="d728" class="lv lw it op b gy ow ox l oy oz">window = Window.partitionBy("country").orderBy("amount")<br/>window_agg = Window.partitionBy("country")</span><span id="fefb" class="lv lw it op b gy pa ox l oy oz">.withColumn("row", F.row_number().over(window)) \<br/>.withColumn("orders", F.count(F.col("amount")).over(window_agg)) \<br/>.withColumn("sales", F.sum(F.col("amount")).over(window_agg)) \<br/>.where(F.col("row") == 1).drop("row") \</span></pre><p id="c27f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果以<code class="fe om on oo op b">stdout</code>的形式写入控制台，并以CSV格式写入亚马逊S3。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="c577" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，使用AWS CLI和<code class="fe om on oo op b">emr</code> API将此作业作为Amazon EMR步骤提交:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="5f29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要查看控制台输出，点击Amazon EMR控制台中的“查看日志”,然后点击<code class="fe om on oo op b">stdout</code>日志文件，如下所示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pd"><img src="../Images/e5517441bf9b3e9bafed6995c0231069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-es_UZsexvBYuXumX_cmQ.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">来自成功的Amazon EMR步骤(Spark作业)的日志</figcaption></figure><p id="129f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据最初的250条销售记录，日志文件应该包含按国家排列的前25个总销售额和订单数。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="8ad6" class="lv lw it op b gy ow ox l oy oz">+------------------+------+------+<br/>|country           |sales |orders|<br/>+------------------+------+------+<br/>|India             |138.80|20    |<br/>|China             |133.80|20    |<br/>|Mexico            |106.86|14    |<br/>|Japan             |100.86|14    |<br/>|Brazil            |96.87 |13    |<br/>|Russian Federation|94.87 |13    |<br/>|United States     |92.86 |14    |<br/>|Nigeria           |58.93 |7     |<br/>|Philippines       |58.92 |8     |<br/>|South Africa      |46.94 |6     |<br/>|Argentina         |42.93 |7     |<br/>|Germany           |39.96 |4     |<br/>|Indonesia         |38.95 |5     |<br/>|Italy             |35.95 |5     |<br/>|Iran              |33.95 |5     |<br/>|South Korea       |33.94 |6     |<br/>|Poland            |30.97 |3     |<br/>|Pakistan          |25.97 |3     |<br/>|Taiwan            |25.96 |4     |<br/>|Mozambique        |23.97 |3     |<br/>|Ukraine           |23.96 |4     |<br/>|Vietnam           |23.96 |4     |<br/>|Venezuela         |22.97 |3     |<br/>|France            |20.98 |2     |<br/>|Peru              |19.98 |2     |<br/>+------------------+------+------+<br/>only showing top 25 rows</span></pre><p id="91ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark脚本也以CSV格式将相同的结果写入亚马逊S3。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pd"><img src="../Images/bcbc7d3a678068fc67bb9f67e4fa1a16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lsZPEeQmkCvli1SmqqDq6A.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">由于Spark作业，CSV文件被写入亚马逊S3</figcaption></figure><p id="f14c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对69个国家的总销售额和订单数进行计算、排序并合并到一个CSV文件中。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">亚马逊S3 CSV文件中前25个国家的简略视图</figcaption></figure><h1 id="c0d2" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">流式查询</h1><p id="5564" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">为了演示Spark结构化流的流查询，我们将使用两个PySpark脚本的组合。第一个脚本<code class="fe om on oo op b">03_streaming_read_kafka_console.py</code>将对Kafka主题中的消息执行流查询和计算，汇总总销售额和订单数。同时，第二个PySpark脚本<code class="fe om on oo op b">04_incremental_sales_kafka.py</code>将从位于亚马逊S3的CSV文件中读取额外的Pagila销售数据，并以每秒两条消息的速度向Kafka主题写入消息。第一个脚本<code class="fe om on oo op b">03_streaming_read_kafka_console.py</code>将以一分钟为增量的微批量将聚合流传输到控制台。Spark结构化流查询使用<a class="ae mt" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#overview" rel="noopener ugc nofollow" target="_blank">微批处理引擎</a>进行处理，该引擎将数据流作为一系列小型批处理作业进行处理。</p><p id="6126" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，第一个脚本执行分组聚合，而不是在<a class="ae mt" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time" rel="noopener ugc nofollow" target="_blank">滑动事件时间窗口</a>上进行聚合。聚合结果表示在计算微批处理时，基于主题中当前所有消息的某个时间点的总销售额。</p><p id="a53b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了进行这一部分的演示，您可以在现有的Amazon EMR集群上作为并发步骤<a class="ae mt" href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-concurrent-steps.html" rel="noopener ugc nofollow" target="_blank">运行两个Spark作业，或者创建与现有集群配置相同的第二个EMR集群，以运行第二个PySpark脚本<code class="fe om on oo op b">04_incremental_sales_kafka.py</code>。使用第二个集群，您可以使用没有核心节点的最小规模的单个主节点集群来节省成本。</a></p><h2 id="e010" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark作业3:将查询流式传输到控制台</h2><p id="8718" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">第一个PySpark脚本<code class="fe om on oo op b">03_streaming_read_kafka_console.py</code>，执行Kafka主题中消息的流查询。然后，该脚本根据总销售额和订单数、国家汇总数据，最后根据总销售额进行排序。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="bca2" class="lv lw it op b gy ow ox l oy oz">.groupBy("country") \<br/>.agg(F.count("amount"), F.sum("amount")) \<br/>.orderBy(F.col("sum(amount)").desc()) \<br/>.select("country",<br/>        (F.format_number(F.col("sum(amount)"), 2)).alias("sales"),<br/>        (F.col("count(amount)")).alias("orders")) \</span></pre><p id="3367" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<code class="fe om on oo op b">processingTime</code>触发参数将结果传输到控制台。触发器定义了流式查询执行和发出新数据的频率。<code class="fe om on oo op b">processingTime</code>参数设置一个触发器，该触发器根据处理时间(例如“5分钟”或“1小时”)定期运行微批处理查询。为了便于演示，触发器当前被设置为一分钟的最小处理时间。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="f66b" class="lv lw it op b gy ow ox l oy oz"><strong class="op iu">.trigger(processingTime="1 minute") \<br/></strong>.outputMode("complete") \<br/>.format("console") \<br/>.option("numRows", 25) \</span></pre><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="3795" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">出于演示目的，我们将直接从EMR集群的主节点运行Spark作业。这种方法将允许我们在微批处理和相关日志事件输出到控制台时轻松查看它们。控制台通常用于测试目的。从集群的主节点提交PySpark脚本是提交Amazon EMR步骤的替代方法。作为<code class="fe om on oo op b">hadoop</code>用户，使用SSH连接到Amazon EMR集群的主节点:</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="355c" class="lv lw it op b gy ow ox l oy oz">export EMR_MASTER=&lt;your-emr-master-dns.compute-1.amazonaws.com&gt;<br/>export EMR_KEY_PATH=path/to/key/&lt;your-ssk-key.pem&gt;</span><span id="c1b9" class="lv lw it op b gy pa ox l oy oz">ssh -i ${EMR_KEY_PATH} hadoop@${EMR_MASTER}</span></pre><p id="4a03" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将PySpark脚本<code class="fe om on oo op b">03_streaming_read_kafka_console.py</code>提交给Spark:</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="e70f" class="lv lw it op b gy ow ox l oy oz">export SPARK_BUCKET="&lt;your-bucket-111222333444-us-east-1&gt;"</span><span id="4330" class="lv lw it op b gy pa ox l oy oz">spark-submit s3a://${SPARK_BUCKET}/spark/<!-- -->03_streaming_read_kafka_console<!-- -->.py</span></pre><p id="b078" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在运行第二个PySpark脚本<code class="fe om on oo op b">04_incremental_sales_kafka.py</code>之前，让第一个脚本运行足够长的时间，以获取Kafka主题中的现有销售数据。在大约两分钟内，您应该会看到第一个小批量的汇总销售结果，标记为“Batch: 0”输出到控制台。这个初始微批应该包含来自Kafka的现有250条消息的聚合结果。流式查询的第一个微批处理结果应该与之前的批处理查询结果相同。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="aa10" class="lv lw it op b gy ow ox l oy oz">-------------------------------------------<br/>Batch: 0<br/>-------------------------------------------<br/>+------------------+------+------+<br/>|country           |sales |orders|<br/>+------------------+------+------+<br/>|India             |138.80|20    |<br/>|China             |133.80|20    |<br/>|Mexico            |106.86|14    |<br/>|Japan             |100.86|14    |<br/>|Brazil            |96.87 |13    |<br/>|Russian Federation|94.87 |13    |<br/>|United States     |92.86 |14    |<br/>|Nigeria           |58.93 |7     |<br/>|Philippines       |58.92 |8     |<br/>|South Africa      |46.94 |6     |<br/>|Argentina         |42.93 |7     |<br/>|Germany           |39.96 |4     |<br/>|Indonesia         |38.95 |5     |<br/>|Italy             |35.95 |5     |<br/>|Iran              |33.95 |5     |<br/>|South Korea       |33.94 |6     |<br/>|Poland            |30.97 |3     |<br/>|Pakistan          |25.97 |3     |<br/>|Taiwan            |25.96 |4     |<br/>|Mozambique        |23.97 |3     |<br/>|Ukraine           |23.96 |4     |<br/>|Vietnam           |23.96 |4     |<br/>|Venezuela         |22.97 |3     |<br/>|France            |20.98 |2     |<br/>|Peru              |19.98 |2     |<br/>+------------------+------+------+<br/>only showing top 25 rows</span></pre><p id="1eba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在批处理输出的正下方，将有一个包含批处理信息的日志条目。在下面的日志条目片段中，注意Spark作业的Kafka消费者组的主题的开始和结束偏移量，从0 ( <em class="le"> null </em>)到250，表示初始销售数据。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="a9dc" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark工作4:增量销售数据</h2><p id="4cb5" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如前所述，第二个PySpark脚本<code class="fe om on oo op b">04_incremental_sales_kafka.py</code>从位于亚马逊S3<code class="fe om on oo op b"><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/data/sales_incremental_large.csv" rel="noopener ugc nofollow" target="_blank">sales_incremental_large.csv</a></code>的第二个CSV文件中读取1，800条额外的销售记录。然后，脚本以每秒两条消息的速度向Kafka主题发布消息。同时，第一个PySpark作业仍在运行并执行流查询，它将使用新的Kafka消息，并在大约15分钟的时间内以一分钟为增量的小批量将汇总的总销售额和订单流传输到控制台。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="7a2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将第二个PySpark脚本作为并发的Amazon EMR步骤提交给第一个EMR集群，或者作为一个步骤提交给第二个Amazon EMR集群。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="499c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该作业在15分钟内以每秒两条消息的速度向Kafka发送总共1800条消息。给定几分钟的启动和关闭时间，作业的总运行时间应该大约为19分钟。为什么跑这么久？我们希望确保作业的运行时间将跨越多个重叠的滑动事件时间窗口。</p><p id="0e76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大约两分钟后，返回到在第一个集群的主节点上运行的第一个Spark作业<code class="fe om on oo op b">03_streaming_read_kafka_console.py</code>的终端输出。只要每分钟都有新消息被消耗，您应该会看到一个新的小批量的汇总销售结果流到控制台。下面我们看到一个批次3的例子，它反映了与之前显示的批次0相比的额外销售额。结果实时反映了当前各个国家的历史销售额，因为销售额是发布给Kafka的。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="acf0" class="lv lw it op b gy ow ox l oy oz">-------------------------------------------<br/>Batch: 5<br/>-------------------------------------------<br/>+------------------+------+------+<br/>|country           |sales |orders|<br/>+------------------+------+------+<br/>|China             |473.35|65    |<br/>|India             |393.44|56    |<br/>|Japan             |292.60|40    |<br/>|Mexico            |262.64|36    |<br/>|United States     |252.65|35    |<br/>|Russian Federation|243.65|35    |<br/>|Brazil            |220.69|31    |<br/>|Philippines       |191.75|25    |<br/>|Indonesia         |142.81|19    |<br/>|South Africa      |110.85|15    |<br/>|Nigeria           |108.86|14    |<br/>|Argentina         |89.86 |14    |<br/>|Germany           |85.89 |11    |<br/>|Israel            |68.90 |10    |<br/>|Ukraine           |65.92 |8     |<br/>|Turkey            |58.91 |9     |<br/>|Iran              |58.91 |9     |<br/>|Saudi Arabia      |56.93 |7     |<br/>|Poland            |50.94 |6     |<br/>|Pakistan          |50.93 |7     |<br/>|Italy             |48.93 |7     |<br/>|French Polynesia  |47.94 |6     |<br/>|Peru              |45.95 |5     |<br/>|United Kingdom    |45.94 |6     |<br/>|Colombia          |44.94 |6     |<br/>+------------------+------+------+<br/>only showing top 25 rows</span></pre><p id="108a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们快进到稍后的微批次，在第二个增量销售作业完成后的某个时间，我们应该看到按国家/地区排列的前25个总计2，050条消息的销售——250条种子消息加上1，800条增量消息。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="6842" class="lv lw it op b gy ow ox l oy oz">-------------------------------------------<br/>Batch: 20<br/>-------------------------------------------<br/>+------------------+--------+------+<br/>|country           |sales   |orders|<br/>+------------------+--------+------+<br/>|China             |1,379.05|195   |<br/>|India             |1,338.10|190   |<br/>|United States     |915.69  |131   |<br/>|Mexico            |855.80  |120   |<br/>|Japan             |831.88  |112   |<br/>|Russian Federation|723.95  |105   |<br/>|Brazil            |613.12  |88    |<br/>|Philippines       |528.27  |73    |<br/>|Indonesia         |381.46  |54    |<br/>|Turkey            |350.52  |48    |<br/>|Argentina         |298.57  |43    |<br/>|Nigeria           |294.61  |39    |<br/>|South Africa      |279.61  |39    |<br/>|Taiwan            |221.67  |33    |<br/>|Germany           |199.73  |27    |<br/>|United Kingdom    |196.75  |25    |<br/>|Poland            |182.77  |23    |<br/>|Spain             |170.77  |23    |<br/>|Ukraine           |160.79  |21    |<br/>|Iran              |160.76  |24    |<br/>|Italy             |156.79  |21    |<br/>|Pakistan          |152.78  |22    |<br/>|Saudi Arabia      |146.81  |19    |<br/>|Venezuela         |145.79  |21    |<br/>|Colombia          |144.78  |22    |<br/>+------------------+--------+------+<br/>only showing top 25 rows</span></pre><p id="252c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将下面批次20与之前批次0的信息输出进行比较。注意题目上卡夫卡消费群的起始偏移量是1986，结束偏移量是2050。这是因为所有消息都已经从主题中被消费和聚合。如果在流式作业仍在运行的同时有额外的消息被流式传输到Kafka，额外的微批处理将继续每一分钟被流式传输到控制台。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="b521" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark作业5:滑动事件时间窗口上的聚合</h2><p id="d7dc" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在前面的示例中，我们实时分析了历史总销售额(例如，<em class="le">定期显示法国与欧洲其他国家相比的当前历史总销售额)</em>。这种方法与在滑动事件时间窗口期间进行的销售相反(例如，<em class="le">在当前的四小时营销促销期间，美国的总销售额是否比前一个促销期间更好)</em>。在许多情况下，特定时期或事件窗口的实时销售额可能是比总销售额更常跟踪的KPI。</p><p id="d842" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们向PySpark脚本添加一个滑动事件时间窗口，我们可以很容易地实时观察滑动事件时间窗口期间的总销售额和订单数。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="4d33" class="lv lw it op b gy ow ox l oy oz">.withWatermark("timestamp", "10 minutes") \<br/>.groupBy("country", <br/>         F.window("timestamp", "10 minutes", "5 minutes")) \<br/>.agg(F.count("amount"), F.sum("amount")) \<br/>.orderBy(F.col("window").desc(), <br/>         F.col("sum(amount)").desc()) \</span></pre><p id="426e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">窗口总计将不包括在流查询开始之前出现在Kafka主题中的销售(<em class="le">消息</em>),也不包括在先前的滑动窗口中。构建正确的查询总是从清楚地理解您试图回答的问题开始。</p><p id="fc67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面，在脚本<code class="fe om on oo op b">05_streaming_read_kafka_console_window.py</code>的微批处理的简化控制台输出中，我们看到了三个10分钟滑动事件时间窗口的结果，其中有五分钟的重叠。销售和订单总计表示在该时间段内销售的数量，该微批次落在当前活动时间段(19:30至19:40 UTC)内。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="72dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用滑动事件时间窗口绘制一段时间内的总销售额，我们将观察到结果并不反映运行总数。总销售额仅在滑动窗口内累计。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pm"><img src="../Images/062951279008eb9b76f1a98b9726ad54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uMps0DN3GxPZaGsdX_wUcA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">5分钟滑动活动时间窗口内的累计销售额</figcaption></figure><p id="79a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将这些结果与前一个脚本的结果进行比较，前一个脚本的总销售额反映了一个运行总数。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pm"><img src="../Images/83fabe13dad768e066f646525e6e31a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sa4fG6qNAboJpKBRqFyseA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">销售总额(无滑动窗口)</figcaption></figure><h2 id="6df6" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark作业6:来自/去往亚马逊MSK的流查询</h2><p id="45a7" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">PySpark脚本<code class="fe om on oo op b">06_streaming_read_kafka_kafka.py</code>执行与前面的脚本<code class="fe om on oo op b">03_streaming_read_kafka_console.py</code>相同的流查询和分组聚合。然而，该作业的结果将被写入亚马逊MSK上的一个新的Kafka主题，而不是输出到控制台。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="3b09" class="lv lw it op b gy ow ox l oy oz"><strong class="op iu">.format("kafka") \<br/></strong>.options(**options_write) \<br/>.option("checkpointLocation", "/checkpoint/kafka/") \</span></pre><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="3c30" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">重复前面脚本中使用的相同过程。重新运行种子数据脚本<code class="fe om on oo op b">01_seed_sales_kafka.py</code>，但是将输入主题更新为一个新名称，比如<code class="fe om on oo op b">pagila.sales.spark.streaming.in</code>。接下来，运行新的脚本，<code class="fe om on oo op b">06_streaming_read_kafka_kafka.py</code>。给脚本时间来启动和消耗来自卡夫卡的250个种子消息。然后，更新输入主题名称，并重新运行增量数据PySpark脚本<code class="fe om on oo op b">04_incremental_sales_kafka.py</code>，与同一集群上的新脚本并发运行，或者在第二个集群上运行。</p><p id="ef0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行时，脚本<code class="fe om on oo op b">06_streaming_read_kafka_kafka.py</code>将不断使用来自新<code class="fe om on oo op b">pagila.sales.spark.streaming.in</code>主题的消息，并将分组聚合结果发布到新主题<code class="fe om on oo op b">pagila.sales.spark.streaming.out</code>。</p><p id="d79f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用Kafka消费者API查看新消息，因为Spark job将它们近乎实时地发布到Kafka。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="95a8" class="lv lw it op b gy ow ox l oy oz">export BBROKERS="b-1.your-cluster.kafka.us-east-1.amazonaws.com:9098,b-2.your-cluster.kafka.us-east-1.amazonaws.com:9098, ..."</span><span id="67e7" class="lv lw it op b gy pa ox l oy oz">bin/kafka-console-consumer.sh \<br/><strong class="op iu">  --topic pagila.sales.spark.streaming.out \<br/></strong>  --from-beginning \<br/>  --property print.key=true \<br/>  --property print.value=true \<br/>  --property print.offset=true \<br/>  --property print.partition=true \<br/>  --property print.headers=true \<br/>  --property print.timestamp=true \<br/>  --bootstrap-server $BBROKERS \<br/>  --consumer.config config/client-iam.properties</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pk"><img src="../Images/42a26f8b2574570660110d2168e7b366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XB-fd68acpbfmHzWrLyNcw.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">通过流式Spark作业发布到Kafka的汇总销售结果(消息)</figcaption></figure><h2 id="e898" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark作业7:批量查询来自MSK的流结果</h2><p id="c3ee" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">运行时，前面的脚本每分钟都会向Kafka主题生成包含非窗口销售聚合的Kafka消息。使用下一个PySpark脚本<code class="fe om on oo op b">07_batch_read_kafka.py</code>，我们可以使用批处理查询来消费这些聚合消息，并在控制台上显示最近的销售总额。每个国家/地区最近的总销售额和订单数应该与前一个脚本的结果相同，代表所有2，050条Kafka消息的聚合— 250条种子消息加上1，800条增量消息。</p><p id="6206" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了获得最新的国家总销售额，我们将使用输出主题中的所有消息，按国家对结果进行分组，从每个国家的销售列中找到最大值(<code class="fe om on oo op b">max</code>)，最后，以降序显示结果排序销售。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="a000" class="lv lw it op b gy ow ox l oy oz">window = Window.partitionBy("country") \<br/>    .orderBy(F.col("timestamp").desc())</span><span id="5618" class="lv lw it op b gy pa ox l oy oz">.withColumn("row", F.row_number().over(window)) \<br/>.where(F.col("row") == 1).drop("row") \<br/>.select("country", "sales", "orders") \</span></pre><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="be60" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将前25个结果写入控制台，我们应该会看到与PySpark脚本的最后一个微批处理(第20批，如上所示)相同的结果。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="7b26" class="lv lw it op b gy ow ox l oy oz">+------------------+------+------+<br/>|country           |sales |orders|<br/>+------------------+------+------+<br/>|India             |948.63|190   |<br/>|China             |936.67|195   |<br/>|United States     |915.69|131   |<br/>|Mexico            |855.80|120   |<br/>|Japan             |831.88|112   |<br/>|Russian Federation|723.95|105   |<br/>|Brazil            |613.12|88    |<br/>|Philippines       |528.27|73    |<br/>|Indonesia         |381.46|54    |<br/>|Turkey            |350.52|48    |<br/>|Argentina         |298.57|43    |<br/>|Nigeria           |294.61|39    |<br/>|South Africa      |279.61|39    |<br/>|Taiwan            |221.67|33    |<br/>|Germany           |199.73|27    |<br/>|United Kingdom    |196.75|25    |<br/>|Poland            |182.77|23    |<br/>|Spain             |170.77|23    |<br/>|Ukraine           |160.79|21    |<br/>|Iran              |160.76|24    |<br/>|Italy             |156.79|21    |<br/>|Pakistan          |152.78|22    |<br/>|Saudi Arabia      |146.81|19    |<br/>|Venezuela         |145.79|21    |<br/>|Colombia          |144.78|22    |<br/>+------------------+------+------+<br/>only showing top 25 rows</span></pre><h2 id="55d0" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark作业8:带静态连接和滑动窗口的流式查询</h2><p id="2bc4" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">PySpark脚本<code class="fe om on oo op b">08_streaming_read_kafka_join_window.py</code>在滑动事件时间窗口上执行与前面的脚本<code class="fe om on oo op b">05_streaming_read_kafka_console_window.py</code>相同的流查询和计算。但是，该脚本不是按国家对销售额和订单进行合计，而是按销售区域对销售额和订单进行合计。销售区域由同一地理区域内的多个国家组成。PySpark脚本从亚马逊S3<code class="fe om on oo op b">sales_regions.csv</code>读取销售地区和国家的静态列表。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">销售区域的简略列表</figcaption></figure><p id="bd9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，脚本在流式查询的结果和静态区域列表之间执行一个<a class="ae mt" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#join-operations" rel="noopener ugc nofollow" target="_blank">连接操作</a>，在<code class="fe om on oo op b">country</code>上连接。使用join，Kafka的流式销售数据被丰富为销售类别。任何国家没有指定销售区域的销售记录都被归类为“未指定”</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="fc7a" class="lv lw it op b gy ow ox l oy oz">.join(df_regions, on=["country"], how="leftOuter") \<br/>.na.fill("Unassigned") \</span></pre><p id="44bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后按销售区域汇总销售额和订单，每分钟将前25名输出到控制台。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="52ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要运行该作业，请重复之前重命名主题(如<code class="fe om on oo op b">pagila.sales.spark.streaming.region)</code>)的过程，然后运行初始销售数据作业、该脚本，最后，与该脚本同时运行增量销售数据作业。下面，我们看到了稍后从Spark作业到控制台的微批处理输出。我们从三个不同的10分钟滑动事件时间窗口(有5分钟的重叠)中，按销售区域看到了三组销售结果。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="5973" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">PySpark脚本9:分组聚合的静态连接</h2><p id="3639" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">作为比较，我们可以从前面的流查询脚本<code class="fe om on oo op b">08_streaming_read_kafka_join_window.py</code>中排除滑动事件时间窗口操作，以获得按销售区域划分的当前、总的、有史以来的销售额。有关详细信息，请参见项目存储库中的脚本<code class="fe om on oo op b"><a class="ae mt" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/09_streaming_read_kafka_join.py" rel="noopener ugc nofollow" target="_blank">09_streaming_read_kafka_join.py</a></code>。</p><pre class="lg lh li lj gt os op ot ou aw ov bi"><span id="d011" class="lv lw it op b gy ow ox l oy oz">-------------------------------------------<br/>Batch: 20<br/>-------------------------------------------<br/>+--------------+--------+------+<br/>|sales_region  |sales   |orders|<br/>+--------------+--------+------+<br/>|Asia &amp; Pacific|5,780.88|812   |<br/>|Europe        |3,081.74|426   |<br/>|Latin America |2,545.34|366   |<br/>|Africa        |1,029.59|141   |<br/>|North America |997.57  |143   |<br/>|Middle east   |541.23  |77    |<br/>|Unassigned    |352.47  |53    |<br/>|Arab States   |244.68  |32    |<br/>+--------------+--------+------+</span></pre><h1 id="7aec" class="nn lw it bd lx no np nq ma nr ns nt md jz nu ka mg kc nv kd mj kf nw kg mm nx bi translated">结论</h1><p id="7ff3" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在这篇文章中，我们了解了如何在Amazon EMR上开始使用Spark结构化流。首先，我们探索了如何在Amazon EMR上使用PySpark以Python编写的作业作为步骤运行，并直接从EMR集群的主节点运行。接下来，我们发现了如何在亚马逊MSK上使用Apache Kafka，使用批处理和流查询来生成和消费消息。最后，我们了解了滑动事件时间窗口上的聚合与分组聚合的比较，以及如何使用微批处理处理结构化流查询。</p><p id="56ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在后续的帖子中，我们将学习如何在Amazon EMR上使用Apache Avro和Apicurio Registry与PySpark来读取和写入Apache Avro格式的消息到Amazon MSK。</p></div><div class="ab cl pn po hx pp" role="separator"><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps"/></div><div class="im in io ip iq"><p id="c1fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇博客代表我自己的观点，而不是我的雇主亚马逊网络服务公司(AWS)的观点。所有产品名称、徽标和品牌都是其各自所有者的财产。</p></div></div>    
</body>
</html>