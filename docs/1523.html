<html>
<head>
<title>Linear Regression: How to overcome underfitting with Locally Weighted Linear Regression (LWLR)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归:如何克服局部加权线性回归(lwr)的拟合不足</h1>
<blockquote>原文：<a href="https://itnext.io/linear-regression-how-to-overcome-underfitting-with-locally-weight-linear-regression-lwlr-e867f0cde4a4?source=collection_archive---------2-----------------------#2018-11-13">https://itnext.io/linear-regression-how-to-overcome-underfitting-with-locally-weight-linear-regression-lwlr-e867f0cde4a4?source=collection_archive---------2-----------------------#2018-11-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/2c12694908dd51d19832df75ff08fdf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDKbncrWqqus8Sj_So5jrw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">线性路径</figcaption></figure><p id="74ac" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在本文中，我们将首先讨论线性回归、它的意义以及如何在Python中实现它。接下来，我们将了解一种用于局部平滑估计以更好地拟合数据的技术。即使用<a class="ae la" href="https://vsoch.github.io/2013/locally-weighted-linear-regression/" rel="noopener ugc nofollow" target="_blank">lwr</a>克服装配不足。</p><p id="cbda" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">获取<a class="ae la" href="https://github.com/Eyongkevin/Linearly-Weighted-Linear-Regression" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上的完整代码</p><h2 id="3f41" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">什么是线性回归</h2><p id="f996" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">让我们首先理解什么是回归。回归是一种有监督的学习，其中我们有一个目标变量或我们想要预测的东西。回归和分类的区别在于，在回归中，我们的目标变量是数字和连续的。</p><p id="7d3a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">LR用于找到目标和一个或多个预测因子之间的线性关系。我们预测的变量被称为(<strong class="ke ir">标准变量、结果变量、内生变量或回归</strong>)，而我们预测所基于的变量被称为(<strong class="ke ir">预测变量、外生变量或回归</strong>)。线性回归有两种类型:<strong class="ke ir">简单回归</strong>或<strong class="ke ir">多重回归</strong>。当只有一个预测变量时，预测方法称为简单LR。简单LR的图总是形成一条直线。</p><p id="8c13" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">LR的一些性质是:</p><ul class=""><li id="fa3b" class="lz ma iq ke b kf kg kj kk kn mb kr mc kv md kz me mf mg mh bi translated"><strong class="ke ir">优点</strong>:结果易于解释，计算成本低</li><li id="b03a" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated"><strong class="ke ir">缺点</strong>:对非线性数据建模较差</li><li id="74f6" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated"><strong class="ke ir">配合</strong>使用:数值、额定值</li></ul><h2 id="7e53" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">寻找与LR最匹配的系列</h2><p id="54e2" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">使用回归时，我们的主要目标是预测数字目标值。一种方法是写出目标值相对于输入的等式。让我们假设，我们想预测我们的能源基础上的食物和我们喝的水的数量。一个可能的等式是:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="b09c" class="lb lc iq ms b gy mw mx l my mz">Energy = 0.0015*food - 0.99*water</span></pre><p id="ba71" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这就是所谓的回归方程。值<em class="na"> 0.0015 </em>和<em class="na"> -0.99 </em>称为<strong class="ke ir">回归权重</strong>。找到这些回归权重的过程称为<strong class="ke ir">回归。</strong></p><p id="07b6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">线性回归意味着你可以将输入与一些常数相乘得到输出。还有一种回归叫做<strong class="ke ir">非线性回归</strong>，但事实并非如此；输出可以是相乘在一起的输入的函数。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="fb05" class="lb lc iq ms b gy mw mx l my mz">Energy = 0.0015*food/water</span></pre><p id="46e3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">就此而言，我们将为此<a class="ae la" href="https://github.com/Eyongkevin/Linearly-Weighted-Linear-Regression/tree/master/data" rel="noopener ugc nofollow" target="_blank">数据</a>寻找最佳拟合线。让我们加载数据:</p><figure class="mn mo mp mq gt jr"><div class="bz fp l di"><div class="nb nc l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">从文件中加载数据</figcaption></figure><blockquote class="nd ne nf"><p id="43a2" class="kc kd na ke b kf kg kh ki kj kk kl km ng ko kp kq nh ks kt ku ni kw kx ky kz ij bi translated"><em class="iq"> loadDataSet() </em>函数打开一个带有制表符分隔值的文本文件，并假定最后一个值是目标值。</p></blockquote><p id="d6c9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了可视化数据，我们将使用这个python代码来绘制数据的线性分布</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="06c8" class="lb lc iq ms b gy mw mx l my mz"># Convert arrays to matrix<br/>xMat = np.mat(data.dataMat)<br/>yMat = np.mat(data.labelMat)</span><span id="d56a" class="lb lc iq ms b gy nj mx l my mz"># Plot<br/>fig = plt.figure(figsize=(20,10))<br/>ax = fig.add_subplot(111)<br/>ax.scatter(xMat[:,1].flatten().A[0], yMat.T[:,0].flatten().A[0])<br/>plt.show()</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/67ef0282e9245bc09b76542d87d70e22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_SdkqXV6GNzfZtaNG_RMqA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">数据线性分布图</figcaption></figure><p id="3a14" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">假设我们的输入数据在矩阵X中，我们的回归权重在向量w中。对于给定的数据X1，我们的预测值由下式给出:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/c43764d02216632de39f36c01b7fa34b.png" data-original-src="https://miro.medium.com/v2/resize:fit:152/format:webp/1*G4ezesKdFRYCW8FDgAWa-Q.png"/></div></figure><p id="e280" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们有<strong class="ke ir"> Xs </strong>和<strong class="ke ir"> ys </strong>，但是怎么才能找到<strong class="ke ir"> ws </strong>？一种方法是找到最小化误差的ws。我们将误差定义为预测的<em class="na"> y </em>和实际的<strong class="ke ir"> y </strong>之差。仅使用误差将允许正值和负值相互抵消，因此我们使用平方误差:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/d539838eebaaa98810e8670b75af21b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*W04NbTCUGWYXY2wR1yQNOw.png"/></div></figure><p id="05cc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">用矩阵符号表示，我们有:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/cc6d1ff40e6c40eef8151728329a12fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*VecDT9NZPFchZbRccaqTkw.png"/></div></figure><p id="c191" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果我们对w求导，我们会得到</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5dc3c5efa7ead67c717f3a713c394bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/format:webp/1*7jCK0IRVs2WLQp8TvxJv3Q.png"/></div></figure><p id="7956" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以将其设置为零，并求解<strong class="ke ir"> w </strong>以获得以下最终等式:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e29693051830f781850708cabdb9e48d.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*VaFJ_2zmipdYmaLlwRoz2w.png"/></div></figure><blockquote class="nd ne nf"><p id="826a" class="kc kd na ke b kf kg kh ki kj kk kl km ng ko kp kq nh ks kt ku ni kw kx ky kz ij bi translated"><strong class="ke ir"> NB </strong>:最终方程有矩阵求逆。因此，在使用它之前，我们必须首先检查矩阵的逆存在，否则我们可能会有一个错误。</p></blockquote><p id="24ea" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们看看最终的等式是如何用Python实现的</p><figure class="mn mo mp mq gt jr"><div class="bz fp l di"><div class="nb nc l"/></div></figure><blockquote class="nd ne nf"><p id="d632" class="kc kd na ke b kf kg kh ki kj kk kl km ng ko kp kq nh ks kt ku ni kw kx ky kz ij bi translated">我们已经使用了<a class="ae la" href="https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.linalg.det.html" rel="noopener ugc nofollow" target="_blank"> linalg.det(m) </a>来找出一个矩阵是否有逆。如果等于零，那么它是一个奇异矩阵，所以没有逆矩阵</p></blockquote><h2 id="bc17" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">绘制并显示最佳拟合线</h2><p id="9b4b" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">到目前为止，我们已经加载了数据，并根据上面的公式实现了线性回归。现在我们将使用它来显示图上的最佳拟合线</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="a757" class="lb lc iq ms b gy mw mx l my mz">ws = standRegres(data.dataMat, data.labelMat)<br/>xMat = mat(data.dataMat)<br/>yMat = mat(data.labelMat)</span><span id="8e7c" class="lb lc iq ms b gy nj mx l my mz"># Our predicted value yHat using the weights (ws)</span><span id="d030" class="lb lc iq ms b gy nj mx l my mz"># Sort it to avoid out of order<br/>xCopy = xMat.copy()<br/>xCopy.sort(0)<br/>yHat = xCopy*ws</span><span id="d6c8" class="lb lc iq ms b gy nj mx l my mz"># Plot<br/>fig = plt.figure()<br/>ax = fig.add_subplot(111)<br/>ax.scatter(xMat[:,1].flatten().A[0], yMat.T[:,0].flatten().A[0])<br/>ax.plot(xCopy[:,1],yHat)<br/>plt.show()</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/fddf7b4151ce6b269e5d0ff9b47be581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsrePwt09mb6fVvThG_AnQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">显示最佳拟合线的图</figcaption></figure><h2 id="f667" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">检查相关系数</h2><p id="2807" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">为了计算预测值<em class="na"> yHat </em>与实际数据<strong class="ke ir"> y </strong>的匹配程度，我们检查两个序列之间的相关性</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="859b" class="lb lc iq ms b gy mw mx l my mz"># Get predicted value(unsorted)<br/># Transpose yHat so we have both vectors as row vectors<br/>yHat = xMat*ws<br/>np.corrcoef(yHat.T, yMat)</span><span id="b1c7" class="lb lc iq ms b gy nj mx l my mz">&gt;&gt;&gt; array([[ 1.        ,  0.98647356],<br/>       [ 0.98647356,  1.        ]])</span></pre><blockquote class="nd ne nf"><p id="2a07" class="kc kd na ke b kf kg kh ki kj kk kl km ng ko kp kq nh ks kt ku ni kw kx ky kz ij bi translated">Numpy.corrcoef()用于求2个数列(矩阵)之间的相关系数，从而知道它们有多相似。</p></blockquote><p id="45c1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从上面的结果来看，对角线上的元素是<strong class="ke ir"> 1.0 </strong>，因为<em class="na"> yMat </em>和<em class="na"> yMat </em>之间的相关性是完美的。然而，我们在<em class="na"> yHat </em>和<em class="na"> yMat </em>之间有一个<strong class="ke ir"> 0.98 </strong>的相关性。因此，我们的模型在预测方面做得很好</p><h2 id="c9a9" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">局部加权线性回归</h2><p id="ccbc" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">线性回归有一个问题，就是它倾向于对数据进行欠拟合。它给出了无偏估计量的最小均方误差。因此，在拟合不足的情况下，我们得不到最好的预测。</p><p id="95b1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">降低均方误差的一种方法是一种称为LWLR的技术。对于LWLR，我们对感兴趣的数据点附近的数据点赋予权重；然后我们计算最小二乘回归。该公式现在变为:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/9406948765e97cdc22d75044252b5259.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*NgdcOS7sK1gpaL9O-T7Q7w.png"/></div></figure><p id="a3a3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> (W) </strong>这是一个用来给数据点加权的矩阵。LWLR使用一个类似于SVM的核来加权附近的点。最常用的内核是一个<strong class="ke ir">高斯</strong>。这分配了由下式给出的权重:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/eb3705d05ba11283f7d062fbed327582.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*YokDDXSJgPPpEIzE8tchhA.png"/></div></figure><ul class=""><li id="dcf6" class="lz ma iq ke b kf kg kj kk kn mb kr mc kv md kz me mf mg mh bi translated">根据上面的公式，数据点<strong class="ke ir"> x </strong>离其他点越近，<strong class="ke ir"> w(i，i) </strong>越大。</li><li id="b549" class="lz ma iq ke b kf mi kj mj kn mk kr ml kv mm kz me mf mg mh bi translated">我们还看到一个常数<strong class="ke ir"> k </strong>，这是一个用户定义的常数，它将决定附近点的权重。所以它决定了衰变发生的速度。对于LWLR，这是我们唯一需要担心的参数</li></ul><p id="dfc6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">优点</strong></p><ul class=""><li id="119d" class="lz ma iq ke b kf kg kj kk kn mb kr mc kv md kz me mf mg mh bi translated">有了合适的k值，我们就可以得到最佳拟合的数据，避免过度拟合和欠拟合</li></ul><p id="8171" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">缺点</strong></p><ul class=""><li id="7f60" class="lz ma iq ke b kf kg kj kk kn mb kr mc kv md kz me mf mg mh bi translated">这涉及到大量的计算。您必须使用整个数据来找到一个估计值</li></ul><p id="7d13" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在Python代码中，我们有以下内容</p><figure class="mn mo mp mq gt jr"><div class="bz fp l di"><div class="nb nc l"/></div></figure><blockquote class="nd ne nf"><p id="e863" class="kc kd na ke b kf kg kh ki kj kk kl km ng ko kp kq nh ks kt ku ni kw kx ky kz ij bi translated">函数<em class="iq"> lwlr() </em>根据输入数据创建矩阵，然后创建一个对角权重矩阵，称为<em class="iq">权重</em>。权重矩阵是具有与数据点一样多的元素的正方形矩阵。该函数接下来迭代所有数据点并计算一个值，该值随着远离<em class="iq">测试点</em>而呈指数衰减。输入<em class="iq"> K </em>控制衰减发生的速度。在我们填充了权重矩阵之后，我们找到了类似于函数<em class="iq">standregregs()</em>的测试点的估计值。<br/> <em class="iq"> lwlrTest() </em>为数据集中的每一点调用<em class="iq"> lwlr() </em>。</p></blockquote><p id="bd63" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们之前说过，有了合适的K值，我们就可以获得数据的最佳拟合，避免过度拟合和欠拟合。在这方面，我们将测试K的3个值(1.0，0.01，0.003)，并查看K的哪个值最符合我们的数据。</p><p id="27ad" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将使用下面的代码来绘制最佳拟合</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="4ed3" class="lb lc iq ms b gy mw mx l my mz"># find extimate yHat for all data points.</span><span id="2da0" class="lb lc iq ms b gy nj mx l my mz">k = 0.01<br/>yHat = lwlrTest(data.dataMat, data.dataMat, data.labelMat, k)</span><span id="c69b" class="lb lc iq ms b gy nj mx l my mz"># Plot needs the data to be sorted. Here, we sort xArr<br/>srtInd = xMat[:,1].argsort(0)<br/>xSort = xMat.copy()<br/>xSort.sort(0)</span><span id="3fd5" class="lb lc iq ms b gy nj mx l my mz"># Plot<br/>fig = plt.figure()<br/>ax = fig.add_subplot(111)<br/>ax.plot(xSort[:,1], yHat[srtInd])<br/>ax.scatter(xMat[:,1].flatten().A[0], mat(data.labelMat).T.flatten().A[0], s=2, c='red')<br/>plt.show()</span></pre><p id="63c5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">情况K=1.0 </strong></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/c8a99f42c62a625458a7bd8d62eec30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwSLYFouOtMKQRFVL6csJQ.png"/></div></div></figure><p id="d1e4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当<em class="na"> K=1.0 </em>时，我们看不到任何变化，它仍然会出现欠拟合。</p><p id="def0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">案例K=0.003 </strong></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/9758efda4b94ec9fec3bb83e01da4f68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kg8iXgswmjHGEZQ3_3Hvqw.png"/></div></div></figure><p id="e6a4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">随着<em class="na"> K=0.003 </em>，我们看到我们的模型经历过拟合。</p><p id="933c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">案例K=0.01 </strong></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/2d4dd23dfc62393614b6c8e287ee144f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qronGUPiTyQA_t8HbpfFIA.png"/></div></div></figure><p id="5cd4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用<em class="na"> K=0.01 </em>，我们有最佳拟合线，不会过度拟合和欠拟合。</p><h2 id="cc08" class="lb lc iq bd ld le lf dn lg lh li dp lj kn lk ll lm kr ln lo lp kv lq lr ls lt bi translated">结束！</h2><p id="9b6e" class="pw-post-body-paragraph kc kd iq ke b kf lu kh ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz ij bi translated">我们看到了如何使用LWLR方法找到没有欠拟合和过拟合的最佳拟合线。然而如上所述，LWLR的一个问题是它涉及大量的计算。</p><p id="b601" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我希望你喜欢读这篇文章，如果你有任何关于比LWLR更好的其他方法的建议，或者我错过了在这篇文章中包括的任何东西，请欢迎你的评论。</p></div></div>    
</body>
</html>