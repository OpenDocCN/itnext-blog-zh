<html>
<head>
<title>Deep Learning in Information Retrieval. Part I: Introduction and Sparse Retrieval</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">信息检索中的深度学习。第一部分:介绍和稀疏检索</h1>
<blockquote>原文：<a href="https://itnext.io/deep-learning-in-information-retrieval-part-i-introduction-and-sparse-retrieval-12de0423a0b9?source=collection_archive---------1-----------------------#2022-12-14">https://itnext.io/deep-learning-in-information-retrieval-part-i-introduction-and-sparse-retrieval-12de0423a0b9?source=collection_archive---------1-----------------------#2022-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/9f8124a69b044800dc4a1c2fe88d5f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rWbWwehwBwHQ1qlr0_OoNw.png"/></div></div></figure><p id="571b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">信息检索(IR) </strong>是计算机科学的一个重要研究领域。这不仅仅是建立像谷歌或必应这样的大型T2搜索引擎。当我们使用网上商店搜索、聊天机器人或接收带有推荐在线课程的自动电子邮件时，我们可能会面对IR系统。信息检索是业界的一个热门领域。大型在线市场、流媒体服务、社交媒体、求职服务和许多其他公司都建立了自己的信息检索系统。搜索或推荐结果的相关性对这些领域的业务绩效有着至关重要的影响。提高百分之几的搜索质量可以带来数百万美元的利润增长。一些公司甚至创建了自己的研发部门，专门研究信息检索方面的问题。现代信息检索中仍有许多问题没有解决。研究人员致力于定义和解决这类问题。例如，<a class="ae kw" href="https://www.kaggle.com/c/trec-covid-information-retrieval" rel="noopener ugc nofollow" target="_blank">TREC-科维德信息检索挑战赛</a>于2020年举行。参与者竞相建立一个疫情文档搜索系统，该系统有助于为生物医学研究人员、临床医生和政策制定者确定一些重要问题的答案。2022年更一般的<a class="ae kw" href="https://fair-trec.github.io/index" rel="noopener ugc nofollow" target="_blank"> TREC 2022年公平排名赛道</a>，与其他挑战一起举行。这个挑战集中在公平地优先编辑维基媒体的文章，以提供来自不同群体的文章的公平曝光。<br/>构建信息检索系统的一个有前途的领域是医学研究。绝大多数临床试验未能达到他们招募患者的目标。有效的患者试验招募是医学研究的主要障碍。这个问题的一个重要解决方案是建立一个信息检索系统，该系统将利用已经以电子健康记录形式存在的大量患者数据(EHR)。在<a class="ae kw" href="https://www.trec-cds.org/2022.html." rel="noopener ugc nofollow" target="_blank"> 2022临床试验跟踪</a>中，参与者被要求建立一种从<a class="ae kw" href="http://ClinicalTrials.gov" rel="noopener ugc nofollow" target="_blank">ClinicalTrials.gov</a>中检索临床试验的方法，这是临床试验所需的注册。</p><p id="c62c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">国际关系领域的研究历史相对较长。<a class="ae kw" href="https://trec.nist.gov/" rel="noopener ugc nofollow" target="_blank">文本检索会议</a> (TREC)成立于1992年。但深度学习的最新进展极大地改变了这一领域。这是“信息检索中的深度学习”系列的第一篇文章。在这一系列文章中，我们将看看最近的深度学习方法是如何应用于现代信息检索系统的。我们将主要关注文本信息检索。图像、声音和其他对象的相似性搜索以及多模态信息检索也是重要且值得注意的研究领域。但是我们不会在本系列中深入探讨这些主题。在本文中，我们从文本信息检索的问题定义、解决信息检索的一般框架、信息检索中的经典算法以及使用深度学习来提高经典信息检索算法的性能的方法开始。</p><h2 id="8035" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated"><strong class="ak">红外系统简介</strong></h2><p id="9bfe" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">什么是<strong class="ka ir">信息检索(IR) </strong>？将信息检索定义为排序问题是合理的。我们有一个<strong class="ka ir">查询</strong>(也称为<strong class="ka ir">上下文</strong>)，它是信息需求的正式陈述。例如，查询可以是搜索字符串、聊天中的问题、我们需要向其显示推荐的用户数据等。我们有一组<strong class="ka ir">候选者</strong>，它们是可用于检索特定<strong class="ka ir">查询</strong>的数据实体。例如，候选可以是段落、文档、图像、视频、音频文件或其他实体。在这一系列文章中，我将重点讨论文本数据。在信息检索中，查询并不识别候选集合中的唯一对象。相反，几个候选可能以不同的<strong class="ka ir">相关度</strong>匹配查询。一些查询可以有强匹配。另一个只能匹配相关性相对较低但仍与特定查询相关的弱结果。这就是为什么大多数IR系统<strong class="ka ir">通过某种相关性分数对结果进行排名</strong>并返回排名靠前的结果列表的原因。因此，信息检索可以定义为通过与特定查询的相关性对数据实体集合进行排序的问题。这是一个非正式的定义，有助于更好地理解IR问题。</p><p id="c1f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有不同的指标来衡量一些信息检索算法或系统在任务排序方面的表现。最重要的指标之一是召回@N和MRR。<strong class="ka ir"> Recall </strong>是检索到的相关结果的分数。<strong class="ka ir"> Recall@N </strong>表示在第一个<strong class="ka ir"> <em class="lv"> N </em> </strong>排名靠前的结果中检索到的相关结果的分数。查询的排序结果列表的倒数排名<strong class="ka ir">是第一个正确结果的排名的乘法倒数。M <strong class="ka ir"> ean倒数排名</strong>是查询数据集的结果倒数排名的平均值:</strong></p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/c62b475f885b78ca8b16b234722e62a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*Z1QR52bBD1vTWwAKfWIDbw.png"/></div></figure><p id="fb39" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<strong class="ka ir"> <em class="lv"> Q </em> </strong>是查询的样本，而<strong class="ka ir"><em class="lv">rankᵢ</em></strong><em class="lv"/>是第<em class="lv">个</em>相关候选项在第<em class="lv">个</em>个查询中的排名位置。</p><p id="12d6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在科学界，有不同的公共基准可以用来衡量IR系统的质量。其中最大的和最受欢迎的开放域IR是MS MARCO数据集。该数据集由1，010，916个匿名问题组成，这些问题从Bing的搜索查询日志中取样，每个问题都有人工生成的答案和182，669个完全由人工重写生成的答案。此外，该数据集包含从Bing检索的3，563，535个web文档中提取的8，841，823个段落，这些段落提供了管理自然语言答案所必需的信息。MARCO女士的作者提出的任务之一是对给定问题的一组检索到的段落进行排序。该任务是大规模开放域信息检索系统的一个很好的基准。因此，该数据集经常在科学论文中用于测试现代信息检索中的新模型和方法。还有TREC复杂答案检索(TREC汽车)数据集。它更小，基于所有维基百科页面的大约700万个段落。在科学论文中，TREC CAR数据集也经常被用作现代IR方法的基准。此外，还有一些其他更小和更老的公共信息检索数据集、不同信息检索问题的基准、不同语言的数据集等。您可以查看<a class="ae kw" href="https://github.com/allenai/ir_datasets/" rel="noopener ugc nofollow" target="_blank"> ir_datasets </a> repo和Python库，该库为许多ir排名数据集提供了一个公共接口。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mb"><img src="../Images/d0cf3b8193d05d41e33d67df0b312f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lz3pwQx_7bCp7kjVRkqrjg.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">典型红外系统管道</figcaption></figure><p id="d8c3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如何构建一个信息检索系统？一种直接的方法是设计一种算法或模型，其对要查询的候选项的相关性进行评分，并建立一种系统，其计算所有候选项的分数并返回排序的结果列表。然而，为每个查询排列整个候选集是不切实际的。这就是为什么大多数信息检索系统通常使用至少两级管道的原因(见上图)。第一阶段通常被称为<strong class="ka ir">候选人检索</strong>阶段。第二阶段和随后的阶段被称为<strong class="ka ir">排名</strong>或<strong class="ka ir">重新排名</strong>阶段。在候选项检索阶段，使用一些快速但不是非常精确的算法来检索可能包含相关结果的候选项的合理子集。然后在排序阶段，使用更复杂的算法和机器学习模型对检索到的子集进行排序，并为最终用户获得最相关的结果。在本文的后续部分，我们将深入IR pipeline的候选人检索阶段，了解它是如何工作的，以及如何应用现代深度学习方法来改善这一阶段的结果。</p><h2 id="0593" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">经典算法(稀疏检索)</h2><p id="43e5" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">经典信息检索系统的核心组件之一是一个倒排索引数据结构。<strong class="ka ir">倒排索引</strong>是存储内容映射的索引，如术语、单词、数字等。到它在数据实体(文档、段落等)集合中的位置。)倒排索引的目的是以当新的数据实体被添加到系统中时增加的处理时间为代价来实现快速搜索。索引设计融合了语言学、认知心理学、数学、信息学和计算机科学的跨学科概念。</p><p id="49fb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">用于构建倒排索引的最简单的文本表示方法之一是<strong class="ka ir">词袋(BoW) </strong>模型。在BoW模型中，数据实体文本<br/>被表示为其单词(<strong class="ka ir">术语</strong>或<strong class="ka ir">标记</strong>)的包(<a class="ae kw" href="https://en.wikipedia.org/wiki/Multiset" rel="noopener ugc nofollow" target="_blank">多重集</a>)，而不考虑语法和词序。让我们看看下面的例子。这里有两个简单的文本:</p><blockquote class="mg mh mi"><p id="bace" class="jy jz lv ka b kb kc kd ke kf kg kh ki mj kk kl km mk ko kp kq ml ks kt ku kv ij bi translated">这篇文章是关于信息检索的。</p><p id="d988" class="jy jz lv ka b kb kc kd ke kf kg kh ki mj kk kl km mk ko kp kq ml ks kt ku kv ij bi translated">信息检索是获取与信息需求相关的信息资源的过程。</p></blockquote><p id="ca58" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">基于这两个文本，为每个文本构建术语列表如下:</p><pre class="lx ly lz ma gt mm mn mo bn mp mq bi"><span id="123e" class="mr ky iq mn b be ms mt l mu mv">"this", "article", "is", "about", "information", "retrieval"</span></pre><pre class="mw mm mn mo bn mp mq bi"><span id="47c2" class="mr ky iq mn b be ms mt l mu mv">"information", "retrieval", "is", "the", "process", "of", "obtaining", "information", "resources", "that", "are", "relevant", "to", "an", "information", "need"</span></pre><p id="28d6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们将两个文本数据集中使用的所有标记收集到单个<strong class="ka ir">词汇表</strong>中，并为词汇表中的每个标记分配一个顺序号，那么我们将能够使用每个包中特定标记的出现次数将每个单词包表示为一个向量。例如，让我们假设我们以如下方式定义词汇:</p><pre class="lx ly lz ma gt mm mn mo bn mp mq bi"><span id="4d8c" class="mr ky iq mn b be ms mt l mx mv"> 0: "about",<br/> 1: "an", <br/> 2: "are", <br/> 3: "article", <br/> 4: "is",<br/> 5: "obtaining",<br/> 6: "of",<br/> 7: "need"<br/> 8: "process",  <br/> 9: "information",<br/>10: "relevant",<br/>11: "retrieval"<br/>12: "resources",<br/>13: "the", <br/>14: "this", <br/>15: "that", <br/>16: "to"</span></pre><p id="b257" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">那么使用这两个文本的特定令牌包的出现次数可以表示为以下两个向量:</p><pre class="lx ly lz ma gt mm mn mo bn mp mq bi"><span id="00a9" class="mr ky iq mn b be ms mt l mx mv">1: [1,0,0,1,1,0,0,0,0,1,0,1,0,0,1,0,0],<br/>2: [0,1,1,0,1,1,1,1,1,3,1,1,1,1,0,1,1]</span></pre><p id="6b8d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">单词袋是一个简单的向量空间模型。它没有考虑到术语在特定文本的上下文中可能具有不同的重要性。例如，像“a”、“the”、“this”这样的常用词几乎总是出现频率很高，但携带的信息量很少。解决这个问题最简单的方法之一是使用<strong class="ka ir"> tf-idf </strong>或t <strong class="ka ir"> erm频率-逆文档频率</strong>模型。TF–IDF计算为两个指标的乘积，<strong class="ka ir">术语频率</strong>和<strong class="ka ir">逆文档频率</strong>。一个数据实体被称为文档。<strong class="ka ir">术语频率</strong>是术语在文档中的相对频率，通常计算为术语在文档中出现的次数除以文档中的术语总数。<strong class="ka ir">逆文档频率</strong>与<strong class="ka ir"> </strong>该术语提供的信息量相关，通常计算为文档总数除以包含该术语的文档数的分数的对数。词汇的计算逆文档频率集通常被称为<strong class="ka ir"> IDF表</strong>。对于上述IDF表中使用以10为底的对数计算的两个文本数据集，如下所示:</p><pre class="lx ly lz ma gt mm mn mo bn mp mq bi"><span id="d960" class="mr ky iq mn b be ms mt l mx mv">"about": 0.30103,<br/>"an": 0.30103, <br/>"are": 0.30103, <br/>"article": 0.30103, <br/>"is": 0,<br/>"obtaining": 0.30103,<br/>"of": 0.30103,<br/>"need": 0.30103<br/>"process": 0.30103,  <br/>"information": 0,<br/>"relevant": 0.30103,<br/>"retrieval": 0,<br/>"resources": 0.30103,<br/>"the": 0.30103, <br/>"this": 0.30103, <br/>"that": 0.30103, <br/>"to": 0.30103</span></pre><p id="7b5a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">计算术语频率，然后计算每个术语的整个tf-idf度量，这两个文本可以表示为以下两个向量:</p><pre class="lx ly lz ma gt mm mn mo bn mp mq bi"><span id="68ec" class="mr ky iq mn b be ms mt l mx mv">1: [0.05017,0.0    ,0.0    ,0.05017,0.0,0.0    ,0.0    ,0.0    ,0.0    ,0.0,0.0    ,0.0,0.0    ,0.0    ,0.05017,0.0    ,0.0    ],<br/>2: [0.0    ,0.01881,0.01881,0.0    ,0.0,0.01881,0.01881,0.01881,0.01881,0.0,0.01881,0.0,0.01881,0.01881,0.0    ,0.01881,0.01881]</span></pre><p id="106d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Tf-ids和词袋都是简单的向量空间模型。在真实情况下(有几十万个术语的庞大词汇表)，它们都产生非零元素数量相对较少的向量。这样的向量被称为<strong class="ka ir">稀疏向量</strong>。使用稀疏向量的信息检索方法通常被称为<strong class="ka ir">稀疏检索</strong>。</p><p id="4f8b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最著名的稀疏检索经典算法之一是Okapi BM25。<strong class="ka ir"> Okapi </strong>BM25函数的一个流行变体如下:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/54489273e9519376e6df73ff55a37cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxbO6uYgkDc1Xllhv_ZSMg.png"/></div></div></figure><p id="ae3c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<strong class="ka ir"> <em class="lv"> D </em> </strong>是文档<em class="lv">，</em> <strong class="ka ir"> <em class="lv"> Q </em> </strong>是包含术语<strong class="ka ir"> <em class="lv"> q₁,…,qₙ </em> </strong>，<strong class="ka ir"> <em class="lv"> k₁ </em> </strong>和<strong class="ka ir"> <em class="lv"> b </em> </strong>是特殊常量，<strong class="ka ir"> <em class="lv"> avgdl </em> </strong>是文档中术语的平均数量，<strong class="ka ir"> | <em class="lv"> <strong class="ka ir"><em class="lv">【f(qᵢ,d】)</em></strong>是术语<strong class="ka ir"><em class="lv"/></strong>在文档<strong class="ka ir"><em class="lv">【d】</em></strong>，<strong class="ka ir"><em class="lv">【idf(qᵢ)</em></strong>中出现的次数，是逆文档频率的变化，通常计算如下:</em></strong></p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/1dc1c07762111aabcc106587b9a06b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*-gg59JI4mtq68qxupue01A.png"/></div></figure><p id="2728" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<strong class="ka ir"><em class="lv">【n(qᵢ】</em></strong>为包含<strong class="ka ir"><em class="lv">qᵢ</em></strong><strong class="ka ir"><em class="lv">n</em></strong>为文档总数。</p><p id="cc39" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本文中，我们将深入BM25算法背后的所有理论。请注意，尽管它是在20世纪80年代实现的，但它仍然在许多信息检索系统中使用。用于稀疏检索的旧的经典算法相对于最近的基于机器学习的方法具有一些优势:</p><ul class=""><li id="f554" class="na nb iq ka b kb kc kf kg kj nc kn nd kr ne kv nf ng nh ni bi translated">索引和检索速度。在索引阶段(计算向量)和大多数情况下在检索阶段，经典算法比现代基于DL的方法快得多。</li><li id="3a0e" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nf ng nh ni bi translated">可解释性。稀疏向量的意义显而易见。我们可以很容易地检查为什么针对特定的查询检索到特定的实体，以及哪些术语具有最大的影响。甚至可以通过简单地删除或更新特定的术语或添加自定义规则来计算术语权重，从而影响结果。</li></ul><p id="aafd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，在现代的信息检索系统中，仍然使用经典的稀疏检索方法。了解它们是如何工作的是很有用的。现代深度学习方法甚至可以用来改进稀疏检索。让我们在下一节看看它是如何工作的。</p><h2 id="e377" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">稀疏检索中的深度学习</h2><p id="b9c4" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">基于Transformer架构变体的深度学习语言模型在跨不同领域的绝大多数NLP任务中取得了最先进的结果。第一个也是最受欢迎的语言模型之一是BERT(来自变形金刚的双向编码器表示)，由Google的Jacob Devlin和他的同事在2018年开发并发布[ <a class="ae kw" href="#0e9b" rel="noopener ugc nofollow"> 3 </a> ]。如果你不熟悉Transformer架构或BERT，那么我推荐<a class="ae kw" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">这篇</a>文章。目前，这种模型有很多更强大的变体:RoBERTa、XLNet等。但是如何将语言模型应用于信息检索，尤其是稀疏检索算法呢？</p><p id="7636" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，让我们来看看一种叫做<strong class="ka ir"> DeepCT </strong>的方法。正如我们在上面看到的，术语频率是一种在查询或文档中识别术语重要性的常用方法。但是它是一个弱信号，尤其是当频率分布是平坦的时候，例如在长查询或文本是句子/段落长度的短文档中。让我们来看看原始论文中的以下示例:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/0a6d3ea493cdb0de16112ae42604a689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CRvjRHpXHF7Gp9E1VW1T7Q.png"/></div></div></figure><p id="cf17" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">两段文字两次提到了“胃”。但是只有第一段是关于‘胃’这个话题的。DeepCT论文提出了一个<strong class="ka ir">深度上下文化术语权重</strong>框架，该框架学习将BERT的上下文化文本表示映射到文本的上下文感知术语权重。当应用于段落时，DeepCT-Index产生可以存储在普通倒排索引中用于检索的术语权重。当应用于查询文本时，DeepCT-Query生成一个加权的单词包查询。典型的第一阶段检索算法可以直接使用这两种类型的术语权重[ <a class="ae kw" href="#df0e" rel="noopener ugc nofollow"> 4 </a> ]。</p><p id="62d0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">DeepCT包括两个主要组件:通过BERT生成上下文化的单词嵌入，以及通过线性回归为每个单词预测术语权重。语境化单词嵌入是一个特征向量，它表征了单词在给定语境中的句法和语义角色。DeepCT将这些特征线性组合成一个单词重要性分数。给定文本中每个单词的基本真实项权重，DeepCT旨在最小化均方误差。从BERT到回归层的DeepCT模型是端到端优化的。用预训练的BERT模型初始化BERT分量，以减少过拟合。它被微调以将上下文化的单词嵌入与术语预测任务对齐。最后一个回归层是从零开始学习的。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/00ddf92f0bead04fa7dfeef4e23e6a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sx9hCFs-1QrEvplLhjM2gg.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">预测术语权重架构</figcaption></figure><p id="8090" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用MSMARCO和TREC-卡尔数据集对DeepCT进行了基准测试。在DeepCT-Index上的BM25检索可以比经典的基于tf的索引精确25%,并且比一些广泛使用的多级检索系统更精确。DeepCT方法的变体也称为<strong class="ka ir">W-索引检索</strong>。</p><p id="72cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">类似的方法可以用于长文档检索。例如，在名为<strong class="ka ir"> HDCT </strong> ( <strong class="ka ir">上下文感知分层文档术语加权框架</strong>)的工作中，提出了一种用于文档索引和检索的框架<a class="ae kw" href="#1eee" rel="noopener ugc nofollow"> 5 </a>。HDCT首先评估一个术语在每篇文章中的语义重要性。这些细粒度的术语权重然后被聚合到文档级的词袋表示中，该词袋表示可以存储到标准的倒排索引中，以便使用BM25等经典的稀疏方法进行高效检索。HDCT建筑来自原始文件:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/158a2a354b3c1eb059859d7f8fca804d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UeQ9L5fABtgQ88cfsdUYvw.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">HDCT建筑</figcaption></figure><p id="ed8e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">像在DeepCT方法中一样，HDCT被训练来预测文章中每个标记的重要性权重。但主要的区别是，HDCT的作者提出了两种弱监督方法来训练HDCT，而不是手动标记每篇文章中每个术语的重要性。第一种方法使用文档的元数据字段(例如标题、关键字和摘要)来基于包含来自文档的标记的字段实例的百分比生成弱标签。第二种方法使用查询和相关文档的数据集，基于从文档中提及术语的相关查询的百分比来生成弱标签。</p><p id="bd60" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在4个文档检索数据集上对HDCT进行了评估:MS-MARCO文档排序数据集、ClueWeb09-B、ClueWeb09-C和ClueWeb12-C。</p><p id="f8f1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当查询和相关文本不能直接通过术语匹配时，改进的术语加权没有帮助。例如，如果在查询中使用同义词，或者查询包含与某些文本间接相关的术语。对于这种情况，另一种称为<strong class="ka ir">文档扩展</strong>的方法开始发挥作用。<strong class="ka ir">文档扩展</strong>是一种通过相关术语或短语扩展文档来提高信息检索质量的方法。有许多方法来进行文档扩展，但是这里我们考虑一些基于深度学习的方法。</p><p id="dc6b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一种方法称为<strong class="ka ir"> Doc2Query </strong>。这是一种简单的方法，它预测将对给定文档发出哪些查询，然后使用普通的序列到序列神经网络扩展这些预测，该神经网络使用由查询和相关文档对组成的数据集进行训练[ <a class="ae kw" href="#f459" rel="noopener ugc nofollow"> 6 </a> ]。原始论文中的下图说明了该方法的思想:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/b0e2da05b253abbd822ab5c7adb0b708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xjl86hN6krtnwYf1mkJQ_w.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">doc2查询思想</figcaption></figure><p id="326c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用top-k随机抽样，使用训练的Doc2Query模型来预测10个查询，然后将生成的查询附加到索引中的每个文档。然后使用BM25算法进行第一阶段检索。这个方法的下一个版本叫做DocTTTTTQuery，以同样的方式训练，但是使用现代的基于transformer的T5模型[ <a class="ae kw" href="#a696" rel="noopener ugc nofollow"> 7 </a> ]。</p><p id="4f3f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">像Doc2Query这样的文档扩展方法可以与W-index检索结合使用。但是一些现代深度学习方法同时进行文档扩展和术语加权。让我们看一些例子。</p><p id="f278" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">称为<strong class="ka ir"> SparTerm </strong>的框架直接学习全词汇空间中的稀疏文本表示。SparTerm包括一个重要性预测器，用于预测词汇表中每个术语的重要性，以及一个门控控制器，用于控制术语激活。这两个模块共同确保最终文本表示的稀疏性和灵活性，将术语加权和扩展统一在同一框架内[ <a class="ae kw" href="#a696" rel="noopener ugc nofollow"> 8 </a> ]。原始论文中的SparTerm架构模式:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/50169cb34e2952857b072081ddfde77b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5mOZkUDLVPylRcTeOlGcZA.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">SparTerm架构</figcaption></figure><p id="ab96" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">重要性预测器输出段落中所有术语的语义重要性。通过优化排序目标，端到端地训练重要性预测器。但与DeepCT不同的是，它不直接符合统计术语重要性标签，而是将重要性视为中间变量，可以通过远程监控信号学习，用于段落排序。</p><p id="66c6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">选通控制器生成二进制选通信号，激活该二进制选通信号的哪些项来表示该通道。它学习激活文章中出现的术语和一些与文章主题相关的其他术语。这就是SparTerm如何通过激活最初没有出现在文章中的附加术语来进行文档扩展。通道-查询/摘要平行语料库用于门控员的训练。因此，文档扩展的方法类似于Doc2Query。但是不同之处在于直接学习段落的稀疏表示，并且联合训练扩展和术语加权模型。在MSMARCO段落检索数据集上，SparTerm优于DeepCT方法。</p><p id="ffde" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">更现代的方法称为<strong class="ka ir"> SPLADE </strong> ( <strong class="ka ir">稀疏词法和扩展</strong>模型)对SparTerm模型进行了轻微但必要的修改，从而显著提高了其性能[ <a class="ae kw" href="#e29b" rel="noopener ugc nofollow"> 9 </a> ]。首先，它对术语权重引入了对数饱和效应，这防止了一些术语占优势，并且自然地确保了表示中的稀疏性。第二，它引入了在BM25取样的批次中使用硬底片的损失。批量否定采样策略被广泛用于训练图像检索模型，并且已经显示出在学习第一阶段排序器中是有效的。第三，将一些正则化技术应用于损失，以获得良好平衡的索引。这些变化允许显著改进SparTerm方法。在这个方法的下一个版本<strong class="ka ir"> SPLADEv2 </strong>中，引入了几个在有效性和效率方面的重大改进。更具体地说，修改了模型的池化机制，并引入了用蒸馏训练的模型[ <a class="ae kw" href="#8367" rel="noopener ugc nofollow"> 10 </a> ]。</p><p id="2cff" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">还有其他深度学习方法进行稀疏检索[ <a class="ae kw" href="#7a37" rel="noopener ugc nofollow"> 11 </a> ]。将深度学习应用于稀疏检索的一般思想是寻找一种获得稀疏文本表示的方法，这种方法将更好地用于像BM25这样的经典稀疏检索算法。</p><h2 id="550b" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">结论</h2><p id="de10" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">在这一部分，我们考虑了信息检索系统的基本概念:倒排索引，词袋，TF-IDF，MRR和NDCG度量，稀疏检索和BM25算法。然后，我们看看使用深度学习来提高稀疏检索性能的不同方法:W索引检索、文档扩展模型和混合方法，如SparTerm或SPLADE。稀疏检索方法有一些优点，如可解释性和有效实现的存在，但其他方法在某些情况下可以优于稀疏检索方法。在下一部分中，我们将看看可用于IR系统的第一阶段检索的替代方法。</p><h2 id="dd95" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated"><strong class="ak">参考文献</strong></h2><ol class=""><li id="d52b" class="na nb iq ka b kb lq kf lr kj nt kn nu kr nv kv nw ng nh ni bi translated"><a class="ae kw" href="https://arxiv.org/abs/1611.09268" rel="noopener ugc nofollow" target="_blank"> Tri Nguyen等人，MARCO女士:人类生成的机器阅读理解数据集，2016，arXiv:1611.09268v3 </a></li><li id="efd3" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated"><a class="ae kw" href="https://arxiv.org/abs/1705.04803" rel="noopener ugc nofollow" target="_blank">费德里科·南尼，巴斯卡尔·米特拉，马特·马格努松，劳拉·迪茨，复杂答案检索基准，2017，arXiv:1705.04803 </a></li><li id="0e9b" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated">雅各布·德夫林、张明蔚、肯顿·李和克里斯蒂娜·图塔诺娃。BERT:用于语言理解的深度双向变压器预训练，2018，arXiv:1810.04805 </li><li id="df0e" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated"><a class="ae kw" href="http://arxiv.org/abs/1910.10687" rel="noopener ugc nofollow" target="_blank">戴，杰米卡兰，面向第一阶段检索的上下文感知句子/段落术语重要性估计，2019，arXiv:1910.10687v2 </a></li><li id="1eee" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated"><a class="ae kw" href="https://dl.acm.org/doi/10.1145/3366423.3380258" rel="noopener ugc nofollow" target="_blank">用于特定搜索的上下文感知文档术语加权，WWW ' 20:2020年网络会议论文集</a>。</li><li id="f459" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated"><a class="ae kw" href="https://arxiv.org/abs/1904.08375" rel="noopener ugc nofollow" target="_blank"> Rodrigo Nogueira等，通过查询预测进行文档扩展，2019，arXiv:1904.08375 </a></li><li id="a696" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated">罗德里戈·诺盖拉，和艾的认知。从doc2query到doctttttquery，2019 </li><li id="b304" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated"><a class="ae kw" href="https://arxiv.org/abs/2010.00768" rel="noopener ugc nofollow" target="_blank">柏杨等，SparTerm:学习基于术语的稀疏表示用于快速文本检索，2020，arXiv:2010.00768 </a></li><li id="e29b" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated"><a class="ae kw" href="https://doi.org/10.1145/3404835.3463098" rel="noopener ugc nofollow" target="_blank"> Thibault Formal、Benjamin Piwowarski和Stéphane Clinchant，SPLADE:第一阶段排名的稀疏词汇和扩展模型，载于第44届国际ACM SIGIR信息检索研究与发展会议论文集(SIGIR '21)。美国纽约州纽约市计算机械协会，邮编:2288–2292，2021，doi:10.1145/3404835.3463098</a></li><li id="8367" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated"><a class="ae kw" href="https://arxiv.org/abs/2109.10086" rel="noopener ugc nofollow" target="_blank"> Thibault Formal，Carlos Lassance，Benjamin Piwowarski，Stéphane Clinchant，SPLADE v2:信息检索的稀疏词法和扩展模型，2021，arXiv:2109.10086 </a></li><li id="7a37" class="na nb iq ka b kb nj kf nk kj nl kn nm kr nn kv nw ng nh ni bi translated"><a class="ae kw" href="https://dl.acm.org/doi/10.1145/3511808.3557456" rel="noopener ugc nofollow" target="_blank"> SpaDE:使用双文档编码器改进第一阶段检索的稀疏表示，CIKM’22:第31届ACM国际信息会议论文集&amp;知识管理，2022，doi:10.1145/3517456</a></li></ol></div></div>    
</body>
</html>