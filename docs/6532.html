<html>
<head>
<title>Hive on Spark with Spark Operator</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Spark运算符在Spark上配置蜂箱</h1>
<blockquote>原文：<a href="https://itnext.io/hive-on-spark-with-spark-operator-9a43ea7ebe06?source=collection_archive---------1-----------------------#2021-12-12">https://itnext.io/hive-on-spark-with-spark-operator-9a43ea7ebe06?source=collection_archive---------1-----------------------#2021-12-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/2bf471756d936e47046f07114ef0ef73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ViKiVFOP1yQNfTxx"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@aaronburden?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚伦·伯顿</a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="dc01" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Spark节俭服务器用作Hive服务器，其执行引擎是spark。正如Kubernetes 中Spark上的<a class="ae kc" rel="noopener ugc nofollow" target="_blank" href="/hive-on-spark-in-kubernetes-115c8e9fa5c1"> Hive所提到的，Spark Thrift Server可以部署到Kubernetes上。对于这种情况，安装在本地机器上的<code class="fe lb lc ld le b">spark-submit</code>已经被用来向kubernetes提交spark thrift服务器。还有一种方法可以向kubernetes提交spark应用程序。在这里，我将向您展示如何使用spark Operator将spark thrift server作为Spark应用程序部署到kubernetes。安装在容器中的<code class="fe lb lc ld le b">spark-submit , spark-submit</code>将用于将spark应用程序部署到kubernetes，而不是本地安装，也就是说，Spark Operator将用于此。</a></p><p id="ab65" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有许多spark操作符，但我写了一个spark操作符(<a class="ae kc" href="https://github.com/cloudcheflabs/dataroaster/tree/master/operators/spark" rel="noopener ugc nofollow" target="_blank"> DataRoaster Spark操作符</a>)来控制Spark应用程序，如与开源数据平台<a class="ae kc" href="https://github.com/cloudcheflabs/dataroaster" rel="noopener ugc nofollow" target="_blank"> DataRoaster </a>相关的spark thrift server。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="bed7" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">先决条件</h1><p id="1412" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在使用spark operator部署spark thrift server之前，需要安装hive metastore。Hive metastore需要卷来保存mysql中的元数据。还需要将卷安装到kubernetes上运行的spark应用程序上。对于本文，将安装NFS服务器来提供卷，即持久卷。</p><h2 id="b3a7" class="mp ln iq bd lo mq mr dn ls ms mt dp lw ko mu mv ma ks mw mx me kw my mz mi na bi translated">安装NFS服务器</h2><p id="818b" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">首先安装nfs二进制文件。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="81dd" class="mp ln iq le b gy nj nk l nl nm">sudo yum install nfs-utils -y;</span></pre><p id="8eda" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建nfs共享目录。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="4399" class="mp ln iq le b gy nj nk l nl nm">sudo mkdir -p /var/nfs_share_dir/nfs-client;<br/>sudo chmod -R 755 /var/nfs_share_dir<br/>sudo chown nfsnobody:nfsnobody /var/nfs_share_dir</span></pre><p id="d2f2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">启动nfs服务。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="9080" class="mp ln iq le b gy nj nk l nl nm">sudo systemctl enable rpcbind<br/>sudo systemctl enable nfs-server<br/>sudo systemctl enable nfs-lock<br/>sudo systemctl enable nfs-idmap<br/>sudo systemctl start rpcbind<br/>sudo systemctl start nfs-server<br/>sudo systemctl start nfs-lock<br/>sudo systemctl start nfs-idmap</span></pre><p id="767f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">配置导出文件并重新启动nfs服务器。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="6c56" class="mp ln iq le b gy nj nk l nl nm"># Configuring the exports file for sharing.<br/>sudo vi /etc/exports;<br/>...<br/>/var/nfs_share_dir  *(rw,sync,no_root_squash,insecure)<br/>/var/nfs_share_dir/nfs-client *(rw,sync,no_root_squash,insecure)<br/>...</span><span id="06bc" class="mp ln iq le b gy nn nk l nl nm"># restart nfs server.<br/>sudo systemctl restart nfs-server;</span><span id="da7e" class="mp ln iq le b gy nn nk l nl nm"># reexport.<br/>sudo exportfs -rav;</span><span id="b6c3" class="mp ln iq le b gy nn nk l nl nm"># firewall.<br/>sudo firewall-cmd --permanent --zone=public --add-service=nfs<br/>sudo firewall-cmd --permanent --zone=public --add-service=mountd<br/>sudo firewall-cmd --permanent --zone=public --add-service=rpc-bind<br/>sudo firewall-cmd --reload</span></pre><p id="4fae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，已经安装了nfs服务器来共享卷。</p><h2 id="7619" class="mp ln iq bd lo mq mr dn ls ms mt dp lw ko mu mv ma ks mw mx me kw my mz mi na bi translated">安装NFS存储类</h2><p id="5ed9" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">为了在kubernetes中动态供应卷，将在kubernetes上安装NFS存储类。</p><p id="96e4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">添加舵回购。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="2bd5" class="mp ln iq le b gy nj nk l nl nm">helm repo add nfs-subdir-external-provisioner <a class="ae kc" href="https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/" rel="noopener ugc nofollow" target="_blank">https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/</a><br/>helm repo update</span></pre><p id="845c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建值文件。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="96a4" class="mp ln iq le b gy nj nk l nl nm">cat &lt;&lt;EOF &gt; nfs-values.yaml<br/>nfs:<br/>  server: &lt;nfs-server-ip&gt;<br/>  path: /var/nfs_share_dir/nfs-client<br/>storageClass:<br/>  name: nfs-external<br/>  accessModes: ReadWriteMany<br/>EOF</span></pre><p id="b99c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">&lt;nfs-server-ip&gt;</code>需要设置您的nfs服务器节点的ip地址。</p><p id="e97b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，用helm安装nfs存储类。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="90c1" class="mp ln iq le b gy nj nk l nl nm">helm install \<br/>nfs-external \<br/>--create-namespace \<br/>--namespace nfs-external \<br/>--values ./nfs-values.yaml \<br/>nfs-subdir-external-provisioner/nfs-subdir-external-provisioner;</span></pre><p id="0820" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们检查一下是否安装了nfs服务器存储类。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="097b" class="mp ln iq le b gy nj nk l nl nm">kubectl get sc<br/>NAME                 PROVISIONER                                                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE<br/>nfs-external         cluster.local/nfs-external-nfs-subdir-external-provisioner   Delete          Immediate           true                   3d3h</span><span id="879a" class="mp ln iq le b gy nn nk l nl nm">...</span></pre><p id="73a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">NFS存储类<code class="fe lb lc ld le b">nfs-external</code>已成功创建。</p><h2 id="64d6" class="mp ln iq bd lo mq mr dn ls ms mt dp lw ko mu mv ma ks mw mx me kw my mz mi na bi translated">准备S3水桶</h2><p id="03e7" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">S3水桶是必要的上传星火申请工作文件到S3水桶。Hive metastore也需要访问S3存储桶。有许多S3兼容的对象存储。你可以从安装在你机器上的MinIO或Ceph S3，或者从AWS S3、OCI对象存储等公共云服务获得S3桶。应获得以下具有凭据的S3存储桶信息。</p><ul class=""><li id="2ea4" class="no np iq kf b kg kh kk kl ko nq ks nr kw ns la nt nu nv nw bi translated">存储桶名称。</li><li id="0b15" class="no np iq kf b kg nx kk ny ko nz ks oa kw ob la nt nu nv nw bi translated">访问键。</li><li id="2f5a" class="no np iq kf b kg nx kk ny ko nz ks oa kw ob la nt nu nv nw bi translated">秘钥。</li><li id="3923" class="no np iq kf b kg nx kk ny ko nz ks oa kw ob la nt nu nv nw bi translated">端点。</li></ul><h1 id="0ca1" class="lm ln iq bd lo lp oc lr ls lt od lv lw lx oe lz ma mb of md me mf og mh mi mj bi translated">在Kubernetes上安装Hive Metastore</h1><p id="5e27" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">有一个github repo可以看到本文中显示的所有代码。克隆源代码。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="024f" class="mp ln iq le b gy nj nk l nl nm">cd ~;<br/>git clone <a class="ae kc" href="https://github.com/mykidong/hive-on-spark-with-spark-operator" rel="noopener ugc nofollow" target="_blank">https://github.com/mykidong/hive-on-spark-with-spark-operator</a>;</span></pre><p id="6dde" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">修改metastore图表的值文件。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="4ed9" class="mp ln iq le b gy nj nk l nl nm">cd ~/hive-on-spark-with-spark-operator/hive-metastore/metastore;<br/>vi dataroaster-values.yaml;<br/>...</span><span id="e71f" class="mp ln iq le b gy nn nk l nl nm">namespace: dataroaster-hivemetastore<br/>s3:<br/>  bucket: &lt;bucket-name&gt;<br/>  accessKey: &lt;access-key&gt;<br/>  secretKey: &lt;secret-key&gt;<br/>  endpoint: <a class="ae kc" href="https://cnobgk2u8blu.compat.objectstorage.ap-seoul-1.oraclecloud.com" rel="noopener ugc nofollow" target="_blank">https://&lt;</a>s3-endpoint&gt;<br/>jdbc:<br/>  user: root<br/>  password: icarus0337</span></pre><p id="d387" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">需要配置<code class="fe lb lc ld le b">&lt;bucket-name&gt;</code>、<code class="fe lb lc ld le b">&lt;access-key&gt;</code>、<code class="fe lb lc ld le b">&lt;secret-key&gt;</code>和<code class="fe lb lc ld le b">&lt;s3-endpoint&gt;</code>。</p><p id="67fb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，用下面的shell在kubernetes上安装hive metastore。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="c34d" class="mp ln iq le b gy nj nk l nl nm">cd ~/hive-on-spark-with-spark-operator/hive-metastore;</span><span id="6511" class="mp ln iq le b gy nn nk l nl nm">./create.sh</span></pre><p id="a54a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个shell将安装mysql服务器，运行作业以在mysql中创建初始模式，并在kubernetes上安装hive metastore服务器。</p><p id="65b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">成功安装hive metastore后，它看起来类似于下面这样。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="02af" class="mp ln iq le b gy nj nk l nl nm">kubectl get po -n dataroaster-hivemetastore<br/>NAME                         READY   STATUS      RESTARTS   AGE<br/>hive-initschema--1-4wwgt     0/1     Error       0          73m<br/>hive-initschema--1-js2mk     0/1     Completed   0          73m<br/>metastore-5c65559cb7-pbbv5   1/1     Running     0          73m<br/>mysql-statefulset-0          1/1     Running     0          73m</span></pre><p id="2107" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并且，可以看到mysql服务器使用的PVC。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="e9c6" class="mp ln iq le b gy nj nk l nl nm">kubectl get pvc -n dataroaster-hivemetastore<br/>NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE<br/>mysql-data-disk   Bound    pvc-c2ce07f1-0a6b-4a91-b478-4f5f30f0d83e   1Gi        RWO            nfs-external   74m</span></pre><h1 id="b519" class="lm ln iq bd lo lp oc lr ls lt od lv lw lx oe lz ma mb of md me mf og mh mi mj bi translated">安装DataRoaster Spark运算符</h1><p id="6f1a" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">让我们安装数据焙烧炉火花运营商与舵图。</p><p id="2fb3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">添加舵库。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="ac79" class="mp ln iq le b gy nj nk l nl nm">helm repo add dataroaster-spark-operator <a class="ae kc" href="https://cloudcheflabs.github.io/helm-repository/" rel="noopener ugc nofollow" target="_blank">https://cloudcheflabs.github.io/helm-repository/</a><br/>helm repo update</span></pre><p id="df52" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">部署火花操作员。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="a9f0" class="mp ln iq le b gy nj nk l nl nm">helm install \<br/>spark-operator \<br/>--create-namespace \<br/>--namespace spark-operator \<br/>--version v1.0.0 \<br/>dataroaster-spark-operator/dataroaster-spark-operator;</span></pre><p id="b0bd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">查看spark操作器的pod状态。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="69da" class="mp ln iq le b gy nj nk l nl nm">kubectl get po -n spark-operator<br/>NAME                              READY   STATUS    RESTARTS   AGE<br/>spark-operator-756fbf4479-s2zjt   1/1     Running   0          52s</span></pre><p id="d9b6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">安装spark operator非常简单。</p><h1 id="d058" class="lm ln iq bd lo lp oc lr ls lt od lv lw lx oe lz ma mb of md me mf og mh mi mj bi translated">使用Spark Operator在Kubernetes上部署Spark Thrift服务器</h1><h2 id="8bd7" class="mp ln iq bd lo mq mr dn ls ms mt dp lw ko mu mv ma ks mw mx me kw my mz mi na bi translated">准备运行spark thrift服务器</h2><p id="d07c" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">让我们与RBAC创建服务帐户运行火花节俭服务器。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="3f23" class="mp ln iq le b gy nj nk l nl nm">kubectl create namespace spark;<br/>kubectl create serviceaccount spark -n spark;<br/>kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=spark:spark --namespace=spark;</span></pre><p id="9878" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用dataroaster spark operator部署spark thrift server需要准备以下事项。</p><ul class=""><li id="8251" class="no np iq kf b kg kh kk kl ko nq ks nr kw ns la nt nu nv nw bi translated">S3秘密是强制性的。它用于将应用程序文件上传到S3存储桶，并访问S3存储桶中的数据。</li><li id="b96d" class="no np iq kf b kg nx kk ny ko nz ks oa kw ob la nt nu nv nw bi translated">spark本地目录的卷，用于存储spark驱动程序和执行器将产生的本地临时数据。spark本地目录有三种卷类型，如<code class="fe lb lc ld le b">hostPath</code>、<code class="fe lb lc ld le b">emptyDir</code>、<code class="fe lb lc ld le b">persistentVolumeClaim</code>。<code class="fe lb lc ld le b">persistentVolumeClaim</code>将用于这个spark节俭服务器。</li></ul><p id="388d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们创造S3的秘密。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="06c9" class="mp ln iq le b gy nj nk l nl nm">cat &lt;&lt;EOF &gt; s3-secret.yaml<br/>apiVersion: v1<br/>kind: Secret<br/>metadata:<br/>  name: s3-secret<br/>  namespace: spark-operator<br/>type: Opaque<br/>data:<br/>  accessKey: $(echo -n "&lt;access-key&gt;" | base64)<br/>  secretKey: $(echo -n "&lt;secret-key&gt;" | base64)<br/>EOF</span><span id="5957" class="mp ln iq le b gy nn nk l nl nm">kubectl apply -f s3-secret.yaml -n spark-operator;</span></pre><p id="0633" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">&lt;access-key&gt;</code>和<code class="fe lb lc ld le b">&lt;secret-key&gt;</code>需要配置。</p><p id="0c12" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要两个用于spark节俭服务器的spark driver和executor pods的PVC。来看看下面。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="fa82" class="mp ln iq le b gy nj nk l nl nm">apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: nfs-pvc-driver<br/>  namespace: spark<br/>  labels: {}<br/>  annotations: {}<br/>spec:<br/>  accessModes:<br/>    - ReadWriteMany<br/>  resources:<br/>    requests:<br/>      storage: 1Gi<br/>  storageClassName: nfs-external<br/>---<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: nfs-pvc-executor<br/>  namespace: spark<br/>  labels: {}<br/>  annotations: {}<br/>spec:<br/>  accessModes:<br/>    - ReadWriteMany<br/>  resources:<br/>    requests:<br/>      storage: 1Gi<br/>  storageClassName: nfs-external</span></pre><p id="e378" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">storageClassName</code>的值是<code class="fe lb lc ld le b">nfs-external</code>，这是之前创建的nfs存储类。使用以下命令，将创建<code class="fe lb lc ld le b">nfs-pvc-driver</code>和<code class="fe lb lc ld le b">nfs-pvc-executor</code>PVC。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="51a0" class="mp ln iq le b gy nj nk l nl nm">kubectl apply -f ~/hive-on-spark-with-spark-operator/spark-pvc.yaml;</span></pre><p id="3e40" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看一下PVC的名称空间<code class="fe lb lc ld le b">spark</code>，因为我们的spark thrift服务器将部署在名称空间<code class="fe lb lc ld le b">spark</code>中。与此相反，上面创建的S3秘密将被运行在名称空间<code class="fe lb lc ld le b">spark-operator</code>中的Spark操作符使用。</p><h2 id="c8f2" class="mp ln iq bd lo mq mr dn ls ms mt dp lw ko mu mv ma ks mw mx me kw my mz mi na bi translated">安装spark节俭服务器的自定义资源</h2><p id="f2e1" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在，使用定制资源<code class="fe lb lc ld le b">SparkApplication</code>部署spark thrift服务器的准备工作已经就绪。让我们来看看spark节俭服务器的定制资源。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="dfc5" class="mp ln iq le b gy nj nk l nl nm">apiVersion: "spark-operator.cloudchef-labs.com/v1alpha1"<br/>kind: SparkApplication<br/>metadata:<br/>  name: spark-thrift-server-minimal<br/>  namespace: spark-operator<br/>spec:<br/>  core:<br/>    applicationType: EndlessRun<br/>    deployMode: Cluster<br/>    container:<br/>      image: "cloudcheflabs/spark:v3.0.3"<br/>      imagePullPolicy: Always<br/>    class: com.cloudcheflabs.dataroaster.hive.SparkThriftServerRunner<br/>    applicationFileUrl: "s3a://mykidong/spark-app/spark-thrift-server-3.0.3-spark-job.jar"<br/>    namespace: spark<br/>    s3:<br/>      bucket: mykidong<br/>      accessKey:<br/>        valueFrom:<br/>          secretKeyRef:<br/>            name: s3-secret<br/>            key: accessKey<br/>      secretKey:<br/>        valueFrom:<br/>          secretKeyRef:<br/>            name: s3-secret<br/>            key: secretKey<br/>      endpoint: "https://any-s3-endpoint"<br/>    hive:<br/>      metastoreUris:<br/>        - "thrift://metastore.dataroaster-hivemetastore.svc:9083"<br/>  driver:<br/>    serviceAccountName: spark<br/>    label:<br/>      application-name: spark-thrift-server-driver<br/>    resources:<br/>      cores: "1"<br/>      limitCores: "1200m"<br/>      memory: "512m"<br/>    volumeMounts:<br/>      - name: driver-local-dir<br/>        mountPath: "/tmp/local-dir"<br/>  executor:<br/>    instances: 1<br/>    label:<br/>      application-name: spark-thrift-server-executor<br/>    resources:<br/>      cores: "1"<br/>      limitCores: "1200m"<br/>      memory: "1g"<br/>    volumeMounts:<br/>      - name: executor-local-dir<br/>        mountPath: "/tmp/local-dir"<br/>  volumes:<br/>    - name: driver-local-dir<br/>      type: SparkLocalDir<br/>      persistentVolumeClaim:<br/>        claimName: nfs-pvc-driver<br/>    - name: executor-local-dir<br/>      type: SparkLocalDir<br/>      persistentVolumeClaim:<br/>        claimName: nfs-pvc-executor</span></pre><p id="90ec" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">spec.applicationFileUrl</code>是下载spark应用文件(例如spark fat jar文件)的url路径。在运行spark应用程序的自定义资源之前，需要将这样的spark应用程序文件上传到S3 bucket。</p><p id="1f3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Spark thrift服务器jar文件可以从这里下载。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="f13f" class="mp ln iq le b gy nj nk l nl nm">curl -L -O <a class="ae kc" href="https://github.com/cloudcheflabs/spark/releases/download/v3.0.3/spark-thrift-server-3.0.3-spark-job.jar" rel="noopener ugc nofollow" target="_blank">https://github.com/cloudcheflabs/spark/releases/download/v3.0.3/spark-thrift-server-3.0.3-spark-job.jar</a>;</span></pre><p id="8a33" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要将spark thrift服务器jar文件上传到S3，可以使用MinIO CLI(mc)。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="cf5f" class="mp ln iq le b gy nj nk l nl nm"># install mc.<br/>sudo yum install wget -y; <br/>sudo wget <a class="ae kc" href="https://dl.min.io/client/mc/release/linux-amd64/mc" rel="noopener ugc nofollow" target="_blank">https://dl.min.io/client/mc/release/linux-amd64/mc</a>; <br/>sudo cp mc /usr/local/bin; <br/>sudo chmod +x /usr/local/bin/mc;</span><span id="c03e" class="mp ln iq le b gy nn nk l nl nm"># add s3 alias.</span><span id="951a" class="mp ln iq le b gy nn nk l nl nm">mc alias set my-s3 \<br/><a class="ae kc" href="https://cnobgk2u8blu.compat.objectstorage.ap-seoul-1.oraclecloud.com" rel="noopener ugc nofollow" target="_blank">https://&lt;</a>s3-endpoint&gt; \<br/>&lt;access-key&gt; \<br/>&lt;secret-key&gt; \<br/>--api S3v4;</span><span id="b7d0" class="mp ln iq le b gy nn nk l nl nm"># upload spark thrift server jar to s3.<br/>mc cp <a class="ae kc" href="https://github.com/cloudcheflabs/spark/releases/download/v3.0.3/spark-thrift-server-3.0.3-spark-job.jar" rel="noopener ugc nofollow" target="_blank">spark-thrift-server-3.0.3-spark-job.jar</a> my-s3/&lt;bucket&gt;/spark-app/spark-thrift-server-3.0.3-spark-job.jar</span></pre><p id="bfbc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">spec.s3.accessKey.valueFrom.secretKeyRef.name</code>是S3以前创造的秘密名称。用s3端点更改<code class="fe lb lc ld le b">spec.s3.endpoint</code>的值。</p><p id="154f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们创建spark thrift服务器的自定义资源。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="4941" class="mp ln iq le b gy nj nk l nl nm">cd ~/hive-on-spark-with-spark-operator;</span><span id="5e0d" class="mp ln iq le b gy nn nk l nl nm">kubectl apply -f spark-thrift-server-minimal.yaml;</span></pre><p id="2255" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">spark thrift server的spark driver和executor pods将在名称空间<code class="fe lb lc ld le b">spark</code>中创建。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="7e51" class="mp ln iq le b gy nj nk l nl nm">kubectl get po -n spark<br/>NAME                                                  READY   STATUS    RESTARTS   AGE<br/>spark-thrift-server-minimal-8daaab7dad4be14a-driver   1/1     Running   0          95s<br/>spark-thrift-server-minimal-bf049d7dad4ce154-exec-1   1/1     Running   0          35s</span></pre><h2 id="e404" class="mp ln iq bd lo mq mr dn ls ms mt dp lw ko mu mv ma ks mw mx me kw my mz mi na bi translated">连接到Spark Thrift服务器以查询数据</h2><p id="c7be" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">要查询与spark thrift服务器连接的数据，需要运行spark示例作业，以便在hive表中创建拼花数据。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="aec0" class="mp ln iq le b gy nj nk l nl nm"># clone dataroaster sources.<br/>cd ~;<br/>git clone <a class="ae kc" href="https://github.com/cloudcheflabs/dataroaster.git" rel="noopener ugc nofollow" target="_blank">https://github.com/cloudcheflabs/dataroaster.git</a>;<br/>cd ~/dataroaster;</span><span id="7932" class="mp ln iq le b gy nn nk l nl nm"># build all.<br/>mvn -e -DskipTests=true clean install;</span></pre><p id="2317" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为spark示例作业需要从本地机器访问hive metastore，所以需要使用<code class="fe lb lc ld le b">port-forward</code>来访问hive metastore服务。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="30e0" class="mp ln iq le b gy nj nk l nl nm">kubectl port-forward svc/metastore  9083 -n dataroaster-hivemetastore;</span></pre><p id="de35" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本地机器上运行spark示例作业。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="1597" class="mp ln iq le b gy nj nk l nl nm">cd ~/dataroaster/components/hive/spark-thrift-server;<br/>mvn -e -Dtest=JsonToParquetTestRunner \<br/>-DmetastoreUrl=localhost:9083 \<br/>-Ds3Bucket=&lt;bucket&gt; \<br/>-Ds3AccessKey=&lt;access-key&gt; \<br/>-Ds3SecretKey=&lt;secret-key&gt; \<br/>-Ds3Endpoint=<a class="ae kc" href="https://cnobgk2u8blu.compat.objectstorage.ap-seoul-1.oraclecloud.com" rel="noopener ugc nofollow" target="_blank">https://&lt;</a>s3-endpoint&gt; \<br/>test;</span></pre><p id="1b00" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">需要配置与S3相关的属性。</p><p id="48c8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">spark实例工作成功后，我们可以用spark thrift服务器查询数据。让我们公开spark节俭服务器的服务。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="d8fc" class="mp ln iq le b gy nj nk l nl nm"># create spark thrift server service.<br/>cat &lt;&lt;EOF &gt; spark-thrift-server-service.yaml<br/>kind: Service<br/>apiVersion: v1<br/>metadata:<br/>  name: spark-thrift-server-service<br/>  namespace: spark<br/>spec:<br/>  type: ClusterIP<br/>  selector:<br/>    spark-role: driver<br/>  ports:<br/>    - name: jdbc-port<br/>      port: 10016<br/>      protocol: TCP<br/>      targetPort: 10016<br/>EOF</span><span id="2660" class="mp ln iq le b gy nn nk l nl nm">kubectl apply -f spark-thrift-server-service.yaml;</span></pre><p id="7fc8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为已经安装在spark operator容器中的spark中包含的直线将用于连接spark thrift服务器，所以让我们列出spark operator的pod状态来访问spark operator pod。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="39ac" class="mp ln iq le b gy nj nk l nl nm">kubectl get po -n spark-operator;<br/>NAME                              READY   STATUS    RESTARTS   AGE<br/>spark-operator-756fbf4479-s2zjt   1/1     Running   0          14m</span></pre><p id="bcd5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们通过访问spark operator容器来连接spark thrift服务器。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="40c7" class="mp ln iq le b gy nj nk l nl nm">kubectl exec -it spark-operator-756fbf4479-s2zjt -n spark-operator -- /opt/spark/spark-3.0.3/bin/beeline -u jdbc:hive2://spark-thrift-server-service.spark.svc:10016;</span></pre><p id="eea7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行查询以激活节俭服务器。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="93df" class="mp ln iq le b gy nj nk l nl nm">0: jdbc:hive2://spark-thrift-server-service.s&gt; show tables;<br/>+-----------+---------------+--------------+<br/>| database  |   tableName   | isTemporary  |<br/>+-----------+---------------+--------------+<br/>| default   | test_parquet  | false        |<br/>+-----------+---------------+--------------+<br/>1 row selected (1.19 seconds)<br/>0: jdbc:hive2://spark-thrift-server-service.s&gt; select * from test_parquet;<br/>+----------------------------------------------------+---------------+--------+-----------+<br/>|                   baseProperties                   |    itemId     | price  | quantity  |<br/>+----------------------------------------------------+---------------+--------+-----------+<br/>| {"eventType":"cart-event","ts":1527304486873,"uid":"any-uid0","version":"7.0"} | any-item-id0  | 1000   | 2         |<br/>| {"eventType":"cart-event","ts":1527304486873,"uid":"any-uid0","version":"7.0"} | any-item-id0  | 1000   | 2         |<br/>| {"eventType":"cart-event","ts":1527304486873,"uid":"any-uid0","version":"7.0"} | any-item-id0  | 1000   | 2         |<br/>| {"eventType":"cart-event","ts":1527304486873,"uid":"any-uid0","version":"7.0"} | any-item-id0  | 1000   | 2         |<br/>| {"eventType":"cart-event","ts":1527304486873,"uid":"any-uid0","version":"7.0"} | any-item-id0  | 1000   | 2         |<br/>| {"eventType":"cart-event","ts":1527304486873,"uid":"any-uid0","version":"7.0"} | any-item-id0  | 1000   | 2         |<br/>| {"eventType":"cart-event","ts":1527304486873,"uid":"any-uid0","version":"7.0"} | any-item-id0  | 1000   | 2         |<br/>| {"eventType":"cart-event","ts":1527304486873,"uid":"any-uid0","version":"7.0"} | any-item-id0  | 1000   | 2         |<br/>+----------------------------------------------------+---------------+--------+-----------+<br/>8 rows selected (11.519 seconds)<br/>0: jdbc:hive2://spark-thrift-server-service.s&gt; select count(*) from test_parquet;<br/>+-----------+<br/>| count(1)  |<br/>+-----------+<br/>| 8         |<br/>+-----------+<br/>1 row selected (2.541 seconds)</span></pre><p id="b1f9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好的，工作正常。</p><h2 id="a40c" class="mp ln iq bd lo mq mr dn ls ms mt dp lw ko mu mv ma ks mw mx me kw my mz mi na bi translated">关闭spark节俭服务器</h2><p id="0601" class="pw-post-body-paragraph kd ke iq kf b kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">要关闭spark thirft服务器，只需删除spark thirft服务器的自定义资源。</p><pre class="nb nc nd ne gt nf le ng nh aw ni bi"><span id="30e2" class="mp ln iq le b gy nj nk l nl nm">kubectl delete sparkapp spark-thrift-server-minimal -n spark-operator;</span></pre><p id="343c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">删除spark thrift server自定义资源后，spark operator会自动杀死spark thrift server的driver pod，从而触发删除spark thrift server的executor pods。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="317c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如这里所看到的，spark thrift server作为spark应用程序可以很容易地部署到kubernetes上。</p><p id="a6f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用spark operator不仅可以部署spark批处理应用程序，还可以轻松部署无休止运行的spark应用程序，如spark流应用程序。好好享受吧！</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="6a3b" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">参考</h1><ul class=""><li id="fd26" class="no np iq kf b kg mk kk ml ko oh ks oi kw oj la nt nu nv nw bi translated">本文源代码:<a class="ae kc" href="https://github.com/mykidong/hive-on-spark-with-spark-operator" rel="noopener ugc nofollow" target="_blank">https://github . com/mykidong/hive-on-spark-with-spark-operator</a></li><li id="9a04" class="no np iq kf b kg nx kk ny ko nz ks oa kw ob la nt nu nv nw bi translated">DataRoaster Spark操作员:<a class="ae kc" href="https://github.com/cloudcheflabs/dataroaster/tree/master/operators/spark" rel="noopener ugc nofollow" target="_blank">https://github . com/cloudcheflabs/data roaster/tree/master/operators/Spark</a></li></ul></div></div>    
</body>
</html>