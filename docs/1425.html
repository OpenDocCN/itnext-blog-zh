<html>
<head>
<title>Machine Learning: Sentiment analysis of movie reviews using Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:基于逻辑回归的电影评论情感分析</h1>
<blockquote>原文：<a href="https://itnext.io/machine-learning-sentiment-analysis-of-movie-reviews-using-logisticregression-62e9622b4532?source=collection_archive---------1-----------------------#2018-10-11">https://itnext.io/machine-learning-sentiment-analysis-of-movie-reviews-using-logisticregression-62e9622b4532?source=collection_archive---------1-----------------------#2018-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="0aa3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我们将重点分析IMDb电影评论数据，并试图预测评论是积极的还是消极的。熟悉一些机器学习概念将有助于理解所使用的代码和算法。我们将使用流行的<strong class="js iu"> scikit-learn </strong>机器学习框架。</p><p id="bb30" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">数据集准备</strong>:</p><p id="0378" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用来自这里的数据集—【http://ai.stanford.edu/~amaas/data/sentiment/】<em class="kp"/></p><p id="ac3e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下载数据集后，删除了不必要的文件/文件夹，因此文件夹结构如下所示—</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi kq"><img src="../Images/fe8c6b9b94bcfad247ec5c893076e80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*NiepgZN_gdoc5D1pbr2HLQ.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">数据集的文件夹结构</figcaption></figure><p id="795a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">将数据载入程序</strong>:</p><p id="4b55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将加载并查看训练和测试数据，以了解数据的性质。在这种情况下，训练和测试数据的格式相似。</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="b7d9" class="lh li it ld b gy lj lk l ll lm">from sklearn.datasets import load_files</span><span id="100c" class="lh li it ld b gy ln lk l ll lm">reviews_train = load_files("aclImdb/train/")<br/>text_train, y_train = reviews_train.data, reviews_train.target<br/><br/>print("Number of documents in train data: {}".format(len(text_train)))<br/>print("Samples per class (train): {}".format(np.bincount(y_train)))<br/><br/>reviews_test = load_files("aclImdb/test/")<br/>text_test, y_test = reviews_test.data, reviews_test.target<br/><br/>print("Number of documents in test data: {}".format(len(text_test)))<br/>print("Samples per class (test): {}".format(np.bincount(y_test)))</span></pre><p id="befb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> scikit </strong> - <strong class="js iu"> learn </strong>提供<strong class="js iu"> load_files </strong>来读取这类文本数据。加载数据后，我们打印了文件数量(训练/测试)和每类样本数量(pos/neg ),如下所示</p><blockquote class="lo lp lq"><p id="6d48" class="jq jr kp js b jt ju jv jw jx jy jz ka lr kc kd ke ls kg kh ki lt kk kl km kn im bi translated">训练数据中的文档数:25000 <br/>样本每班(训练):【12500 12500】<br/>测试数据中的文档数:25000 <br/>样本每班(测试):【12500 12500】</p></blockquote><p id="7164" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到总共25000个样本的训练和测试数据，每类12500个。</p><p id="7ccb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">将文本数据表示为单词包</strong>:</p><p id="3bb0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们希望将单词出现次数统计为一个单词包，其中包括图表中的以下步骤——</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lu"><img src="../Images/152b9d70f29f348bac8f442c783e6d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmGtC3uYZboHh8GIG4N5tA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">文字处理包[1]</figcaption></figure><p id="27b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了将输入数据集表示为单词包，我们将使用<strong class="js iu">计数矢量器</strong>并调用它的<strong class="js iu">转换</strong>方法。<strong class="js iu">计数矢量器</strong>是一个转换器，将输入文档转换成稀疏的特征矩阵。</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="ddeb" class="lh li it ld b gy lj lk l ll lm">from sklearn.feature_extraction.text import CountVectorizer</span><span id="5d5b" class="lh li it ld b gy ln lk l ll lm">vect = CountVectorizer(min_df=5, ngram_range=(2, 2))<br/>X_train = vect.fit(text_train).transform(text_train)<br/>X_test = vect.transform(text_test)<br/><br/>print("Vocabulary size: {}".format(len(vect.vocabulary_)))<br/>print("X_train:\n{}".format(repr(X_train)))<br/>print("X_test: \n{}".format(repr(X_test)))<br/><br/>feature_names = vect.get_feature_names()<br/>print("Number of features: {}".format(len(feature_names)))</span></pre><p id="6c59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">计数矢量器</strong>与两个参数一起使用—</p><ol class=""><li id="a098" class="lz ma it js b jt ju jx jy kb mb kf mc kj md kn me mf mg mh bi translated"><strong class="js iu"> min_df </strong> ( = 5):定义一个词作为一个特征的最小频率</li><li id="9923" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn me mf mg mh bi translated"><strong class="js iu"> ngram_range </strong> (= (2，2)):ngram _ range参数是一个元组。它定义了所考虑的令牌序列的最小和最大长度。在这种情况下，这个长度是2。因此，这将找到2个标记的序列，如“但是”，“智者”等。</li></ol><p id="1292" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">结果矩阵中的每个条目都被视为一个特征。上述代码片段的输出如下—</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="da74" class="lh li it ld b gy lj lk l ll lm">Vocabulary size: <strong class="ld iu">129549</strong><br/>X_train:<br/>&lt;<strong class="ld iu">25000x129549</strong> sparse matrix of type '&lt;class 'numpy.int64'&gt;'<br/> with 3607330 stored elements in Compressed Sparse Row format&gt;<br/>X_test: <br/>&lt;<strong class="ld iu">25000x129549</strong> sparse matrix of type '&lt;class 'numpy.int64'&gt;'<br/> with 3392376 stored elements in Compressed Sparse Row format&gt;<br/>Number of features: <strong class="ld iu">129549</strong></span></pre><p id="edc8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总共找到了129549个特征。</p><p id="b353" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">模型开发:</strong></p><p id="9995" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用<strong class="js iu">逻辑回归</strong>进行模型开发，对于像我们这样的高维稀疏数据，<strong class="js iu">逻辑回归</strong>通常效果最佳。</p><p id="1f84" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在开发模型时，我们需要做另外两件事</p><ol class=""><li id="d60a" class="lz ma it js b jt ju jx jy kb mb kf mc kj md kn me mf mg mh bi translated"><strong class="js iu">网格搜索</strong>:用于逻辑回归的参数调整。我们想确定什么样的系数'<strong class="js iu"> C </strong>值能提供更好的精度。</li><li id="102b" class="lz ma it js b jt mi jx mj kb mk kf ml kj mm kn me mf mg mh bi translated"><strong class="js iu">交叉验证</strong>:为了避免数据过拟合。</li></ol><p id="c19b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要了解更多关于<strong class="js iu"> GridSearch </strong>和<strong class="js iu">交叉验证</strong>的信息，请参考【2】。</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="6d57" class="lh li it ld b gy lj lk l ll lm">from sklearn.model_selection import GridSearchCV<br/>from sklearn.linear_model import LogisticRegression</span><span id="3190" class="lh li it ld b gy ln lk l ll lm">param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}<br/>grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)<br/>grid.fit(X_train, y_train)<br/><br/>print("Best cross-validation score: {:.2f}".format(grid.best_score_))<br/>print("Best parameters: ", grid.best_params_)<br/>print("Best estimator: ", grid.best_estimator_)</span></pre><p id="9953" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这里，我们用<strong class="js iu"> GridSearchCV </strong>进行5重交叉验证。拟合训练数据后，我们看到“C”的最佳分数、最佳参数和最佳估计量(我们将使用的模型)。</p><p id="d38c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上述代码片段的输出如下</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="7083" class="lh li it ld b gy lj lk l ll lm">Best cross-validation score: 0.88<br/>Best parameters:  {'C': 1}<br/>Best estimator:  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,<br/>          intercept_scaling=1, max_iter=100, multi_class='warn',<br/>          n_jobs=None, penalty='l2', random_state=None,  solver='warn',<br/>          tol=0.0001, verbose=0, warm_start=False)</span></pre><p id="c8e2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们有一个C = 1的模型，准确率为88%。</p><p id="247e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们想画出最好和最差的前25个特征。</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="c393" class="lh li it ld b gy lj lk l ll lm">import matplotlib.pyplot as plt<br/>import mglearn</span><span id="4d3b" class="lh li it ld b gy ln lk l ll lm">mglearn.tools.visualize_coefficients(grid.best_estimator_.coef_, feature_names, n_top_features=25)<br/>plt.show()</span></pre><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mn"><img src="../Images/1b497cafcccf5d4b4d7c69854912acd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CmX0q0lqtx4yHvJ1Kj-KrQ.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">常见的最佳和最差特征</figcaption></figure><p id="286b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">* mglearn是本书附带的库[1]。你可以从—<a class="ae ko" href="https://github.com/amueller/mglearn" rel="noopener ugc nofollow" target="_blank">https://github.com/amueller/mglearn</a>下载</p><p id="7937" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">进行预测</strong>:</p><p id="287d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们将使用训练好的模型对我们的测试数据进行预测。</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="b6cd" class="lh li it ld b gy lj lk l ll lm">lr = grid.best_estimator_<br/>lr.fit(X_train, y_train)<br/>lr.predict(X_test)<br/>print("Score: {:.2f}".format(lr.score(X_test, y_test)))</span></pre><p id="137d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">预测的输出显示超过测试数据88%的分数。</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="2d35" class="lh li it ld b gy lj lk l ll lm">Score: 0.88</span></pre><p id="093f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了检查我们的模型在单个数据上的表现，我们将对正面的电影评论和负面的电影评论分别进行预测。</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="4340" class="lh li it ld b gy lj lk l ll lm">pos = ["I've seen this story before but my kids haven't. Boy with troubled past joins military, faces his past, falls in love and becomes a man. "<br/>       "The mentor this time is played perfectly by Kevin Costner; An ordinary man with common everyday problems who lives an extraordinary "<br/>       "conviction, to save lives. After losing his team he takes a teaching position training the next generation of heroes. The young troubled "<br/>       "recruit is played by Kutcher. While his scenes with the local love interest are a tad stiff and don't generate enough heat to melt butter, "<br/>       "he compliments Costner well. I never really understood Sela Ward as the neglected wife and felt she should of wanted Costner to quit out of "<br/>       "concern for his safety as opposed to her selfish needs. But her presence on screen is a pleasure. The two unaccredited stars of this movie "<br/>       "are the Coast Guard and the Sea. Both powerful forces which should not be taken for granted in real life or this movie. The movie has some "<br/>       "slow spots and could have used the wasted 15 minutes to strengthen the character relationships. But it still works. The rescue scenes are "<br/>       "intense and well filmed and edited to provide maximum impact. This movie earns the audience applause. And the applause of my two sons."]</span><span id="671f" class="lh li it ld b gy ln lk l ll lm">print("Pos prediction: {}". format(lr.predict(vect.transform(pos))))</span></pre><p id="084e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个输出—</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="4d34" class="lh li it ld b gy lj lk l ll lm">Pos prediction: [1]</span></pre><p id="38b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，1表示它预测了一个积极的评价。</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="0d8d" class="lh li it ld b gy lj lk l ll lm">neg = ["David Bryce\'s comments nearby are exceptionally well written and informative as almost say everything "<br/>       "I feel about DARLING LILI. This massive musical is so peculiar and over blown, over produced and must have "<br/>       "caused ruptures at Paramount in 1970. It cost 22 million dollars! That is simply irresponsible. DARLING LILI "<br/>       "must have been greenlit from a board meeting that said \"hey we got that Pink Panther guy and that Sound Of Music gal... "<br/>       "lets get this too\" and handed over a blank cheque. The result is a hybrid of GIGI, ZEPPELIN, HALF A SIXPENCE, some MGM 40s "<br/>       "song and dance numbers of a style (daisies and boaters!) so hopelessly old fashioned as to be like musical porridge, and MATA HARI "<br/>       "dramatics. The production is colossal, lush, breathtaking to view, but the rest: the ridiculous romance, Julie looking befuddled, Hudson "<br/>       "already dead, the mistimed comedy, and the astoundingly boring songs deaden this spectacular film into being irritating. LILI is"<br/>       " like a twee 1940s mega musical with some vulgar bits to spice it up. STAR! released the year before sadly crashed and now is being "<br/>       "finally appreciated for the excellent film is genuinely is... and Andrews looks sublime, mature, especially in the last half hour......"<br/>       "but LILI is POPPINS and DOLLY frilly and I believe really killed off the mega musical binge of the 60s..... "<br/>       "and made Andrews look like Poppins again... which I believe was not Edwards intention. Paramount must have collectively fainted "<br/>       "when they saw this: and with another $20 million festering in CATCH 22, and $12 million in ON A CLEAR DAY and $25 million in PAINT YOUR WAGON...."<br/>       "they had a financial abyss of CLEOPATRA proportions with $77 million tied into 4 films with very uncertain futures. Maybe they should have asked seer "<br/>       "Daisy Gamble from ON A CLEAR DAY ......LILI was very popular on immediate first release in Australia and ran in 70mm cinemas for months but it failed "<br/>       "once out in the subs and the sticks and only ever surfaced after that on one night stands with ON A CLEAR DAY as a Sunday night double. Thank "<br/>       "god Paramount had their simple $1million (yes, ONE MILLION DOLLAR) film LOVE STORY and that $4 million dollar gangster pic THE GODFATHER "<br/>       "also ready to recover all the $77 million in just the next two years....for just $5m.... incredible!"]</span><span id="0479" class="lh li it ld b gy ln lk l ll lm">print("Neg prediction: {}". format(lr.predict(vect.transform(neg))))</span></pre><p id="c27b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个输出—</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="222c" class="lh li it ld b gy lj lk l ll lm">Neg prediction: [0]</span></pre><p id="0e51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这里，0表示它预测了负面评价。</p><p id="7c12" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">完整源代码</strong>:</p><pre class="kr ks kt ku gt lc ld le lf aw lg bi"><span id="215c" class="lh li it ld b gy lj lk l ll lm">from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.datasets import load_files<br/>from sklearn.model_selection import GridSearchCV<br/>import numpy as np<br/>import mglearn<br/>import matplotlib.pyplot as plt<br/><br/>reviews_train = load_files("aclImdb/train/")<br/>text_train, y_train = reviews_train.data, reviews_train.target<br/><br/>print("Number of documents in train data: {}".format(len(text_train)))<br/>print("Samples per class (train): {}".format(np.bincount(y_train)))<br/><br/>reviews_test = load_files("aclImdb/test/")<br/>text_test, y_test = reviews_test.data, reviews_test.target<br/><br/>print("Number of documents in test data: {}".format(len(text_test)))<br/>print("Samples per class (test): {}".format(np.bincount(y_test)))<br/><br/><br/>vect = CountVectorizer(min_df=5, ngram_range=(2, 2))<br/>X_train = vect.fit(text_train).transform(text_train)<br/>X_test = vect.transform(text_test)<br/><br/>print("Vocabulary size: {}".format(len(vect.vocabulary_)))<br/>print("X_train:\n{}".format(repr(X_train)))<br/>print("X_test: \n{}".format(repr(X_test)))<br/><br/>feature_names = vect.get_feature_names()<br/>print("Number of features: {}".format(len(feature_names)))<br/><br/>param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}<br/>grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)<br/>grid.fit(X_train, y_train)<br/><br/>print("Best cross-validation score: {:.2f}".format(grid.best_score_))<br/>print("Best parameters: ", grid.best_params_)<br/>print("Best estimator: ", grid.best_estimator_)<br/><br/>mglearn.tools.visualize_coefficients(grid.best_estimator_.coef_, feature_names, n_top_features=25)<br/>plt.show()<br/><br/>lr = grid.best_estimator_<br/>lr.predict(X_test)<br/>print("Score: {:.2f}".format(lr.score(X_test, y_test)))<br/><br/>pos = ["I've seen this story before but my kids haven't. Boy with troubled past joins military, faces his past, falls in love and becomes a man. "<br/>       "The mentor this time is played perfectly by Kevin Costner; An ordinary man with common everyday problems who lives an extraordinary "<br/>       "conviction, to save lives. After losing his team he takes a teaching position training the next generation of heroes. The young troubled "<br/>       "recruit is played by Kutcher. While his scenes with the local love interest are a tad stiff and don't generate enough heat to melt butter, "<br/>       "he compliments Costner well. I never really understood Sela Ward as the neglected wife and felt she should of wanted Costner to quit out of "<br/>       "concern for his safety as opposed to her selfish needs. But her presence on screen is a pleasure. The two unaccredited stars of this movie "<br/>       "are the Coast Guard and the Sea. Both powerful forces which should not be taken for granted in real life or this movie. The movie has some "<br/>       "slow spots and could have used the wasted 15 minutes to strengthen the character relationships. But it still works. The rescue scenes are "<br/>       "intense and well filmed and edited to provide maximum impact. This movie earns the audience applause. And the applause of my two sons."]<br/>print("Pos prediction: {}". format(lr.predict(vect.transform(pos))))<br/><br/>neg = ["David Bryce\'s comments nearby are exceptionally well written and informative as almost say everything "<br/>       "I feel about DARLING LILI. This massive musical is so peculiar and over blown, over produced and must have "<br/>       "caused ruptures at Paramount in 1970. It cost 22 million dollars! That is simply irresponsible. DARLING LILI "<br/>       "must have been greenlit from a board meeting that said \"hey we got that Pink Panther guy and that Sound Of Music gal... "<br/>       "lets get this too\" and handed over a blank cheque. The result is a hybrid of GIGI, ZEPPELIN, HALF A SIXPENCE, some MGM 40s "<br/>       "song and dance numbers of a style (daisies and boaters!) so hopelessly old fashioned as to be like musical porridge, and MATA HARI "<br/>       "dramatics. The production is colossal, lush, breathtaking to view, but the rest: the ridiculous romance, Julie looking befuddled, Hudson "<br/>       "already dead, the mistimed comedy, and the astoundingly boring songs deaden this spectacular film into being irritating. LILI is"<br/>       " like a twee 1940s mega musical with some vulgar bits to spice it up. STAR! released the year before sadly crashed and now is being "<br/>       "finally appreciated for the excellent film is genuinely is... and Andrews looks sublime, mature, especially in the last half hour......"<br/>       "but LILI is POPPINS and DOLLY frilly and I believe really killed off the mega musical binge of the 60s..... "<br/>       "and made Andrews look like Poppins again... which I believe was not Edwards intention. Paramount must have collectively fainted "<br/>       "when they saw this: and with another $20 million festering in CATCH 22, and $12 million in ON A CLEAR DAY and $25 million in PAINT YOUR WAGON...."<br/>       "they had a financial abyss of CLEOPATRA proportions with $77 million tied into 4 films with very uncertain futures. Maybe they should have asked seer "<br/>       "Daisy Gamble from ON A CLEAR DAY ......LILI was very popular on immediate first release in Australia and ran in 70mm cinemas for months but it failed "<br/>       "once out in the subs and the sticks and only ever surfaced after that on one night stands with ON A CLEAR DAY as a Sunday night double. Thank "<br/>       "god Paramount had their simple $1million (yes, ONE MILLION DOLLAR) film LOVE STORY and that $4 million dollar gangster pic THE GODFATHER "<br/>       "also ready to recover all the $77 million in just the next two years....for just $5m.... incredible!"]<br/>print("Neg prediction: {}". format(lr.predict(vect.transform(neg))))</span></pre><p id="f1fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">更多需要考虑的事情</strong>:</p><p id="175b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">文本分析需要考虑的还有— <strong class="js iu">词汇化</strong>、<strong class="js iu">词干化</strong>，以及<strong class="js iu"> <em class="kp">词频—逆文档频率</em>(TF–IDF)</strong>等。你可以在网上或者从[1]中学到如何使用它们。但是对于这个示例项目来说，我发现这些技术大大增加了执行时间，而没有显著提高准确性。</p><p id="7983" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我希望这篇文章对一些人有帮助，如果不是很多人的话。这是我关于机器学习主题的第一篇文章，我不是这个领域的专家，还在学习中。如果你喜欢这篇文章，请关注我这里或者上 <a class="ae ko" href="https://twitter.com/meraj_enigma" rel="noopener ugc nofollow" target="_blank"> <em class="kp">推特</em> </a> <em class="kp">。</em></p><p id="225f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参考文献:</strong></p><p id="7d7e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[1]<a class="ae ko" href="http://shop.oreilly.com/product/0636920030515.do" rel="noopener ugc nofollow" target="_blank">http://shop.oreilly.com/product/0636920030515.do</a></p><p id="bd99" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2]<a class="ae ko" href="http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb" rel="noopener ugc nofollow" target="_blank">http://nb viewer . jupyter . org/github/rhi ever/Data-Analysis-and-Machine-Learning-Projects/blob/master/Example-Data-science-notebook/Example % 20 Machine % 20 Learning % 20 note book . ipynb</a></p><p id="6ef5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3]<a class="ae ko" href="https://medium.com/@rnbrown/more-nlp-with-sklearns-countvectorizer-add577a0b8c8" rel="noopener">https://medium . com/@ rn brown/more-NLP-with-sk learns-count vectorizer-add 577 a 0 b 8 c 8</a></p></div></div>    
</body>
</html>