<html>
<head>
<title>Top Machine Learning NLP Tools for Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python的顶级机器学习NLP工具</h1>
<blockquote>原文：<a href="https://itnext.io/top-machine-learning-nlp-tools-for-python-5b2587521b73?source=collection_archive---------5-----------------------#2021-09-06">https://itnext.io/top-machine-learning-nlp-tools-for-python-5b2587521b73?source=collection_archive---------5-----------------------#2021-09-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/0776eaa5148919905798cb2fa0232083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yr3JGvQ0E1cVvZDk"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kf" href="https://unsplash.com/@heyerlein?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> h heyerlein </a>拍摄</figcaption></figure><h1 id="812e" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="5418" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">本文的目标是给出一个快速的<strong class="lg iu">概述</strong>，并举例说明使用<a class="ae kf" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Python </strong> </a>执行自然语言处理(<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> NLP </strong> </a> <strong class="lg iu"> ) </strong>任务的工具。在本文中，我将只关注<strong class="lg iu">文本</strong>数据，而不是音频或视频处理。</p><p id="1486" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这篇文章是我以前的<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/introduction-to-natural-language-processing-nlp-tools-for-python-cf39af3cfc64">文章</a>的摘录，在那里我给出了什么是<strong class="lg iu"> NLP </strong>的概述，如果你不熟悉NLP，我建议你先看看这篇<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/introduction-to-natural-language-processing-nlp-tools-for-python-cf39af3cfc64">文章</a>。我假设你之前有<a class="ae kf" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Python </strong> </a>和<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">机器学习</strong> </a>的知识。</p><h1 id="ba1e" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">什么是NLP？</h1><p id="1218" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">简单来说，NLP是<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">机器学习</strong> </a>专注于从<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>自然语言中提取洞察的一个领域。你的目标是让计算机理解我们自己的语言。</p><p id="f1de" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">自然语言处理的一些实用的<strong class="lg iu">例子</strong>是语音识别、翻译、情感分析、主题建模、词汇分析、实体提取等等。</p><p id="7a3f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">使用所有这些工具和算法，你可以从自然语言中提取结构化数据，这些数据可以被计算机处理。此外，<strong class="lg iu"> NLP </strong>任务的输出通常是机器学习算法，该算法将使用这些原始数据来进行<strong class="lg iu">预测</strong>。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/2505ea7015b04dddf27a39ed337f55a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cQAIGyxGk2qZXAbd"/></div></div></figure><p id="aea6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">通过<strong class="lg iu">将许多算法结合在一起</strong>，您可以提取有用的数据，这些数据可用于广泛的场景，例如:</p><ul class=""><li id="c543" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">欺诈检测</li><li id="95e9" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://aylien.com/solutions/risk-intelligence" rel="noopener ugc nofollow" target="_blank">风险情报</a></li><li id="cda1" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">电子邮件分类</li><li id="ccaf" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://pythonspot.com/python-sentiment-analysis/" rel="noopener ugc nofollow" target="_blank">情绪分析</a></li></ul><h1 id="d27b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">NLP工具</h1><p id="e75d" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在我之前的<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/introduction-to-natural-language-processing-nlp-tools-for-python-cf39af3cfc64">文章</a>中，我谈到了管理原始数据的重要性。你大概听说过<strong class="lg iu">花在<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>机器学习的80% </strong>时间是<strong class="lg iu">数据准备</strong>:数据清洗、<a class="ae kf" href="https://en.wikipedia.org/wiki/Data_wrangling" rel="noopener ugc nofollow" target="_blank">数据角力</a>、特征工程等。对于NLP来说尤其如此，因为我们的主要目标是将文本转换成计算机可以使用的数字T21。</p><p id="55c5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">简而言之，我们可以获取原始文本数据，加载它，清理它，标记它，最后执行特征工程，将文本转换为数字，以便它可以用于实体提取或分类。现在，我们将回顾Python中可用的一些工具，<strong class="lg iu">我们将从更通用/低级的工具开始，然后转向更专业和更高级别的库和工具。</strong></p><h2 id="e19d" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">sci kit-学习</h2><p id="8f7a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>是一个著名的<strong class="lg iu">Python通用ML库</strong>。广泛用于各种各样的机器学习任务，如<a class="ae kf" href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning" rel="noopener ugc nofollow" target="_blank">分类</a>、<a class="ae kf" href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning" rel="noopener ugc nofollow" target="_blank">回归</a>或<a class="ae kf" href="https://scikit-learn.org/stable/modules/clustering.html#clustering" rel="noopener ugc nofollow" target="_blank">聚类</a>。还可以通过连接多个模型来构建管道。</p><p id="544d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为了<strong class="lg iu">安装</strong>和<a class="ae kf" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>运行:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="527a" class="na kh it nn b gy nr ns l nt nu">pip install -U scikit-learn</span></pre><p id="12c1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>附带了许多<a class="ae kf" href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> NLP特性</strong> </a>包括分词器、词袋、词频、分类器等等。例如，要处理文本、标记化、删除停用词并使用词袋构建特征向量，我们可以使用<code class="fe nv nw nx nn b"><a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">CountVectorizer</strong></a></code>来一气呵成地完成所有这些工作:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="cebc" class="na kh it nn b gy nr ns l nt nu"><strong class="nn iu">from</strong> <strong class="nn iu">sklearn.feature_extraction.text</strong> <strong class="nn iu">import</strong> CountVectorizer<br/>count_vect = CountVectorizer()<br/>X_train_counts = count_vect.fit_transform(email.data)</span></pre><p id="29e0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">结果是一个带有文本数据数字表示的向量，可以用作分类器的输入。<code class="fe nv nw nx nn b">email.data</code>包含邮件正文。</p><p id="98bf" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它还带有<code class="fe nv nw nx nn b"><a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">TfidfTransformer</strong></a></code> <strong class="lg iu"> </strong>，使用<strong class="lg iu"> tf-idf </strong>算法实现相同的方法:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="cc03" class="na kh it nn b gy nr ns l nt nu"><strong class="nn iu">from</strong> <strong class="nn iu">sklearn.feature_extraction.text</strong> <strong class="nn iu">import</strong> TfidfTransformer<br/>X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)</span></pre><p id="9a92" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们可以很容易地训练一个模型来进行预测。首先，我们训练一个模型，例如<strong class="lg iu">多项式</strong></p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="38c2" class="na kh it nn b gy nr ns l nt nu"><strong class="nn iu">from</strong> <strong class="nn iu">sklearn.naive_bayes</strong> <strong class="nn iu">import</strong> MultinomialNB<br/>clf = MultinomialNB().fit(X_train_tfidf, email_span.target)</span></pre><p id="4b5c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">目标列包含电子邮件是否是Span。上面的代码训练我们的模型。现在我们可以预测:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="ff54" class="na kh it nn b gy nr ns l nt nu">emails_test = ['Buy this', 'OpenGL on the GPU is fast']<br/>X_new_counts = count_vect.transform(emails_test)<br/>X_new_tfidf = tfidf_transformer.transform(X_new_counts)<br/>predicted = clf.predict(X_new_tfidf)</span></pre><p id="719d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Scikit-learn是一个非常著名且易于使用的库，是你工具箱中的必备工具。我推荐这个库，因为你已经用它来完成其他的ML任务，并且你想增加NLP功能。然而，还有其他更专业的自然语言处理库。</p><h2 id="c6ef" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">NLTK</h2><p id="faeb" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> NLTK </strong> </a>是著名的NLP库。它不仅仅是一个库，它是一个平台，不仅提供库，还提供简单易用的接口来连接超过50个语料库和词汇资源。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/febc5264b9ac495b5b4c487a8b3182a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GuQ7HtIknzktwzUc"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">NLTK功能</figcaption></figure><p id="2d5c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它拥有用于分类、标记化、词干化、标记、解析和语义推理的文本处理库，用于工业级自然语言处理库的包装器，以及一个活跃的论坛。</p><p id="f571" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">要安装NLTK运行:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="e295" class="na kh it nn b gy nr ns l nt nu">pip install --user -U nltk</span></pre><p id="fdb9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">建议您也下载一些语料库来运行:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="eccf" class="na kh it nn b gy nr ns l nt nu"><strong class="nn iu">import</strong> <strong class="nn iu">nltk</strong><br/>nltk.download()</span></pre><p id="f96b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">应该会打开一个新窗口，显示NLTK下载程序。您可以选择并下载许多易于导入的数据集。</p><p id="2277" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">得益于大量的<strong class="lg iu">语料库数据</strong>，NLTK在<strong class="lg iu">学术界</strong>中非常<strong class="lg iu">受欢迎</strong>，因为你可以很容易地迭代并使用容易访问的数据集处理你的模型。它还提供了许多工具和算法，例如，要显示一个词汇树，你可以简单地运行:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="45f0" class="na kh it nn b gy nr ns l nt nu"><strong class="nn iu">from</strong> <strong class="nn iu">nltk.corpus</strong> <strong class="nn iu">import</strong> treebank<br/>t = treebank.parsed_sents('wsj_0001.mrg')[0]<br/>t.draw()</span></pre><p id="c911" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它还支持词干/词汇化和<a class="ae kf" href="https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/" rel="noopener ugc nofollow" target="_blank">词性标注</a>。查看<a class="ae kf" href="https://likegeeks.com/nlp-tutorial-using-python-nltk/" rel="noopener ugc nofollow" target="_blank">这篇</a> <a class="ae kf" href="https://likegeeks.com/nlp-tutorial-using-python-nltk/" rel="noopener ugc nofollow" target="_blank">文章</a>以了解有关NLTK功能的更多信息。我向那些想学习和尝试自然语言处理的人推荐这个库，但是它不是我生产就绪自然语言处理的首选。</p><h2 id="87ec" class="na kh it bd ki nb nc dn km nd ne dp kq lp nf ng ku lt nh ni ky lx nj nk lc nl bi translated">空间</h2><p id="b515" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">T15】SpacyT17】是我最喜欢的<strong class="lg iu">NLP</strong>T20】库。它是一个开源的库，致力于使执行任何NLP任务变得非常容易。不像</a><a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_Language_Toolkit" rel="noopener ugc nofollow" target="_blank"> NLTK </a>用在学术上；<strong class="lg iu"> Spacy是为现实世界的使用</strong>而构建的，这得益于其出色的<strong class="lg iu">性能</strong>，大量的<strong class="lg iu">优化</strong>和<strong class="lg iu">易于使用的</strong>API。</p><p id="5e79" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它支持许多自然的<a class="ae kf" href="https://spacy.io/usage/models#languages" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">语言</strong> </a>开箱即用，它提供许多<a class="ae kf" href="https://spacy.io/usage/models#download" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">训练好的管道</strong> </a>随时可以用作你的起点。它提供了一个很棒的<a class="ae kf" href="https://spacy.io/usage/linguistic-features#tokenization" rel="noopener ugc nofollow" target="_blank">标记器</a>和许多<a class="ae kf" href="https://spacy.io/usage/linguistic-features" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">语言特性</strong> </a>，比如<a class="ae kf" href="https://spacy.io/usage/linguistic-features#pos-tagging" rel="noopener ugc nofollow" target="_blank">词性标注</a>、词汇化、<a class="ae kf" href="https://spacy.io/usage/linguistic-features#entity-linking" rel="noopener ugc nofollow" target="_blank">实体链接</a>、依存解析等等！</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/b4cf5b7c742f60c9208ceecef88d4e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3bMvAWhBq3ED5Vnt.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">空间特征:<a class="ae kf" href="https://spacy.io/usage/facts-figures" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage/facts-figures</a></figcaption></figure><p id="0674" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> SpaCy </strong>支持<a class="ae kf" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">深度学习</strong> </a> <strong class="lg iu"> </strong>工作流，你可以在其中训练和部署神经网络模型，或者将由流行的<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>库训练的外部模型，如<a class="ae kf" href="https://en.wikipedia.org/wiki/TensorFlow" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>或<a class="ae kf" href="https://en.wikipedia.org/wiki/PyTorch" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>插入到你的管道中。</p><p id="4d5e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我真正喜欢SpaCy的是它的入门如此简单！</p><p id="6af2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">安装空间</strong></p><p id="b45d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">有关基于您的平台的详细<a class="ae kf" href="https://spacy.io/usage" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">安装说明</strong> </a>，请参考文档。使用画中画时，只需:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="7ab7" class="na kh it nn b gy nr ns l nt nu">pip install -U pip setuptools wheel<br/>pip install -U spacy<br/>python -m spacy download en_core_web_sm</span></pre><p id="2953" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">由于<strong class="lg iu">预训练管道</strong>是使用大型文本语料库训练的，所以很容易开始。首先，你需要导入Spacy，然后你可以加载一个预先训练好的模型，有<a class="ae kf" href="https://spacy.io/models/en" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">不同尺寸</strong> </a> <strong class="lg iu"> </strong>可供选择；然后你可以创建你的<strong class="lg iu"> <em class="ny"> nlp </em> </strong>对象包含你的原始数据:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="3566" class="na kh it nn b gy nr ns l nt nu">import spacynlp = spacy.load("en_core_web_sm")<br/>doc = nlp("Apple is looking at buying U.K. startup for $1 billion")</span></pre><p id="935c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> SpaCy </strong>为您处理所有的数据准备和标记化，这是在您调用<strong class="lg iu"> <em class="ny"> nlp </em> </strong>对象时完成和计算的，该对象接受一个文本字符串，并返回一个包含标记化文本、POS注释、命名实体等等的已处理的<code class="fe nv nw nx nn b">Doc</code>。即使对一个<code class="fe nv nw nx nn b">Doc</code>进行了处理(拆分成单个单词并进行注释),它仍然保存着<strong class="lg iu">原始文本的所有信息。</strong></p><p id="301f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们可以很容易地遍历令牌:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="b0c4" class="na kh it nn b gy nr ns l nt nu">for token in doc:<br/>    print(token.text)</span></pre><p id="27fb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这将打印令牌列表。</p><p id="e7f3" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">位置标记</strong></p><p id="7bca" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> POS tagging </strong> </a>也是为你计算的:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="a4e9" class="na kh it nn b gy nr ns l nt nu">for token in doc:<br/>    print(token.text, token.pos_, token.dep_)</span></pre><p id="b47c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这将打印:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="89a4" class="na kh it nn b gy nr ns l nt nu">Apple PROPN nsubj<br/>is AUX aux<br/>looking VERB ROOT<br/>at ADP prep<br/>buying VERB pcomp<br/>U.K. PROPN dobj<br/>startup NOUN advcl<br/>for ADP prep<br/>$ SYM quantmod<br/>1 NUM compound<br/>billion NUM pobj</span></pre><p id="45d6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">词向量相似度</strong></p><p id="4000" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">相似度是通过比较“<a class="ae kf" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>”词语嵌入量来确定的。可以使用像<a class="ae kf" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>这样的算法生成单词向量。大多数SpaCy管道包都带有内置的单词向量，使它们可以作为<code class="fe nv nw nx nn b"><a class="ae kf" href="https://spacy.io/api/token#vector" rel="noopener ugc nofollow" target="_blank">Token.vector</a></code>属性使用。例如给出下面的句子:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="3bd3" class="na kh it nn b gy nr ns l nt nu">tokens = nlp("dog cat banana afskfsd")for token in tokens:<br/>    print(token.text, token.has_vector, token.vector_norm)</span></pre><p id="2a2e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们得到这样的输出:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="1c62" class="na kh it nn b gy nr ns l nt nu">dog True 7.0336733 False<br/>cat True 6.6808186 False<br/>banana True 6.700014 False<br/>afskfsd False 0.0 True</span></pre><p id="a2cd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">预测相似性对于建立推荐系统或者标记重复是有用的。</p><p id="2b8e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">命名实体识别(NER) </strong></p><p id="dd88" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">使用SpaCy提取实体非常简单:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="346a" class="na kh it nn b gy nr ns l nt nu">nlp = spacy.load("en_core_web_sm")<br/>doc = nlp("Apple is looking at buying U.K. startup for $1 billion")for ent in doc.ents:<br/>    print(ent.text, ent.start_char, ent.end_char, ent.label_)</span></pre><p id="27da" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这将输出:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="a0cf" class="na kh it nn b gy nr ns l nt nu">Apple 0 5 ORG<br/>U.K. 27 31 GPE<br/>$1 billion 44 54 MONEY</span></pre><p id="97bb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如您所见，SpaCy提供了一个非常简单的API和<strong class="lg iu">强大的预训练管道</strong>，允许开发人员轻松构建生产就绪的NLP解决方案。</p><p id="d996" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">但是我如何训练模型来检测其他实体呢？我在这篇文章<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/nlp-named-entity-recognition-ner-with-spacy-and-python-dabaf843cab2"><strong class="lg iu"/></a><strong class="lg iu"/>中解释了这个过程，但是基本上你需要做的就是<strong class="lg iu">使用一些将在下一节介绍的工具给数据</strong>加标签，<strong class="lg iu">训练模型</strong>，最后像使用预训练模型一样使用。这是代码:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="008a" class="na kh it nn b gy nr ns l nt nu">import spacy<br/>import random<br/>import jsonnlp = spacy.blank("en")<br/>ner = nlp.create_pipe("ner")<br/>nlp.add_pipe(ner)<br/>ner.add_label("OIL")# Start the training<br/>nlp.begin_training()# Loop for 40 iterations<br/>for itn in range(40):<br/>    # Shuffle the training data<br/>    random.shuffle(TRAINING_DATA)<br/>    losses = {}    # Batch the examples and iterate over them<br/>    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):<br/>        texts = [text for text, entities in batch]<br/>        annotations = [entities for text, entities in batch]        # Update the model<br/>        nlp.update(texts, annotations, losses=losses, drop=0.3)<br/>    print(losses)</span></pre><p id="0ed4" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这个例子中，我们使用了<em class="ny"> TRAINING_DATA </em>对象作为输入，该对象包含带有训练数据的句子，其中标注了与<em class="ny"> OIL </em>相关的单词。然后，我们使用深度学习算法(神经网络)通过几次迭代来训练模型。现在我们可以将模型保存到磁盘:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="c3d7" class="na kh it nn b gy nr ns l nt nu">nlp.to_disk("oil.model")</span></pre><p id="b185" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">并像其他模型一样加载它。我们可以像使用预训练模型一样使用它:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="50bf" class="na kh it nn b gy nr ns l nt nu">for ent in doc.ents:<br/>    print(ent.text, ent.start_char, ent.end_char, ent.label_)</span></pre><p id="d23d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">或者使用<a class="ae kf" href="https://explosion.ai/demos/displacy" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">显示</strong> </a>库获得更好的可视化效果:</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/25a34efd749007ee34bef4fc636a8704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9iq7fGhd53HRY_Yt.png"/></div></div></figure><p id="f7cf" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">模式匹配</strong></p><p id="2979" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> Spacy </strong>还提供了<strong class="lg iu">匹配器</strong>，可以很容易地用来查找特定的子字符串、数字等。我们还可以根据词性标签设置规则。</p><p id="dfb7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是代码:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="d3d4" class="na kh it nn b gy nr ns l nt nu">import spacy# Import the Matcher<br/>from spacy.matcher import Matchernlp = spacy.load("en_core_web_sm")<br/>doc = nlp(example)# Initialize the Matcher with the shared vocabulary<br/>matcher = Matcher(nlp.vocab)# Add the pattern to the matcher<br/>matcher.add("OIL_PATTERN", None, [{"LOWER": "oil"}], [{"LOWER": "petroleum"}])# Use the matcher on the doc<br/>matches = matcher(doc)<br/>print("Matches:", [doc[start:end].text for match_id, start, end in matches])</span></pre><p id="81a2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">你应该会看到印在笔记本上的火柴。</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="0412" class="na kh it nn b gy nr ns l nt nu">Matches: ['petroleum', 'oil']</span></pre><h1 id="479b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">根西姆</h1><p id="c708" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">根思</strong> </a> <strong class="lg iu"> m </strong>是一个强大的专注于<a class="ae kf" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">主题</strong> <strong class="lg iu">造型</strong> </a>的库。它提供了许多算法，如<a class="ae kf" href="https://towardsdatascience.com/nlp-with-lda-latent-dirichlet-allocation-and-text-clustering-to-improve-classification-97688c23d98" rel="noopener" target="_blank"> LDA </a>记忆，它是一个<strong class="lg iu">无监督</strong>算法。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/67e1dbe53edcf38e18a73eb675e715b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*TBpMyvBrbpZF3_fX.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">https://radimrehurek.com/gensim/的例子</figcaption></figure><p id="1d7a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><code class="fe nv nw nx nn b">gensim</code>的核心概念是:</p><ul class=""><li id="5212" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-document" rel="noopener ugc nofollow" target="_blank">文件</a>:输入文本。</li><li id="5785" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-corpus" rel="noopener ugc nofollow" target="_blank">文集</a>:文件的集合。</li><li id="43fa" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-vector" rel="noopener ugc nofollow" target="_blank"> Vector </a>:文档的数学上的方便表示。</li><li id="47e7" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-model" rel="noopener ugc nofollow" target="_blank">模型</a>:将向量从一种表示转换成另一种表示的算法。</li></ul><p id="966b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是我们已经看过的核心概念。为了使用<strong class="lg iu"> LDA </strong>进行主题建模，首先我们需要从数据中创建一个<strong class="lg iu">字典</strong>，然后将其转换为<a class="ae kf" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词袋</a>模型:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="6149" class="na kh it nn b gy nr ns l nt nu">from gensim import corporadictionary = corpora.Dictionary(input_text)<br/>corpus = [dictionary.doc2bow(text) for text in input_text]</span></pre><p id="fbbd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">然后我们可以使用LDA:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="51d7" class="na kh it nn b gy nr ns l nt nu">import gensim<br/>NUM_TOPICS = 2 # how many topic we want to extractldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=12)topics = ldamodel.print_topics(num_words=5)<br/>for topic in topics:<br/>    print(topic)</span></pre><p id="ed2a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">正如我们前面提到的，我们需要事先选择要提取多少主题，然后我们可以使用前面创建的语料库和字典来创建模型。代码将打印两个主题，每个主题有5个示例单词。</p><p id="6f4d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们可以使用<a class="ae kf" href="https://pypi.python.org/pypi/pyLDAvis/2.1.1" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> pyLDAvis </strong> </a>这个神奇的库来可视化结果:</p><pre class="mi mj mk ml gt nm nn no np aw nq bi"><span id="ede0" class="na kh it nn b gy nr ns l nt nu">import pyLDAvis.gensimlda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)pyLDAvis.display(lda_display)</span></pre><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/92a1066a8840f2a736800219713cbb6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eRTPMWAYlh4yn2h_.png"/></div></div></figure><p id="7da1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">检查这个<a class="ae kf" href="https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb#topic=0&amp;lambda=1&amp;term=" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">笔记本</strong> </a>获得一个关于<a class="ae kf" href="https://pypi.python.org/pypi/pyLDAvis/2.1.1" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> pyLDAvis </strong> </a>功能的<a class="ae kf" href="https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb#topic=0&amp;lambda=1&amp;term=" rel="noopener ugc nofollow" target="_blank">演示</a>。</p><h1 id="2069" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><p id="27fd" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们可以这样总结NLP:<strong class="lg iu">它结合了一套工具和技术来将复杂的自然语言转换成机器可读的数据。</strong></p><p id="5d79" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如你所见，像任何机器学习任务一样，<strong class="lg iu">大部分工作都是准备和标记数据</strong>，这一部分不仅耗时，而且<strong class="lg iu">至关重要</strong>。对于NLP来说，<strong class="lg iu">正确地预处理数据</strong>是取得好结果最重要的因素。</p><p id="7447" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们已经回顾了一些工具，它们可以帮助你非常容易地做到这一点，所以你可以<strong class="lg iu">更多地关注你业务领域</strong>而不是NLP的通用数据准备任务。我最喜欢的工具<a class="ae kf" href="https://en.wikipedia.org/wiki/SpaCy" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Spacy </strong> </a>，可以让你快速建立模型，而不会陷入文本处理的复杂细节中。</p><p id="b596" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="ny">记得来</em> <strong class="lg iu"> <em class="ny">拍拍</em> </strong> <em class="ny">如果你喜欢这篇文章和</em> <a class="ae kf" href="https://javier-ramos.medium.com/subscribe" rel="noopener"> <em class="ny"> </em> <strong class="lg iu"> <em class="ny">关注</em></strong><em class="ny"/><strong class="lg iu"><em class="ny">me</em></strong></a><em class="ny">或</em> <a class="ae kf" href="https://javier-ramos.medium.com/membership" rel="noopener"> <strong class="lg iu"> <em class="ny">订阅</em> </strong> </a> <em class="ny">获取更多更新！</em></p></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><p id="d96d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://javier-ramos.medium.com/subscribe" rel="noopener"> <strong class="lg iu">订阅</strong> </a>获得<strong class="lg iu">通知</strong>当我发表一篇文章和<a class="ae kf" href="https://javier-ramos.medium.com/membership" rel="noopener"> <strong class="lg iu">加入Medium.com</strong></a>访问百万或文章！</p></div></div>    
</body>
</html>