<html>
<head>
<title>EdgeFS cluster with Rook in Google Cloud</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Google Cloud中带有Rook的EdgeFS集群</h1>
<blockquote>原文：<a href="https://itnext.io/edgefs-cluster-with-rook-in-google-cloud-885227625b9b?source=collection_archive---------1-----------------------#2018-12-21">https://itnext.io/edgefs-cluster-with-rook-in-google-cloud-885227625b9b?source=collection_archive---------1-----------------------#2018-12-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ff53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你碰巧在GCP云中运行Kubernetes，那么你可能会有兴趣通过Rook EdgeFS Operator尝试一下这个新项目。它不仅支持地理透明数据访问的新用例(想想多云工作负载！)，而且它还可以显著节省云成本！</p><p id="0708" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着托管Kubernetes解决方案的公共云或私有云产品的发展，将复杂的存储基础架构视为托管服务的需求日益增长。</p><p id="1658" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了实现这一目标，现在有了新的选项，即Kubernetes的Rook Orchestration和EdgeFS作为地理透明的数据I/O平台。<br/> EdgeFS具有独特的功能集，如全局重复数据删除和git式容错，以及其他功能，为NFS和iSCSI持久性卷提供现成的Kubernetes CSI集成，运行在完全不可变的存储系统之上。众所周知，EdgeFS是一个考虑到“git”架构的存储解决方案，其中所有修改都是全局不可变的、版本化的、自识别的和全局可访问的。使用这种架构的好例子是跨多个数据中心或云对相同数据集的一致和容错访问。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/202ce52a9e5e1813e608580a5e0d4b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CBvF5A9GW1ClfOcXv8Byew.png"/></div></figure><p id="2902" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了企业级的可靠性和可用性特性，EdgeFS还展示了在云中运行时令人印象深刻的性能结果。由于其内置的重复数据消除和压缩支持，它还是一个非常经济高效的解决方案。</p><p id="a962" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了将EdgeFS作为托管的Kubernetes服务来运营，我们最近增加了对Rook Operator的支持。Rook v0.9版本现在提供了初始集成。使用Rook Operator EdgeFS，可以通过自定义资源定义(CRD)结构将数据节点作为StatefulSet服务进行管理。Rook提供了跨云提供商和在Kubernetes运行的任何环境中使用相同存储解决方案的灵活性。您将不再局限于GCP或其他特定于供应商的存储解决方案。</p><p id="bff3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章是为那些已经熟悉Kubernetes、听说过GCP但从未尝试过EdgeFS的人设计的。所以，让我在这里过一遍基本步骤，我相信你将能够填补空白(如果有的话),或者在最后提出意见/问题。</p><h1 id="f7e3" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">测试配置</h1><p id="ccbd" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">我想配置4节点EdgeFS群集，准备使用CSI接口，通过透明的S3 API访问为横向扩展NFS导出提供服务。目标是建立如下所示的托管存储基础架构:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/df42881e7fec8bee28078dc1ed996ddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*PH-sN1qujkNug-N7C56jyw.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">EdgeFS Rook CRDs呈现为一个</figcaption></figure><p id="f0d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中，车CRD由YAML文件集cluster.yaml、nfs.yaml和csi.yaml表示。从技术上讲，csi定义不是车分布的一部分，但由于紧密集成和假设，我认为CSI是EdgeFS车运算符的一部分。</p><p id="3748" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">NFS GW 1..n是用于访问EdgeFS导出桶的一组NFS网关。</p><p id="27ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">S3 GW..n是通过Kubernetes负载平衡器服务类型连接的一组S3 API兼容网关。</p><p id="edb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">gRPC管理器是一个代理，它平衡和委派来自CSI插件的请求。它还充当工具箱的角色，即管理员可以在其中执行EdgeFS CLI的UNIX shell。</p><p id="8fa1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目标节点1..n是一组数据节点，为一些磁盘服务。目标可以服务于各种不同的配置，可以是全HDD、元数据卸载到SSD的混合HDD/SSD以及全SSD。</p><h1 id="aaa8" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">将GCP·库伯内特公司设置为存储服务</h1><p id="db8e" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">首先，一旦你登录到你的谷歌帐户，并在GCP控制台创建了一个项目(在我的例子中，它被称为“我的第一个项目”)，导航到Kubernetes引擎部分，并点击“创建集群”。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mb"><img src="../Images/1f63a8b035c1b021f45d2fe06fa9fd64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2HHRzQFG5o7sFDMOrhUcmg.png"/></div></div></figure><p id="c99a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">集群规模很重要，这里的假设是本文中的资源分配将用作最佳配置的示例。在根据I/O性能和可用性要求设计初始安装时，需要应用某些规则。要考虑的最佳要求的最小子集:</p><ul class=""><li id="fb81" class="mg mh iq jp b jq jr ju jv jy mi kc mj kg mk kk ml mm mn mo bi translated">解除GW吊舱接入点和目标吊舱的隔离。虽然在同一个节点上混合GW和目标功能是可以的，但不建议这样做，因为这会对性能/可用性产生负面影响。例如，现在滚动升级会突然影响NFS功能的可用性。或者，激进的NFS客户端可以中断目标CPU内核，从而在共享内核的情况下导致尾部延迟显著增加。</li><li id="e31d" class="mg mh iq jp b jq mp ju mq jy mr kc ms kg mt kk ml mm mn mo bi translated">目标pod CPU核心数公式需要考虑介质类型。固态硬盘/ NVMe运行速度更高，单核可能不够。每个SSD至少分配2个CPU内核是一个很好的经验法则。虽然每个SSD个CPU内核可以很好地工作，但它会增加多线程应用程序在更高I/O时的尾部延迟。</li><li id="ccc4" class="mg mh iq jp b jq mp ju mq jy mr kc ms kg mt kk ml mm mn mo bi translated">目标pod内存公式需要考虑工作负载。如果工作负载是小型I/O随机负载，主要服务于多线程应用程序，请考虑为每个设备分配2 GB。除此之外，Target的其他功能(如协调服务和后台操作)还需要2GB的最低内存。</li><li id="c470" class="mg mh iq jp b jq mp ju mq jy mr kc ms kg mt kk ml mm mn mo bi translated">GW pod CPU和内存公式需要考虑工作负载。如果工作负载是小I/O随机的，有许多多线程应用程序，增加更多的CPU和内存会有所帮助。不要忘记，NFS和S3的EdgeFS服务可以通过随时添加更多的pod进行水平扩展。</li></ul><p id="4110" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">记住这一点，让我们继续创建区域类型集群(其中所有的Kubernetes节点将在相同的地理位置运行)。此外，因为我计划在不同的节点上运行1个NFS GW pod，默认池大小必须为4台服务器，并且预期EdgeFS将为主机设置故障域策略，因此3个副本将安全地存储在3台不同的服务器上。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mu"><img src="../Images/0272a321a9827fe51afa99e778de3670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t6bQWuFFof88gW04DfqHyg.png"/></div></div></figure><p id="5da4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于网关和计算节点，我们需要选择稍有不同的配置文件，从而为服务和应用提供更多CPU和内存:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mv"><img src="../Images/60ab97714f34527d424e6b479410f2cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y91uRGIT5hFQlRhin72IEw.png"/></div></div></figure><p id="9b33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们应该标记我们计划用作专用EdgeFS网关的节点，这可以通过GCloud“元数据”编辑界面来完成:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mw"><img src="../Images/f3d39b228498743ced66d1d082b6715c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4uQrAa9mFVheAz3sPLHUw.png"/></div></div></figure><p id="728f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过将“rook-edgefs-nodetype”设置为“gateway ”,我们可以为存储服务自动计划程序策略启用关联。我们的NFS和S3服务单元将自动转向网关节点，Kubernetes将负责在集群中配置的所有专用网关之间实现复杂的最佳资源平衡。如果没有提供标签，那么服务将在目标节点上运行。这是一个“可以”的默认行为，但不是最佳行为。</p><p id="e767" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，创建集群并等待它准备就绪:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/531a1746e00aee069b2bbc5580947a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*AKiYQc8oBLh9x7sKSaaBDw.png"/></div></figure><p id="1487" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦集群开始运行，点击集群名称，复制gcloud example命令并将其粘贴到您的工作站控制台中，其中已经预装了kubectl，可以使用了。请遵循gcloud提供的关于如何设置gcloud命令的说明。</p><p id="8440" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">EdgeFS支持两种主要的机制来部署目标pod——在预先格式化、预先挂载的目录之上或在原始磁盘之上。在本文中，我想让我们演示一下原始磁盘的配置。混合使用硬盘和固态硬盘可以实现最佳性价比配置，其中硬盘用于容量，固态硬盘用于日志/元数据。要实现这一点，请按照gcloud文档中的步骤，为集群中的每个目标节点创建2个HDD和1个SSD:</p><p id="c7b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae my" href="https://cloud.google.com/compute/docs/disks/add-persistent-disk#create_disk" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/compute/docs/disks/add-persistent-disk # create _ disk</a></p><p id="7292" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以通过“虚拟机实例详细信息”来确认结果，例如，在我的例子中，我创建的所有4个目标节点看起来都是这样的:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mz"><img src="../Images/2f46d84757424264b42f7332bbe822bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tfiqjEvpCvgqpRJftIsjuw.png"/></div></div></figure><p id="61c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Rook Operator的帮助下，EdgeFS将自动检测设备并创建最佳布局。</p><h1 id="e777" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">设置EdgeFS Rook运算符</h1><p id="967c" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">现在我们到了您需要登录到工作站控制台并为我们想要尝试的配置准备Rook CRDs的部分。</p><p id="3081" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在继续之前，使用“kubectl get pod”命令，确保可以看到kubernetes集群。请注意，Google Cloud用户需要明确授予用户创建角色的权限。确保执行以下命令以获得必要的访问控制:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="311b" class="nf ku iq nb b gy ng nh l ni nj">kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user <strong class="nb ir">$(</strong>gcloud config get-value account<strong class="nb ir">)</strong></span></pre><p id="e388" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在克隆车库并导航到edgefs示例:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="1607" class="nf ku iq nb b gy ng nh l ni nj">git clone <a class="ae my" href="https://github.com/rook/rook.git" rel="noopener ugc nofollow" target="_blank">https://github.com/rook/rook.git</a><br/>cd rook/cluster/examples/kubernetes/edgefs</span></pre><p id="46fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过运行下面的命令来安装EdgeFS操作符应该足够了。然而，在撰写本文时，需要将操作符图像更改为edgefs/edgefs-operator:latest:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="9187" class="nf ku iq nb b gy ng nh l ni nj">kubectl create -f operator.yaml</span></pre><p id="bee4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用以下命令监控结果:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="7092" class="nf ku iq nb b gy ng nh l ni nj">kubectl get po -n rook-edgefs-system<br/>NAME                                READY   STATUS    RESTARTS   AGE<br/>rook-discover-5h6fj                 1/1     Running   0          26s<br/>rook-discover-dcrlr                 1/1     Running   0          26s<br/>rook-discover-sh4r6                 1/1     Running   0          26s<br/>rook-discover-shjxl                 1/1     Running   0          26s<br/>rook-discover-v7zvg                 1/1     Running   0          26s<br/>rook-edgefs-operator-6d54bb-v47mv   1/1     Running   0          28s</span></pre><p id="a7c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，打开cluster.yaml文件，找到“kind:Cluster”CRD，进行修改，使其看起来类似于以下内容:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="4410" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，我希望将全部元数据卸载到SSD，并且因为在测试中使用了大部分16K或更大的有效负载，所以优化了有效负载页面大小。</p><p id="ced9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的其他重要参数是自我描述的“useAllNodes”和“useAllDevices”。我希望能够自动发现和调配我所有包含未使用的硬盘和固态硬盘的节点。和“/var/lib/edgefs”目录来保存目标pod的本地状态信息。</p><p id="51a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，保存它并执行CRD创建:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="c55f" class="nf ku iq nb b gy ng nh l ni nj">kubectl create -f cluster.yaml</span></pre><p id="84da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您希望看到的是mgr和target-* pod显示为正在运行:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="7ab8" class="nf ku iq nb b gy ng nh l ni nj">kubectl get po -n rook-edgefs<br/>NAME                             READY   STATUS    RESTARTS   AGE<br/>rook-edgefs-mgr-6f9dd99b-j9pf9   1/1     Running   0          47s<br/>rook-edgefs-target-0             3/3     Running   0          47s<br/>rook-edgefs-target-1             3/3     Running   0          47s<br/>rook-edgefs-target-2             3/3     Running   0          47s<br/>rook-edgefs-target-3             3/3     Running   0          47s<br/>rook-edgefs-target-4             3/3     Running   0          47s</span></pre><p id="6e6d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中，mgr pod是gRPC代理，target-* pod是我们的数据和网关节点。请注意，目标数据单元和网关单元之间没有明显的区别。Gateway pod是完全相同的构造，运行相同的软件，但不服务于任何磁盘。</p><p id="b221" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是时候初始化集群了，这可以通过工具箱工具来完成:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="7b2a" class="nf ku iq nb b gy ng nh l ni nj">kubectl exec -it -n rook-edgefs rook-edgefs-target-0 -c daemon -- env COLUMNS=$COLUMNS LINES=$LINES TERM=linux toolbox</span><span id="5567" class="nf ku iq nb b gy nm nh l ni nj">Welcome to EdgeFS Toolbox.<br/>Hint: type efscli to begin</span><span id="2f06" class="nf ku iq nb b gy nm nh l ni nj">#</span></pre><p id="3db5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">FlexHash是一个重要的EdgeFS构造。它代表以“行”组织的磁盘的动态发现集群布局。任何低级别的读或写I/O都将使用这种布局来协商网络上的有效负载传递。</p><p id="88ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">初始化FlexHash映射(它是自动发现的，现在可以使用了):</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="6782" class="nf ku iq nb b gy ng nh l ni nj"># efscli system init</span><span id="903e" class="nf ku iq nb b gy nm nh l ni nj">System Initialization<br/>=====================</span><span id="b342" class="nf ku iq nb b gy nm nh l ni nj">Please review auto discovered FlexHash Table:</span><span id="8fc9" class="nf ku iq nb b gy nm nh l ni nj">from_checkpoint 0<br/>zonecount 0<br/>numrows 8<br/>pid 1<br/>genid 1544406738001966<br/>failure_domain 1<br/>leader 0<br/>servercount 4<br/>vdevcount 8</span><span id="1987" class="nf ku iq nb b gy nm nh l ni nj">Please confirm initial configuration? [y/n]: y<br/>Sent message to daemon: FH_CPSET.1544406738001966<br/>Successfully set FlexHash table to GenID=1544406738001966<br/>System GUID: 0B936E29335A480BB8DA0D7E9A395CCB</span></pre><p id="17a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们创建逻辑站点命名空间“cltest”和租户“test”:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="b283" class="nf ku iq nb b gy ng nh l ni nj">efscli cluster create cltest<br/>efscli tenant create cltest/test</span></pre><h1 id="ac1f" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">EdgeFS数据输入/输出飞机的飞行前验证</h1><p id="dd7f" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">EdgeFS附带了FIO集成I/O引擎，可以在此时快速执行这些引擎来验证整体I/O性能特征:文件、块和对象。</p><p id="9a16" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">I/O生成器在专用网关节点上执行，以模拟接近生产环境。专用网关节点可以与“池-1”中定义的一样多，并且配置文件必须更加计算密集型。除了在I/O生成器内存上下文中，读取数据不会在本地缓存，即重新启动FIO将清除缓存的数据，并强制通过网络从目标数据节点重新读取这些数据。</p><p id="abc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于文件和数据块，FIO文件如下所示:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="c0a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此FIO文件将生成16个文件，数据集比目标数据节点上的可用内存总量(4 x 8GB)大3.5倍。随机80/20工作负载，具有对齐的32K数据块、直接C接口(无NFS或iSCSI开销)、复制计数1、模拟压缩(50%)和重复数据消除系数(87%)。</p><p id="658e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">登录我们的专用网关(通常为target-0)并执行:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="e8b9" class="nf ku iq nb b gy ng nh l ni nj">fio edgefs-file.fio<br/>...</span><span id="7ee3" class="nf ku iq nb b gy nm nh l ni nj">clat percentiles (usec):<br/>70.00th=[ 1080], 80.00th=[ 1368], 90.00th=[ 1848], 95.00th=[ 2320]<br/>read: IOPS=10.7k, BW=333MiB/s (349MB/s)(128GiB/394029msec)</span><span id="9bf4" class="nf ku iq nb b gy nm nh l ni nj">clat percentiles (usec):<br/>70.00th=[ 1768], 80.00th=[ 2040], 90.00th=[ 2512], 95.00th=[ 3024]<br/>write: IOPS=2661, BW=83.2MiB/s (87.3MB/s)(32.3GiB/394029msec)<br/>...</span></pre><p id="f89d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们来分析一下结果。首先，延迟在1到1.2毫秒之间，95%的尾部在2到3毫秒之间。这可能取决于硬盘如何配置的内部因素，但对我来说，这看起来像是一些有效的缓存正在发生，这是可以的。其次，谷歌提供的硬盘广告上写着350个读取IOPS，700个写入。也就是说，如果直接用于80/20工作负载，我们的8块硬盘只能提供大约3080 IOPS。为了直观地比较这一点:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nn"><img src="../Images/f135b0eed1bea2ab33014369a6e70128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yTDQcCqw68UugYYGzVdrvQ.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">16个线程，16个文件，每个文件10GB，32KB，80/20，与谷歌提供的硬盘最大容量相比，IOPS</figcaption></figure><p id="7b1d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了解释这一点，EdgeFS的优势主要来自数据I/O减少(动态重复数据删除、压缩)和智能使用元数据卸载技术到内存/SSD。不要忘记提及使用轻量级UDP/IP而不是TCP/IP的网络结构，传输都是无连接的，并且在其自己的数据放置/检索协议逻辑内操作。</p><p id="bcbd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于对象，FIO文件如下所示:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="943d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该FIO文件将生成65536个大小为1MB的对象(32 * 2G /1MB ),使用复制计数为1的直接C库接口(无S3开销)平均分布在4个存储桶bk1-bk4上。该数据集比目标数据节点上的可用内存总量(4 x 8GB)大2倍左右。我们首先以rwmixread=0 (100%写入)运行它，然后以rwmixread=100 (100%读取)重新运行它。</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="fe9b" class="nf ku iq nb b gy ng nh l ni nj">fio edgefs-object.fio<br/>...<br/>write: IOPS=1128, BW=2257MiB/s (2367MB/s)(64.0GiB/29035msec)</span><span id="00b5" class="nf ku iq nb b gy nm nh l ni nj">read: IOPS=1676, BW=3352MiB/s (3515MB/s)(64.0GiB/19550msec)</span></pre><p id="20b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们试着分析和理解这个结果。在此示例中，每个调配的HDD设备都限制在60MB/s，也就是说，我们的8个设备不可能达到480MB/s以上。我们调配群集SSD的方式是，我们不将它们用于数据缓存，仅用于元数据卸载和写入日志，但读取速度惊人，接近<strong class="jp ir"> 3.5GB/s、</strong>，写入速度惊人，接近<strong class="jp ir"> 2.5GB/s </strong>！为了形象化这一点:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi no"><img src="../Images/65a77251987bdb725f884561f2fd8312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nelZz40boFglJsEXY3PrcA.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">吞吐量32个线程，2MB对象跨4个存储桶，而Google提供的硬盘，MB/s</figcaption></figure><p id="019b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了解释这一点，写I/O受到HDD的限制，EdgeFS需要做更多的工作来处理元数据和数据放置。借助read I/O EdgeFS，在通过网关的网络接口发送/接收数据块之前，利用重复数据消除和压缩，这一点做得非常好。</p><h1 id="8640" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">配置服务</h1><p id="4c97" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">至此，配置已验证，特性已预测试，我们可以继续创建我们需要的服务:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="55f5" class="nf ku iq nb b gy ng nh l ni nj"># bucket, aligned on 32K chunk size<br/>efscli bucket create cltest/test/files -s 32768 -r 1 -t 1</span><span id="334b" class="nf ku iq nb b gy nm nh l ni nj"># bucket, aligned on 4MB chunk size<br/>efscli bucket create cltest/test/objects -s 4194304 -r 1 -t 1</span><span id="cf09" class="nf ku iq nb b gy nm nh l ni nj"># NFS service, serving Tenant<br/>efscli service create nfs files<br/>efscli service serve files cltest/test/files</span><span id="78e1" class="nf ku iq nb b gy nm nh l ni nj"># S3 service, serving buckets we want to be transparently accessed<br/>efscli service create s3 objects<br/>efscli service serve objects cltest/test</span></pre><p id="cb08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面创建了两个名为“文件”和“对象”的服务。这些服务定义保存在集群本身中，由gRPC管理器用来与CSI框架通信。</p><p id="35a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在是时候为这两种服务创建车CRD定义了:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="309e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，我将gateway的可用内存分成两个12GB的部分，用于每个服务，并使CRD名称(“文件”、“对象”)与相应的EdgeFS服务相匹配。</p><p id="e106" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还要注意，两个服务CRD都定义了放置条件“rook-edgefs-nodetype”=“gateway”，从而告诉Kubernetes调度程序尝试找到所需的资源。</p><p id="cbe1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们创建CRD:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="d319" class="nf ku iq nb b gy ng nh l ni nj">kubectl create -f edgefs-s3-gateway.yaml<br/>kubectl create -f edgefs-nfs-gateway.yaml</span><span id="9c29" class="nf ku iq nb b gy nm nh l ni nj"># kubectl get po --all-namespaces|grep s3-objects<br/>rook-edgefs   rook-edgefs-s3-objects-23ffe   1/1   Running   0   33s<br/># kubectl get po --all-namespaces|grep nfs-files<br/>rook-edgefs   rook-edgefs-nfs-files-345fx    1/1   Running   0   7s</span></pre><p id="1eb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">服务现在正在运行，我们可以检查ClusteIP是否可用:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="3932" class="nf ku iq nb b gy ng nh l ni nj"># kubectl get svc -n rook-edgefs | grep s3-objects<br/>rook-edgefs-s3-objects   NodePort    10.11.241.165 9982:31138/TCP</span></pre><p id="5a43" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此时，CSI插件应该能够连接并使用动态或静态配置的持久卷的存储服务。</p><p id="7a6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">跟随Rook <a class="ae my" href="https://rook.io/docs/rook/v0.9/edgefs-csi.html" rel="noopener ugc nofollow" target="_blank"> EdgeFS CSI文档</a>了解如何设置它的细节。</p><h1 id="5e2b" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">集群拆卸</h1><p id="39d8" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">使用原始磁盘的EdgeFS(未创建文件系统！)作为其存储介质。连接到群集目标节点的HDD和SSD已分区并在使用中。如果我们想拆除集群，我们可能还想删除所有分区并清除磁盘。有两种方法可以做到这一点:1)只需简单地删除集群，并为每个节点上的每个设备手动执行“wipefs -a /dev/DEV”命令；2)使用内置的EdgeFS工具重新使用当前配置和zap设备。在后一种情况下，如果您希望重新配置某些配置参数，但不希望重新发现磁盘或进行手动清理，这将非常有用。</p><p id="34c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了实现这一点，通过车集群CRD有3个选项可用:</p><ul class=""><li id="112e" class="mg mh iq jp b jq jr ju jv jy mi kc mj kg mk kk ml mm mn mo bi translated">devices resurrectmode:" restoreZapWait ":zap所有正在使用的磁盘，并等待直到" kubectl delete -f cluster.yaml "</li><li id="f6ee" class="mg mh iq jp b jq mp ju mq jy mr kc ms kg mt kk ml mm mn mo bi translated">devices resurrectmode:" restore zap ":在目标守护进程启动之前清除所有正在使用的磁盘。当您希望从头开始但保持相同的配置时，这对于集群重启非常有用</li><li id="de53" class="mg mh iq jp b jq mp ju mq jy mr kc ms kg mt kk ml mm mn mo bi translated">设备恢复模式:" restore ":不要更换磁盘，而是尝试恢复集群CRD删除之前使用的配置。如果上次运行时/var/lib/edgefs目录中还有配置，那么将从该目录中提取配置。</li></ul><p id="73c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如何在切换设备时重启集群的示例:</p><pre class="km kn ko kp gt na nb nc nd aw ne bi"><span id="8893" class="nf ku iq nb b gy ng nh l ni nj">kubectl delete -f cluster.yaml<br/># edit cluster.yaml and add devicesResurrectMode: “restoreZap”<br/>kubectl create -f cluster.yaml</span></pre><h1 id="52c2" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论</h1><p id="7a8d" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">通过在GCP运行EdgeFS Rook Operator，分布式EdgeFS存储的部署和管理现在大大简化了。</p><p id="d7a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，部署EdgeFS作为Kubernetes运营商管理的服务不仅非常容易，它还证明了<strong class="jp ir"> <em class="np">它可以在GCP提供最大限度的硬盘驱动器、固态硬盘和网络资源</em> </strong>的基础上显著提升性能。当然，这意味着成比例的云成本节约！</p><p id="fcfc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这只是GCP的一种配置。我期待听到您使用EdgeFS Operator配置Rook以优化您的云配置的性能和成本的经验！</p><p id="e208" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请记住，任何支持Kubernetes的环境都可以使用Rook和EdgeFS作为后备存储。这种方法是可移植的，使其成为任何云原生环境的良好选择，无论是在公共云中、内部还是在边缘物联网角落。</p><p id="75e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我还没有谈到的EdgeFS特性之一是它的设计，通过提供地理透明的数据访问来支持多云存储工作负载。请继续关注这方面的更多文章！</p><h2 id="9d59" class="nf ku iq bd kv nq nr dn kz ns nt dp ld jy nu nv lh kc nw nx ll kg ny nz lp oa bi translated">前进</h2><p id="552b" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">将Rook EdgeFS Operator作为存储架构的焦点，您将能够灵活地提供块、对象存储或横向扩展网络文件系统来满足各种需求，避免供应商锁定，并享受现代分布式Kubernetes本地存储。</p><p id="53ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">即将到来的EdgeFS与Crossplane和其他Kubernetes联邦和工作负载管理解决方案的集成将允许云原生应用程序真正跨云和开放。</p><p id="2b7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我很期待看到EdgeFS to Rook！</p><p id="72aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我代表EdgeFS和Rook开发者邀请您加入我们充满活力的Rook <a class="ae my" href="https://rook-slackin.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">社区</a>和EdgeFS <a class="ae my" href="https://edgefs.slack.com/" rel="noopener ugc nofollow" target="_blank">社区</a>，亲自体验一下<a class="ae my" href="http://edgefs.io" rel="noopener ugc nofollow" target="_blank"> EdgeFS </a>。欢迎投稿和反馈！</p></div></div>    
</body>
</html>