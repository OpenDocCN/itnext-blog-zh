<html>
<head>
<title>Exploring Popular Open-source Stream Processing Technologies: Part 1 of 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索流行的开源流处理技术:第1部分，共2部分</h1>
<blockquote>原文：<a href="https://itnext.io/exploring-popular-open-source-stream-processing-technologies-part-1-of-2-31069337ba0e?source=collection_archive---------0-----------------------#2022-09-24">https://itnext.io/exploring-popular-open-source-stream-processing-technologies-part-1-of-2-31069337ba0e?source=collection_archive---------0-----------------------#2022-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8700" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Apache Spark结构化流、Apache Kafka流、Apache Flink和Apache Pinot与Apache超集的简短演示</h2></div><p id="8d9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据<a class="ae le" href="https://www.techtarget.com/searchdatamanagement/definition/stream-processing" rel="noopener ugc nofollow" target="_blank">TechTarget</a>,<em class="lf">流处理是一种数据管理技术，涉及</em> <strong class="kk iu"> <em class="lf">摄取连续的数据流，以实时快速分析、过滤、转换或增强数据</em> </strong> <em class="lf">。处理后，数据被传递给应用程序、数据存储或另一个流处理引擎。</em> " <a class="ae le" href="https://www.confluent.io/learn/batch-vs-real-time-data-processing/" rel="noopener ugc nofollow" target="_blank"> Confluent </a>，一家完全管理的Apache Kafka市场领导者，将流处理定义为"<em class="lf">一种软件范例，它在连续的数据流还在运动的时候</em> <strong class="kk iu"> <em class="lf">接收、处理和管理它们</em> </strong> <em class="lf">。</em></p><h1 id="f72b" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">批处理与流处理</h1><p id="4a2e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">同样，根据<a class="ae le" href="https://www.confluent.io/learn/batch-vs-real-time-data-processing/" rel="noopener ugc nofollow" target="_blank">汇流</a>，<em class="lf">批处理是指对一组已经存储了一段时间的数据进行处理和分析。</em>“批处理示例可能包括每日零售数据，这些数据在每夜商店关门后汇总并制成表格。相反，“<em class="lf">流数据处理发生在数据流经系统时。这导致事件发生时的分析和报告。</em>“举一个类似的例子，销售数据流不是在夜间进行批处理，而是全天连续处理、汇总和分析——实时跟踪销量、购买趋势、库存水平和营销计划绩效。</p><h1 id="bf98" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">有界与无界数据</h1><p id="67da" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">根据Packt出版社的书，<a class="ae le" href="https://subscription.packtpub.com/book/big-data-and-business-intelligence/9781788296403/1/ch01lvl1sec8/unbounded-data-and-continuous-processing" rel="noopener ugc nofollow" target="_blank">学习Apache Apex </a>，<em class="lf">有界数据是有限的；它有始有终。无界数据是一个不断增长的、本质上无限的数据集。批处理通常在有界数据上执行，而流处理通常在无界数据上执行。</em></p><h1 id="3205" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">流处理技术</h1><p id="b1ee" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">有许多技术可用于执行流处理。其中包括专有定制软件、商业现货(COTS)软件、由软件即服务(或SaaS)提供商、云解决方案提供商(CSP)、商业开源软件(COSS)公司提供的完全托管服务，以及来自<a class="ae le" href="https://www.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache软件基金会</a>和<a class="ae le" href="https://www.linuxfoundation.org/" rel="noopener ugc nofollow" target="_blank"> Linux基金会</a>的流行开源项目。</p><p id="0d50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇由两部分组成的博文和在<a class="ae le" href="https://youtu.be/m2IjTPjKbUk" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上的视频演示探索了四个流行的开源软件(OSS)流处理项目:<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Apache Spark结构化流</a>、<a class="ae le" href="https://kafka.apache.org/documentation/streams/" rel="noopener ugc nofollow" target="_blank"> Apache Kafka流</a>、<a class="ae le" href="https://flink.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Flink </a>和<a class="ae le" href="https://pinot.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Pinot </a>与<a class="ae le" href="https://superset.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache超集</a>。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/db2425d2402db4cd031b9cc7b140e3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DlZvpWpto6EwNNzG_v35Fw.png"/></div></div></figure><p id="57f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章使用了开源项目，使得跟随演示变得更加容易，并且将成本保持在最低。然而，您可以很容易地用开源项目替代您喜欢的SaaS、CSP或COSS服务产品。</p><h2 id="6a1f" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">Apache Spark结构化流</h2><p id="1a1f" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">根据Apache Spark <a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">文档</a>，<em class="lf">结构化流(Structured Streaming)是建立在</em><a class="ae le" href="https://spark.apache.org/sql/" rel="noopener ugc nofollow" target="_blank"><em class="lf">Spark SQL</em></a><em class="lf">引擎之上的一个可扩展的容错流处理引擎。您可以像表达静态数据上的批处理计算一样表达您的流计算。</em><em class="lf">此外，使用微批处理引擎来处理结构化流查询，该引擎将数据流作为一系列小批量作业来处理，从而实现低至100毫秒的端到端延迟以及一次容错保证。</em>“在这篇文章中，我们将使用一系列用<a class="ae le" href="https://pypi.org/project/pyspark/" rel="noopener ugc nofollow" target="_blank"> PySpark </a>编写的Apache Spark结构化流作业来检查批处理和流处理。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nb"><img src="../Images/559d2dd427aa2cc95e3effe35de88073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*74SsUKPuIS80S88AUSRhNA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">从Spark UI看到的Spark结构化流作业统计</figcaption></figure><h2 id="dfb8" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">阿帕奇卡夫卡溪流</h2><p id="edcd" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">根据Apache Kafka <a class="ae le" href="https://kafka.apache.org/documentation/streams/" rel="noopener ugc nofollow" target="_blank">文档</a>,<em class="lf">Kafka Streams[aka KStreams]是一个用于构建应用程序和微服务的客户端库，其中的输入和输出数据存储在Kafka集群中。它结合了在客户端编写和部署标准Java和Scala应用程序的简单性和Kafka服务器端集群技术的优势。</em>“在本文中，我们将研究一个用Java编写的KStreams应用程序，它执行流处理和增量聚合。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nb"><img src="../Images/bd6ffa744043bcdbe72469f4fc104f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wjeA2jLpaLgaF3zKv8xcfw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">在JetBrains IntelliJ IDEA中构建KStreams应用程序的uber JAR</figcaption></figure><h2 id="667d" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">阿帕奇弗林克</h2><p id="159a" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">根据Apache Flink <a class="ae le" href="https://flink.apache.org/flink-architecture.html" rel="noopener ugc nofollow" target="_blank">文档</a>，<em class="lf"> Apache Flink是一个框架和分布式处理引擎，用于无界和有界数据流上的有状态计算。Flink设计用于在所有常见的集群环境中运行，以内存速度和任意规模执行计算。</em>“进一步来说，<em class="lf"> Apache Flink擅长处理无界和有界数据集。对时间和状态的精确控制使Flink的运行时能够在无界流上运行任何类型的应用程序。有界流由专门为固定大小的数据集设计的算法和数据结构在内部处理，从而产生出色的性能。“在本文中，我们将研究一个用Java编写的Flink应用程序，它执行流处理、增量聚合和多流连接。</em></p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nb"><img src="../Images/8c4dbef06aca1c3d4c7e435a131f5a73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZG6qk9jWyd6S0nV9sgsaA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">阿帕奇Flink仪表盘显示了本文演示的Flink管道</figcaption></figure><h2 id="994b" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">阿帕奇皮诺</h2><p id="7c0a" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">根据Apache Pinot的文档，“<em class="lf"> Pinot是一个实时分布式OLAP数据存储，旨在提供超低延迟分析，即使在极高的吞吐量下也是如此。它可以直接从流数据源(如Apache Kafka和Amazon Kinesis)获取数据，并使事件可以立即进行查询。它还可以从诸如Hadoop HDFS、亚马逊S3、Azure ADLS和谷歌云存储等批量数据源中获取数据。</em>“在帖子中，我们将使用<a class="ae le" href="https://docs.pinot.apache.org/users/user-guide-query/querying-pinot" rel="noopener ugc nofollow" target="_blank"> SQL </a>从<a class="ae le" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank">Apache Flink生成的</a>Apache Kafka中查询无界数据流。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nb"><img src="../Images/51b868b924090a55ca0327a4a59394e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lE6HadOl15jIC-BVzWG29w.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">Apache Pinot查询控制台显示了本文中演示的表格</figcaption></figure><h1 id="d408" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">流式数据源</h1><p id="8754" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">我们必须首先找到一个良好的无界数据源来探索或演示这些流技术。理想情况下，流数据源应该足够复杂，以允许多种类型的分析，并使用商业智能(BI)和仪表板工具可视化不同的方面。此外，流式数据源应该具有一定程度的一致性和可预测性，同时显示合理水平的可变性和随机性。</p><p id="ab85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们将使用开源的<a class="ae le" href="https://github.com/garystafford/streaming-sales-generator" rel="noopener ugc nofollow" target="_blank">流式合成销售数据生成器</a>项目，该项目由我开发，并在GitHub上提供。该项目的高度可配置的、基于Python的合成数据生成器为一系列Apache Kafka主题生成一个无界的产品列表、销售交易和库存补货活动流。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ng"><img src="../Images/cb5172d432c4262c33bed2aa320c0312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dfUNqIGhO-Y6hxawX2iJsA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">流式合成销售数据生成器向Apache Kafka发布消息</figcaption></figure><h1 id="18ac" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">源代码</h1><p id="b75e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">这篇文章中展示的所有源代码都是开源的，可以在<a class="ae le" href="https://github.com/garystafford/" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。有三个独立的GitHub项目:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="0e3d" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">码头工人</h1><p id="2c20" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">为了便于跟随演示，我们将使用Docker Swarm来提供流工具。或者，您可以使用Kubernetes(例如，创建一个掌舵图)或您首选的CSP或SaaS管理的服务。本演示不要求您使用付费服务。</p><p id="74e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两个Docker Swarm堆栈位于<a class="ae le" href="https://github.com/garystafford/streaming-sales-generator" rel="noopener ugc nofollow" target="_blank">流式合成销售数据生成器</a>项目中:</p><ol class=""><li id="12b7" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated"><a class="ae le" href="https://github.com/garystafford/streaming-sales-generator/blob/main/docker/spark-kstreams-stack.yml" rel="noopener ugc nofollow" target="_blank">流堆栈—第1部分</a> : <a class="ae le" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Kafka </a>，<a class="ae le" href="https://zookeeper.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Zookeeper </a>，<a class="ae le" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>，<a class="ae le" href="https://github.com/provectus/kafka-ui" rel="noopener ugc nofollow" target="_blank">Apache Kafka的UI</a>，以及<a class="ae le" href="https://github.com/garystafford/kstreams-kafka-demo" rel="noopener ugc nofollow" target="_blank"> KStreams应用</a></li><li id="2baa" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated"><a class="ae le" href="https://github.com/garystafford/streaming-sales-generator/blob/main/docker/flink-pinot-superset-stack.yml" rel="noopener ugc nofollow" target="_blank">流栈—第二部分</a> : <a class="ae le" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇卡夫卡</a>，<a class="ae le" href="https://zookeeper.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇动物园管理员</a>，<a class="ae le" href="https://flink.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇弗林克</a>，<a class="ae le" href="https://pinot.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇皮诺</a>，<a class="ae le" href="https://superset.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇超集</a>，阿帕奇卡夫卡的<a class="ae le" href="https://github.com/provectus/kafka-ui" rel="noopener ugc nofollow" target="_blank">UI</a>，以及<a class="ae le" href="https://jupyter.org/" rel="noopener ugc nofollow" target="_blank">项目Jupyter </a> (JupyterLab)。*</li></ol><p id="e7b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">* Jupyter容器可用作运行PySpark作业的Spark容器的替代物(遵循与Spark相同的步骤，如下)</em></p><h1 id="335a" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">演示#1: Apache Spark</h1><p id="c81e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">在四个演示的第一个中，我们将研究两个用<a class="ae le" href="https://pypi.org/project/pyspark/" rel="noopener ugc nofollow" target="_blank"> PySpark </a>编写的<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Apache Spark结构化流</a>作业，演示批处理(<code class="fe nx ny nz oa b"><a class="ae le" href="https://github.com/garystafford/streaming-sales-generator/blob/main/apache_spark_examples/spark_batch_kafka.py" rel="noopener ugc nofollow" target="_blank">spark_batch_kafka.py</a></code>)和流处理(<code class="fe nx ny nz oa b"><a class="ae le" href="https://github.com/garystafford/streaming-sales-generator/blob/main/apache_spark_examples/spark_streaming_kafka.py" rel="noopener ugc nofollow" target="_blank">spark_streaming_kafka.py</a></code>)。我们将从Kafka主题<code class="fe nx ny nz oa b">demo.purchases</code>的单个数据流中读取数据，并写入控制台。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ob"><img src="../Images/e55767b8e8f6940f784804ed86122c77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jt1Ig2Fy5P4UqrROzV0ouw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">Apache Spark演示的高级工作流</figcaption></figure><h2 id="a943" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">部署流式堆栈</h2><p id="c1be" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">首先，部署包含<a class="ae le" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Kafka </a>、<a class="ae le" href="https://zookeeper.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Zookeeper </a>、<a class="ae le" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>、Apache Kafka的<a class="ae le" href="https://github.com/provectus/kafka-ui" rel="noopener ugc nofollow" target="_blank">UI</a>和<a class="ae le" href="https://github.com/garystafford/kstreams-kafka-demo" rel="noopener ugc nofollow" target="_blank"> KStreams应用程序</a>容器的第一个<a class="ae le" href="https://github.com/garystafford/streaming-sales-generator/blob/main/docker/spark-kstreams-stack.yml" rel="noopener ugc nofollow" target="_blank"> streaming Docker Swarm堆栈</a>。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="de52" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">堆栈将需要几分钟才能完全部署。完成后，堆栈中应该总共有六个容器在运行。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi oc"><img src="../Images/5751c4c1bcef1df3231e5872394f4210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lncYz91ja31hTfqsNaDhhQ.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">查看Docker流堆栈的六个容器</figcaption></figure><h2 id="7e03" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">销售生成器</h2><p id="2d74" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">启动流数据发生器之前，确认或修改<code class="fe nx ny nz oa b"><a class="ae le" href="https://github.com/garystafford/streaming-sales-generator/blob/main/configuration/configuration.ini" rel="noopener ugc nofollow" target="_blank">configuration/configuration.ini</a></code>。特别是三个配置项将决定流数据生成器运行多长时间以及它产生多少数据。出于测试目的，我们将设置相对快速地生成事务事件的时间。我们还将设置足够高的事件数量，以便有时间探索Spark作业。使用下面的设置，生成器应该平均运行大约50-60分钟:(((5秒+ 2秒)/2)*1000个事务)/60秒=平均大约58分钟。如有必要，您可以再次运行生成器或增加事务的数量。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">来自项目<code class="fe nx ny nz oa b">configuration.ini file</code>的代码片段</figcaption></figure><p id="86af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为后台服务启动流数据生成器:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="97a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">流数据生成器将开始向三个Apache Kafka主题写入数据:<code class="fe nx ny nz oa b">demo.products</code>、<code class="fe nx ny nz oa b">demo.purchases</code>和<code class="fe nx ny nz oa b">demo.inventories</code>。我们可以通过登录Apache Kafka容器并使用Kafka CLI来查看这些主题及其消息:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="51a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面，我们看到一些来自<code class="fe nx ny nz oa b">demo.purchases</code>主题的示例消息:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ng"><img src="../Images/99efd737cb4997ee9a3e0965d22ff4b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*chmvJFtfzy8jxU9bRhFnNw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">消费来自卡夫卡<code class="fe nx ny nz oa b">demo.purchases</code>主题的信息</figcaption></figure><p id="186c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者，您可以使用Apache Kafka的UI，可以在端口9080上访问。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nb"><img src="../Images/6ba76deb130ece08c4bdd837b0d1c2fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u5ayUpt0BhR6yHWkHv-TXA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">查看Apache Kafka用户界面中的主题</figcaption></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nb"><img src="../Images/713f0998d103e9c8ee33d8acfa279858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6iG07iEw9UlpajOfeyxcPg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">使用Apache Kafka的用户界面查看<code class="fe nx ny nz oa b">demo.purchases</code>主题中的消息</figcaption></figure><h2 id="5c2c" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">准备火花</h2><p id="7334" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">接下来，准备Spark容器来运行Spark作业:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">作为根用户准备Spark容器实例</figcaption></figure><h2 id="1671" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">运行Spark作业</h2><p id="84ca" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">接下来，将作业从项目复制到Spark容器，然后执行回容器:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="aa1f" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">用Spark进行批处理</h2><p id="94e4" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">第一个Spark作业<code class="fe nx ny nz oa b"><a class="ae le" href="https://github.com/garystafford/streaming-sales-generator/blob/main/apache_spark_examples/spark_batch_kafka.py" rel="noopener ugc nofollow" target="_blank">spark_batch_kafka.py</a></code>，根据从<code class="fe nx ny nz oa b">demo.purchases</code>主题消费的现有消息，聚合售出的商品数量和每种产品的总销售额。我们在第一个例子中使用了<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> PySpark DataFrame </a>类的<code class="fe nx ny nz oa b">read()</code>和<code class="fe nx ny nz oa b">write()</code>方法，从Kafka读取数据并写入控制台。我们可以很容易地把结果写回给卡夫卡。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">批处理Spark job的<code class="fe nx ny nz oa b">summarize_sales()</code>方法片段</figcaption></figure><p id="099f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">批处理作业对结果进行排序，并按总销售额将前25项输出到控制台。作业应该运行完成并成功退出。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ng"><img src="../Images/73eb9228046552323ed1b22410ce033c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dbRfdQ6K1c4PjnKLWr_LcA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">按总销售额列出的前25个项目的批结果</figcaption></figure><p id="175f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要运行批处理Spark作业，请使用以下命令:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">运行批处理Spark作业</figcaption></figure><h2 id="ba76" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">用Spark进行流处理</h2><p id="8dfe" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">流处理Spark作业<code class="fe nx ny nz oa b"><a class="ae le" href="https://github.com/garystafford/streaming-sales-generator/blob/main/apache_spark_examples/spark_streaming_kafka.py" rel="noopener ugc nofollow" target="_blank">spark_streaming_kafka.py</a></code>还根据从<code class="fe nx ny nz oa b">demo.purchases</code>主题消费的消息，汇总售出的商品数量和每件商品的总销售额。但是，如下面的代码片段所示，该作业不断聚合来自Kafka的数据流，显示任意十分钟的<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#types-of-time-windows" rel="noopener ugc nofollow" target="_blank">滑动窗口</a>内的前十个产品总数，有五分钟的重叠，并每分钟更新控制台的输出。我们使用<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> PySpark DataFrame </a>类的<code class="fe nx ny nz oa b">readStream()</code>和<code class="fe nx ny nz oa b">writeStream()</code>方法，而不是第一个例子中面向批处理的<code class="fe nx ny nz oa b">read()</code>和<code class="fe nx ny nz oa b">write()</code>方法。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">流处理Spark job的<code class="fe nx ny nz oa b">summarize_sales()</code>方法片段</figcaption></figure><p id="06d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">较短的事件时间窗口更容易进行演示，在生产中，每小时、每天、每周或每月的窗口更适合进行销售分析。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ng"><img src="../Images/8ad84ef0108206e55ecc52f54e0a65d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bQaYVpnTEEQXfztJo4ahVw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">微批处理表示当前十分钟窗口的实时总数</figcaption></figure><p id="e71b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要运行流处理Spark作业，请使用以下命令:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">运行流处理Spark作业</figcaption></figure><p id="98b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以很容易地计算滑动事件时间窗口内销售数据流与聚合的运行总数(项目中包含的<em class="lf">示例作业)。</em></p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ng"><img src="../Images/634dea01c25f765cee52392cc1a9e701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Y5HKoV7ah_Lsh08VeGHKQ.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">微批处理表示数据流的运行总数，而不是使用事件时间窗口</figcaption></figure><p id="5885" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完成后，一定要终止流处理Spark作业，否则它们会继续运行，等待更多数据。</p><h1 id="6baf" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">演示#2:阿帕奇卡夫卡流</h1><p id="ebfe" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">接下来，我们将检查<a class="ae le" href="https://kafka.apache.org/documentation/streams/" rel="noopener ugc nofollow" target="_blank">阿帕奇卡夫卡流</a> ( <em class="lf">又名KStreams </em>)。在本文的这一部分，我们还将使用三个GitHub资源库项目中的第二个项目<code class="fe nx ny nz oa b"><a class="ae le" href="https://gist.github.com/garystafford/77b5edf9d8fb150ca1311e9cdbb8cc91" rel="noopener ugc nofollow" target="_blank">kstreams-kafka-demo</a></code>。该项目包含一个用Java编写的KStreams应用程序，它执行流处理和增量聚合。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi od"><img src="../Images/49425e9f6b22e46da97ececf3d17111f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xH64p4S00gAYttXpdzuKrg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">KStreams演示的高级工作流</figcaption></figure><h2 id="945f" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">KStreams应用程序</h2><p id="ea3d" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">KStreams应用程序使用<code class="fe nx ny nz oa b"><a class="ae le" href="https://kafka.apache.org/32/javadoc/org/apache/kafka/streams/StreamsBuilder.html" rel="noopener ugc nofollow" target="_blank">StreamBuilder()</a></code>类的一个实例不断地使用来自<code class="fe nx ny nz oa b">demo.purchases</code> Kafka主题(<em class="lf">源</em>)的消息流。然后，它汇总售出商品的数量和每件商品的总销售额，维护运行总数，然后将它们流式传输到新的<code class="fe nx ny nz oa b">demo.running.totals</code>主题(<em class="lf"> sink </em>)。所有这些都使用了<code class="fe nx ny nz oa b"><a class="ae le" href="https://kafka.apache.org/32/javadoc/org/apache/kafka/streams/KafkaStreams.html" rel="noopener ugc nofollow" target="_blank">KafkaStreams()</a></code> Kafka客户端类的一个实例。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">KStreams应用程序的<code class="fe nx ny nz oa b">kStreamPipeline()</code>方法片段</figcaption></figure><h2 id="2117" class="mp lh it bd li mq mr dn lm ms mt dp lq kr mu mv ls kv mw mx lu kz my mz lw na bi translated">运行应用程序</h2><p id="4f1b" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">对于这个演示，我们至少有三种选择来运行KStreams应用程序:1)从我们的IDE本地运行，2)从命令行本地运行编译后的JAR，或者3)将编译后的JAR复制到Docker映像中，作为Swarm堆栈的一部分进行部署。你可以选择任何一个选项。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">在本地编译和运行KStreams应用程序</figcaption></figure><p id="f3cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将继续使用用于Apache Spark演示的相同的<a class="ae le" href="https://github.com/garystafford/streaming-sales-generator/blob/main/docker/spark-kstreams-stack.yml" rel="noopener ugc nofollow" target="_blank">流Docker Swarm堆栈</a>。我已经使用项目源代码中的<a class="ae le" href="https://openjdk.org/projects/jdk/17/" rel="noopener ugc nofollow" target="_blank"> OpenJDK 17 </a>和<a class="ae le" href="https://gradle.org/" rel="noopener ugc nofollow" target="_blank"> Gradle </a>编译了一个单独的<a class="ae le" href="https://plugins.gradle.org/plugin/com.github.johnrengelman.shadow" rel="noopener ugc nofollow" target="_blank"> uber JAR </a>文件。然后我创建并发布了一个Docker映像，它已经是运行堆栈的一部分。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">用于构建KStreams应用程序Docker映像的Docker文件</figcaption></figure><p id="11df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们之前为Spark演示运行了销售生成器，所以在<code class="fe nx ny nz oa b">demo.purchases</code>主题中有现有数据。重新运行销售生成器(<code class="fe nx ny nz oa b">nohup python3 ./producer.py &amp;</code>)以生成新的数据流。查看KStreams应用程序的结果，该应用程序自使用Kafka CLI或Apache Kafka的UI部署堆栈以来一直在运行:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="7e48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面，在顶部的终端窗口中，我们看到了KStreams应用程序的输出。使用KStream的<code class="fe nx ny nz oa b"><a class="ae le" href="https://kafka.apache.org/32/javadoc/org/apache/kafka/streams/kstream/KStream.html" rel="noopener ugc nofollow" target="_blank">peek()</a></code>方法，应用程序在处理<code class="fe nx ny nz oa b">Purchase</code>和<code class="fe nx ny nz oa b">Total</code>实例并将其写入Kafka时，将它们输出到控制台。在下方的终端窗口中，我们看到新消息作为连续流发布到输出主题<code class="fe nx ny nz oa b">demo.running.totals</code>。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ng"><img src="../Images/2a62be1db7abbb4d4ff7ac9c880db3aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-RVmGx6dsOPdjhNHZdMLg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">KStreams应用程序执行流处理和结果输出流</figcaption></figure><h1 id="2d9c" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">第二部分</h1><p id="f813" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">在这篇由两部分组成的文章的第二部分中，我们继续探索四个流行的开源流处理项目。我们将介绍<a class="ae le" href="https://flink.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇弗林克</a>和<a class="ae le" href="https://pinot.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇皮诺</a>。此外，我们将把<a class="ae le" href="https://superset.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache超集</a>合并到演示中，构建一个实时仪表板来可视化我们的流处理结果。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi oe"><img src="../Images/fa8c4cc1150cc5107a18b4c0446aadd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJTh-smKy7dzZ_3cqwzAPA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">Apache超集仪表板显示来自Apache Pinot实时表的数据</figcaption></figure></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="da69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇博客代表我的观点，而不是我的雇主亚马逊网络服务公司(AWS)的观点。所有产品名称、徽标和品牌都是其各自所有者的财产。除非另有说明，所有图表和插图都是作者的财产。</p></div></div>    
</body>
</html>