<html>
<head>
<title>Demystifying Kubernetes networking using tcpdump</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用tcpdump揭开Kubernetes网络的神秘面纱</h1>
<blockquote>原文：<a href="https://itnext.io/demystifying-kubernetes-networking-using-tcpdump-f760f0fb4967?source=collection_archive---------1-----------------------#2022-06-01">https://itnext.io/demystifying-kubernetes-networking-using-tcpdump-f760f0fb4967?source=collection_archive---------1-----------------------#2022-06-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6ac1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated">你有没有想过在Kubernetes环境中，数据包是如何到达目的地的？一个数据包在从它的Kubernetes节点“退出”之前要经过多少跳？像<a class="ae ku" href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/" rel="noopener ugc nofollow" target="_blank"> kube-proxy </a>这样的组件和像<a class="ae ku" href="https://istio.io/" rel="noopener ugc nofollow" target="_blank"> Istio </a>和<a class="ae ku" href="https://linkerd.io/" rel="noopener ugc nofollow" target="_blank"> Linkerd </a>这样的服务网格解决方案如何影响这个数据包的流量？</p><p id="3aef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">观察包的最好方法之一当然是tcpdump。在本文中，我们将试图揭开Kubernetes和Istio网络行为的神秘面纱，并通过使用来自tcpdump的TCP流来了解这些工具在实践中是如何工作的。</p></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><p id="9341" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们进一步讨论用例之前，描述一下我们的实验室环境是有意义的。我们的Kubernetes集群在版本<code class="fe lc ld le lf b">1.21</code>上，它在iptables模式下运行kube-proxy，而Istio在版本<code class="fe lc ld le lf b">1.13.4</code>上。</p><p id="f68a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">从吊舱内部观察</strong></p><p id="29e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于我们的第一个实验，我们将有一个默认的Nginx安装，我们将使用它作为目标；另一个独立的ubuntu pod将是我们数据包的来源。我们的Nginx Kubernetes服务以<code class="fe lc ld le lf b">10.0.185.118</code>为IP，Nginx pod <code class="fe lc ld le lf b">10.244.4.224</code>和ubuntu pod将会是<code class="fe lc ld le lf b">10.244.10.89</code>。</p><p id="720d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将从ubuntu pod向Nginx Kubernetes服务发出一个简单的HTTP <code class="fe lc ld le lf b">HEAD</code>请求开始。在ubuntu pod中，我们将并行运行一个tcpdump会话，在该会话中，我们可以看到数据包飞过。</p><p id="d958" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<code class="fe lc ld le lf b">curl</code>完成HTTP请求后，我们看到下面的TCP流:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">从Pod内部取出的Tcpdump</figcaption></figure><p id="eee0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们深入了解一下我们在这里看到了什么。</p><p id="648a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">前四个包是UDP包，它们是DNS查询和应答，由<code class="fe lc ld le lf b">curl</code>触发以获得我们想要的服务的IP。接下来的三个包是标准的<a class="ae ku" href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Connection_establishment" rel="noopener ugc nofollow" target="_blank"> TCP三次握手</a>，接下来的四个包是保存HTTP信息的实际包，最后三个包用于<a class="ae ku" href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Connection_termination" rel="noopener ugc nofollow" target="_blank">TCP连接终止</a>。就TCP流而言，没有什么值得观察的。</p><p id="5e69" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">从Kubernetes节点内部观察</strong></p><p id="ba7d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，我们如何从服务的IP ( <code class="fe lc ld le lf b">10.0.185.118</code>)到分配给特定Nginx pod的IP(<code class="fe lc ld le lf b">10.244.4.224</code>)。在流中，我们没有看到任何IP属于Nginx pod ( <code class="fe lc ld le lf b">10.244.4.224</code>)的数据包。</p><p id="b2b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们试着深入一个层次，从Kubernetes节点层次观察事物。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">从Kubernetes节点获取的Tcpdump</figcaption></figure><p id="12e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一个容易观察到的现象是，每个数据包似乎出现了四次。幸运的是，最新的tcpdump特性之一允许我们看到数据包来自的接口。如您所见，单个数据包在离开Kubernetes节点之前会经过四个接口。使用<code class="fe lc ld le lf b">ip a</code>查看我们的节点中的接口，我们看到几十个接口，其中大多数接口的名称中都有一个<code class="fe lc ld le lf b">veth*</code>前缀。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">Kubernetes节点接口</figcaption></figure><p id="b9dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些是大多数Kubernetes <a class="ae ku" href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni" rel="noopener ugc nofollow" target="_blank"> CNIs </a>为每个节点中调度的每个pod创建的虚拟接口。如果我们也使用<code class="fe lc ld le lf b">brctl show</code>查看我们节点中的桥，我们会看到所有的<code class="fe lc ld le lf b">veth*</code>虚拟接口都是<code class="fe lc ld le lf b">cbr0</code>桥的一部分。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">Kubernetes节点桥</figcaption></figure><p id="a6dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该信息也可在<code class="fe lc ld le lf b">ip a</code>输出中获得，其中指定该虚拟接口的主接口是<code class="fe lc ld le lf b">cbr0</code>。如果我们在系统中看到使用<code class="fe lc ld le lf b">ip route</code>的现有路由，我们会看到<code class="fe lc ld le lf b">eth0</code>用于“退出”节点。<br/><code class="fe lc ld le lf b">eth0</code>和<code class="fe lc ld le lf b">enP41291s1</code>似乎也是成对的，因为我们看到<code class="fe lc ld le lf b">eth0</code>被声明为<code class="fe lc ld le lf b">enP41291s1</code>的主机。<br/>所以现在我们知道了，为了让一个pod内的数据包离开节点，它从它的虚拟接口到网桥，然后到节点的接口<code class="fe lc ld le lf b">eth0</code>和<code class="fe lc ld le lf b">enP41291s1</code>。</p><p id="284a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二个观察结果是，在前面的流中，我们从pod内部看到了服务的IP和端口。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">带有服务IP的单个TCP数据包</figcaption></figure><p id="d8ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，在下一个数据包中，我们看到目的IP和端口已被转换为pod的IP和端口。什么会导致这种情况发生？</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">带有pod IP的单个TCP数据包</figcaption></figure><p id="2d81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你使用Kubernetes已经有一段时间了，你应该熟悉它的一个核心组件kube-proxy。Kube-proxy负责完成这项翻译工作。在我们的实验室中，kube-proxy设置为使用iptables，所以如果我们检查Kubernetes节点中的iptables规则，我们将看到下面的行。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">kubernetes服务的iptables规则</figcaption></figure><p id="946a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ku" href="https://kubernetes.io/docs/concepts/services-networking/service/#ips-and-vips" rel="noopener ugc nofollow" target="_blank">iptables模式下的Kube-proxy</a>确保我们始终拥有将服务IP转换为pods IPs的最新规则。</p></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><p id="8c47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Istio观察</strong></p><p id="f56e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如今，在Kubernetes上运行服务网格解决方案是很常见的事情。这些解决方案对我们的TCP流有什么影响吗？</p><p id="489e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我想想…</p><p id="12df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们的实验室中，我们将使用Istio，但Linkerd应该具有大致相同的行为。我们将再次使用ubuntu pod，我们将在相同的Kubernetes名称空间中针对一个简单的Nginx服务，该服务启用了Istio，因此其中的所有pod都有Istio。<br/>让我们从ubuntu pod使用<code class="fe lc ld le lf b">curl</code>启动一个HTTP <code class="fe lc ld le lf b">HEAD</code>请求，并使用tcpdump从Kubernetes节点查看数据包流。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">Istio注入pod的Kubernetes节点内的Tcpdump</figcaption></figure><p id="549b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于上述流程，我们可以进行一些有趣的观察。</p><p id="4ae0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一个观察结果是，我们没有看到以服务的IP为目的地的数据包。与前面来自Kubernetes节点的TCP流不同，这里我们看到3次握手部分中的第一个数据包将pod的IP作为目的地。因此，不知何故，我们跳过了kube-proxy提供的从服务ip到Pod IP的转换，如上所述。</p><p id="5672" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们知道Istio的一些关键特征，就可以解释这一点。第一个是Istio安装自己的iptables规则，以便在pod初始化期间借助<code class="fe lc ld le lf b">istio-init</code>容器拦截所有流量。它安装的iptables规则可以通过一个简单的<code class="fe lc ld le lf b">kubectl logs &lt;pod-name&gt; -c istio-init</code>找到。第二个特性是Istio拥有所有Kubernetes服务及其相关端点(pods IPs)的更新地图。任何人都可以使用<code class="fe lc ld le lf b">istioctl proxy-config endpoints &lt;pod-name&gt;</code>检查<code class="fe lc ld le lf b">istio-proxy</code>知道哪些端点。为了保持这个地图不断更新<code class="fe lc ld le lf b">istio-proxy</code>集装箱不断与istiod pod对话。如果您启动tcpdump并使用端口<code class="fe lc ld le lf b">15012</code> (Istio的<a class="ae ku" href="https://istio.io/latest/docs/ops/deployment/requirements/#ports-used-by-istio" rel="noopener ugc nofollow" target="_blank">配置端口</a>)过滤数据包，就可以很容易地观察到这一点。<br/>知道了这些，我们就可以理解，当Istio截获一个以服务IP为目的IP的数据包时，它会直接使用一个pod的IP来启动连接。</p><p id="6bd8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二个观察结果是，与没有任何Istio的流相比，我们看到了更多的数据包。这与Istio增加的TLS开销有关，因为<a class="ae ku" href="https://istio.io/latest/docs/concepts/security/#mutual-tls-authentication" rel="noopener ugc nofollow" target="_blank">Istio pod之间的所有通信都是通过TLS </a>进行的。</p><p id="900b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们观察到的最后一点是，我们没有看到关闭连接的<code class="fe lc ld le lf b">FIN</code> TCP数据包。虽然<code class="fe lc ld le lf b">curl</code>已经退出并且连接应该已经关闭，但是我们似乎保持连接打开。</p></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><p id="920b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们最后的观察是有趣的，所以让我们尝试一些额外的东西来获得更多的数据。我们将在一分钟内触发多个请求，而不是只做一个请求。下面是我们在运行请求循环时看到的几个单独curl请求的TCP数据包。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">具有多个curl请求的Istio注入pod的Tcpdump内部节点</figcaption></figure><p id="b307" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，在我们的最后一个流中有一些有趣的观察。</p><p id="f43f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一个问题是，我们似乎没有为每个请求建立新的TCP连接。在一个<code class="fe lc ld le lf b">while true</code>循环中使用bash中的普通<code class="fe lc ld le lf b">curl</code>,人们可能会认为每次<code class="fe lc ld le lf b">curl</code>调用都会创建一个新的TCP连接。上面的TCP流表明已经建立了一个TCP连接，我们使用这个连接。这也解释了我们在数据包完成后没有看到任何<code class="fe lc ld le lf b">FIN</code>数据包的事实。</p><p id="1d2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二个观察结果是，我们有流向不同目的IP的流，它们属于我们服务的不同pod。因此，尽管DNS返回服务IP，我们的TCP连接直接对我们服务的不同pod开放。这是Istio在本地保存每个服务的端点并如前所述直接使用它们的另一个证据。</p><p id="7b0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的观察都表明Istio维护的连接池的存在。</p><p id="fe75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然上面的tcpdump捕获是从节点级别进行的，但是让我们仔细检查一下pod内部的情况，我们可以看到还没有应用iptables规则的本地接口的流量。</p><p id="8061" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看来自pod网络内部的数据包:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ll lm l"/></div></figure><p id="b164" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们在这里可以观察到，虽然我们在<code class="fe lc ld le lf b">lo</code>接口上看到了3次握手数据包，但我们没有看到它们通过<code class="fe lc ld le lf b">eth0</code>接口，该接口用于退出pod的网络。还值得注意的是，第一个SYN包的目的端口是<code class="fe lc ld le lf b">15001</code>，这是<a class="ae ku" href="https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/" rel="noopener ugc nofollow" target="_blank">默认的istio-proxy监听端口</a>。</p><p id="582a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">实际情况是，为了让Istio保持连接池对任何其他容器透明，使用iptables拦截所有流量，并拥有自己的连接，这些连接不同于主容器启动的连接。</p></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><p id="fa6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，我们试图使用一个一直以来最受欢迎的工具tcpdump来验证和观察Kubernetes世界中与网络相关的一些理论和工具。当然，这只是我们在Kubernetes世界中看到的网络用例的一小部分，但是希望在阅读完这篇文章之后，您会对将来如何在Kubernetes中调试和解释网络行为有一个更好的想法。</p></div></div>    
</body>
</html>