<html>
<head>
<title>A first look at AWS Inferentia</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动气象站推理初探</h1>
<blockquote>原文：<a href="https://itnext.io/a-first-look-at-aws-inferentia-b9672e8f8b8f?source=collection_archive---------1-----------------------#2019-12-14">https://itnext.io/a-first-look-at-aws-inferentia-b9672e8f8b8f?source=collection_archive---------1-----------------------#2019-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e762" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在AWS re:Invent 2019上推出，<a class="ae kl" href="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener ugc nofollow" target="_blank"> AWS Inferentia </a>是一款高性能机器学习推理芯片，由AWS定制设计:其目的是提供经济高效、低延迟的大规模预测。推理出现在<a class="ae kl" href="https://aws.amazon.com/ec2/instance-types/inf1/" rel="noopener ugc nofollow" target="_blank">亚马逊EC2 inf1 </a>实例中，一个新的实例家族也在re:Invent上<a class="ae kl" href="https://aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/" rel="noopener ugc nofollow" target="_blank">推出</a>。</p><p id="80b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将向你展示如何开始使用推理和张量流。请注意，也支持<a class="ae kl" href="https://mxnet.apache.org" rel="noopener ugc nofollow" target="_blank"> Apache MXNet </a>、<a class="ae kl" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>和<a class="ae kl" href="https://onnx.ai" rel="noopener ugc nofollow" target="_blank"> ONNX </a>。</p><h2 id="c835" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">推理入门</h2><p id="959b" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated"><a class="ae kl" href="https://www.youtube.com/watch?v=17r1EapAxpk" rel="noopener ugc nofollow" target="_blank"> CMP324 </a>分组会议是对推理的一个很好的介绍，Alexa用例是一个难得的引擎盖下的一瞥。它非常值得你花时间。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lp lq l"/></div></figure><p id="3b69" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之，每个推理芯片拥有4个神经元。其中的每一个都实现了一个“高性能<a class="ae kl" href="https://en.wikipedia.org/wiki/Systolic_array" rel="noopener ugc nofollow" target="_blank">脉动阵列</a>矩阵乘法引擎”(说得好，Gadi)，并且还配备了一个大的片上高速缓存。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/b4a05d0d6859e7f76cfa322ec4bf66c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IQmXRvF9uIyNlzb-voaIUQ.png"/></div></div></figure><p id="ed55" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">芯片相互连接，这使得:</p><ul class=""><li id="c682" class="ly lz iq jp b jq jr ju jv jy ma kc mb kg mc kk md me mf mg bi translated">将一个模型划分到多个内核(和推理芯片，如果有几个可用的话)，将它100%存储在缓存内存中。</li><li id="97c4" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">通过内核管道全速传输数据流，无需应对外部存储器访问造成的延迟。</li></ul><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mm"><img src="../Images/a6f24c88321af4419ce5da0b18c70832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xSIJzmWfkef7cxnBSMpdyQ.png"/></div></div></figure><p id="719c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">或者，您可以在同一个推理芯片上运行不同模型的推理。这是通过将神经元分成神经元组，并在不同的组上加载不同的模型来实现的。</p><h2 id="dd95" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">神经元SDK</h2><p id="03f4" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">为了在推理上运行，首先需要将模型编译成硬件优化的表示。然后，可以使用特定的运行时来加载、执行和分析它们。这些操作可以通过<a class="ae kl" href="https://github.com/aws/aws-neuron-sdk" rel="noopener ugc nofollow" target="_blank"> AWS Neuron SDK </a>中可用的命令行工具来执行，或者通过框架API来执行。</p><div class="mn mo gp gr mp mq"><a href="https://github.com/aws/aws-neuron-sdk" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">aws/aws-neuron-sdk</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">AWS Neuron概述入门AWS Neuron是一个软件开发工具包(SDK ),支持高性能的深度…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">github.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne lw mq"/></div></div></a></div><p id="4b4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们开始吧！</p><h2 id="8182" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">为模型编译启动EC2实例</h2><p id="b9cc" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">这第一步不需要一个<em class="nf"> inf1 </em>实例。事实上，您应该使用一个<strong class="jp ir">计算优化实例</strong>来进行快速且经济的编译。为了避免任何软件配置，你还应该使用<a class="ae kl" href="https://aws.amazon.com/machine-learning/amis/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">深度学习AMI </strong> </a>，它预装了Neuron SDK和最新的框架。</p><p id="e627" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在撰写本文时，最新的深度学习AMI for Amazon Linux 2为26.0版本，其标识符为<a class="ae kl" href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=ami-08e68326c36bf3710" rel="noopener ugc nofollow" target="_blank"><em class="nf">AMI-08e 68326 c 36 BF 3710</em></a>。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ng"><img src="../Images/580856cc67e3291c55dbcd01d0b351c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AWzrZcLnyTz5vCtiJAezJw.png"/></div></div></figure><p id="9734" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用这个AMI，我启动了一个<em class="nf"> c5d.4xlarge </em>实例。不需要特殊设置:只需确保在安全组中允许SSH访问。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nh"><img src="../Images/598b2545d723938d683e14d8b53f003b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*94uEjdipHKN2SPasjdDmdQ.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">系列、实例名称、vCPUs、RAM、存储</figcaption></figure><p id="8c60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦实例启动，我对它使用ssh，我会看到熟悉的深度学习AMI横幅，告诉我<a class="ae kl" href="https://docs.conda.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Conda </a>环境可用于TensorFlow和Apache MXNet。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="0889" class="km kn iq nn b gy nr ns l nt nu">====================================================================<br/> __|  __|_  )<br/> _|  (     / Deep Learning AMI (Amazon Linux 2) Version 26.0<br/>___|\___|___|<br/>====================================================================</span><span id="0c85" class="km kn iq nn b gy nv ns l nt nu">Please use one of the following commands to start the required environment with the framework of your choice:<br/>for MXNet(+Keras2) with Python3 (CUDA 10.1 and Intel MKL-DNN) <br/>source activate mxnet_p36<br/>for MXNet(+Keras2) with Python2 (CUDA 10.1 and Intel MKL-DNN) <br/>source activate mxnet_p27<br/><strong class="nn ir">for MXNet(+AWS Neuron) with Python3 <br/>source activate aws_neuron_mxnet_p36</strong><br/>for TensorFlow(+Keras2) with Python3 (CUDA 10.0 and Intel MKL-DNN) source activate tensorflow_p36<br/>for TensorFlow(+Keras2) with Python2 (CUDA 10.0 and Intel MKL-DNN) source activate tensorflow_p27<br/><strong class="nn ir">for TensorFlow(+AWS Neuron) with Python3<br/>source activate aws_neuron_tensorflow_p36</strong><br/>for TensorFlow 2(+Keras2) with Python3 (CUDA 10.0 and Intel MKL-DNN) ssource activate tensorflow2_p36<br/>for TensorFlow 2(+Keras2) with Python2 (CUDA 10.0 and Intel MKL-DNN) ssource activate tensorflow2_p27<br/>for PyTorch with Python3 (CUDA 10.1 and Intel MKL) <br/>source activate pytorch_p36<br/>for PyTorch with Python2 (CUDA 10.1 and Intel MKL)<br/>source activate pytorch_p27<br/>for Chainer with Python2 (CUDA 10.0 and Intel iDeep)<br/>source activate chainer_p27<br/>for Chainer with Python3 (CUDA 10.0 and Intel iDeep)<br/>source activate chainer_p36<br/>for base Python2 (CUDA 10.0)<br/>source activate python2<br/>for base Python3 (CUDA 10.0) <br/>source activate python3</span><span id="17d0" class="km kn iq nn b gy nv ns l nt nu">Official Conda User Guide: <a class="ae kl" href="https://docs.conda.io/projects/conda/en/latest/user-guide/" rel="noopener ugc nofollow" target="_blank">https://docs.conda.io/projects/conda/en/latest/user-guide/</a><br/>AWS Deep Learning AMI Homepage: <a class="ae kl" href="https://aws.amazon.com/machine-learning/amis/" rel="noopener ugc nofollow" target="_blank">https://aws.amazon.com/machine-learning/amis/</a><br/>Developer Guide and Release Notes: <a class="ae kl" href="https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html" rel="noopener ugc nofollow" target="_blank">https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html</a><br/>Support: <a class="ae kl" href="https://forums.aws.amazon.com/forum.jspa?forumID=263" rel="noopener ugc nofollow" target="_blank">https://forums.aws.amazon.com/forum.jspa?forumID=263</a><br/>For a fully managed experience, check out Amazon SageMaker at <a class="ae kl" href="https://aws.amazon.com/sagemaker" rel="noopener ugc nofollow" target="_blank">https://aws.amazon.com/sagemaker</a><br/><strong class="nn ir">When using INF1 type instances, please update regularly using the instructions at: </strong><a class="ae kl" href="https://github.com/aws/aws-neuron-sdk/tree/master/release-notes" rel="noopener ugc nofollow" target="_blank"><strong class="nn ir">https://github.com/aws/aws-neuron-sdk/tree/master/release-notes</strong></a><br/>====================================================================</span></pre><p id="cffb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我激活适当的环境，它提供了所有需要的依赖项。</p><blockquote class="nw nx ny"><p id="f482" class="jn jo nf jp b jq jr js jt ju jv jw jx nz jz ka kb oa kd ke kf ob kh ki kj kk ij bi translated">F <!-- -->或者这篇文章的其余部分，任何以<em class="iq">(AWS _ neuron _ tensor flow _ p36)</em>为前缀的shell命令都应该在Conda环境中运行。</p></blockquote><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="c28e" class="km kn iq nn b gy nr ns l nt nu">$ source activate aws_neuron_tensorflow_p36<br/>(aws_neuron_tensorflow_p36) $</span></pre><p id="8d77" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我升级了<em class="nf"> tensorflow-neuron </em>包。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="33af" class="km kn iq nn b gy nr ns l nt nu">(aws_neuron_tensorflow_p36) $ conda install numpy=1.17.2 --yes<br/>(aws_neuron_tensorflow_p36) $ conda update tensorflow-neuron</span></pre><p id="3011" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在准备获取一个模型并编译它。</p><h2 id="f30f" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">编译模型</h2><p id="30d5" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">下面的代码抓取了一个在<a class="ae kl" href="http://image-net.org" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>数据集上预先训练的<a class="ae kl" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>图像分类模型，并将其存储在<em class="nf"> resnet50 </em>目录中。</p><p id="c41b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，它编译它进行推理。我强调了所需的一行代码:其他的都是普通的TensorFlow。然后，编译后的模型保存在<em class="nf"> ws_resnet50 </em>目录中，并保存在一个ZIP文件中，以便于复制到<em class="nf"> inf1 </em>实例中。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="785e" class="km kn iq nn b gy nr ns l nt nu">import os<br/>import time<br/>import shutil<br/>import tensorflow as tf<br/><strong class="nn ir">import tensorflow.neuron as tfn</strong><br/>import tensorflow.compat.v1.keras as keras<br/>from tensorflow.keras.applications.resnet50 import ResNet50<br/>from tensorflow.keras.applications.resnet50 import preprocess_input<br/><br/># Create a workspace<br/>WORKSPACE = './ws_resnet50'<br/>os.makedirs(WORKSPACE, exist_ok=True)<br/><br/># Prepare export directory (old one removed)<br/>model_dir = os.path.join(WORKSPACE, 'resnet50')<br/>compiled_model_dir = os.path.join(WORKSPACE, 'resnet50_neuron')<br/>shutil.rmtree(model_dir, ignore_errors=True)<br/>shutil.rmtree(compiled_model_dir, ignore_errors=True)<br/><br/># Instantiate Keras ResNet50 model<br/>keras.backend.set_learning_phase(0)<br/>keras.backend.set_image_data_format('channels_last')<br/><br/>model = ResNet50(weights='imagenet')<br/><br/># Export SavedModel<br/>tf.saved_model.simple_save(<br/>    session            = keras.backend.get_session(),<br/>    export_dir         = model_dir,<br/>    inputs             = {'input': model.inputs[0]},<br/>    outputs            = {'output': model.outputs[0]})<br/><br/><strong class="nn ir"># Compile using Neuron<br/>tfn.saved_model.compile(model_dir, compiled_model_dir)    </strong><br/><br/># Prepare SavedModel for uploading to Inf1 instance<br/>shutil.make_archive('./resnet50_neuron', 'zip', WORKSPACE, 'resnet50_neuron')</span></pre><p id="a3d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那一个<a class="ae kl" href="https://github.com/aws/aws-neuron-sdk/blob/master/docs/tensorflow-neuron/api-compilation-python-api.md" rel="noopener ugc nofollow" target="_blank"> API </a>就够了！令人印象深刻。</p><blockquote class="nw nx ny"><p id="1379" class="jn jo nf jp b jq jr js jt ju jv jw jx nz jz ka kb oa kd ke kf ob kh ki kj kk ij bi translated">超级用户会喜欢阅读关于CLI编译器的文章。</p></blockquote><p id="bc75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行这段代码会产生预期的输出。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="f0cb" class="km kn iq nn b gy nr ns l nt nu">(aws_neuron_tensorflow_p36) $ python compile_resnet.py</span><span id="4999" class="km kn iq nn b gy nv ns l nt nu">&lt;output removed&gt;<br/>Downloading data from <a class="ae kl" href="https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5" rel="noopener ugc nofollow" target="_blank">https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5</a><br/>102973440/102967424 [==============================] - 2s 0us/step<br/>&lt;output removed&gt;<br/>INFO:tensorflow:fusing subgraph neuron_op_d6f098c01c780733 with neuron-cc<br/>INFO:tensorflow:Number of operations in TensorFlow session: 4638<br/>INFO:tensorflow:Number of operations after tf.neuron optimizations: 556<br/>INFO:tensorflow:Number of operations placed on Neuron runtime: 554<br/>INFO:tensorflow:Successfully converted ./ws_resnet50/resnet50 to ./ws_resnet50/resnet50_neuron</span></pre><p id="054b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我简单地将ZIP文件复制到亚马逊S3桶中，这可能是与用于推理的<em class="nf"> inf1 </em>实例共享它的最简单方式。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="aba2" class="km kn iq nn b gy nr ns l nt nu">$ ls *.zip<br/>resnet50_neuron.zip<br/>$ aws s3 mb s3://jsimon-inf1-useast1 <br/>$ aws s3 cp resnet50_neuron.zip s3://jsimon-inf1-useast1<br/>upload: ./resnet50_neuron.zip to s3://jsimon-inf1-useast1/resnet50_neuron.zip</span></pre><p id="f164" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好吧，让我们点燃其中一个。</p><h2 id="1187" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">用张量流预测推理</h2><p id="d44b" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">使用与上面相同的AMI，我启动了一个<em class="nf">in f1 . xlage</em>实例。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oc"><img src="../Images/e5653dbe827b6a3a9023d10d7205f53d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hMip9-P6o7RaPt46Qk9csg.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">系列、实例名称、vCPUs、RAM、存储</figcaption></figure><p id="19de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦这个实例启动，我就ssh到它，并更新Neuron CLI工具。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="94d3" class="km kn iq nn b gy nr ns l nt nu">$ sudo yum install aws-neuron-tools aws-neuron-runtime-base aws-neuron-runtime -y</span></pre><p id="ba56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，我可以使用<a class="ae kl" href="https://github.com/aws/aws-neuron-sdk/tree/master/docs/neuron-tools" rel="noopener ugc nofollow" target="_blank"> <em class="nf"> neuron-ls </em> </a>查看硬件属性。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi od"><img src="../Images/33581f9035ecfde278b2de18102ccd19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDcXAWoafSbF6Mcc0mi-DQ.png"/></div></div></figure><p id="262c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4个神经核心，如预期的那样。“东”和“西”列显示了与其他推理芯片的连接:因为这个实例只有一个，所以它们是空的。</p><p id="45ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我从我的S3桶中检索编译后的模型，并提取它。我还下载了一个测试图像。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="f987" class="km kn iq nn b gy nr ns l nt nu">$ aws s3 cp s3://jsimon-inf1-useast1/resnet50_neuron.zip .<br/>download: s3://jsimon-inf1-useast1/resnet50_neuron.zip to resnet50_neuron.zip<br/>$ unzip resnet50_neuron.zip<br/>Archive: resnet50_neuron.zip<br/> creating: resnet50_neuron/<br/> creating: resnet50_neuron/variables/<br/> inflating: resnet50_neuron/saved_model.pb<br/>$ curl -O <a class="ae kl" href="https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg</a></span></pre><p id="ba0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用下面的代码，我加载并转换测试图像。然后，我加载编译后的模型，并使用它对图像进行分类。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="aaf7" class="km kn iq nn b gy nr ns l nt nu">import os<br/>import time<br/>import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.keras.preprocessing import image<br/>from tensorflow.keras.applications import resnet50<br/><br/>tf.keras.backend.set_image_data_format('channels_last')<br/><br/># Create input from image<br/>img_sgl = image.load_img('kitten_small.jpg', target_size=(224, 224))<br/>img_arr = image.img_to_array(img_sgl)<br/>img_arr2 = np.expand_dims(img_arr, axis=0)<br/>img_arr3 = resnet50.preprocess_input(img_arr2)<br/><br/># Load model<br/>COMPILED_MODEL_DIR = './resnet50_neuron/'<br/>predictor_inferentia = tf.contrib.predictor.from_saved_model(COMPILED_MODEL_DIR)<br/><br/># Run inference<br/>model_feed_dict={'input': img_arr3}<br/>infa_rslts = predictor_inferentia(model_feed_dict);<br/><br/># Display results<br/>print(resnet50.decode_predictions(infa_rslts["output"], top=5)[0])</span></pre><p id="17e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您能猜出这里有多少行特定于推理的代码吗？答案是<strong class="jp ir">零</strong>。我们无缝地使用<a class="ae kl" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/predictor" rel="noopener ugc nofollow" target="_blank"><em class="nf">TF . contrib . predictor</em></a>API。呜哇！</p><p id="ebad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行这段代码产生了预期的输出，我们看到了图像的前5个类。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="5a24" class="km kn iq nn b gy nr ns l nt nu">(aws_neuron_tensorflow_p36) $ python infer_resnet50.py</span><span id="d7ef" class="km kn iq nn b gy nv ns l nt nu">&lt;output removed&gt;<br/>[('n02123045', 'tabby', 0.6918919), ('n02127052', 'lynx', 0.12770271), ('n02123159', 'tiger_cat', 0.08277027), ('n02124075', 'Egyptian_cat', 0.06418919), ('n02128757', 'snow_leopard', 0.009290541)]</span></pre><p id="1454" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们看看如何使用<a class="ae kl" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"> TensorFlow Serving，</a>加载编译后的模型，这对于生产部署来说是一个非常好的选择。</p><h2 id="6ac4" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">用张量流服务进行推理预测</h2><p id="4a5f" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">首先，我们需要正确地打包模型，并将其移动到反映其版本的目录中。我们这里只有一个，所以让我们将保存的模型移动到名为“1”的目录中。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="507e" class="km kn iq nn b gy nr ns l nt nu">$ pwd<br/>/home/ec2-user/resnet50_neuron<br/>$ mkdir 1<br/>$ mv * 1</span></pre><p id="e024" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">TensorFlow服务的第一次用户经常会对文件布局感到困惑，所以它应该是这样的。这个目录是您应该使用<em class="nf"> model_base_path </em>参数传递给TensorFlow服务器的目录。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f0cab7e73eb3cb9c88e1aaadbd1d8cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*2BvyHsk-yVimJFYgKumYEQ.png"/></div></figure><p id="e6dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们可以启动TensorFlow服务，并加载编译后的模型。再说一次，这是香草张量流。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="384f" class="km kn iq nn b gy nr ns l nt nu">(aws_neuron_tensorflow_p36) $ tensorflow_model_server_neuron <br/>--model_name=resnet50 <br/>--model_base_path=/home/ec2-user/resnet50_neuron <br/>--port=8500</span><span id="1bdd" class="km kn iq nn b gy nv ns l nt nu">2019–12–13 16:16:27.704882: I tensorflow_serving/core/loader_harness.cc:87] <strong class="nn ir">Successfully loaded servable version {name: resnet50 version: 1}</strong><br/>2019–12–13 16:16:27.706241: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:8500</span></pre><p id="949c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦TensorFlow服务启动并运行，我们可以使用下面的脚本来加载测试图像，并发送它进行预测。冒着重复我自己的风险…这是香草张量流:)</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="72b2" class="km kn iq nn b gy nr ns l nt nu">import numpy as np<br/>import grpc<br/>import tensorflow as tf<br/>from tensorflow.keras.preprocessing import image<br/>from tensorflow.keras.applications.resnet50 import preprocess_input<br/>from tensorflow.keras.applications.resnet50 import decode_predictions<br/>from tensorflow_serving.apis import predict_pb2<br/>from tensorflow_serving.apis import prediction_service_pb2_grpc<br/><br/>if __name__ == '__main__':<br/>    chan = grpc.insecure_channel('localhost:8500')<br/>    stub = prediction_service_pb2_grpc.PredictionServiceStub(chan)</span><span id="d7ad" class="km kn iq nn b gy nv ns l nt nu">    img_file="kitten_small.jpg"      <br/>    img = image.load_img(img_file, target_size=(224, 224))<br/>    img_array = preprocess_input(image.img_to_array(img)[None, ...])</span><span id="7973" class="km kn iq nn b gy nv ns l nt nu">    request = predict_pb2.PredictRequest()<br/>    request.model_spec.name = 'resnet50'<br/>    request.inputs['input'].CopyFrom(<br/>        tf.contrib.util.make_tensor_proto(<br/>            img_array, shape=img_array.shape)<br/>    )</span><span id="0d07" class="km kn iq nn b gy nv ns l nt nu">    result = stub.Predict(request)<br/>    prediction = tf.make_ndarray(result.outputs['output'])<br/>    print(decode_predictions(prediction))</span></pre><p id="4a1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行这段代码产生了预期的输出，我们看到了图像的前5个类。</p><pre class="lk ll lm ln gt nm nn no np aw nq bi"><span id="42e6" class="km kn iq nn b gy nr ns l nt nu">(aws_neuron_tensorflow_p36) $ python tfserving_resnet50.py</span><span id="f27a" class="km kn iq nn b gy nv ns l nt nu">&lt;output removed&gt;<br/>[[(‘n02123045’, ‘tabby’, 0.6918919), (‘n02127052’, ‘lynx’, 0.12770271), (‘n02123159’, ‘tiger_cat’, 0.08277027), (‘n02124075’, ‘Egyptian_cat’, 0.06418919), (‘n02128757’, ‘snow_leopard’, 0.009290541)]]</span></pre><p id="9cd2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我录了这个演示，可以在YouTube上看到。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="of lq l"/></div></figure><h2 id="ecb6" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">潜得更深</h2><p id="9a67" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">今天到此为止。希望我给你介绍清楚了AWS推理，以及它有多好用！编译我们的模型只需要一行代码。</p><p id="4182" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想深入了解，我强烈推荐我的同事叶在re:Invent举办的精彩的<a class="ae kl" href="https://github.com/awshlabs/reinvent19Inf1Lab" rel="noopener ugc nofollow" target="_blank">工作坊</a>。其中一个实验向您展示了如何将32位浮点(FP32) ResNet50模型编译为16位浮点(FP16)。众所周知，通过降低算术复杂度，这种技术可以在保持准确性的同时提高性能。事实上，在一个<em class="nf"> inf1.2xlarge </em>的实例中，FP16模型提供了令人印象深刻的<strong class="jp ir">每秒1500个图像分类</strong>！</p><p id="8a3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一如既往的感谢您的阅读。很乐意在这里或者在<a class="ae kl" href="https://twitter.com/julsimon" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上回答问题。</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><p id="0042" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">推理神经核心完全是硬核。他们破坏一切\m/</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="of lq l"/></div></figure></div></div>    
</body>
</html>