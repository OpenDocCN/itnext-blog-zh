<html>
<head>
<title>Apache Spark Internals: Tips and Optimizations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark内部:技巧和优化</h1>
<blockquote>原文：<a href="https://itnext.io/apache-spark-internals-tips-and-optimizations-8c3cad527ea2?source=collection_archive---------0-----------------------#2020-12-12">https://itnext.io/apache-spark-internals-tips-and-optimizations-8c3cad527ea2?source=collection_archive---------0-----------------------#2020-12-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/fbd1d467192359ae9bf9ea55872558b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-Ha45keR7bEqwfic"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">由<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kf" href="https://unsplash.com/@bkgoh785?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> BK GOH </a>拍摄</figcaption></figure><h1 id="d0fa" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="c142" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这篇文章的目标是更深入地挖掘<strong class="lg iu">Apache</strong><a class="ae kf" href="http://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">Spark</strong></a>的内部，以便更好地理解Spark如何工作，这样我们就可以编写<strong class="lg iu">最佳代码，最大化并行性，最小化数据混乱</strong>。</p><p id="785e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是我的<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/spark-cassandra-all-you-need-to-know-tips-and-optimizations-d3810cc0bd4e"> <strong class="lg iu">上一篇文章</strong> </a>的摘录，我推荐在这篇文章之后阅读。我假设您已经对Spark有了基本的了解。</p><h1 id="f641" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">火花优化</h1><p id="c92d" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在Spark中，您编写转换数据的代码，这些代码被<strong class="lg iu">惰性</strong>评估，并在幕后被转换为<strong class="lg iu">查询计划</strong>，当您调用诸如<em class="mh">收集</em>()或<em class="mh">写入</em>)之类的动作时，查询计划被具体化。Spark将数据分成<strong class="lg iu">个分区</strong>，由<strong class="lg iu">个执行器</strong>处理，每个执行器处理一组分区。在单个分区内执行的操作称为<strong class="lg iu">窄操作</strong>，包括<em class="mh"> map() </em>或<em class="mh"> filter() </em>等函数。另一方面，聚合是<strong class="lg iu">宽操作</strong>，需要跨节点移动数据，这是非常昂贵的。查询计划本身可以有两种主要类型:逻辑计划和物理计划，我们将在后面讨论。</p><p id="bdc5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">记住，关于Spark性能的主要规则是:<strong class="lg iu">最小化数据混洗</strong>。这是由Spark  中的<a class="ae kf" href="https://medium.com/@dvcanton/wide-and-narrow-dependencies-in-apache-spark-21acf2faf031" rel="noopener"> <strong class="lg iu">宽操作引起的，这种连接或聚合<strong class="lg iu">非常昂贵</strong>因为数据混洗。</strong></a></p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/b81e55cb31b686469852935e9b824409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*GAcwcvjx6HvYJe09.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">宽依赖性与窄依赖性。来源:<a class="ae kf" href="https://medium.com/@dvcanton/wide-and-narrow-dependencies-in-apache-spark-21acf2faf031" rel="noopener">https://medium . com/@ dv canton/wide-and-narrow-dependencies-in-Apache-spark-21 ACF 2 faf 031</a></figcaption></figure><p id="7b07" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">总是试图减少数据混洗的次数，实际上，我们将要谈到的大多数优化的目标都是试图这样做:<strong class="lg iu">减少通过网络发送的数据量。</strong></p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/f6aa6ea7e1f629e526e8e6bbe064d877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*CxrNzE6JV_Ynedct.jpg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">来源:https://luminousmen.com/<a class="ae kf" href="https://luminousmen.com/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="ca5a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在让我们回顾一下一些优化…</p><h1 id="8f17" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">早期过滤</h1><p id="d69c" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这真的很简单但是<strong class="lg iu">非常重要</strong>。<strong class="lg iu">尽可能早地过滤数据</strong>这样你就不会处理那些以后会被丢弃的数据。越是可以将过滤器下推到数据源，特别是可以通过分区键限制查询的地方，越好。如果可能，在filter语句中指定分区键的所有组成部分。此外，如果您打算不止一次地使用来自磁盘的一组数据，请确保使用<strong class="lg iu"> <em class="mh">缓存</em> </strong> <em class="mh"> () </em>将其保存在Spark内存中，而不是每次都从磁盘中读取。</p><p id="eb4f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一个很好的经验是在过滤数据后使用<strong class="lg iu"><em class="mh">coalesce</em></strong><em class="mh">()</em>方法来减少分区的数量。</p><h1 id="6d81" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">火花分区和火花连接</h1><p id="9257" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这是Spark中<strong class="lg iu">关键的</strong>，我真的推荐<a class="ae kf" href="https://luminousmen.com/post/spark-tips-partition-tuning" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">这篇</strong> <strong class="lg iu">文章</strong> </a>里面详细解释了不同的优化。</p><h2 id="ada0" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated"><strong class="ak">设置正确的分区数量</strong></h2><p id="6fdb" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">您需要<strong class="lg iu">设置</strong> <strong class="lg iu">正确的分区数量到</strong> <strong class="lg iu">最大化您集群中的并行度</strong>。如果您的分区太少，那么您将无法利用集群中所有可用的核心，从而浪费资源。拥有太多分区将导致管理许多小任务的额外开销，以及降低性能的数据移动。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/81c97976f2d231a543cf6414d38064f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nNXlxGCrcMsQutLc"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">火花隔板。来源:<a class="ae kf" href="https://luminousmen.com/post/spark-tips-partition-tuning" rel="noopener ugc nofollow" target="_blank">https://luminousmen.com/post/spark-tips-partition-tuning</a></figcaption></figure><p id="5e2a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">对Spark的一般建议是让集群中可用于应用的内核数量达到分区数量的4x，当然，这取决于您的使用案例。当拿不准的时候，最好是<strong class="lg iu">被</strong> <em class="mh"> </em> <strong class="lg iu">错在更大数量的任务</strong>一边。这需要与执行程序的数量和每个执行程序的内存保持一致，我们将在后面讨论。</p><h2 id="e4e5" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated"><strong class="ak">避免数据偏斜</strong></h2><p id="fca8" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一般来说，您希望您的数据在所有节点上平均分布，这在执行连接时特别重要。您的连接键应该均匀分布，以避免数据倾斜。<strong class="lg iu">倾斜的数据会导致并行处理的性能下降</strong>甚至OOM(内存不足)崩溃，应该避免。</p><p id="87ab" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">通过查看<strong class="lg iu"> Spark UI </strong>并检查每项任务花费的时间，您可以<strong class="lg iu">诊断数据的数据偏斜度</strong>。如果一个或多个任务比其他任务花费的时间长，那么就有<strong class="lg iu">不平衡的分区</strong>。</p><p id="d18c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一些<strong class="lg iu">解决方案</strong>可能是:</p><ul class=""><li id="1383" class="nb nc it lg b lh mc ll md lp nd lt ne lx nf mb ng nh ni nj bi translated"><strong class="lg iu">将数据</strong>重新分区到更均匀分布的密钥上。这是首选选项，使用一个<strong class="lg iu">键，在执行任何宽操作之前自然地将数据</strong>分布到各个分区。</li><li id="2fa7" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated">使用<a class="ae kf" href="https://medium.com/appsflyer/salting-your-spark-to-scale-e6f1c87dd18" rel="noopener"> <strong class="lg iu">加盐</strong> </a>合成一个额外的随机密钥，以便更好地分配数据。</li><li id="5e43" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated"><strong class="lg iu">将数据分为偏斜数据和非偏斜数据</strong>，并通过重新分配偏斜数据来并行处理它们。</li><li id="bee9" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated">如果要连接的数据集很小，比如一个事实表，使用<a class="ae kf" href="https://acadgild.com/blog/broadcast-variables-and-accumulators-in-spark" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">广播变量</strong> </a>，我们将在后面讨论。这对于在事实表上进行查找很有用。</li><li id="5ec8" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated">使用<a class="ae kf" href="https://mungingdata.com/apache-spark/broadcast-joins/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">广播连接</strong> </a>当连接两个数据集且其中一个很小时，这与广播变量具有相同的好处。一个更高级的特性是迭代广播连接，我们分割数据，做许多小的广播连接，而不是大的。</li></ul><p id="4a75" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们更详细地看看联播吧…</p><h2 id="e875" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated"><strong class="ak">火花连接</strong></h2><p id="56a7" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">默认情况下，Spark user<a class="ae kf" href="https://en.wikipedia.org/wiki/Sort-merge_join" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">Sort Merge</strong><strong class="lg iu">Join</strong></a>非常适合大型数据集。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/009c457af69f8938fcdd445c48a34de2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/0*_1a3pdQ_0j4LR7u0.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">排序合并联接。来源:<a class="ae kf" href="https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c" rel="noopener" target="_blank">https://towards data science . com/the-art-of-joining-in-spark-dcbd 33d 693 c</a></figcaption></figure><p id="4059" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这个想法是在连接之前对分区进行排序，以减少数据混乱的数量，但是排序本身是一个昂贵的操作，这个连接的性能会根据连接两端的源数据而有很大的变化，如果数据已经混乱，那么它非常快，如果没有，Spark将需要执行交换和排序，这将导致数据混乱。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/e9bc907d81f4e1d2dc390265420f6d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SIDW_O1Us7TaIG-u.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">需要交换时排序合并联接。来源:<a class="ae kf" href="https://towardsdatascience.com/should-i-repartition-836f7842298c" rel="noopener" target="_blank">https://towards data science . com/should-I-repartition-836 f 7842298 c</a></figcaption></figure><p id="d88a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">当两个数据集都很大时，这种连接技术非常有效，但是当您将一个表与一个小的事实表连接时，这种优势就丧失了。在这种情况下，特别是如果你有一个集群有足够的内存可用，你可以使用<strong class="lg iu">广播加入</strong>。</p><h2 id="42b4" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated"><strong class="ak">广播加入</strong></h2><p id="e942" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在广播连接的情况下，<strong class="lg iu"> Spark会将数据的副本发送给每个执行器，并保存在内存中</strong>，这可以将性能提高70%，在某些情况下甚至更高。广播连接的概念类似于我们将在后面讨论的广播变量，但是广播连接是由Spark自动处理的，您需要做的只是告诉Spark您想要广播哪个表:</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="c174" class="mo kh it ns b gy nw nx l ny nz">df = fact_table.join(<strong class="ns iu">broadcast</strong>(dimension_table),                        fact_table.col("dimension_id") === dimension_table.col("id"))</span></pre><p id="ce31" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">请注意，当使用广播连接时，数据在内核之间共享，但每个执行器都有自己的副本，因此平衡内核和执行器很重要，我们将在后面讨论这一点。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/438680ddcfa92be27a43735bbe69aacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*fwK3fYHRadWwRTtx.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">广播加入。来源:<a class="ae kf" href="https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c" rel="noopener" target="_blank">https://towards data science . com/the-art-of-joining-in-spark-dcbd 33d 693 c</a></figcaption></figure><p id="01df" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">有关Spark joins的更多信息，请查看本文。</p><h2 id="914d" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated"><strong class="ak">在昂贵或多重连接之前重新分区</strong></h2><p id="f875" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="lg iu"> <em class="mh"> repartition() </em> </strong>方法允许我们更改集群上的数据分布。这将导致数据混乱，代价很高，但是如果在某些类型的连接之前或者在多个连接之前进行，只要指定列名，就可以<strong class="lg iu">提高性能。正如我们之前看到的，Spark需要知道数据分布，以便在排序合并连接之前使用它。这可以使用<strong class="lg iu">存储桶</strong>来完成，它以预混洗和可能预排序的状态存储数据，其中关于存储桶的信息存储在metastore中。但是，即使您的数据已经排序并在磁盘上对其中一个表进行了混洗，Spark也不会知道，仍然会对两个表进行重新排序和完全混洗。这就是为什么需要<strong class="lg iu">对数据进行重新分区，以匹配连接</strong>一端的分区，并且可以减少数据混乱，因此只有连接的一端(表)分布在网络上。其思想是，通过指定列，Spark将元数据添加到逻辑计划中，因此它知道不需要移动数据。例如，如果一个表按ID划分为100个分区。通过重新划分数据，您可以避免数据混乱:</strong></p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="a818" class="mo kh it ns b gy nw nx l ny nz">df<strong class="ns iu">.repartition(100, "id")</strong>.join(..., "id")</span></pre><p id="c615" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">目标是在连接的两端有相同数量的分区，以避免数据交换。如果分区数量与此属性不同，也会发生数据混洗:</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="edd0" class="mo kh it ns b gy nw nx l ny nz">spark.sql.shuffle.partitions // default 200</span></pre><p id="70ca" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它控制shuffle期间的<strong class="lg iu">个分区，</strong>个分区，排序合并连接使用它在连接之前对数据进行重新分区和排序。我非常推荐阅读<a class="ae kf" href="https://towardsdatascience.com/should-i-repartition-836f7842298c" rel="noopener" target="_blank"> <strong class="lg iu">这篇</strong> <strong class="lg iu">文章</strong> </a>，它更详细地介绍了连接和数据分区是如何工作的。</p><p id="2b17" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一个很好的经验法则是在多次连接或非常昂贵的连接之前尝试重新分区，以避免一次又一次的排序合并连接重新洗牌。</p><p id="3640" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果您需要减少分区的数量，请使用<strong class="lg iu">联合</strong>而不是<em class="mh">重新分区()</em>方法，因为它可以最大限度地减少数据混乱，并且不会触发数据交换。在一个过滤器之后，记得使用<em class="mh"> coalesce()，</em>，因为你会有更少的数据，这比<em class="mh"> repartition() </em>更有效，因为它最小化了数据混乱。此外，在重新分区后，始终保持或缓存您的数据，以最大限度地减少数据混乱。但是请记住，重新分区本身是一项开销很大的操作，它会在整个集群中移动数据，所以尽量只在完全必要时使用一次；并且永远记得先做窄操作。</p><p id="bc77" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">重要的是要记住<strong class="lg iu">一些宽操作，比如group by，会改变分区的数量</strong>。</p><h2 id="9aa2" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated"><strong class="ak">写入存储前重新分区</strong></h2><p id="5c13" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">以一种优化阅读的方式编写数据总是一个好主意。在HDFS，当执行基于列的操作时，您希望使用列格式，如<a class="ae kf" href="https://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Parquet </strong> </a>来提高读取操作的性能。这个我在<a class="ae kf" href="https://towardsdatascience.com/big-data-file-formats-explained-dfaabe9e8b33" rel="noopener" target="_blank"> <strong class="lg iu">这个</strong> <strong class="lg iu">篇</strong> </a>里讲过。其思想是，如果使用正确的文件夹结构创建分区，那么读操作不需要扫描磁盘中的所有数据，而只需要扫描指定的文件夹/分区。</p><p id="942e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">您可以使用以下方法在写入时对数据进行分区:</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="a982" class="mo kh it ns b gy nw nx l ny nz">df.write.partitionBy('key').json('/path...')</span></pre><p id="8883" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">通过将数据存储在不同的文件夹中，<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/olap-query-engines-for-big-data-5f17b88d6ebc"> <strong class="lg iu">查询引擎</strong> </a>可以跳过从磁盘扫描的大量数据，提高读取<strong class="lg iu">性能</strong>。例如，在<strong class="lg iu"> Spark SQL </strong>中，如果您按日期分区，如果您在<em class="mh"> where </em>子句中指定日期，那么只读取一个文件夹，而不是整个数据集，因此查询将花费不到一秒钟而不是几分钟。</p><p id="3210" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果文件夹结构是这样的:</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="4c39" class="mo kh it ns b gy nw nx l ny nz">/mydata/2020-02-02/...<br/>/mydata/2020-02-03/...<br/>/mydata/2020-02-04/...<br/>/mydata/2020-02-05/...</span></pre><p id="93e4" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">然后，通过运行以下命令:</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="83cb" class="mo kh it ns b gy nw nx l ny nz">SELECT * from mydata WHERE date = '2020-02-03'</span></pre><p id="1da6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">仅读取<em class="mh">2020–02–03</em>的数据，而不是整个数据集。</p><h2 id="e396" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated"><strong class="ak">避免磁盘溢出</strong></h2><p id="222f" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当加入大型数据集时，Spark需要在数据洗牌期间存储中间数据，如果执行器没有足够的内存，它会将其移动到磁盘，然后加入将变得非常慢，请确保为每个执行器设置正确的内存量(<em class="mh"> spark.executor.memory </em>)，并减少您的数据大小。您也可以通过设置来更改缓冲区的大小:<em class="mh">spark . shuffle . file . buffer</em></p><h2 id="866f" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated">设置正确数量的执行器、内核和内存</h2><p id="263f" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">重要的是<strong class="lg iu">根据分区数量</strong>设置执行人数量。你的目标是最大化并行性，并且<strong class="lg iu">确保你的Spark执行器在整个任务期间都很忙，并且所有的内核都在节点中使用</strong>。</p><p id="7cc7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">记住，每个执行器处理一个数据子集，也就是一组分区。此外，每个执行器使用一个或多个内核，如属性所设置:</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="fd36" class="mo kh it ns b gy nw nx l ny nz">spark.executor.cores</span></pre><p id="5dbb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">带纱运行时设置为1。</p><p id="867e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在Spark中，我们通过将数据分割成分区来实现并行性，这是Spark分割数据的方式。分区分布在不同的节点上，每个节点都有一组执行器。然后每个执行器管理一个或多个分区。请注意，在计算级别，每个执行器执行一组任务，因此每个执行器将向同一个分区应用一组函数。所以，这就是为什么您可能希望每个执行器有<strong class="lg iu">多个内核，这样您就可以并行运行独立的任务。重要的是要明白，每个执行器在内存中都有自己的本地和独立数据，包括广播变量(将在后面讨论)和累加器，这两者都使用相当多的内存；但是，这些在内核之间共享。因此，每个执行器只有一个内核意味着需要为每个执行器复制所有数据。此外，I/O操作，尤其是对可拆分文件格式的I/O操作，可以在读写分区时利用多个内核，从而最大限度地提高吞吐量。</strong></p><p id="3a97" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">总之，当您使用广播变量、累加器或者您读取/写入大量数据时，您希望每个执行器有多个内核，但不能太多，否则一些内核将无法使用。更多信息请查看<a class="ae kf" href="https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/><strong class="lg iu">篇</strong> </a>。</p><p id="d3a4" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">根据经验，每个执行器3-5个内核是一个不错的选择<strong class="lg iu">。当然，这取决于分区的数量和分区的大小。如果您有非常小的分区，并且不像广播变量那样使用很多内存，那么建议使用较少的内核。</strong></p><p id="cd45" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">你的目标是找到执行器和内核数量之间的平衡，以及每个执行器的合适内存量。您可以在运行<a class="ae kf" href="https://spark.apache.org/docs/latest/submitting-applications.html" rel="noopener ugc nofollow" target="_blank"> spark-submit </a>时设置执行器内存、执行器数量和内核数量。</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="1a0c" class="mo kh it ns b gy nw nx l ny nz"><em class="mh"># Run on a Spark standalone cluster in cluster deploy mode with supervise</em><br/>./bin/spark-submit <strong class="ns iu">\</strong><br/>  <strong class="ns iu">--class</strong> org.apache.spark.examples.SparkPi <strong class="ns iu">\</strong><br/>  <strong class="ns iu">--master</strong> spark://207.184.161.138:7077 <strong class="ns iu">\</strong><br/>  <strong class="ns iu">--deploy-mode</strong> cluster <strong class="ns iu">\</strong><br/>  <strong class="ns iu">--supervise</strong> <strong class="ns iu">\</strong><br/>  <strong class="ns iu">--executor-memory</strong> 20G <strong class="ns iu">\</strong><br/>  <strong class="ns iu">--total-executor-cores</strong> 100 <strong class="ns iu">\</strong><br/>  /path/to/examples.jar <strong class="ns iu">\</strong><br/>  1000</span></pre><p id="ec70" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">也可以将它们作为Spark属性传递。</p><p id="abd3" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为每个执行器设置正确的<strong class="lg iu">内存</strong>也很重要，这需要基于您使用的累加器、广播变量以及在执行连接时数据的大小，正如我们之前看到的，数据是无序的。确保你<strong class="lg iu">设置了警报</strong>，这样你就知道数据溢出是非常低效的。</p><h2 id="f76e" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated">广播变量</h2><p id="8c59" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们已经讨论过广播连接，它非常有用并且非常受欢迎，例如，因为连接小数据集和大数据集是很常见的；当您用从<em class="mh"> CSV </em>文件上传的小事实表连接您的表时。</p><p id="aa76" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这种情况下，想法是<strong class="lg iu">将数据复制到每个执行器</strong>，因此不需要移动任何数据，并且连接是在本地完成的，因为连接的一端完全存储在每个节点的内存中。这是Spark中广播的思想，对于连接和变量都是如此。</p><p id="4fe5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">连接由Spark在幕后为您管理，因此易于使用。需要在代码中创建广播变量。您可以存储任何<em class="mh"> JVM </em>对象，只要它是可序列化的。我们将在下一节讨论序列化。</p><p id="cc50" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">要创建广播变量:</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="a7b9" class="mo kh it ns b gy nw nx l ny nz">val df = spark.sparkContext.broadcast(data)</span></pre><p id="d1b0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">注意，数据不能是数据帧或数据集，需要是一个常规对象，所以需要调用<em class="mh"> collect() </em>方法获取所有数据，然后发送给执行者。</p><h2 id="29d2" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated">Spark序列化</h2><p id="867a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有几篇文章和几本书教你如何优化你的Spark代码，然而，你能做的提高所有代码的Spark性能的最有效的事情就是<strong class="lg iu">去除Java序列化。</strong></p><p id="7b8a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Java序列化效率很低，也不安全；会拖慢你的工作。</p><p id="dc60" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">您需要了解Spark如何运行应用程序。简而言之，驱动程序需要序列化您的代码并将其发送到所有节点，因此广播变量和作业本身需要通过网络传输，此外，中间数据和元数据也需要序列化。这种情况在发动机罩下经常发生，对于您的应用来说，它可能是一个瓶颈<strong class="lg iu">。</strong></p><p id="23dd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果您在Spark中遇到过著名的“<em class="mh">任务不可序列化</em>”错误，那么您应该知道我在说什么。理解代码的哪些部分需要序列化，哪些不需要是至关重要的。您的目标是识别这些对象，并通过使用另一种可序列化的格式来优化它们。要解决连载问题，我真的建议看一看<a class="ae kf" href="https://medium.com/swlh/spark-serialization-errors-e0eebcf0f6e6" rel="noopener"> <strong class="lg iu">这篇</strong> <strong class="lg iu">文章</strong> </a>。一般来说，<strong class="lg iu">确保传递给闭包的所有对象都是可序列化的。</strong></p><p id="6b21" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Java序列化最著名的Spark替代方案是<a class="ae kf" href="https://github.com/EsotericSoftware/kryo" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Kryo序列化</strong> </a> <strong class="lg iu"> </strong>，可以将序列化性能提高几个数量级。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c8b8663149b87e27d5608c0eebd4444f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/0*bKGM9FKca86FQKTK.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">Kryo性能vs Java，来源:<a class="ae kf" href="https://docs.mulesoft.com/mule-runtime/3.9/improving-performance-with-the-kryo-serializer" rel="noopener ugc nofollow" target="_blank">https://docs . mulesoft . com/mule-runtime/3.9/用kryo序列化器提高性能</a></figcaption></figure><p id="1f85" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">许多Spark开发人员选择不使用它，因为您需要使用以下方法注册您使用的每个类:</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="3daf" class="mo kh it ns b gy nw nx l ny nz">Kryo kryo = new Kryo();<br/>kryo.register(SomeClass.class);</span></pre><p id="a0e0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">但是我真的建议你从项目一开始就启用Kryo来利用性能提升。<strong class="lg iu">注意</strong>如果你依赖数据集API，那么你可能不需要Kryo，因为你的类将使用比Kryo更有效的钨编码器，我们将在后面讨论它们。</p><h2 id="04ad" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated">火花催化剂</h2><p id="7fa0" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我把最好的优化留到了最后。Spark有一个秘密武器，可以极大地提高你的工作效率，最棒的是你几乎不用做任何事情就可以使用它，它在引擎盖下运行。我们之前已经接触过这个特性:<a class="ae kf" href="https://databricks.com/glossary/catalyst-optimizer" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">火花催化发动机</strong> </a> <strong class="lg iu">。它基本上以一种最佳的方式重写了你的代码。</strong></p><p id="59a5" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Catalyst在数据框API和部分数据集API中可用。<strong class="lg iu">RDD API</strong>中没有。它是<a class="ae kf" href="https://spark.apache.org/sql/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Spark SQL </strong> </a>层的一部分，其背后的想法是使用RDBMS世界多年来完成的所有优化，并将它们带到<a class="ae kf" href="https://spark.apache.org/sql/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Spark SQL </strong> </a>。由于SQL提供了一个已知的数学模型，Spark Catalyst可以理解数据，做出假设并优化代码。在引擎盖下，Spark运行一个复杂的工作流程，将你的代码完全改写成一个更难理解但更有效的代码。</p><p id="770e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">把Spark数据框架API想象成一种声明性的语言，而不是真正的代码。你就像是在这个引擎上写东西，这个引擎会解释你的代码并优化它。这是在Spark中触发动作时发生的过程:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/6a915f43c9f86beea5c1ffcfec47cb85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*biwWNKp1oYnh3d2z"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">催化剂工作流程</figcaption></figure><p id="d12b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">简而言之，<strong class="lg iu">它创建了一个优化的逻辑计划，并将其划分为多个物理计划</strong>。然后，它为物理计划生成代码，并将代码分发给执行者(还记得为什么序列化很重要吗？).Catalyst通过在逻辑查询计划上应用一系列转换，如谓词下推、列修剪和常量折叠，从逻辑查询计划生成优化的物理查询计划。</p><p id="fafb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Catalyst Optimizer有两种类型的优化:</p><ul class=""><li id="0e59" class="nb nc it lg b lh mc ll md lp nd lt ne lx nf mb ng nh ni nj bi translated"><strong class="lg iu">基于成本的优化器</strong>:由于数据帧是基于SQL的，Catalyst可以计算每条路径的成本并分析哪条路径更便宜，然后执行那条路径来提高查询执行。</li><li id="5a0a" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated"><strong class="lg iu">基于规则的优化器</strong>:包括常量合并、谓词下推、投影修剪、空传播、布尔简化和其他规则。</li></ul><p id="f9d1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Catalyst还会<strong class="lg iu">自动执行广播连接</strong>当连接的一端很小时，可以使用该属性设置阈值:</p><pre class="mj mk ml mm gt nr ns nt nu aw nv bi"><span id="fd9c" class="mo kh it ns b gy nw nx l ny nz">spark.sql.autoBroadcastJoinThreshold</span></pre><h2 id="6c34" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated">钨项目</h2><p id="d3fe" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">数据帧和数据集API也有利于<a class="ae kf" href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">项目</strong> <strong class="lg iu">钨</strong> </a>，该项目旨在修复我们之前提到的<strong class="lg iu">序列化</strong>问题、内存管理和<strong class="lg iu">性能</strong>。它使用优化的内存格式和不需要垃圾收集的堆外内存。</p><p id="e4cb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它通过执行以下操作来优化Spark作业的CPU和内存效率:</p><ol class=""><li id="ed9e" class="nb nc it lg b lh mc ll md lp nd lt ne lx nf mb od nh ni nj bi translated">堆外内存管理使用二进制内存数据表示，这是钨行格式。</li><li id="1b21" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb od nh ni nj bi translated">高速缓存局部性，这是关于高速缓存感知计算和高速缓存感知布局，以实现高高速缓存命中率。</li><li id="b770" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb od nh ni nj bi translated">全阶段代码生成(CodeGen)。</li></ol><p id="395e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这就是为什么在使用数据集API时需要使用<a class="ae kf" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Encoders.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">编码器</strong> </a>的原因，这些负责堆外优化。您可以在数据框和数据集API中免费获得，但数据集会得到更好的优化。更多信息请查看这篇<a class="ae kf" href="https://www.linkedin.com/pulse/catalyst-tungsten-apache-sparks-speeding-engine-deepak-rajak/?articleId=6674601890514378752" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">伟大的</strong> <strong class="lg iu">文章</strong> </a>。</p><ul class=""><li id="ac18" class="nb nc it lg b lh mc ll md lp nd lt ne lx nf mb ng nh ni nj bi translated"><strong class="lg iu"><em class="mh">RDDs呢？</em> </strong>催化剂毫无办法，这是自我实现的方法。实际上Catalyst生成RDD代码，这是最终结果。此外，您需要管理序列化。</li><li id="2bf6" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated"><strong class="lg iu"><em class="mh">Catalyst是否优化数据集API？</em> </strong>是和否。最初，数据集API较慢，因为Catalyst " <em class="mh">看不到</em>"您在代码中做了什么，因为数据集语法比固定的SQL语法更广泛。但是如果你用的是最新版本的Spark ( <em class="mh"> 3.0 </em>)，那么大部分优化都是可用的。尽管如此，数据框架API在优化您的查询方面会更有效一些；对于数据集，你需要更加小心，但是如果你写了好的Spark代码，差别是很小的。此外，数据集具有<a class="ae kf" href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" rel="noopener ugc nofollow" target="_blank">更好的空间和速度性能</a>，数据集钨优化优于数据帧。我的建议是<strong class="lg iu"> Spark SQL和数据帧</strong>应该由<strong class="lg iu">数据科学家</strong>使用，他们对Spark没有深入的了解，Catalyst会为他们优化代码。<strong class="lg iu">数据集API是Scala工程师的理想选择</strong>,他们对Spark内部有很好的了解。</li></ul><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/ed8a301c16aa3186cf19d5fa41a4f6fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*suTAyCgtlpfbBGqw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">Spark APIs比较。来源:<a class="ae kf" href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" rel="noopener ugc nofollow" target="_blank">https://databricks . com/blog/2016/07/14/a-tale-of-three-Apache-spark-APIs-rdds-data frames-and-datasets . html</a></figcaption></figure><ul class=""><li id="32e5" class="nb nc it lg b lh mc ll md lp nd lt ne lx nf mb ng nh ni nj bi translated"><strong class="lg iu"> <em class="mh">如果使用数据集API，需要Kryo吗？</em> </strong>是也不是。钨将使用编码器管理你的case类，但仍有其他部分的代码需要序列化，启用Kryo会有所帮助。如果你使用数据集API，我建议跳过Kryo，除非你需要最大化性能。</li></ul><h2 id="570a" class="mo kh it bd ki mp mq dn km mr ms dp kq lp mt mu ku lt mv mw ky lx mx my lc mz bi translated">其他Spark优化</h2><ul class=""><li id="f1d3" class="nb nc it lg b lh li ll lm lp of lt og lx oh mb ng nh ni nj bi translated">尽可能使用<strong class="lg iu"><em class="mh">reduce by/aggregate by</em></strong>方法，而不是<strong class="lg iu"> <em class="mh"> groupBy </em> </strong>，因为它们减少了数据混乱。它工作得更快，因为Spark在混合数据之前知道每个分区上的公共键的组合输出。</li><li id="09b4" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated">避免<strong class="lg iu"> <em class="mh"> reduceByKey </em> </strong>当输入输出值类型不同时，用use <code class="fe oi oj ok ns b"><strong class="lg iu">aggregateByKey</strong> </code>代替；<strong class="lg iu">如果类型相同，使用<em class="mh"> reduceByKey </em>如果不相同<em class="mh">aggregate by key</em>T40】。</strong></li><li id="0c4f" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated">在重新分区和写入之前，使用<strong class="lg iu"><em class="mh">local check point</em></strong><em class="mh">()</em>方法。这是为了在混洗阶段和写入操作之间创建阶段屏障。</li><li id="8365" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated">不要用<strong class="lg iu"> <em class="mh">收</em> </strong> <em class="mh"> () </em>的方法。它会尝试将所有数据移动到驱动程序中，在那里它可能会耗尽内存并崩溃。仅将它用于调试目的或创建定义为小尺寸的广播变量。</li><li id="a877" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated">使用正确的<a class="ae kf" href="https://towardsdatascience.com/big-data-file-formats-explained-dfaabe9e8b33" rel="noopener" target="_blank"> <strong class="lg iu">文件格式</strong> </a>。<a class="ae kf" href="https://avro.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Avro </strong> </a> <strong class="lg iu">用于行级操作</strong> <a class="ae kf" href="https://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">用于列级操作</strong> </a> <strong class="lg iu">或ORC</strong>等Spark SQL。压缩数据并确保格式是可拆分的，这样它就可以被Spark阅读器并行读取。<strong class="lg iu">可拆分格式有lso、bzip2、snappy </strong>等。不可分割的gzip，zip，lz4…</li><li id="49d2" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated"><strong class="lg iu">监控和可观察性</strong>关键是火花。在写入磁盘之前，作为最后一步，使用<strong class="lg iu"> <em class="mh"> explain() </em> </strong>方法来验证我们谈到的所有优化都发生了。检查预期的Catalyst优化是否已完成，过滤器是否已推送到源，查询是否已重新安排，数据混乱是否已最小化，等等。启用<a class="ae kf" href="https://supergloo.com/spark-monitoring/spark-performance-monitoring-history-server/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Spark历史服务器</strong> </a>来分析和观察您的作业的进展，并检测任何可能导致性能问题的变化。</li></ul><p id="4018" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如您所见，所有这些优化的目标都是减少数据移动。为此，总是<strong class="lg iu">尽可能多地过滤</strong>数据。尝试<strong class="lg iu">最小化宽操作</strong>。如果数据集将被多次使用，则缓存这些数据集。在执行连接或写入磁盘之前，确保您的<strong class="lg iu">数据被均匀分区</strong>。注意数据局部性，记得使用<strong class="lg iu"> Spark UI </strong>和<em class="mh"> explain() </em>方法来理解Spark逻辑和物理计划。</p><h1 id="4abc" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><p id="ef99" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在本文中，我们展开了Spark 的<strong class="lg iu">内部，以便能够理解它如何工作以及如何优化它。关于Spark，我们可以用两个要点来总结我们的经验:</strong></p><ul class=""><li id="5acb" class="nb nc it lg b lh mc ll md lp nd lt ne lx nf mb ng nh ni nj bi translated"><strong class="lg iu">最小化数据洗牌，最大化数据局部性。</strong></li><li id="b543" class="nb nc it lg b lh nk ll nl lp nm lt nn lx no mb ng nh ni nj bi translated"><strong class="lg iu">使用数据帧或数据集高级API来利用Spark优化。</strong></li></ul><p id="da2a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果你是Scala开发人员，请记住Spark高阶函数与Scala中的不同，你真的需要理解Spark和Catalyst引擎是如何工作的。</p><p id="9221" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> <em class="mh">祝您大数据之旅好运！</em>T3】</strong></p><p id="abfc" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mh">如果你喜欢这篇文章，记得鼓掌，并关注我的更多更新！</em></p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="d351" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我希望你喜欢这篇文章。欢迎发表评论或分享这篇文章。跟随<a class="ae kf" href="https://twitter.com/JavierRamosRod" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"><em class="mh">me</em></strong></a><strong class="lg iu"><em class="mh"/></strong><em class="mh">进行未来岗位。</em></p></div></div>    
</body>
</html>