<html>
<head>
<title>What is NLP? An Introduction to Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是NLP？自然语言处理导论</h1>
<blockquote>原文：<a href="https://itnext.io/what-is-nlp-an-introduction-to-natural-language-processing-f48ff68a2e90?source=collection_archive---------2-----------------------#2021-10-13">https://itnext.io/what-is-nlp-an-introduction-to-natural-language-processing-f48ff68a2e90?source=collection_archive---------2-----------------------#2021-10-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/041963d3e09b3ebaaa576f820c29cb73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h8QRSZZsEgUQpoD0"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">阿玛多·洛雷罗在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="2d3a" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="b26b" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这篇文章的目的是给已经具备基本的<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>机器学习技能并想进入<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">机器学习<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"/>的</a>领域的人一个自然语言处理(<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">【NLP】</strong></a>)的快速介绍。</p><p id="a170" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我将从什么是<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>以及它在<strong class="lg iu">人工智能</strong> <strong class="lg iu">生态系统</strong>中的位置开始一个非常简短的概述。在本文中，我将只关注<strong class="lg iu">文本</strong> <strong class="lg iu">数据</strong>而不是音频或视频处理。</p><p id="9418" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这篇文章是我之前的<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/introduction-to-natural-language-processing-nlp-tools-for-python-cf39af3cfc64">文章</a>的摘录，其中也谈到了Python中可用的不同工具。</p><h1 id="2fa2" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">什么是NLP？</h1><p id="1302" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">简单来说，NLP就是<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">机器学习</strong> </a>专注于从<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>自然语言中提取洞察的一个领域。你的目标是让计算机理解我们自己的语言。</p><p id="2665" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">自然语言处理的一些实际例子有语音识别、翻译、情感分析、主题建模、词汇分析、实体提取等等。</p><p id="8081" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">使用所有这些工具和算法，你可以<strong class="lg iu">从自然语言中提取结构化数据</strong>，这些数据可以被计算机处理。此外，<strong class="lg iu"> NLP </strong>任务的输出通常是机器学习算法，该算法将使用这些原始数据来进行<strong class="lg iu">预测</strong>。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/4ce736fe09f1c665d5710c1e7dc60dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LE9_W6u9LQQxBvgl"/></div></div></figure><p id="d1fd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">通过<strong class="lg iu">将许多算法结合在一起</strong>，您可以提取有用的数据，这些数据可用于广泛的场景，例如:</p><ul class=""><li id="e45f" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated">欺诈检测</li><li id="4bb9" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://aylien.com/solutions/risk-intelligence" rel="noopener ugc nofollow" target="_blank">风险情报</a></li><li id="83a4" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated">电子邮件分类</li><li id="8819" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://pythonspot.com/python-sentiment-analysis/" rel="noopener ugc nofollow" target="_blank">情绪分析</a></li></ul><h1 id="72b7" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">常见NLP任务</h1><p id="9c9c" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你大概听说过<strong class="lg iu">花在<a class="ae kf" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>机器学习的80% </strong>时间是<strong class="lg iu">数据准备</strong>:数据清洗、<a class="ae kf" href="https://en.wikipedia.org/wiki/Data_wrangling" rel="noopener ugc nofollow" target="_blank">数据角力</a>、特征工程等。对于NLP来说尤其如此，因为我们的主要目标是将文本转换成计算机可以使用的数字。</p><h1 id="8a96" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">数据准备</h1><p id="a139" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这些任务包括初始准备，如加载文本数据，删除停用词，小写字母，删除页眉和页脚或任何其他无用的文本；还有更多。</p><p id="0033" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> Pandas </strong> </a>是这个探索阶段的必备工具，因为它可以让你轻松地<strong class="lg iu">实验</strong>数据，并与其他团队成员和利益相关者分享你的见解和成果。</p><p id="cf51" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">记号化</strong> </a>是另一个重要的任务，有很多记号化器可用，它的配置将取决于你要实现的目标。这个想法是将文本分割成单词，这些单词以后可以被量化并表示为数字。除了将文本拆分成单词之外，您可能还想将它们拆分成元组。<strong class="lg iu">句子分割</strong>是一个类似的过程，但目标是将文本分割成完整的句子。</p><p id="92d9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">词法分析</strong>是自然语言处理中的一项重要任务，目标是将单词还原为其基本形式。有两种方法可以做到这一点:</p><ul class=""><li id="c3cb" class="mm mn it lg b lh mc ll md lp mo lt mp lx mq mb mr ms mt mu bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a><strong class="lg iu"/>词干化是将单词还原为基本形式的过程(<em class="na">如</em>)，“close”将是“closed”、“closing”、“close”、“closer”等的词根。).我这样做是基于规则，而不是字典。换句话说，结果可能是也可能不是一个真正的单词，它更像是一个前缀。</li><li id="50d3" class="mm mn it lg b lh mv ll mw lp mx lt my lx mz mb mr ms mt mu bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">引理化</strong> </a> <strong class="lg iu"> </strong>的任务是仅移除屈折词尾，并返回单词的基本字典形式，也称为<strong class="lg iu">引理</strong>。换句话说，它从字典中产生一个真实的单词。</li></ul><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/2ccd5956a66a451ca75475dda549369e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*F65BiZ1O_Wpcp8kF"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">词干化和词汇化的区别</figcaption></figure><p id="d80b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">注意，根据你想要达到的目标<strong class="lg iu">，每种方法都有自己的优点和缺点</strong>。如果你不在乎单词的意思，而只想对文本进行分类，那么词干提取会更快更容易。</p><p id="e1ca" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一旦你已经阅读了文本数据，删除不必要的内容，删除停用词，小写的文本，标记的文本和减少单词的词条；您已经准备好应用NLP算法。请注意，根据您的任务和您计划使用的算法，数据准备会有所不同，一些任务(如词条化或小写字母)可能不需要和/或不推荐使用文本。</p><h1 id="cc43" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">特征提取和文本分类</h1><p id="3abd" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在机器学习中，给你一组列或<strong class="lg iu">特征</strong>，你用它们来预测一些结果。NLP的棘手之处在于能够从非结构化的文本中提取有意义的特征。这就是<strong class="lg iu">特征提取</strong>的目标；完成这些之后，您可以应用其他ML算法进行文本分类。</p><p id="0438" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在NLP中，你还有<a class="ae kf" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">有监督的</a>和<a class="ae kf" href="https://en.wikipedia.org/wiki/Unsupervised_learning" rel="noopener ugc nofollow" target="_blank">无监督的</a>算法；简而言之，<strong class="lg iu">监督的</strong>算法需要<strong class="lg iu">标记的数据</strong>而非监督的不需要。我们将在本文后面讨论标记数据，但为了简单起见；让我们用一个简单的例子来解释这个过程:图像你想检测垃圾邮件。您已经标记了<a class="ae kf" href="https://www.kaggle.com/balaka18/email-spam-classification-dataset-csv" rel="noopener ugc nofollow" target="_blank">数据</a>，其中包含邮件正文以及该邮件是否为垃圾邮件。</p><p id="d168" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在机器学习中，你通常会有一个列列表(<strong class="lg iu">特征</strong>)和一个<strong class="lg iu">目标</strong> (Spam/NotSpam)。例如，如果您试图预测一所房子的价格，您将拥有诸如卧室数量、大小、到学校的距离等列。这些都是可以转换成数字的数字或类别；并且可以容易地输入到机器学习算法中。但是我们如何用自然语言做到这一点呢？</p><p id="56d6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> NLP特征提取</strong>算法用于将单词转换成包含足够信息的数字表示，以便将其输入到统计模型中。为此，我们将文本转换成一种称为<strong class="lg iu">特征向量</strong>的数字表示。<a class="ae kf" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">袋词模型</strong> </a>是实现这一点的简单方法。在这种情况下，我们只是将每个单词转换成一个数字，即该单词在语料库中出现的次数。例如，如果我们有:</p><pre class="mi mj mk ml gt nb nc nd ne aw nf bi"><span id="8dc8" class="ng kh it nc b gy nh ni l nj nk">'All my cats in a row',<br/>'When my cat sits down, she looks like a Furby toy!',</span></pre><p id="15b7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这将被转换为:</p><pre class="mi mj mk ml gt nb nc nd ne aw nf bi"><span id="71d2" class="ng kh it nc b gy nh ni l nj nk">{'all': 0, 'cat': 1, 'cats': 2, 'down': 3, 'furby': 4, 'in': 5, 'like': 6, 'looks': 7, 'my': 8, 'row': 9, 'she': 10, 'sits': 11, 'toy': 12, 'when': 13 }</span></pre><p id="7712" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在我们有数字了！每个单词将是高维向量中的一列，该向量可以被馈送给<a class="ae kf" href="https://en.wikipedia.org/wiki/Neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>或任何其他算法，并用于分类/回归。我们还可以<a class="ae kf" href="https://iq.opengenus.org/normalization-in-detail/" rel="noopener ugc nofollow" target="_blank">归一化</a>这些值，以便为某些算法获得更好的结果。</p><p id="1556" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在我们的<em class="na">垃圾邮件</em>示例中，机器算法可能会“学习”到，如果出现大量类似“购买”的特定单词，那么它将是垃圾邮件。为了做到这一点，我们真的需要删除经常出现的停用词，并将词减少到其引理，这样它们就不会被计算在内，而是单独的特征。<strong class="lg iu">这就是数据准备如此重要的原因。</strong></p><p id="ca49" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">然而，词频是非常基本的，大多数时候，你会对出现在你的输入文本中但不在一般语料库中的词感兴趣。此时<a class="ae kf" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> TF-IDF </strong> </a>前来救援，定义如下:</p><blockquote class="nl"><p id="cf61" class="nm nn it bd no np nq nr ns nt nu mb dk translated"><em class="nv">TF-IDF中的高权重通过该术语在整个文档集合中的高术语频率(在给定文档中)和低文档频率来达到。</em></p></blockquote><p id="8925" class="pw-post-body-paragraph le lf it lg b lh nw lj lk ll nx ln lo lp ny lr ls lt nz lv lw lx oa lz ma mb im bi translated">它结合了两种算法:</p><p id="6efe" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">词频</strong></p><pre class="mi mj mk ml gt nb nc nd ne aw nf bi"><span id="6201" class="ng kh it nc b gy nh ni l nj nk"><strong class="nc iu">TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)</strong></span></pre><p id="619c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">逆文档频率</strong></p><pre class="mi mj mk ml gt nb nc nd ne aw nf bi"><span id="4986" class="ng kh it nc b gy nh ni l nj nk"><strong class="nc iu">IDF(t) = log_e(Total number of documents / Number of documents with term t in it)</strong></span></pre><p id="3c31" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">将文本转换为矢量的另一种技术是<a class="ae kf" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">单词嵌入</strong> </a> <strong class="lg iu"> </strong>，它将单词<strong class="lg iu"> </strong>转换为n维矢量。像“汽车”和“车辆”这样的相关单词将映射到相似的n维向量，而像“狗”和“箭头”这样的单词在向量空间中将是遥远的。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/d22634c46d17c4d4763de00caf7378dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hRqhArSPkDjK_YmS.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">自然语言处理中两种主要特征工程技术综述</figcaption></figure><p id="217d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">总而言之，<strong class="lg iu">我们使用NLP中的特征提取来从文本中提取特征，</strong>因此它们可以被馈送到用于<strong class="lg iu">文本分类的监督机器学习模型中</strong>。使用这些技术的一些例子是垃圾邮件检测或<a class="ae kf" href="https://en.wikipedia.org/wiki/Sentiment_analysis" rel="noopener ugc nofollow" target="_blank">情感分析</a>。稍后，我们将看到一些简化文本分类过程的库。</p><h1 id="7827" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">词性标注</h1><p id="981d" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">词性标注</strong> </a> ( <strong class="lg iu"> POS </strong>)是将句子中的词标注为名词、形容词、动词等的过程。这些算法自动用正确的标签标记文本数据的内容，其他算法可以使用输出来检测主题、相似性等。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/d90c2725a0a53df27d76d5e1be5da240.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/0*rlOkD3OjwwWKaZ-c"/></div></figure><p id="70ab" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Support_vector_machine" rel="noopener ugc nofollow" target="_blank"> SVM </a>、<a class="ae kf" href="https://en.wikipedia.org/wiki/Maximum_entropy_classifier" rel="noopener ugc nofollow" target="_blank">最大熵分类器</a>、<a class="ae kf" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm" rel="noopener ugc nofollow" target="_blank">最近邻法</a>等机器学习算法可以用于<strong class="lg iu"> POS </strong>，大部分可以达到95%以上的准确率。</p><h1 id="df20" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">命名实体识别(NER)</h1><p id="2996" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Named-entity_recognition" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu"/></a>是一种<strong class="lg iu">监督</strong>技术，用于将<a class="ae kf" href="https://en.wikipedia.org/wiki/Unstructured_data" rel="noopener ugc nofollow" target="_blank">非结构化文本</a>中提到的实体分类成预定义的类别，如人名、组织、地点、<a class="ae kf" href="https://en.wikipedia.org/wiki/Medical_classification" rel="noopener ugc nofollow" target="_blank">医疗代码</a>、时间表达式、数量、货币值、百分比等。目标是<strong class="lg iu">检测你的文本中的类别，这样你就可以提取关于文本内容的见解</strong>或者它是否在谈论某个主题。这对于<strong class="lg iu">风险检测</strong>、分类、欺诈检测等非常有用。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/081d36742f8b181c77c5a54a26e3781a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*j_-TP0aCrg97rkKr"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">NER在行动中检测人员，日期，组织…</figcaption></figure><p id="5c3f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这篇<a class="ae kf" rel="noopener ugc nofollow" target="_blank" href="/nlp-named-entity-recognition-ner-with-spacy-and-python-dabaf843cab2"> <strong class="lg iu">文章</strong> </a>中，我给出了一个完整的<strong class="lg iu"> NER </strong>的端到端例子。</p><h1 id="4a0b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">主题建模</h1><p id="9e66" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">主题建模</strong> </a>是一种<strong class="lg iu">无监督</strong>方法，用于发现文档集合中出现的抽象“主题”。它可以用于文本挖掘或发现隐藏的语义结构。主题建模技术产生的“主题”是一组相似的单词。</p><p id="c569" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu"> LDA </strong> </a>算法常用于主题建模。像其他非监督算法一样，你需要预先选择你想要提取多少主题。</p><h1 id="19b8" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><p id="9e04" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们可以这样总结NLP:<strong class="lg iu">它结合了一套工具和技术，将复杂的自然语言转换成机器可读的数据。</strong></p><p id="566e" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">特征工程</strong>和提取是<strong class="lg iu">自然语言处理</strong>中的关键步骤。像<strong class="lg iu"> TF-IDF </strong>和单词嵌入这样的技术被用来将单词转换成可以被统计模型用于文本分类的数字。<strong class="lg iu">命名实体提取(NER) </strong>是一个非常有用的算法，允许我们从文本数据中提取类别和其他见解。文本分类和NER都是<strong class="lg iu">监督的</strong>算法，你需要提供带标签的数据。</p><p id="cc79" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">主题建模</strong>可用于聚集文章内的相关主题，例如创建实时<a class="ae kf" href="https://aylien.com/blog/getting-started-with-real-time-topic-clustering" rel="noopener ugc nofollow" target="_blank">新闻内容主题集群</a>。这是一个<strong class="lg iu">无监督</strong>算法。</p><p id="6981" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">许多组织花费大量资源建立复杂的模型来改善结果，而不是关注数据。良好的<strong class="lg iu">数据准备和特征工程</strong>可以比任何ML算法更快更好地提高模型性能。</p><p id="bb98" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="na">记得来</em> <strong class="lg iu"> <em class="na">拍拍</em> </strong> <em class="na">如果你喜欢这篇文章和</em> <a class="ae kf" href="https://javier-ramos.medium.com/subscribe" rel="noopener"> <em class="na"> </em> <strong class="lg iu"> <em class="na">关注</em></strong><em class="na"/><strong class="lg iu"><em class="na">me</em></strong></a><em class="na">或</em><a class="ae kf" href="https://javier-ramos.medium.com/membership" rel="noopener"><em class="na">订阅</em>  </a> <em class="na">获取更多更新！</em></p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="31af" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我希望你喜欢这篇文章。欢迎发表评论或分享这篇文章。跟随<a class="ae kf" href="https://javier-ramos.medium.com/subscribe" rel="noopener"><strong class="lg iu"><em class="na">me</em></strong></a><em class="na">进行未来岗位。</em></p></div></div>    
</body>
</html>