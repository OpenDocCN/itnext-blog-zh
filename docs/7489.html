<html>
<head>
<title>Things to consider to submit Spark Jobs on Kubernetes in cluster mode</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">以集群模式在Kubernetes上提交Spark作业需要考虑的事项</h1>
<blockquote>原文：<a href="https://itnext.io/things-to-consider-to-submit-spark-jobs-on-kubernetes-766402c21716?source=collection_archive---------1-----------------------#2022-10-09">https://itnext.io/things-to-consider-to-submit-spark-jobs-on-kubernetes-766402c21716?source=collection_archive---------1-----------------------#2022-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d5afe91bb23402adb3ec115d1c5e6e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rs388Thu-xbqHOrp"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@30daysreplay?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">30日在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上播放社交媒体营销</a></figcaption></figure><p id="327d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很难在kubernetes上提交spark jobs。正如之前在kubernetes的spark上的<a class="ae kc" rel="noopener ugc nofollow" target="_blank" href="/hive-on-spark-in-kubernetes-115c8e9fa5c1"> Hive的帖子</a>中提到的，其中显示spark thrift server作为一个普通的Spark作业提交到Kubernetes上，有许多事情需要考虑。</p><p id="cfad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我将向您展示在kubernetes上以<code class="fe lb lc ld le b">cluster</code>模式提交spark作业时，应该考虑哪些事情来避免问题。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="3a0b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从下面的git repo中克隆源代码，以理解本文中显示的源代码。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="dbc3" class="lu lv iq le b gy lw lx l ly lz"><a class="ae kc" href="https://github.com/mykidong/hard-to-submit-spark-jobs-on-kubernetes.git" rel="noopener ugc nofollow" target="_blank">https://github.com/mykidong/hard-to-submit-spark-jobs-on-kubernetes.git</a></span></pre><h1 id="f92c" class="ma lv iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">为自己创造火花</h1><p id="52b2" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">在大多数情况下，可以下载的预构建的spark发行版并不适合您的需求。您必须构建spark来满足您的需求，例如，使用正确的hadoop版本，支持hive和kubernetes等。</p><h2 id="51b4" class="lu lv iq bd mb nc nd dn mf ne nf dp mj ko ng nh mn ks ni nj mr kw nk nl mv nm bi translated">检查火花源</h2><p id="65ac" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">用正确的标签检查火花源。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="7652" class="lu lv iq le b gy lw lx l ly lz">export SPARK_VERSION=3.2.2;<br/>git clone https://github.com/apache/spark.git .;<br/>git checkout v$SPARK_VERSION;</span></pre><h2 id="2f47" class="lu lv iq bd mb nc nd dn mf ne nf dp mj ko ng nh mn ks ni nj mr kw nk nl mv nm bi translated">在<code class="fe lb lc ld le b">pom.xml</code>中添加与S3相关的依赖项</h2><p id="7c76" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">如今，S3是流行的存储，所有的数据，如拼花地板，冰山表将被保存为数据湖。要使用spark jobs处理S3的数据，您必须在spark source中将S3相关的依赖项添加到<code class="fe lb lc ld le b">pom.xml</code>中，以避免spark jobs在<code class="fe lb lc ld le b">cluster</code>模式下提交到kubernetes时出现依赖项缺失问题。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="b9c3" class="lu lv iq le b gy lw lx l ly lz">&lt;dependency&gt;<br/>   &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;<br/>   &lt;artifactId&gt;hadoop-aws&lt;/artifactId&gt;<br/>   &lt;version&gt;3.2.2&lt;/version&gt;<br/>   &lt;exclusions&gt;<br/>      &lt;exclusion&gt;<br/>         &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;<br/>         &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;<br/>      &lt;/exclusion&gt;<br/>      &lt;exclusion&gt;<br/>         &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;<br/>         &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;<br/>      &lt;/exclusion&gt;<br/>      &lt;exclusion&gt;<br/>         &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;<br/>         &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;<br/>      &lt;/exclusion&gt;<br/>   &lt;/exclusions&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>   &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;<br/>   &lt;artifactId&gt;aws-java-sdk-s3&lt;/artifactId&gt;<br/>   &lt;version&gt;1.11.563&lt;/version&gt;<br/>&lt;/dependency&gt;</span></pre><p id="4dbb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些依赖项用于提交带有<code class="fe lb lc ld le b">— packages com.amazonaws:aws-java-sdk-s3:1.11.563,org.apache.hadoop:hadoop-aws:3.2.2</code>选项的spark作业。</p><h2 id="34f6" class="lu lv iq bd mb nc nd dn mf ne nf dp mj ko ng nh mn ks ni nj mr kw nk nl mv nm bi translated">更改火花-提交</h2><p id="7fed" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">即使spark作业驱动程序pod用<code class="fe lb lc ld le b">exit code: 1</code>返回了作业失败，spark提交也会用<code class="fe lb lc ld le b">exit code 0</code>返回作业成功。为了避免这种情况，需要将<code class="fe lb lc ld le b">$SPARK_HOME/bin</code>目录中的<code class="fe lb lc ld le b">spark-submit</code> shell文件修改如下。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="e6c3" class="lu lv iq le b gy lw lx l ly lz"><strong class="le ir">#!/usr/bin/env bash<br/><br/></strong>#<br/># Licensed to the Apache Software Foundation (ASF) under one or more<br/># contributor license agreements.  See the NOTICE file distributed with<br/># this work for additional information regarding copyright ownership.<br/># The ASF licenses this file to You under the Apache License, Version 2.0<br/># (the "License"); you may not use this file except in compliance with<br/># the License.  You may obtain a copy of the License at<br/>#<br/>#    http://www.apache.org/licenses/LICENSE-2.0<br/>#<br/># Unless required by applicable law or agreed to in writing, software<br/># distributed under the License is distributed on an "AS IS" BASIS,<br/># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/># See the License for the specific language governing permissions and<br/># limitations under the License.<br/>#<br/><br/>if [ -z "${SPARK_HOME}" ]; then<br/>  source "$(dirname "$0")"/find-spark-home<br/>fi<br/><br/># disable randomized hash for string in Python 3.3+<br/>export PYTHONHASHSEED=0<br/><br/><br/><br/># check deployment mode.<br/>if echo "$@" | grep -q "\-\-deploy-mode cluster";<br/>then<br/>    echo "cluster mode..";<br/>    # temp log file for spark job.<br/>    export TMP_LOG="/tmp/spark-job-log-$(date '+%Y-%m-%d-%H-%M-%S').log";<br/>    exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@" |&amp; tee ${TMP_LOG};<br/>    # when exit code 1 is contained in spark log, then return exit 1.<br/>    if cat ${TMP_LOG} | grep -q "exit code: 1";<br/>    then<br/>        echo "exit code: 1";<br/>        rm -rf ${TMP_LOG};<br/>        exit 1;<br/>    else<br/>        echo "job succeeded.";<br/>        rm -rf ${TMP_LOG};<br/>        exit 0;<br/>    fi<br/>elif echo "$@" | grep -q "\-\-conf spark.submit.deployMode=cluster";<br/>then<br/>    echo "cluster mode..";<br/>    # temp log file for spark job.<br/>    export TMP_LOG="/tmp/spark-job-log-$(date '+%Y-%m-%d-%H-%M-%S').log";<br/>    exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@" |&amp; tee ${TMP_LOG};<br/>    # when exit code 1 is contained in spark log, then return exit 1.<br/>    if cat ${TMP_LOG} | grep -q "exit code: 1";<br/>    then<br/>        echo "exit code: 1";<br/>        rm -rf ${TMP_LOG};<br/>        exit 1;<br/>    else<br/>        echo "job succeeded.";<br/>        rm -rf ${TMP_LOG};<br/>        exit 0;<br/>    fi<br/>else<br/>    echo "client mode..";<br/>    exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"<br/>fi</span></pre><p id="2e4e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看看下面的部分被替换成了<code class="fe lb lc ld le b">exec “${SPARK_HOME}”/bin/spark-class org.apache.spark.deploy.SparkSubmit “$@”</code>的spark-submit文件的行尾。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="5304" class="lu lv iq le b gy lw lx l ly lz"># check deployment mode.<br/>if echo "$@" | grep -q "\-\-deploy-mode cluster";<br/>then<br/>    echo "cluster mode..";<br/>    # temp log file for spark job.<br/>    export TMP_LOG="/tmp/spark-job-log-$(date '+%Y-%m-%d-%H-%M-%S').log";<br/>    exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@" |&amp; tee ${TMP_LOG};<br/>    # when exit code 1 is contained in spark log, then return exit 1.<br/>    if cat ${TMP_LOG} | grep -q "exit code: 1";<br/>    then<br/>        echo "exit code: 1";<br/>        rm -rf ${TMP_LOG};<br/>        exit 1;<br/>    else<br/>        echo "job succeeded.";<br/>        rm -rf ${TMP_LOG};<br/>        exit 0;<br/>    fi<br/>elif echo "$@" | grep -q "\-\-conf spark.submit.deployMode=cluster";<br/>then<br/>    echo "cluster mode..";<br/>    # temp log file for spark job.<br/>    export TMP_LOG="/tmp/spark-job-log-$(date '+%Y-%m-%d-%H-%M-%S').log";<br/>    exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@" |&amp; tee ${TMP_LOG};<br/>    # when exit code 1 is contained in spark log, then return exit 1.<br/>    if cat ${TMP_LOG} | grep -q "exit code: 1";<br/>    then<br/>        echo "exit code: 1";<br/>        rm -rf ${TMP_LOG};<br/>        exit 1;<br/>    else<br/>        echo "job succeeded.";<br/>        rm -rf ${TMP_LOG};<br/>        exit 0;<br/>    fi<br/>else<br/>    echo "client mode..";<br/>    exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"<br/>fi</span></pre><p id="bae7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果spark作业以<code class="fe lb lc ld le b">cluster</code>模式提交，<code class="fe lb lc ld le b">spark-submit</code> shell将检查从spark作业驱动程序箱返回的日志行是否包含<code class="fe lb lc ld le b">exit code: 1</code>。在<code class="fe lb lc ld le b">client</code>模式下提交spark作业时，会执行<code class="fe lb lc ld le b">exec “${SPARK_HOME}”/bin/spark-class org.apache.spark.deploy.SparkSubmit “$@”</code>原来的spark提交命令。</p><h2 id="e5b8" class="lu lv iq bd mb nc nd dn mf ne nf dp mj ko ng nh mn ks ni nj mr kw nk nl mv nm bi translated">包装火花分布</h2><p id="3029" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">你必须用选项构建spark源代码，例如，hive，spark thrift server，kubernetes对3.2.2版本hadoop的支持。在JDK 1.8中运行以下命令来构建和打包spark。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="e66e" class="lu lv iq le b gy lw lx l ly lz">export MAVEN_OPTS="-Xss64m -Xmx2500m -XX:ReservedCodeCacheSize=1g"<br/>./dev/make-distribution.sh --name custom-spark --pip --tgz -DskipTests=true -Dhadoop.version=3.2.2 -Phive -Phive-thriftserver -Pkubernetes;</span></pre><h2 id="5d61" class="lu lv iq bd mb nc nd dn mf ne nf dp mj ko ng nh mn ks ni nj mr kw nk nl mv nm bi translated">构建并推送Spark Docker映像</h2><p id="f9a0" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">经过上面的包装火花，<code class="fe lb lc ld le b">spark-3.2.2-bin-custom-spark.tgz</code>就会被创造出来。Untar它，并建立这样的火花码头形象。把码头报告换成你的。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="4e5c" class="lu lv iq le b gy lw lx l ly lz"># spark version.<br/>export SPARK_VERSION=3.2.2;<br/><br/># create and push spark and spark-py docker image.<br/>bin/docker-image-tool.sh -r docker.io/cloudcheflabs -t v$SPARK_VERSION -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build;<br/>bin/docker-image-tool.sh -r cloudcheflabs -t v$SPARK_VERSION -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile push;</span></pre><h1 id="f714" class="ma lv iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">上传火花uberjar到S3桶</h1><p id="c421" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">要提交spark作业，你编译的java代码需要像这样打包成uberjar。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="1bac" class="lu lv iq le b gy lw lx l ly lz">mvn -e -DskipTests=true clean install shade:shade;</span></pre><p id="fd37" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将打包的spark uberjar上传到S3 bucket，以便在向kubernetes提交spark作业时使用类似于<code class="fe lb lc ld le b">s3a://your-bucket/spark-jobs/your-spark-uberjar.jar</code>的文件选项。例如，可以使用MinIO CLI <code class="fe lb lc ld le b">mc</code>将uberjar上传到S3 bucket。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="b8de" class="lu lv iq le b gy lw lx l ly lz">mc cp <!-- -->your-spark-uberjar.jar<!-- --> your-s3/<!-- -->your-bucket<!-- -->/spark-jobs/;</span></pre><p id="3933" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，你可以像这样在kubernetes上以<code class="fe lb lc ld le b">cluster</code>模式提交spark jobs。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="0dc8" class="lu lv iq le b gy lw lx l ly lz">export SPARK_VERSION=3.2.2<br/>export SPARK_IMAGE=cloudcheflabs/spark:v${SPARK_VERSION};<br/>export SPARK_MASTER=k8s://https://your-kubernetes-master<br/>export NAMESPACE=spark;<br/>export S3_BUCKET=your-bucket;<br/>export S3_ACCESS_KEY=xxx;<br/>export S3_SECRET_KEY=xxx;<br/>export S3_ENDPOINT=https://s3-endpoint-xxx;<br/>...<br/><br/>spark-submit \<br/>--master $SPARK_MASTER \<br/>--deploy-mode cluster \<br/>--name your-spark-job-name \<br/>--class YourClass \<br/>--packages com.amazonaws:aws-java-sdk-s3:1.11.563,org.apache.hadoop:hadoop-aws:3.2.2 \<br/>...<br/>--conf spark.hadoop.fs.defaultFS=s3a://${S3_BUCKET} \<br/>--conf spark.hadoop.fs.s3a.access.key=${S3_ACCESS_KEY} \<br/>--conf spark.hadoop.fs.s3a.secret.key=${S3_SECRET_KEY} \<br/>--conf spark.hadoop.fs.s3a.connection.ssl.enabled=true \<br/>--conf spark.hadoop.fs.s3a.endpoint=$S3_ENDPOINT \<br/>--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \<br/>--conf spark.hadoop.fs.s3a.fast.upload=true \<br/>--conf spark.hadoop.fs.s3a.path.style.access=true \<br/>--conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \<br/>--conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \<br/>--conf spark.executor.instances=3 \<br/>--conf spark.executor.memory=10G \<br/>--conf spark.executor.cores=2 \<br/>--conf spark.driver.memory=5G \<br/>s3a://your-bucket/spark-jobs/your-spark-uberjar.jar;</span></pre><p id="89c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看一下spark job uberjar <code class="fe lb lc ld le b">s3a://your-bucket/spark-jobs/your-spark-uberjar.jar</code>的执行文件。</p><h1 id="1bc8" class="ma lv iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">将Kubernetes上提交的Spark作业与工作流集成</h1><p id="e32d" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">Spark作业可以从运行在Kubernetes上的类似<code class="fe lb lc ld le b">Airflow</code>的工作流任务中提交。对于在kubernetes上从workflow提交spark作业的更方便的方法，我们将使用在kubernetes上运行的Livy。</p><h2 id="c94c" class="lu lv iq bd mb nc nd dn mf ne nf dp mj ko ng nh mn ks ni nj mr kw nk nl mv nm bi translated">使用Livy</h2><p id="2ff2" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">Spark作业可以通过Livy提供的REST API提交。</p><p id="571c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来看看李维的<code class="fe lb lc ld le b">Dockerfile</code>。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="6451" class="lu lv iq le b gy lw lx l ly lz">FROM cloudcheflabs/spark:v3.2.2<br/><br/><br/># env.<br/>ENV <em class="nn">LIVY_HOME </em>/opt/livy<br/>ENV <em class="nn">LIVY_USER </em>livy<br/>ENV <em class="nn">KUBECONFIG </em>${<em class="nn">LIVY_HOME</em>}/.kube/config<br/><br/><br/>RUN useradd -ms /bin/bash -d ${<em class="nn">LIVY_HOME</em>} ${<em class="nn">LIVY_USER</em>}<br/><br/><br/># install livy.<br/>RUN set -eux; \<br/>    apt install -y unzip curl; \<br/>    mkdir -p ${<em class="nn">LIVY_HOME</em>}/.kube; \<br/>    cd ${<em class="nn">LIVY_HOME</em>}; \<br/>    curl -L -O https://dlcdn.apache.org/incubator/livy/0.7.1-incubating/apache-livy-0.7.1-incubating-bin.zip; \<br/>    unzip apache-livy-0.7.1-incubating-bin.zip; \<br/>    cp -rv apache-livy-0.7.1-incubating-bin/* .; \<br/>    rm -rf apache-livy-0.7.1-incubating-bin/; \<br/>    rm -rf apache-livy-0.7.1-incubating-bin.zip;<br/><br/># add run shell.<br/>ADD run-livy.sh ${<em class="nn">LIVY_HOME</em>}<br/><br/># add  kubeconfig.<br/>ADD config ${<em class="nn">LIVY_HOME</em>}/.kube<br/><br/># add permissions.<br/>RUN chown ${<em class="nn">LIVY_USER</em>}: -R ${<em class="nn">LIVY_HOME</em>}<br/><br/># change work directory.<br/>USER ${<em class="nn">LIVY_USER</em>}<br/>RUN chmod +x ${<em class="nn">LIVY_HOME</em>}/*.sh<br/>WORKDIR ${<em class="nn">LIVY_HOME</em>}</span></pre><p id="a6fe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Livy <code class="fe lb lc ld le b">Dockerfile</code>扩展了前面部分创建的<code class="fe lb lc ld le b">cloudcheflabs/spark:v3.2.2</code>的映像，这意味着通过REST API的spark提交请求将在前面部分修改并构建为spark docker映像的<code class="fe lb lc ld le b">spark-submit</code>的情况下执行。</p><p id="0a4f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要在kubernetes上运行Livy，首先要像这样构建livy docker映像。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="fa5f" class="lu lv iq le b gy lw lx l ly lz">cd ~/hard-to-submit-spark-jobs-on-kubernetes/components/livy/docker;<br/><br/><br/>chmod a+x build-docker.sh;<br/><br/>export TAG=0.7.1;<br/><br/>./build-docker.sh \<br/>--image=cloudcheflabs/livy:${TAG} \<br/>;</span></pre><p id="a8f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用你的替换docker repo。</p><p id="e7c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像这样用舵轮图部署李维。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="4b3b" class="lu lv iq le b gy lw lx l ly lz">## install livy with helm.<br/>cd ~/hard-to-submit-spark-jobs-on-kubernetes/components/livy/chart;<br/><br/>export TAG=0.7.1;<br/><br/>helm upgrade \<br/>livy \<br/>--install \<br/>--create-namespace \<br/>--namespace livy \<br/>--set image=cloudcheflabs/livy:${TAG} \<br/>--set livy.storageClass=nfs \<br/>.;</span></pre><p id="7082" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lb lc ld le b">ReadWriteMany</code>livy PVC需要使用支持的存储类，如<code class="fe lb lc ld le b">nfs</code>。</p><p id="529e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">检查livy pod是否正在运行。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="2aa9" class="lu lv iq le b gy lw lx l ly lz">kubectl get po -n livy<br/>NAME                   READY   STATUS    RESTARTS   AGE<br/>livy-c6d54ff44-7lr45  1/1     Running   0          10h</span></pre><p id="55fb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并确保修改后的<code class="fe lb lc ld le b">spark-submit</code>将用于在Livy pod的kubernetes上提交spark作业。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="1a9e" class="lu lv iq le b gy lw lx l ly lz">kubectl exec -it livy-c6d54ff44-7lr45 -n livy -- cat /opt/spark/bin/spark-submit</span><span id="7406" class="lu lv iq le b gy no lx l ly lz">#!/usr/bin/env bash<br/><br/>#<br/># Licensed to the Apache Software Foundation (ASF) under one or more<br/># contributor license agreements.  See the NOTICE file distributed with<br/># this work for additional information regarding copyright ownership.<br/># The ASF licenses this file to You under the Apache License, Version 2.0<br/># (the "License"); you may not use this file except in compliance with<br/># the License.  You may obtain a copy of the License at<br/>#<br/>#    http://www.apache.org/licenses/LICENSE-2.0<br/>#<br/># Unless required by applicable law or agreed to in writing, software<br/># distributed under the License is distributed on an "AS IS" BASIS,<br/># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/># See the License for the specific language governing permissions and<br/># limitations under the License.<br/>#<br/><br/>if [ -z "${SPARK_HOME}" ]; then<br/>  source "$(dirname "$0")"/find-spark-home<br/>fi<br/><br/># disable randomized hash for string in Python 3.3+<br/>export PYTHONHASHSEED=0<br/><br/># check deployment mode.<br/>if echo "$@" | grep -q "\-\-deploy-mode cluster";<br/>then<br/>    echo "cluster mode..";<br/>    # temp log file for spark job.<br/>    export TMP_LOG="/tmp/spark-job-log-$(date '+%Y-%m-%d-%H-%M-%S').log";<br/>    exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@" |&amp; tee ${TMP_LOG};<br/>    # when exit code 1 is contained in spark log, then return exit 1.<br/>    if cat ${TMP_LOG} | grep -q "exit code: 1";<br/>    then<br/>        echo "exit code: 1";<br/>        rm -rf ${TMP_LOG};<br/>        exit 1;<br/>    else<br/>        echo "job succeeded.";<br/>        rm -rf ${TMP_LOG};<br/>        exit 0;<br/>    fi<br/>elif echo "$@" | grep -q "\-\-conf spark.submit.deployMode=cluster";<br/>then<br/>    echo "cluster mode..";<br/>    # temp log file for spark job.<br/>    export TMP_LOG="/tmp/spark-job-log-$(date '+%Y-%m-%d-%H-%M-%S').log";<br/>    exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@" |&amp; tee ${TMP_LOG};<br/>    # when exit code 1 is contained in spark log, then return exit 1.<br/>    if cat ${TMP_LOG} | grep -q "exit code: 1";<br/>    then<br/>        echo "exit code: 1";<br/>        rm -rf ${TMP_LOG};<br/>        exit 1;<br/>    else<br/>        echo "job succeeded.";<br/>        rm -rf ${TMP_LOG};<br/>        exit 0;<br/>    fi<br/>else<br/>    echo "client mode..";<br/>    exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"<br/>fi</span></pre><h2 id="88de" class="lu lv iq bd mb nc nd dn mf ne nf dp mj ko ng nh mn ks ni nj mr kw nk nl mv nm bi translated">创建DAG来调用气流中的Livy</h2><p id="995f" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">我们需要创建一个DAG来调用Livy REST API提交kubernetes上的spark作业。<code class="fe lb lc ld le b">LivyOperator</code>用于在kubernetes上提交spark作业。</p><pre class="lm ln lo lp gt lq le lr ls aw lt bi"><span id="b473" class="lu lv iq le b gy lw lx l ly lz">now = datetime.now()<br/>date_time = now.strftime("%m/%d/%Y, %H:%M:%S")<br/>livy_job_name = "your-spark-job-example-" + date_time<br/>t2 = LivyOperator(<br/>        task_id="spark-job-example",<br/>        livy_conn_id='livy_default',<br/>        polling_interval=5,<br/>        file="s3a://your-bucket/spark-jobs/your-spark-uberjar.jar",<br/>        class_name="YourClass",<br/>        args=[<br/>            "xxx",<br/>            "xxx",<br/>            "xxx",<br/>            "xxx",<br/>            "xxx"<br/>        ],<br/>        name=livy_job_name,<br/>        driver_memory="5G",<br/>        driver_cores=1,<br/>        executor_memory="10G",<br/>        executor_cores=2,<br/>        num_executors=3,<br/>        conf={<br/>            "spark.submit.deployMode": "cluster",<br/>            "spark.jars.packages": "com.amazonaws:aws-java-sdk-s3:1.11.563,org.apache.hadoop:hadoop-aws:3.2.2",<br/>            ...<br/>            "spark.hadoop.fs.defaultFS": "s3a://your-bucket",<br/>            "spark.hadoop.fs.s3a.access.key": "xxx",<br/>            "spark.hadoop.fs.s3a.secret.key": "xxx",<br/>            "spark.hadoop.fs.s3a.connection.ssl.enabled": "true",<br/>            "spark.hadoop.fs.s3a.endpoint": "https://xxx",<br/>            "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",<br/>            "spark.hadoop.fs.s3a.fast.upload": "true",<br/>            "spark.hadoop.fs.s3a.path.style.access": "true",<br/>            "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider",<br/>            "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp"<br/>        }<br/>    )</span></pre><p id="e762" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您需要添加部署模式<code class="fe lb lc ld le b">“spark.submit.deployMode”: “cluster”</code>的配置，并添加<code class="fe lb lc ld le b">livy_default</code>与<code class="fe lb lc ld le b">livy-service.livy.svc</code>主机和<code class="fe lb lc ld le b">8998</code>端口的连接。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="f2d6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样。</p></div></div>    
</body>
</html>