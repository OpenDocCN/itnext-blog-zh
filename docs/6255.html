<html>
<head>
<title>Stream Processing with Apache Spark, Kafka, Avro, and Apicurio Registry on AWS using Amazon MSK and EMR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用亚马逊MSK和EMR在AWS上使用Apache Spark、Kafka、Avro和Apicurio Registry进行流处理</h1>
<blockquote>原文：<a href="https://itnext.io/stream-processing-with-apache-spark-kafka-avro-and-apicurio-registry-on-amazon-emr-and-amazon-13080defa3be?source=collection_archive---------0-----------------------#2021-09-30">https://itnext.io/stream-processing-with-apache-spark-kafka-avro-and-apicurio-registry-on-amazon-emr-and-amazon-13080defa3be?source=collection_archive---------0-----------------------#2021-09-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="38f5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在事件流分析架构中使用注册表将模式与消息分离</h2></div><p id="a18f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上一篇文章中，<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/getting-started-with-spark-structured-streaming-and-kafka-on-aws-using-amazon-msk-and-amazon-emr-91b1f2ef0162">使用亚马逊MSK和亚马逊EMR </a>开始使用AWS上的Spark结构化流和Kafka，我们了解了亚马逊EMR上的Apache Spark和Spark结构化流(<em class="lf">fka Amazon Elastic MapReduce</em>)以及针对Apache Kafka(亚马逊MSK)的亚马逊托管流。我们使用批处理和流式查询从Kafka消费消息和向Kafka发布消息。在那篇文章中，我们使用在每个PySpark脚本中定义为<a class="ae le" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html" rel="noopener ugc nofollow" target="_blank"> StructType </a> ( <code class="fe lg lh li lj b">pyspark.sql.types.StructType</code>)的模式来序列化和反序列化来自JSON的消息。同样，我们为从亚马逊S3读取和写入的CSV格式的数据文件构建了类似的结构。</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="9c56" class="ls lt it lj b gy lu lv l lw lx">schema = StructType([<br/>    StructField("payment_id", IntegerType(), False),<br/>    StructField("customer_id", IntegerType(), False),<br/>    StructField("amount", FloatType(), False),<br/>    StructField("payment_date", TimestampType(), False),<br/>    StructField("city", StringType(), True),<br/>    StructField("district", StringType(), True),<br/>    StructField("country", StringType(), False),<br/>])</span></pre><p id="81fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇后续文章中，我们将以Apache Avro格式在亚马逊MSK上读写消息。我们将把Avro格式的Kafka消息的键和值模式存储在Apicurio Registry中，并检索模式，而不是在PySpark脚本中对模式进行硬编码。我们还将使用注册表来存储CSV格式数据文件的模式。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ly"><img src="../Images/7d4d0f6e25f62152f736e82d7cd968ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGnUIP5Xj3w1ODheHBEl-A.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">请注意本文演示的体系结构中添加了注册中心</figcaption></figure><h1 id="64ab" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">视频演示</h1><p id="6934" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">除了这篇文章，YouTube<a class="ae le" href="https://youtu.be/xthbYl7xC74" rel="noopener ugc nofollow" target="_blank">上还有一个视频演示。</a></p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">为了获得最佳效果，请在YouTube<a class="ae le" href="https://youtu.be/xthbYl7xC74" rel="noopener ugc nofollow" target="_blank">上以1080p高清观看</a></figcaption></figure><h1 id="dd48" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">技术</h1><p id="9bba" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">在上一篇文章<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/getting-started-with-spark-structured-streaming-and-kafka-on-aws-using-amazon-msk-and-amazon-emr-91b1f2ef0162">使用亚马逊MSK和亚马逊EMR在AWS上开始使用Spark结构化流和Kafka</a>中，我们了解了Apache Spark、Apache Kafka、亚马逊EMR和亚马逊MSK。</p><div class="ni nj gp gr nk nl"><a rel="noopener  ugc nofollow" target="_blank" href="/getting-started-with-spark-structured-streaming-and-kafka-on-aws-using-amazon-msk-and-amazon-emr-91b1f2ef0162"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">使用亚马逊MSK和亚马逊EMR在AWS上开始使用Spark结构化流和Kafka</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">使用批处理查询和Spark结构化流，使用Apache Kafka探索Apache Spark</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">itnext.io</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz me nl"/></div></div></a></div><p id="35d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在之前的一篇文章<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/hydrating-a-data-lake-using-log-based-change-data-capture-cdc-with-debezium-apicurio-and-kafka-799671e0012f">中，我们探讨了Apache Avro和Apicurio Registry。</a></p><div class="ni nj gp gr nk nl"><a rel="noopener  ugc nofollow" target="_blank" href="/hydrating-a-data-lake-using-log-based-change-data-capture-cdc-with-debezium-apicurio-and-kafka-799671e0012f"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">通过Debezium、Apicurio和Kafka使用基于日志的变更数据捕获(CDC)来补充数据湖…</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">使用亚马逊MSK、Apache Kafka Connect、Debezium、Apicurio Registry和…将数据从亚马逊RDS导入亚马逊S3</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">itnext.io</p></div></div><div class="nu l"><div class="oa l nw nx ny nu nz me nl"/></div></div></a></div><h2 id="8d14" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">阿帕奇火花</h2><p id="e532" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">根据<a class="ae le" href="https://spark.apache.org/docs/latest/index.html" rel="noopener ugc nofollow" target="_blank">文档</a>，Apache Spark是一个用于大规模数据处理的统一分析引擎。Spark提供了Java、Scala、Python (PySpark)和r的高级API，Spark提供了支持通用执行图(<em class="lf">又名有向无环图或Dag</em>)的优化引擎。此外，Spark支持一组丰富的高级工具，包括用于SQL和结构化数据处理的Spark SQL，用于机器学习的MLlib，用于图形处理的GraphX，以及用于增量计算和流处理的结构化流。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi om"><img src="../Images/be5e9807273fac1cecdc1c04ef268a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxG7RAvda8tWiHYmTaVk3Q.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">根据<a class="ae le" href="https://trends.google.com/trends/explore?geo=US&amp;q=%2Fm%2F0ndhxqz,PySpark,%2Fm%2F0g55vt5,%2Fg%2F11bywl5x_j" rel="noopener ugc nofollow" target="_blank">谷歌趋势</a>，随着时间的推移，人们对Apache Spark和PySpark的兴趣比Hive和Presto大</figcaption></figure><h2 id="4573" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">火花结构化流</h2><p id="c196" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">根据<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">文档</a>的说法，Spark结构化流是一个基于Spark SQL引擎的可扩展和容错的流处理引擎。您可以像表达静态数据上的批处理计算一样表达您的流计算。Spark SQL引擎将不断递增地运行它，并随着流数据的不断到达更新最终结果。简而言之，结构化流提供了快速、可伸缩、容错、端到端、恰好一次的流处理，而无需用户对流进行推理。</p><h2 id="4973" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">阿帕奇Avro</h2><p id="5817" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">Apache Avro将自己描述为一个数据序列化系统。<a class="ae le" href="https://avro.apache.org/docs/current/" rel="noopener ugc nofollow" target="_blank"> Apache Avro </a>是一种紧凑、快速、<a class="ae le" href="https://en.wikipedia.org/wiki/Comparison_of_data-serialization_formats#Overview" rel="noopener ugc nofollow" target="_blank">的二进制数据格式</a>类似于<a class="ae le" href="https://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Parquet </a>、<a class="ae le" href="https://thrift.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Thrift </a>、MongoDB的<a class="ae le" href="https://docs.mongodb.com/manual/reference/bson-types/" rel="noopener ugc nofollow" target="_blank"> BSON </a>和Google的<a class="ae le" href="https://en.wikipedia.org/wiki/Protocol_Buffers" rel="noopener ugc nofollow" target="_blank">协议缓冲区</a> ( <em class="lf"> protobuf </em>)。然而，与列存储格式如<a class="ae le" href="https://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Parquet </a>和<a class="ae le" href="https://orc.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache ORC </a>相比，Apache Avro是一种基于行的存储格式。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi on"><img src="../Images/c04a5d1885bcc4e07f9684e235c48f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MvgLqcwv0ui-oQe0-QVa9w.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">序列化的Avro格式消息，其键和值以[非人类可读]二进制格式显示</figcaption></figure><p id="f9ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Avro依赖于模式。读取Avro数据时，写入数据时使用的模式始终存在。根据<a class="ae le" href="https://avro.apache.org/docs/current/" rel="noopener ugc nofollow" target="_blank">文档</a>，模式允许每个数据被写入而没有每个值的开销，使得序列化快速且小。模式还有助于使用动态脚本语言，因为数据及其模式是完全自描述的。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi oo"><img src="../Images/0f5040850863eb6d15a19aa7d83cf63d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uNrKsANtlJ1FQcGRonVtwg.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">根据<a class="ae le" href="https://trends.google.com/trends/explore?geo=US&amp;q=%2Fg%2F11c54dt85y,%2Fm%2F0by0k1s,%2Fg%2F11gy75t900" rel="noopener ugc nofollow" target="_blank"> Google Trends </a>，随着时间的推移，人们对Apache Avro的兴趣超过了Parquet和ORC</figcaption></figure><h2 id="16ff" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">Apicurio注册表</h2><p id="73d1" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">我们可以通过使用模式注册中心，例如<a class="ae le" href="https://docs.confluent.io/platform/current/schema-registry/index.html" rel="noopener ugc nofollow" target="_blank">汇合模式注册中心</a>或<a class="ae le" href="https://www.apicur.io/registry/" rel="noopener ugc nofollow" target="_blank"> Apicurio注册中心</a>，将数据从模式中分离出来。根据Apicurio的说法，在消息和事件流架构中，发布到主题和队列的数据通常必须使用模式进行序列化或验证(例如，<a class="ae le" href="https://avro.apache.org/docs/current/" rel="noopener ugc nofollow" target="_blank"> Apache Avro </a>、<a class="ae le" href="https://json-schema.org/" rel="noopener ugc nofollow" target="_blank"> JSON模式</a>，或<a class="ae le" href="https://developers.google.com/protocol-buffers" rel="noopener ugc nofollow" target="_blank"> Google协议缓冲区</a>)。当然，模式可以打包在每个应用程序中。尽管如此，在外部系统[schema registry]中注册模式，然后从每个应用程序中引用它们通常是更好的架构模式。</p><blockquote class="op oq or"><p id="f6ff" class="ki kj lf kk b kl km ju kn ko kp jx kq os ks kt ku ot kw kx ky ou la lb lc ld im bi translated"><em class="it">在外部系统中注册模式，然后从每个应用程序中引用它们通常是更好的架构模式。</em></p></blockquote><h2 id="35b6" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">亚马逊电子病历</h2><p id="fe3f" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">根据AWS <a class="ae le" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank">文档</a>，Amazon EMR(<em class="lf">fka Amazon Elastic MapReduce</em>)是一个基于云的大数据平台，使用开源工具处理海量数据，如<a class="ae le" href="https://aws.amazon.com/emr/features/spark/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>、<a class="ae le" href="https://hadoop.apache.org/" rel="noopener ugc nofollow" target="_blank"> Hadoop </a>、<a class="ae le" href="https://aws.amazon.com/emr/features/hive/" rel="noopener ugc nofollow" target="_blank"> Hive </a>、<a class="ae le" href="https://aws.amazon.com/emr/features/hbase/" rel="noopener ugc nofollow" target="_blank"> HBase </a>、<a class="ae le" href="https://aws.amazon.com/blogs/big-data/use-apache-flink-on-amazon-emr/" rel="noopener ugc nofollow" target="_blank"> Flink </a>、<a class="ae le" href="https://aws.amazon.com/emr/features/hudi/" rel="noopener ugc nofollow" target="_blank">胡迪</a>和<a class="ae le" href="https://aws.amazon.com/emr/features/presto/" rel="noopener ugc nofollow" target="_blank"> Presto【等Amazon EMR是一项完全托管的AWS服务，通过自动执行配置容量和调整集群等耗时的任务，可以轻松设置、操作和扩展您的大数据环境。</a></p><p id="ae91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks.html" rel="noopener ugc nofollow" target="_blank">亚马逊EMR on EKS </a>，亚马逊EMR自2020年12月起的部署选项，允许你在<a class="ae le" href="https://aws.amazon.com/eks/" rel="noopener ugc nofollow" target="_blank">亚马逊弹性Kubernetes服务</a>(亚马逊EKS)上运行亚马逊EMR。借助EKS部署选项，您可以专注于运行分析工作负载，而Amazon EMR on EKS为开源应用构建、配置和管理容器。</p><p id="f63a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不熟悉Amazon EMR for Spark，特别是PySpark，我推荐最近的两部分系列文章，<a class="ae le" href="https://medium.com/swlh/running-pyspark-applications-on-amazon-emr-e536b7a865ca" rel="noopener">在Amazon EMR上运行PySpark应用程序:在Amazon Elastic MapReduce上与PySpark交互的方法</a>。</p><div class="ni nj gp gr nk nl"><a href="https://medium.com/swlh/running-pyspark-applications-on-amazon-emr-e536b7a865ca" rel="noopener follow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">在Amazon EMR上运行PySpark应用程序</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">Amazon Elastic MapReduce上与PySpark交互的方法</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">medium.com</p></div></div><div class="nu l"><div class="ov l nw nx ny nu nz me nl"/></div></div></a></div><h2 id="7451" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">阿帕奇卡夫卡</h2><p id="f671" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">根据<a class="ae le" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank">文档</a>，Apache Kafka是一个开源的分布式事件流平台，被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。</p><h2 id="a498" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">亚马逊MSK</h2><p id="772a" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">Apache Kafka集群在生产环境中的设置、扩展和管理极具挑战性。根据AWS <a class="ae le" href="https://aws.amazon.com/msk/" rel="noopener ugc nofollow" target="_blank">文档</a>，亚马逊MSK是一项完全托管的AWS服务，让你可以轻松构建和运行使用<a class="ae le" href="https://aws.amazon.com/streaming-data/what-is-kafka/" rel="noopener ugc nofollow" target="_blank"> Apache Kafka </a>处理流媒体数据的应用程序。有了亚马逊MSK，您可以使用原生Apache Kafka APIs来填充数据湖，将更改传入和传出数据库，并支持机器学习和分析应用程序。</p><h1 id="3665" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">先决条件</h1><p id="6f9d" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">与上一篇文章类似，这篇文章将主要关注在Amazon EMR上配置和运行Apache Spark作业。为了跟进，您需要在AWS上部署和配置以下资源:</p><ol class=""><li id="afee" class="ow ox it kk b kl km ko kp kr oy kv oz kz pa ld pb pc pd pe bi translated">亚马逊S3桶(容纳所有Spark/EMR资源)；</li><li id="f8dd" class="ow ox it kk b kl pf ko pg kr ph kv pi kz pj ld pb pc pd pe bi translated">亚马逊MSK集群(使用<a class="ae le" href="https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html#how-to-use-iam-access-control" rel="noopener ugc nofollow" target="_blank"> IAM访问控制</a>)；</li><li id="a5ff" class="ow ox it kk b kl pf ko pg kr ph kv pi kz pj ld pb pc pd pe bi translated">安装了Kafka APIs并且能够连接到亚马逊MSK的亚马逊EKS容器或EC2实例；</li><li id="597d" class="ow ox it kk b kl pf ko pg kr ph kv pi kz pj ld pb pc pd pe bi translated">安装了Apicurio Registry的亚马逊EKS容器或EC2实例，能够连接到亚马逊MSK ( <em class="lf">如果使用Kafka进行后端存储</em>)并被亚马逊EMR访问；</li><li id="cf9a" class="ow ox it kk b kl pf ko pg kr ph kv pi kz pj ld pb pc pd pe bi translated">确保亚马逊MSK配置有<code class="fe lg lh li lj b">auto.create.topics.enable=true</code>；该设置默认为<code class="fe lg lh li lj b">false</code>；</li></ol><p id="c376" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的架构图显示，该演示在同一个AWS账户和AWS区域<code class="fe lg lh li lj b">us-east-1</code>内使用了三个独立的VPC，分别用于亚马逊EMR、亚马逊MSK和亚马逊EKS。使用<a class="ae le" href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html" rel="noopener ugc nofollow" target="_blank"> VPC对等</a>连接三个VPC。确保您在亚马逊EMR、亚马逊MSK和亚马逊EKS安全组中公开了正确的入口端口和相应的CIDR范围。为了增加安全性和节约成本，使用一个<a class="ae le" href="https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html" rel="noopener ugc nofollow" target="_blank"> VPC端点</a>用于亚马逊EMR和亚马逊S3之间的私人通信。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ly"><img src="../Images/7d4d0f6e25f62152f736e82d7cd968ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGnUIP5Xj3w1ODheHBEl-A.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">这篇文章演示的高级架构</figcaption></figure><h1 id="8914" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">源代码</h1><p id="6875" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">这篇文章和亚马逊MSK系列的前三篇文章的所有源代码，包括这里演示的Python和PySpark脚本，都是开源的，位于<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上。</p><div class="ni nj gp gr nk nl"><a href="https://github.com/garystafford/kafka-connect-msk-demo" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">GitHub—garystaf得起/kafka-connect-msk-demo:</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">在这篇文章中，使用变更数据捕获(CDC)、Apache Kafka和Kubernetes on AWS-GitHub来补充数据湖…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">github.com</p></div></div><div class="nu l"><div class="pk l nw nx ny nu nz me nl"/></div></div></a></div><h1 id="354e" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">目标</h1><p id="9b0f" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">我们将运行一个Spark结构化流PySpark作业，以使用来自Apache Kafka的实时销售数据的模拟事件流。然后，我们将用销售区域来丰富(<em class="lf"> join </em>)销售数据，并在一个滑动事件时间窗口内按区域合计销售额和订单量。接下来，我们将把这些聚合结果流回Kafka。最后，我们将使用一个批处理查询来消费Kafka的汇总销售结果，并在控制台中显示它们。</p><p id="c8d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Kafka消息将以Apache Avro格式编写。Kafka主题消息键和值的模式以及CSV格式的销售和销售区域数据的模式都将存储在Apricurio Registry中。Python和PySpark脚本将使用Apricurio Registry的REST API来读写Avro模式工件。</p><p id="0135" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将以Avro格式编写Kafka消息密钥，并将密钥的Avro模式存储在注册表中。这只是为了演示，并不是必需的。Kafka消息密钥不是必需的，也没有必要在Avro中存储密钥和值。</p><p id="2882" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">模式演化、兼容性和验证是重要的考虑因素，但超出了本文的范围。</p><h2 id="8c3a" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">PySpark脚本</h2><p id="0db6" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">根据<a class="ae le" href="http://spark.apache.org/docs/latest/api/python/" rel="noopener ugc nofollow" target="_blank">文档</a>，PySpark是Python中Apache Spark的一个接口。PySpark允许您使用Python API编写Spark应用程序。PySpark支持Spark的大部分功能，如Spark SQL、DataFrame、Streaming、MLlib(机器学习)和Spark Core。本文涵盖了三个PySpark脚本和一个新的助手Python脚本:</p><ol class=""><li id="e206" class="ow ox it kk b kl km ko kp kr oy kv oz kz pa ld pb pc pd pe bi translated"><a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/10_create_schemas.py" rel="noopener ugc nofollow" target="_blank">10 _ create _ schemas . py</a>:Python脚本使用REST API在Apricurio注册表中创建所有Avro模式；</li><li id="3201" class="ow ox it kk b kl pf ko pg kr ph kv pi kz pj ld pb pc pd pe bi translated"><a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/11_incremental_sales_avro.py" rel="noopener ugc nofollow" target="_blank">11 _ incremental _ sales _ avro . py</a>:py spark脚本模拟销售数据发布到Kafka的事件流，持续时间约30分钟；</li><li id="7d92" class="ow ox it kk b kl pf ko pg kr ph kv pi kz pj ld pb pc pd pe bi translated"><a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/12_streaming_enrichment_avro.py" rel="noopener ugc nofollow" target="_blank">12 _ streaming _ enrichment _ avro . py</a>:py spark脚本使用流查询实时读取来自Kafka的消息，丰富销售数据，聚合区域销售结果，并将结果作为流写回Kafka；</li><li id="1ebe" class="ow ox it kk b kl pf ko pg kr ph kv pi kz pj ld pb pc pd pe bi translated"><a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/13_batch_read_results_avro.py" rel="noopener ugc nofollow" target="_blank">13 _ batch _ read _ results _ avro . py</a>:py spark脚本使用批处理查询从Kafka读取汇总的区域销售结果，并在控制台显示；</li><li id="3d0e" class="ow ox it kk b kl pf ko pg kr ph kv pi kz pj ld pb pc pd pe bi translated"><a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/13_batch_read_results_avro.ipynb" rel="noopener ugc nofollow" target="_blank">13 _ batch _ read _ results _ avro . ipynb</a>:Jupyter笔记本扩展版<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/13_batch_read_results_avro.py" rel="noopener ugc nofollow" target="_blank">13 _ batch _ read _ results _ avro . py</a>。本文未涉及的内容；</li></ol><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pl"><img src="../Images/d9a03e928f53ce64e5f94ca0b9ccf6d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wmwCBwyu7ICQ3yE6exSlGQ.gif"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">本文演示的数据管道(脚本名称显示在最后)</figcaption></figure><h1 id="afc8" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">准备</h1><p id="4d90" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">为了准备您的亚马逊EMR资源，请查看前一篇文章中的说明，<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/getting-started-with-spark-structured-streaming-and-kafka-on-aws-using-amazon-msk-and-amazon-emr-91b1f2ef0162">使用亚马逊MSK和亚马逊EMR在AWS上开始使用Spark结构化流和Kafka</a>。这里是一个回顾，这个职位需要一些补充。</p><h2 id="e164" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">亚马逊S3</h2><p id="d9cf" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">我们将从收集和复制必要的文件到你的亚马逊S3桶开始。bucket将作为Amazon EMR引导脚本、Spark所需的其他JAR文件、PySpark脚本和CSV格式数据文件的位置。</p><p id="ba5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将要运行的Spark作业需要一组额外的JAR文件。从<a class="ae le" href="https://mvnrepository.com/artifact/org.apache.spark" rel="noopener ugc nofollow" target="_blank"> Maven Central </a>和GitHub下载jar，并将它们放在<code class="fe lg lh li lj b"><a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/tree/main/pyspark/emr_jars" rel="noopener ugc nofollow" target="_blank">emr_jars</a></code>项目目录中。这些jar将包括<a class="ae le" href="https://github.com/aws/aws-msk-iam-auth" rel="noopener ugc nofollow" target="_blank"> AWS MSK IAM Auth </a>、<a class="ae le" href="https://aws.amazon.com/blogs/developer/java-sdk-bundle/" rel="noopener ugc nofollow" target="_blank"> AWS SDK </a>、<a class="ae le" href="https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients" rel="noopener ugc nofollow" target="_blank"> Kafka客户端</a>、<a class="ae le" href="https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10" rel="noopener ugc nofollow" target="_blank">用于Kafka的Spark SQL</a>、<a class="ae le" href="https://mvnrepository.com/artifact/org.apache.spark/spark-streaming" rel="noopener ugc nofollow" target="_blank"> Spark流</a>以及其他依赖项。与上一篇相比，Avro 多了一个罐子。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="cbc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更新<code class="fe lg lh li lj b">SPARK_BUCKET</code>环境变量，然后使用AWS <code class="fe lg lh li lj b">s3</code> API将JARs、PySpark脚本、样本数据和EMR引导脚本从GitHub项目存储库的本地副本上传到您的亚马逊S3存储桶。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><h2 id="271f" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">亚马逊电子病历</h2><p id="f5ef" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">GitHub项目资源库包括一个样本AWS CloudFormation <a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/cloudformation/stack.yml" rel="noopener ugc nofollow" target="_blank">模板</a>和一个相关联的JSON格式CloudFormation <a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/cloudformation/dev.json" rel="noopener ugc nofollow" target="_blank">参数文件</a>。云形成模板<code class="fe lg lh li lj b"><a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/cloudformation/stack.yml" rel="noopener ugc nofollow" target="_blank">stack.yml</a></code>，接受几个环境<a class="ae le" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html" rel="noopener ugc nofollow" target="_blank">参数</a>。为了与您的环境相匹配，您需要更新参数值，例如SSK密钥、子网和S3存储桶。该模板将构建一个最小规模的Amazon EMR集群，在现有的VPC中有一个主节点和两个核心节点。您可以轻松地修改模板和参数，以满足您的需求和预算。</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="cb51" class="ls lt it lj b gy lu lv l lw lx">aws cloudformation deploy \<br/>    --stack-name spark-kafka-demo-dev \<br/>    --template-file ./cloudformation/stack.yml \<br/>    --parameter-overrides file://cloudformation/dev.json \<br/>    --capabilities CAPABILITY_NAMED_IAM</span></pre><p id="045d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CloudFormation模板有两个基本的Spark配置项——要在EMR上安装的应用程序列表和引导脚本部署。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="c080" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面，我们看到了EMR引导shell脚本，<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/emr_bootstrap/bootstrap_actions.sh" rel="noopener ugc nofollow" target="_blank"> bootstrap_actions.sh </a>。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="ae19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">引导脚本执行了几项任务，包括将我们之前复制到亚马逊S3的额外JAR文件部署到EMR集群节点。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pn"><img src="../Images/38a8613d0bf75941e3c0af31cd55fc85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O5jMDyGCu6kkcyyvK9VCeg.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">Amazon EMR集群“引导操作”选项卡</figcaption></figure><h2 id="5714" class="ls lt it bd ml ob oc dn mp od oe dp mt kr of og mv kv oh oi mx kz oj ok mz ol bi translated">参数存储</h2><p id="8f99" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">本演示中的PySpark脚本将从<a class="ae le" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html" rel="noopener ugc nofollow" target="_blank"> AWS系统管理器(AWS SSM)参数存储</a>中获取配置值。配置值包括亚马逊MSK引导代理列表、包含EMR/Spark资产的亚马逊S3存储桶以及Apicurio Registry REST API基本URL。使用参数存储可以确保没有敏感的或特定于环境的配置被硬编码到PySpark脚本中。修改并执行<code class="fe lg lh li lj b"><a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/ssm_params.sh" rel="noopener ugc nofollow" target="_blank">ssm_params.sh</a></code>脚本来创建AWS SSM参数存储参数。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><h1 id="39d5" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">在Apricurio注册表中创建模式</h1><p id="b627" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">为了创建本演示所需的模式，项目中包含了一个Python脚本，<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/10_create_schemas.py" rel="noopener ugc nofollow" target="_blank"> 10_create_schemas.py </a>。该脚本使用Python <code class="fe lg lh li lj b"><a class="ae le" href="https://docs.python-requests.org/en/latest/" rel="noopener ugc nofollow" target="_blank">requests</a></code>模块与Apricurio Registry的<a class="ae le" href="https://access.redhat.com/webassets/avalon/d/Red_Hat_Integration-2020-Q2-Getting_Started_with_Service_Registry-en-US/files/registry-rest-api.htm#operation/getArtifactByGlobalId" rel="noopener ugc nofollow" target="_blank"> REST API </a>进行交互，并创建六个新的基于Avro的模式工件。Python脚本使用与命令行相同的HTTP方法和资源端点，通过<code class="fe lg lh li lj b">curl</code>或<code class="fe lg lh li lj b">wget</code>与Apricurio Registry的REST API交互。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">用于在Apricurio注册表中创建新Avro模式工件的等效curl命令</figcaption></figure><p id="41c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Apricurio Registry支持几种常见的<a class="ae le" href="https://www.apicur.io/registry/docs/apicurio-registry/1.3.3.Final/getting-started/assembly-registry-reference.html#registry-artifact-types" rel="noopener ugc nofollow" target="_blank">工件类型</a>，包括AsyncAPI规范、Apache Avro模式、GraphQL模式、JSON模式、Apache Kafka Connect模式、OpenAPI规范、Google protocol buffers模式、Web服务定义语言和XML模式定义。我们将使用注册表来存储Avro模式，用于Kafka和CSV数据源和接收器。</p><p id="bd13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管Apricurio Registry不支持<a class="ae le" href="https://digital-preservation.github.io/csv-schema/" rel="noopener ugc nofollow" target="_blank"> CSV模式</a>，但是为了与PySpark一起使用，我们可以将CSV格式的销售和销售区域数据的模式作为JSON格式的Avro模式存储在注册表中。</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="52d4" class="ls lt it lj b gy lu lv l lw lx">{<br/>  "name": "Sales",<br/>  "type": "record",<br/>  "doc": "Schema for CSV-format sales data",<br/>  "fields": [<br/>    {<br/>      "name": "payment_id",<br/>      "type": "int"<br/>    },<br/>    {<br/>      "name": "customer_id",<br/>      "type": "int"<br/>    },<br/>    {<br/>      "name": "amount",<br/>      "type": "float"<br/>    },<br/>    {<br/>      "name": "payment_date",<br/>      "type": "string"<br/>    },<br/>    {<br/>      "name": "city",<br/>      "type": [<br/>        "string",<br/>        "null"<br/>      ]<br/>    },<br/>    {<br/>      "name": "district",<br/>      "type": [<br/>        "string",<br/>        "null"<br/>      ]<br/>    },<br/>    {<br/>      "name": "country",<br/>      "type": "string"<br/>    }<br/>  ]<br/>}</span></pre><p id="7a4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们可以从注册表中检索JSON格式的Avro模式，将其转换为PySpark StructType，并将其关联到用于持久存储CSV文件中的销售数据的DataFrame。</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="2caa" class="ls lt it lj b gy lu lv l lw lx">root<br/> |-- payment_id: integer (nullable = true)<br/> |-- customer_id: integer (nullable = true)<br/> |-- amount: float (nullable = true)<br/> |-- payment_date: string (nullable = true)<br/> |-- city: string (nullable = true)<br/> |-- district: string (nullable = true)<br/> |-- country: string (nullable = true)</span></pre><p id="d1f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用注册表允许我们避免预先在PySpark脚本中将模式硬编码为StructType。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="161e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">提交PySpark脚本作为EMR步骤。EMR将以运行PySpark作业的相同方式运行Python脚本。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="c7d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Python脚本在Apricurio Registry中创建了六个模式工件，如下图Apricurio Registry基于浏览器的用户界面所示。模式包括两个Kafka主题的两个键/值对和两个CSV格式的销售和销售区域数据的键/值对。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi po"><img src="../Images/e73969a447bf432f68904995cd9edae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwB_UoU96Qr1WQWNKjmyMw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">Apricurio Registry基于浏览器的UI中的工件</figcaption></figure><p id="7aec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以选择使用Apricurio Registry为每个模式启用验证和兼容性规则。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi po"><img src="../Images/df3f0d7417c524cf88773657dd191390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50SuiGXTX8cr98Ii7TRvbw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">Apricurio Registry基于浏览器的用户界面中的内容规则选项</figcaption></figure><p id="b6cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个Avro模式工件都作为JSON对象存储在注册表中。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi po"><img src="../Images/b05775d2acfbbc919dd5c548d624f331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOidhkR8IGDowhJi9mmi_Q.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">Apricurio注册表基于浏览器的用户界面中作为JSON的Avro模式的详细视图</figcaption></figure><h1 id="80f9" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">模拟销售事件流</h1><p id="724e" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">接下来，我们将模拟大约30分钟内发布到Kafka的销售数据的事件流。PySpark脚本<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/11_incremental_sales_avro.py" rel="noopener ugc nofollow" target="_blank">11 _ incremental _ sales _ avro . py</a>从位于S3的CSV文件中读取1，800条销售记录到DataFrame ( <code class="fe lg lh li lj b">pyspark.sql.DataFrame</code>)中。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="e395" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark脚本首先使用Python <code class="fe lg lh li lj b"><a class="ae le" href="https://docs.python-requests.org/en/latest/" rel="noopener ugc nofollow" target="_blank">requests</a></code>模块和Apricurio Registry的REST API ( <code class="fe lg lh li lj b">get_schema()</code>)从Apricurio Registry检索CSV数据的JSON格式Avro模式。</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="3f6e" class="ls lt it lj b gy lu lv l lw lx">csv_sales_schema = get_schema("pagila.sales.csv")</span></pre><p id="de93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">JSON格式的模式:</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="2eba" class="ls lt it lj b gy lu lv l lw lx">{<br/>  "name": "Sales",<br/>  "type": "record",<br/>  "doc": "Schema for CSV-format sales data",<br/>  "fields": [<br/>    {<br/>      "name": "payment_id",<br/>      "type": "int"<br/>    },<br/>    {<br/>      "name": "customer_id",<br/>      "type": "int"<br/>    },<br/>    {<br/>      "name": "amount",<br/>      "type": "float"<br/>    },<br/>    {<br/>      "name": "payment_date",<br/>      "type": "string"<br/>    },<br/>    {<br/>      "name": "city",<br/>      "type": [<br/>        "string",<br/>        "null"<br/>      ]<br/>    },<br/>    {<br/>      "name": "district",<br/>      "type": [<br/>        "string",<br/>        "null"<br/>      ]<br/>    },<br/>    {<br/>      "name": "country",<br/>      "type": "string"<br/>    }<br/>  ]<br/>}</span></pre><p id="3ba7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，该脚本使用一个空的DataFrame从JSON格式的Avro模式创建一个StructType】)。Avro列类型被转换为Spark SQL类型。唯一明显的问题是Spark如何错误地处理每一列的<code class="fe lg lh li lj b">nullable</code>值。请注意，Spark中的列可空性是一个优化语句，而不是对象类型的强制。产生的StructType用于将CSV数据读入数据帧(<code class="fe lg lh li lj b">read_from_csv()</code>)。</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="b48a" class="ls lt it lj b gy lu lv l lw lx">root<br/> |-- payment_id: integer (nullable = true)<br/> |-- customer_id: integer (nullable = true)<br/> |-- amount: float (nullable = true)<br/> |-- payment_date: string (nullable = true)<br/> |-- city: string (nullable = true)<br/> |-- district: string (nullable = true)<br/> |-- country: string (nullable = true)</span></pre><p id="72e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于Avro格式的Kafka键和值模式，我们在所有脚本中使用相同的方法<code class="fe lg lh li lj b">get_schema()</code>。生成的JSON格式的模式然后被传递给<code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.avro.functions.to_avro.html" rel="noopener ugc nofollow" target="_blank">to_avro</a>()</code>和<code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.avro.functions.from_avro.html" rel="noopener ugc nofollow" target="_blank">from_avro</a>()</code>方法，以向Kafka读写Avro格式的消息。这两种方法都是<code class="fe lg lh li lj b">pyspark.sql.avro.functions</code>模块的一部分。Avro列类型与Spark SQL类型相互转换。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="408c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark脚本<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/11_incremental_sales_avro.py" rel="noopener ugc nofollow" target="_blank">11 _ incremental _ sales _ avro . py</a>使用REST API和<code class="fe lg lh li lj b">get_schema()</code>方法从Apicurio注册表中检索Kafka主题键和值的模式。然后，脚本获取数据帧的每一个<a class="ae le" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html" rel="noopener ugc nofollow" target="_blank">行</a> ( <code class="fe lg lh li lj b">pyspark.sql.Row</code>)，一次一行，并将它们写入Kafka主题<code class="fe lg lh li lj b">pagila.sales.avro</code>，在每次写入之间添加一点延迟。</p><p id="b993" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们必须运行PySpark脚本<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/11_incremental_sales_avro.py" rel="noopener ugc nofollow" target="_blank">11 _ incremental _ sales _ avro . py</a>，同时运行PySpark脚本<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/12_streaming_enrichment_avro.py" rel="noopener ugc nofollow" target="_blank">12 _ streaming _ enrichment _ avro . py</a>，以模拟事件流。我们将在文章的下一部分开始这两个脚本。</p><h1 id="81f6" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结构化流的流处理</h1><p id="6e7e" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">PySpark脚本<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/12_streaming_enrichment_avro.py" rel="noopener ugc nofollow" target="_blank">12 _ streaming _ enrichment _ avro . py</a>使用流查询从Kafka主题<code class="fe lg lh li lj b">pagila.sales.avro</code>中实时读取销售数据消息，丰富销售数据，汇总区域销售结果，并每两分钟将结果以小批量写回Kafka。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="5ee8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark脚本在Kafka主题<code class="fe lg lh li lj b">pagila.sales.avro</code>的流式销售数据和包含基于公共<code class="fe lg lh li lj b">country</code>列的销售区域的CSV文件之间执行流到批连接。卡夫卡的销售数据:</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">来自第一个Kafka主题的流查询的销售数据</figcaption></figure><p id="5f81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CSV文件中的销售区域数据:</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">亚马逊S3 CSV文件中的销售区域数据</figcaption></figure><p id="3550" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，PySpark脚本按照销售区域，在滑动的10分钟<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time" rel="noopener ugc nofollow" target="_blank">事件时间窗口</a>内执行销售额和订单数量的流聚合，该窗口有5分钟的窗口重叠和10分钟的<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking" rel="noopener ugc nofollow" target="_blank">水印</a>。该脚本每两分钟向Kafka主题<code class="fe lg lh li lj b">pagila.sales.summary.avro</code>写入结果。下面是使用DataStreamWriter接口(<code class="fe lg lh li lj b">pyspark.sql.streaming.DataStreamWriter</code>)写入外部存储(在本例中是Kafka)的结果流数据帧的示例。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">返回到第二个Kafka主题的汇总销售结果</figcaption></figure><p id="c6db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了将结果写回Kafka，脚本使用其REST API从Apicurio注册表中检索第二个Kafka主题的消息键和值的Avro格式模式。</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="7e91" class="ls lt it lj b gy lu lv l lw lx">sales_summary_key = get_schema("pagila.sales.summary.avro-key")<br/>sales_summary_value = get_schema("pagila.sales.summary.avro-value")</span></pre><p id="e1b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">密钥模式:</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="dc97" class="ls lt it lj b gy lu lv l lw lx">{<br/>  "name": "Key",<br/>  "type": "int",<br/>  "doc": "Schema for pagila.sales.summary.avro Kafka topic key"<br/>}</span></pre><p id="868a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">价值模式:</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="00fc" class="ls lt it lj b gy lu lv l lw lx">{<br/>  "name": "Value",<br/>  "type": "record",<br/>  "doc": "Schema for pagila.sales.summary.avro Kafka topic value",<br/>  "fields": [<br/>    {<br/>      "name": "region",<br/>      "type": "string"<br/>    },<br/>    {<br/>      "name": "sales",<br/>      "type": "float"<br/>    },<br/>    {<br/>      "name": "orders",<br/>      "type": "int"<br/>    },<br/>    {<br/>      "name": "window_start",<br/>      "type": "long",<br/>      "logicalType": "timestamp-millis"<br/>    },<br/>    {<br/>      "name": "window_end",<br/>      "type": "long",<br/>      "logicalType": "timestamp-millis"<br/>    }<br/>  ]<br/>}</span></pre><p id="7acf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用<code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.avro.functions.to_avro.html" rel="noopener ugc nofollow" target="_blank">to_avro</a>()</code>方法应用于流数据帧的<code class="fe lg lh li lj b">pagila.sales.summary.avro-value</code>模式。</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="d337" class="ls lt it lj b gy lu lv l lw lx">root<br/> |-- region: string (nullable = false)<br/> |-- sales: float (nullable = true)<br/> |-- orders: integer (nullable = false)<br/> |-- window_start: long (nullable = true)<br/> |-- window_end: long (nullable = true)</span></pre><p id="f532" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">提交此流式PySpark脚本，<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/12_streaming_enrichment_avro.py" rel="noopener ugc nofollow" target="_blank">12 _ streaming _ enrichment _ avro . py</a>，作为EMR步骤。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="b5d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">等待大约两分钟，让第三个PySpark脚本有时间完全开始它的流查询。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi po"><img src="../Images/58bcbf46cc6db65e032bbba8fdd84362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bk5URj918smndwX5ONSvlw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">在Amazon EMR集群上运行的PySpark结构化流作业</figcaption></figure><p id="fb79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，提交第二个PySpark脚本，<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/11_incremental_sales_avro.py" rel="noopener ugc nofollow" target="_blank">11 _ incremental _ sales _ avro . py</a>，作为EMR步骤。这两个PySpark脚本将在您的Amazon EMR集群上并发运行，或者使用两个不同的EMR集群。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="6fb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark脚本<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/11_incremental_sales_avro.py" rel="noopener ugc nofollow" target="_blank">11 _ incremental _ sales _ avro . py</a>应该运行大约30分钟。</p><figure class="lk ll lm ln gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pn"><img src="../Images/f56e9d03f80a38d6138b5a68ea7de3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJSPDJWg5tD2nzoaSvmKJw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">在第二个Amazon EMR集群上完成的销售数据模拟事件流</figcaption></figure><p id="e1c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此期间，脚本<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/12_streaming_enrichment_avro.py" rel="noopener ugc nofollow" target="_blank">12 _ streaming _ enrichment _ avro . py</a>将以Avro格式向第二个Kafka主题<code class="fe lg lh li lj b">pagila.sales.summary.avro</code>以两分钟为间隔写入微批的汇总销售结果。PySpark的<code class="fe lg lh li lj b">stdout</code>日志中记录的微批次示例如下所示。注意<code class="fe lg lh li lj b">min</code>和<code class="fe lg lh li lj b">max</code> <code class="fe lg lh li lj b">eventTime</code>，表示两分钟的微批次间隔。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">作为Avro写入Kafka的微批处理的流式查询结果</figcaption></figure><p id="6018" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦这个PySpark脚本<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/11_incremental_sales_avro.py" rel="noopener ugc nofollow" target="_blank">11 _ incremental _ sales _ avro . py</a>完成，再等几分钟，然后停止流式PySpark脚本<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/12_streaming_enrichment_avro.py" rel="noopener ugc nofollow" target="_blank">12 _ streaming _ enrichment _ avro . py</a>。</p><h1 id="2ec3" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">查看结果</h1><p id="c31a" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">要从Kafka检索并显示之前PySpark脚本的流计算结果，我们可以使用最终的PySpark脚本，<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/13_batch_read_results_avro.py" rel="noopener ugc nofollow" target="_blank">13 _ batch _ read _ results _ avro . py</a>。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="b2a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">提交最终的PySpark脚本，<a class="ae le" href="https://github.com/garystafford/kafka-connect-msk-demo/blob/main/pyspark/pyspark_scripts/13_batch_read_results_avro.py" rel="noopener ugc nofollow" target="_blank">13 _ batch _ read _ results _ avro . py</a>，作为EMR步骤。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="3b75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个最终的PySpark脚本使用存储在Apicurio Registry中的模式对Kafka主题<code class="fe lg lh li lj b">pagila.sales.summary.avro</code>中的所有Avro格式的聚合销售消息执行批处理查询。</p><pre class="lk ll lm ln gt lo lj lp lq aw lr bi"><span id="464b" class="ls lt it lj b gy lu lv l lw lx">sales_summary_key = get_schema("pagila.sales.summary.avro-key")<br/>sales_summary_value = get_schema("pagila.sales.summary.avro-value")</span></pre><p id="7cd3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，该脚本按销售区域将最终销售结果汇总到<code class="fe lg lh li lj b">stdout</code>作业日志中。下面，我们来看一个例子，在一个10分钟的活动时间滑动窗口内实时汇总亚太地区的销售额和订单。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div></figure><p id="8c09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark使用Spark SQL函数<code class="fe lg lh li lj b">Window.partitionBy()</code>和<code class="fe lg lh li lj b">row_number().over(window)</code>的组合，根据最新的时间戳，在每个滑动的10分钟<a class="ae le" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time" rel="noopener ugc nofollow" target="_blank">事件时间窗口</a>的每个10分钟窗口内，按地区汇总最终销售额。</p><figure class="lk ll lm ln gt lz"><div class="bz fp l di"><div class="pm nh l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">按活动时间窗口列出的销售结果表(为清晰起见，添加了窗口中断)</figcaption></figure><h1 id="c591" class="mk lt it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论</h1><p id="8ee8" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">在这篇文章中，我们学习了如何在Amazon EMR上使用PySpark、Apache Avro格式和Apircurio注册表开始使用Spark结构化流。我们分离了Kafka消息的键和值模式，以及以CSV格式存储在S3的数据模式，将这些模式存储在一个注册表中。</p></div><div class="ab cl pp pq hx pr" role="separator"><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu"/></div><div class="im in io ip iq"><p id="a8cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">这篇博客代表我自己的观点，不代表我的雇主亚马逊网络服务公司。所有产品名称、徽标和品牌都是其各自所有者的财产。</em></p></div></div>    
</body>
</html>