<html>
<head>
<title>How to Set Kubernetes Resource Requests and Limits — A Saga to Improve Cluster Stability and Efficiency</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何设置Kubernetes资源请求和限制——提高集群稳定性和效率的传奇</h1>
<blockquote>原文：<a href="https://itnext.io/how-to-set-kubernetes-resource-requests-and-limits-a-saga-to-improve-cluster-stability-and-a7b1800ecff1?source=collection_archive---------3-----------------------#2019-11-13">https://itnext.io/how-to-set-kubernetes-resource-requests-and-limits-a-saga-to-improve-cluster-stability-and-a7b1800ecff1?source=collection_archive---------3-----------------------#2019-11-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/33524c70fb2983a451572870c7012f71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8FKPeSkbkavMYYTE7BgCA.png"/></div></div></figure><h1 id="f75a" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">一个谜</h1><p id="a5c5" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">这一切都始于9月1日，就在我们的集群从1.11升级到1.12之后。几乎在第二天，我们就开始在<code class="fe lx ly lz ma b">kubelet</code>上看到Datadog报告的警报。有时候我们会收到几封(3 - 5封)，有时候一天会收到10多封。警报监视器基于Datadog check - <code class="fe lx ly lz ma b">kubernetes.kubelet.check</code>，每当<code class="fe lx ly lz ma b">kubelet</code>进程在一个节点上停止时就会触发警报监视器。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mb"><img src="../Images/5791507de869a96f3b3934591b4356b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAsumvq4CUW4KunHvOGGaw.png"/></div></div></figure><p id="aaee" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">我们知道<a class="ae ml" href="https://kubernetes.io/docs/concepts/overview/components/#kubelet" rel="noopener ugc nofollow" target="_blank"> kubelet </a>在Kubernetes的日程安排中起着重要作用。如果不能在节点中正常运行，将会直接从功能群集中删除该节点。如果有更多的节点出现问题<code class="fe lx ly lz ma b">kubelet</code>，那么集群就会降级。现在，想象一下早上醒来有16个提醒。这真是太可怕了。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/335bc72683db181a8e13d59a41d00046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cUMwVoIRT3Msv5pi"/></div></div></figure><p id="fba9" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">真正让我们困惑的是，在有问题的节点上运行的所有服务似乎都是无害的。在某些情况下，只有少数几个正在运行的服务，并且之前有一些高CPU使用率。当潜在的罪犯可能已经离开现场时，很难指出任何事情，因此没有留下任何痕迹供我们进一步诊断。有趣的是，在我们的服务中没有任何明显的性能影响，比如请求延迟。这个小事实给整个事情增添了更多的神秘色彩。</p><p id="396b" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">这种现象每天大约在同一时间(太平洋时间早上5:30)开始，通常在中午之前停止，周末除外。在某种程度上，我觉得我可以用这些数据狗提醒我的闹钟。一点也不好玩，这次挑战让我有些白发。</p><h1 id="6740" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">我们的调查</h1><p id="e2d3" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">从一开始，我们就知道这将是一项艰巨的调查，需要系统的方法。为了简洁起见，我将只列出一些我们尝试过的关键实验，不赘述。尽管它们是很好的调查步骤，但我不认为它们对这篇文章很重要。以下是我们尝试的内容</p><ul class=""><li id="5635" class="mn mo it lb b lc mg lg mh lk mp lo mq ls mr lw ms mt mu mv bi translated">我们将集群从1.12升级到1.13</li><li id="e7e1" class="mn mo it lb b lc mw lg mx lk my lo mz ls na lw ms mt mu mv bi translated">我们创建了一些受感染的节点，并把我们所有的cronjobs转移到这些节点上</li><li id="d61d" class="mn mo it lb b lc mw lg mx lk my lo mz ls na lw ms mt mu mv bi translated">我们创建了更多受感染的节点，并将大多数消耗CPU的工作人员转移到这些节点上</li><li id="efb5" class="mn mo it lb b lc mw lg mx lk my lo mz ls na lw ms mt mu mv bi translated">我们将集群扩展了近20%，从42个节点扩展到50个节点</li><li id="ad90" class="mn mo it lb b lc mw lg mx lk my lo mz ls na lw ms mt mu mv bi translated">我们再次缩减了集群，因为我们没有看到任何改进</li><li id="ccff" class="mn mo it lb b lc mw lg mx lk my lo mz ls na lw ms mt mu mv bi translated">我们回收(删除并重新创建)了所有先前报告kubelet问题的节点，结果第二天又有新的节点跟进</li><li id="adc2" class="mn mo it lb b lc mw lg mx lk my lo mz ls na lw ms mt mu mv bi translated">在你我之间，我甚至认为Datadog警报可能会被打破，因为没有任何明显的服务性能影响。但我无法让自己结案，因为我知道罪犯可能仍然逍遥法外。</li></ul><p id="a037" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">幸运的是，经过大量的政治迫害，这引起了我的注意</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/2ff106de495e72c06b7616592ed4f483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oqqp13YqTZKffnQeeVWEGQ.png"/></div></div></figure><p id="c29e" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">我们看到10个<code class="fe lx ly lz ma b">buffer-publish</code>pod被安排到一个节点，持续大约10分钟，但很快就被终止了。与此同时，CPU使用率激增，<code class="fe lx ly lz ma b">kubelet</code>喊道，在终止后的几分钟内，pod从节点上消失了。</p><p id="9575" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">难怪发出警报后我们什么也找不到。但是我想，这些豆荚有什么特别之处呢？我们有的唯一事实是高CPU使用率。现在，让我们看看资源请求/限制</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="nc nd l"/></div></figure><blockquote class="ne nf ng"><p id="50fb" class="kz la nh lb b lc mg le lf lg mh li lj ni mi lm ln nj mj lq lr nk mk lu lv lw im bi translated"><em class="it">CPU/内存</em> <strong class="lb iu"> <em class="it">请求</em> </strong> <em class="it">参数告诉Kubenetes最初应该分配多少资源</em></p><p id="24d6" class="kz la nh lb b lc mg le lf lg mh li lj ni mi lm ln nj mj lq lr nk mk lu lv lw im bi translated"><em class="it">CPU/内存</em> <strong class="lb iu"> <em class="it">限制</em> </strong> <em class="it">参数告诉Kubenetes在所有情况下应该给定的最大资源</em></p></blockquote><p id="a6a0" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">这里有一个<a class="ae ml" href="http://blog.kubecost.com/blog/requests-and-limits/" rel="noopener ugc nofollow" target="_blank">帖子</a>在解释这个概念方面做得更好。我强烈推荐全文阅读。向kubecost 的团队致敬！</p><p id="2bb6" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">现在，回到我们现在的位置。CPU请求/限制比率是10，应该没问题吧？开始时，我们为一个pod分配0.1个CPU，并将最大使用量限制为1个CPU。这样，我们有一个保守的开始，同时仍然有某种，尽管是任意的上限。感觉我们几乎是在遵循最佳实践！</p><p id="549e" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">然后我想，这一点意义都没有。当在单个节点中调度10个pod时，该参数允许的总CPU是10个CPU，但是在一个<code class="fe lx ly lz ma b">m4.xlarge</code>节点中没有10个CPU。在我们的高峰时段，比如说美国太平洋时间早上5:30醒来，会发生什么？现在我几乎可以想象出一幅可怕的画面，这些节点杀死了所有的CPU，甚至连<code class="fe lx ly lz ma b">kubelet</code>也开始死亡，然后整个节点就崩溃了。</p><p id="c416" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">那么现在，我们能做些什么呢？</p><h1 id="ad18" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">补救措施</h1><p id="0a3d" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">显然，最简单的方法是降低CPU的限制，这样这些pod就会在杀死一个节点之前杀死自己。但我觉得这不太对劲。如果他们真的需要那么多CPU来进行正常操作，那么节流(在这个上有更多<a class="ae ml" href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/" rel="noopener ugc nofollow" target="_blank">说明)不会导致低性能。</a></p><p id="075a" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">好吧，增加CPU请求，让这些单元更加分散，而不是被安排到单个节点，怎么样？这听起来像一个更好的计划，这就是我们实施的计划。以下是详细情况:</p><h2 id="6df5" class="nl kc it bd kd nm nn dn kh no np dp kl lk nq nr kp lo ns nt kt ls nu nv kx nw bi translated">算出你通常需要多少</h2><p id="8bcc" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我使用过去一周的数据狗指标<code class="fe lx ly lz ma b">kubernetes.cpu.usage.total</code>来衡量最大报告值，以给我一些参考</p><p id="f84e" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">你可以看到它通常保持在200米以下(0.2个CPU)。这告诉我，CPU请求的这个值很难出错。</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/67ecbc135f8b2470063fff9e61935e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ms8jDZMpKI2mrDA4peSyBw.png"/></div></div></figure><h2 id="3451" class="nl kc it bd kd nm nn dn kh no np dp kl lk nq nr kp lo ns nt kt ls nu nv kx nw bi translated">给它设限</h2><p id="3ed7" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">这是棘手的部分，就像生活中大多数棘手的事情一样，没有简单的解决方法。根据我的经验，一个好的开始应该是2倍的请求。在这种情况下，它将是400m (0.4个CPU)。更改之后，我花了一些时间观察服务性能指标，以确保性能不会受到CPU节流的影响。如果是的话，我可能需要把它提高到一个更合理的数字。这是一个反复的过程，直到你做对为止。</p><h2 id="4e55" class="nl kc it bd kd nm nn dn kh no np dp kl lk nq nr kp lo ns nt kt ls nu nv kx nw bi translated">注意比例</h2><p id="687f" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">关键是不要让低请求欺骗Kubernetes将所有pods调度到一个节点上，这样只会以难以置信的高限制耗尽所有CPU。理想情况下，请求/限额不应相距太远，比如在2到5倍的范围内。否则，应用程序会被认为过于尖锐，甚至有某种漏洞。如果是这种情况，深入了解应用程序足迹是明智的。</p><h2 id="6a52" class="nl kc it bd kd nm nn dn kh no np dp kl lk nq nr kp lo ns nt kt ls nu nv kx nw bi translated">定期复习</h2><p id="3449" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">只要应用程序是活动的，它们就会发生变化，它们的足迹也是如此。确保你有某种回顾过程，带你回到第一步(计算出它通常需要多少)。这是让事情保持最佳状态的唯一方法。</p><h1 id="e174" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">利润</h1><p id="7b63" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">那么，成功了吗？你打赌！我们的集群中有相当多的服务具有不成比例的请求/限制。在我调整了这些重载服务之后，集群运行更加稳定了，这是它现在的样子👇</p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/d40ed113eb3a88d3fbfa7d5cabe443d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sck4hij5_2bpJB-w"/></div></div></figure><p id="bb10" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated">等等！题目中承诺的效率怎么样？请注意改变后乐队变得更加狭窄。这表明集群中的CPU资源得到了更加均匀的利用。这随后使得放大具有线性效果，这是非常有效的。</p><h1 id="3d5d" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结束语</h1><p id="ef92" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">与在一组专用计算实例上部署每个服务不同，面向服务的架构允许许多服务共享一个Kubernetes集群。正因为如此，每个服务现在都有责任指定自己的资源需求。这一步不可掉以轻心。不稳定的集群会影响所有驻留的服务，并且故障排除通常具有挑战性。诚然，并不是所有人都对这种新配置有经验。在过去的好时光里，我们所需要的只是在一些服务器上部署我们的一个东西，并根据我们的喜好进行缩放。我想这可能是为什么我在Kubernetes中没有看到很多关于资源参数的讨论。通过这篇文章，我希望能帮助一些正在为这个新概念而奋斗的人(我知道我做到了)。更重要的是，也许向掌握其他技巧的人学习。如果你对此有任何想法，请随时在<a class="ae ml" href="https://twitter.com/stevenc81" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上联系我。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="a373" class="pw-post-body-paragraph kz la it lb b lc mg le lf lg mh li lj lk mi lm ln lo mj lq lr ls mk lu lv lw im bi translated"><em class="nh">最初发表于</em><a class="ae ml" href="https://gist.github.com/stevenc81/086d5ed7435ee66d4ea697e6d4461ca2" rel="noopener ugc nofollow" target="_blank">T5【http://github.com】</a><em class="nh">。</em></p></div></div>    
</body>
</html>