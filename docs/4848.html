<html>
<head>
<title>Kafka Connect in a nutshell</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卡夫卡《一言以蔽之》</h1>
<blockquote>原文：<a href="https://itnext.io/kafka-connect-in-a-nutshell-e0a0f57e7cdb?source=collection_archive---------0-----------------------#2020-10-06">https://itnext.io/kafka-connect-in-a-nutshell-e0a0f57e7cdb?source=collection_archive---------0-----------------------#2020-10-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/00e47e93c1c4cfa9c739dc8aa9b0b9ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CT_z7DYfeIeSvNGK.jpg"/></div></div></figure><div class=""/><p id="afc3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Kafka Connect是Apache Kafka平台的一部分。它用于连接Kafka与外部服务，如文件系统和数据库。在这个故事中，你将学习它解决什么问题以及如何运行它。</p><h1 id="6f90" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">为什么卡夫卡连接？</h1><p id="9dd3" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">Apache Kafka用于微服务架构、日志聚合、变更数据捕获(CDC)、集成、流平台和数据采集层到数据湖。无论你用Kafka做什么，数据都是从<strong class="kd jf">源</strong>流向<strong class="kd jf">接收器</strong>。</p><p id="5a2b" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">需要<strong class="kd jf">时间</strong>和<strong class="kd jf">知识</strong>才能<strong class="kd jf">恰当地</strong>实现一个卡夫卡式的消费者或生产者。关键是输入和输出经常重复。许多公司将卡夫卡的数据转移到HDFS/S3和Elasticsearch。如果我们可以用一个尺寸适合所有人的实现会怎么样？</p><p id="d4b1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">没有重新发明轮子的意义</strong>。Kafka Connect解决了这个问题。它是一个连接卡夫卡和外部组件的平台。我们可以用<strong class="kd jf">连接器</strong>配置输入和输出。它可以独立运行，也可以分布式运行。因此，我们拥有了可扩展的容错平台。</p><h1 id="0c9e" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">可供选择的事物</h1><p id="3099" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">我想到的替代方案是:</p><ul class=""><li id="38c6" class="mc md je kd b ke kf ki kj km me kq mf ku mg ky mh mi mj mk bi translated"><a class="ae ml" href="https://gobblin.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇goblin</a></li><li id="72df" class="mc md je kd b ke mm ki mn km mo kq mp ku mq ky mh mi mj mk bi translated"><a class="ae ml" href="https://www.elastic.co/logstash" rel="noopener ugc nofollow" target="_blank">日志存储</a></li><li id="e75f" class="mc md je kd b ke mm ki mn km mo kq mp ku mq ky mh mi mj mk bi translated"><a class="ae ml" href="https://www.fluentd.org/" rel="noopener ugc nofollow" target="_blank">流动的</a></li><li id="6afb" class="mc md je kd b ke mm ki mn km mo kq mp ku mq ky mh mi mj mk bi translated">阿帕奇尼菲</li></ul><h1 id="447f" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">连接器</h1><p id="4557" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">连接器可在<a class="ae ml" href="https://www.confluent.io/hub/" rel="noopener ugc nofollow" target="_blank">汇流毂</a>网站上找到。它们既可以免费获得，也可以付费获得，可以在合流平台中获得。例如:Cassandra连接器有一个<a class="ae ml" href="https://www.confluent.io/hub/confluentinc/kafka-connect-cassandra" rel="noopener ugc nofollow" target="_blank">付费版本</a>(来自Confluent)，但也有一个<a class="ae ml" href="https://www.confluent.io/hub/datastax/kafka-connect-cassandra-sink" rel="noopener ugc nofollow" target="_blank">免费版本</a>来自DataStax。如果您没有找到现成的解决方案，您可以<a class="ae ml" href="https://docs.confluent.io/current/connect/devguide.html" rel="noopener ugc nofollow" target="_blank">自行实施连接器</a>。</p><h1 id="25ad" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">环境</h1><p id="f8bb" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">在这种情况下，Kafka、Zookeeper和Minio将在Docker上运行。Kafka Connect将在主机上运行。它是Apache Kafka的一部分，所以只需下载并解压二进制文件。Conduktor是一个非常方便的GUI Kafka工具。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="095a" class="na la je mw b gy nb nc l nd ne">version: '2'<br/> <br/>services:<br/>  zookeeper:<br/>    image: 'bitnami/zookeeper:3'<br/>    ports:<br/>      - '2181:2181'<br/>    volumes:<br/>      - 'zookeeper_data:/bitnami'<br/>    environment:<br/>      - ALLOW_ANONYMOUS_LOGIN=yes<br/><br/>  kafka:<br/>    image: 'bitnami/kafka:2'<br/>    ports:<br/>      - '9092:9092'<br/>      - '29092:29092'<br/>    volumes:<br/>      - 'kafka_data:/bitnami'<br/>    environment:<br/>      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181<br/>      - ALLOW_PLAINTEXT_LISTENER=yes<br/>      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_HOST://:29092<br/>      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT<br/>      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092<br/>    depends_on:<br/>      - zookeeper<br/>      <br/>  minio:<br/>    image: 'bitnami/minio:latest'<br/>    ports:<br/>      - '9000:9000'<br/>    environment:<br/>      - MINIO_ACCESS_KEY=minio-access-key<br/>      - MINIO_SECRET_KEY=minio-secret-key<br/> <br/><br/>volumes:<br/>  zookeeper_data:<br/>    driver: local<br/>  kafka_data:<br/>    driver: local</span></pre><h1 id="5403" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">独立模式—写入文件</h1><p id="8995" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">Apache Connect可以以两种模式运行:独立模式和分布式模式。第一个用于测试和开发。要运行它，你需要配置文件。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="88ad" class="na la je mw b gy nb nc l nd ne">bootstrap.servers=localhost:29092<br/><br/>key.converter=org.apache.kafka.connect.storage.StringConverter<br/>value.converter=org.apache.kafka.connect.storage.StringConverter<br/><br/>offset.storage.file.filename=/tmp/connect.offsets</span></pre><p id="c7d2" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">键和值转换器定义了Kafka读写数据的格式。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="ea8f" class="na la je mw b gy nb nc l nd ne">name=standalone-file-sink<br/>connector.class=FileStreamSink<br/>tasks.max=1<br/>file=output.txt<br/>topics=standalone-test</span></pre><p id="20d4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这些领域似乎很明显。行刑的GIF下面。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="8c0c" class="na la je mw b gy nb nc l nd ne">kafka_2.13-2.6.0/bin/connect-standalone.sh connect-standalone.properties connect-file-sink.properties</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/12c6b53e0838ca571f7306a6f254b778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/0*txTfOD0h7ezie6ny.gif"/></div></figure><h1 id="e815" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">分布式模式</h1><p id="df7a" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">这种模式提供了可伸缩性和容错能力。你只需要用同一个group.id运行多个workers，任务编排是平台自己提供的。在DataGen示例中，您将看到当您杀死一个工人时Kafka Connect的行为。下面是其中一个工人的配置文件。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="2efc" class="na la je mw b gy nb nc l nd ne">bootstrap.servers=localhost:29092<br/>group.id=connect-cluster<br/>key.converter=org.apache.kafka.connect.json.JsonConverter<br/>value.converter=org.apache.kafka.connect.json.JsonConverter<br/><br/>key.converter.schemas.enable=false<br/>value.converter.schemas.enable=false<br/><br/>offset.storage.topic=connect-offsets<br/>offset.storage.replication.factor=1<br/><br/>config.storage.topic=connect-configs<br/>config.storage.replication.factor=1<br/><br/>status.storage.topic=connect-status<br/>status.storage.replication.factor=1<br/><br/>#rest.host.name=<br/>rest.port=8083<br/><br/>plugin.path=/tmp/plugins/</span></pre><ul class=""><li id="b1a0" class="mc md je kd b ke kf ki kj km me kq mf ku mg ky mh mi mj mk bi translated">group.id =像消费者组一样工作。集群中的工作线程必须具有相同的group.id。</li><li id="6875" class="mc md je kd b ke mm ki mn km mo kq mp ku mq ky mh mi mj mk bi translated">Kafka Connect将数据保存在… Kafka。您必须指出主题的名称和预期的复制因子。</li><li id="cb3b" class="mc md je kd b ke mm ki mn km mo kq mp ku mq ky mh mi mj mk bi translated">我们将连接器放入plugin.path提供的目录中。所有工人都应该可以访问相同的连接器。</li></ul><p id="57b4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们要做的就是运行Kafka Connect</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="7d6a" class="na la je mw b gy nb nc l nd ne">kafka_2.13-2.6.0/bin/connect-distributed.sh connect-distributed-1.properties</span></pre><h2 id="4bea" class="na la je bd lb ng nh dn lf ni nj dp lj km nk nl ln kq nm nn lr ku no np lv nq bi translated">数据生成—数据生成源</h2><p id="f662" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">我们将使用Kafka Connect Datagen。它是生成数据的连接器。这对于一些概念验证是有用的。只需下载它，解压并放入plugin.path中给出的目录。</p><p id="1571" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Kafka Connect中的任务是使用REST API运行的。首先，您需要准备连接器的配置。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="6e97" class="na la je mw b gy nb nc l nd ne">{<br/>    "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",<br/>    "kafka.topic": "distributed-test",<br/>    "quickstart": "ratings",<br/>    "key.converter": "org.apache.kafka.connect.storage.StringConverter",<br/>    "value.converter": "org.apache.kafka.connect.json.JsonConverter",<br/>    "max.interval": 100,<br/>    "tasks.max": "1"<br/>}</span></pre><p id="a450" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后将其发送到…/connectors/name/config URL</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="b913" class="na la je mw b gy nb nc l nd ne">curl -X PUT -H 'Content-Type:application/json' http://localhost:8083/connectors/source-datagen/config -d @connect-datagen-source.json</span></pre><p id="d947" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是我们也可以使用其他工具，例如<a class="ae ml" href="https://www.conduktor.io/" rel="noopener ugc nofollow" target="_blank">导管</a>。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/32efc0eea57df6acb1c9f7b8c2d3ffe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/0*0J5T5JztXzwTHdMs.png"/></div></figure><p id="6736" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面你可以看到我运行Datagen连接器，然后启动和停止Kafka Connect workers的GIF。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/47c2c04bb9b58df30e50374ac8467294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lAVoT-Jt4hrJrE_C.gif"/></div></div></figure><h2 id="7a36" class="na la je bd lb ng nh dn lf ni nj dp lj km nk nl ln kq nm nn lr ku no np lv nq bi translated">将数据移出— AWS S3接收器(MinIO)</h2><p id="940e" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">数据已经生成，让我们继续将它发送到其他系统。最初我想在这里使用Elasticsearch Sink，但我无法回避卡夫卡图书馆缺乏番石榴的问题。你可以在这里找到AWS S3连接器。与Datagen相同的步骤。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="fd97" class="na la je mw b gy nb nc l nd ne">{<br/>    "name":"sink-minio",<br/>    "connector.class":"io.confluent.connect.s3.S3SinkConnector",<br/>    "tasks.max":"1",<br/>    "topics":"distributed-test",<br/>    "s3.bucket.name":"test",<br/>    "s3.part.size":"5242880",<br/>    "s3.compression.type":"gzip",<br/>    "flush.size":"100",<br/>    "aws.secret.access.key": "minio-secret-key",<br/>    "aws.access.key.id": "minio-access-key",<br/>    "store.url":"http://localhost:9000",<br/>    "value.converter": "org.apache.kafka.connect.json.JsonConverter",<br/>    "format.class": "io.confluent.connect.s3.format.json.JsonFormat",<br/>    "storage.class": "io.confluent.connect.s3.storage.S3Storage",<br/>    "schema.compatibility": "NONE",<br/>    "schemas.enable":false<br/>}</span></pre><ul class=""><li id="94d2" class="mc md je kd b ke kf ki kj km me kq mf ku mg ky mh mi mj mk bi translated">store.url用MinIO指向本地容器</li><li id="8153" class="mc md je kd b ke mm ki mn km mo kq mp ku mq ky mh mi mj mk bi translated">flush.size —将包含在一个包中的记录数</li><li id="4cfb" class="mc md je kd b ke mm ki mn km mo kq mp ku mq ky mh mi mj mk bi translated">s3.compression.type —每个包都将被压缩(或不被压缩)</li><li id="260e" class="mc md je kd b ke mm ki mn km mo kq mp ku mq ky mh mi mj mk bi translated">value.converter记录可以是特定的格式。这里我们用JSON。</li></ul><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="9b42" class="na la je mw b gy nb nc l nd ne">curl -X PUT -H 'Content-Type:application/json' http://localhost:8083/connectors/sink-minio/config -d @connect-minio-sink.json</span></pre><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e4353477c2cd454d4588b70364eb39d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*debn4d-84fGq8tW8.png"/></div></div></figure><p id="c099" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">MinIO任务已添加。我们可以检查数据是否真的进入了桶中。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/8b32dd34a702ef54df8bb48ef118de57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3r1zuKA9D9bPBk2X.png"/></div></div></figure><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/03cf42bfaf6235d29a1bc60c8211d431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/0*vvnUSFLSIcrBfThD.png"/></div></figure><h1 id="861f" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">摘要</h1><p id="57fe" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">如果数据是直接往返于Kafka，Kafka Connect可能是个不错的选择。对于更高级的操作，我更倾向于在流环境中看到解决方案，如Logstash或Spark结构化流。</p><h1 id="0fa2" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">被卖方收回的汽车</h1><p id="5e3b" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated"><a class="ae ml" href="https://github.com/zorteran/wiadro-danych-kafka-connect-nutshell" rel="noopener ugc nofollow" target="_blank">https://github . com/zorteran/wia dro-dany ch-Kafka-connect-null</a></p></div></div>    
</body>
</html>