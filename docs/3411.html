<html>
<head>
<title>Parsing 18 billion JSON lines with Go</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Go解析180亿个JSON行</h1>
<blockquote>原文：<a href="https://itnext.io/parsing-18-billion-lines-json-with-go-738be6ee5ed2?source=collection_archive---------0-----------------------#2019-12-10">https://itnext.io/parsing-18-billion-lines-json-with-go-738be6ee5ed2?source=collection_archive---------0-----------------------#2019-12-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6ece" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我的雇主<a class="ae kl" href="https://shopgun.com" rel="noopener ugc nofollow" target="_blank"> Tjek </a>处，我们最近决定重建我们的事件管道，并将其转移到<a class="ae kl" href="https://cloud.google.com/bigquery/" rel="noopener ugc nofollow" target="_blank">谷歌BigQuery </a>以降低堆栈的复杂性，并移除一些不再可维护的服务。</p><p id="bf4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">BigQuery提供了一系列很好的工具来查询和可视化我们的数据，这将使我们的许多内部团队能够直接处理数据并与客户共享，而不必向工程部门提出请求。</p><p id="48b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">替代旧http服务的新服务很容易编写，但是随之而来的问题是将历史数据转移到BigQuery中。</p><p id="d41a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了迁移旧数据，我们必须将过去10年积累的整个数据集回填到BigQuery中，这就是如何完成的故事。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/f7f8c22cda7ea58ccf320fe72c34e1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O82Eo-bJF7xBjeeP26Wf9Q.png"/></div></div></figure><h1 id="7b26" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">闪回</h1><p id="9c0a" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">这是在一个早先的帖子中描述的平台上完成的</p><div class="mb mc gp gr md me"><a rel="noopener  ugc nofollow" target="_blank" href="/kubernetes-in-production-shopgun-2c280f0c0923"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ir gy z fp mj fr fs mk fu fw ip bi translated">生产中的kubernetes @ shop gun</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">—建造、装箱、运输</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">itnext.io</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms kw me"/></div></div></a></div><p id="7258" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里还描述了上述平台的构建管道</p><div class="mb mc gp gr md me"><a rel="noopener  ugc nofollow" target="_blank" href="/building-a-kubernetes-ci-cd-pipeline-on-aws-with-codepipeline-codebuild-shopgun-43ccf76277b5"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ir gy z fp mj fr fs mk fu fw ip bi translated">使用CodePipeline &amp; CodeBuild @ Shopgun在AWS上构建Kubernetes CI/CD管道</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">手动构建容器并部署到集群可能非常繁琐。</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">itnext.io</p></div></div><div class="mn l"><div class="mt l mp mq mr mn ms kw me"/></div></div></a></div><h1 id="831e" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">流动状态</h1><p id="c662" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">旧的事件管道本身就是一头野兽，它由用<a class="ae kl" href="https://www.erlang.org/" rel="noopener ugc nofollow" target="_blank"> Erlang </a>编写的多个服务组成，并由一个<a class="ae kl" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank"> Kafka集群</a>T18】Citus postgres集群提供支持</p><ul class=""><li id="e893" class="mu mv iq jp b jq jr ju jv jy mw kc mx kg my kk mz na nb nc bi translated">一个http服务器，它将从我们的客户端接收事件，并将其发布到一个脏事件主题。</li><li id="3844" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">使用脏事件主题的重复数据消除服务在干净事件主题中发布新事件</li><li id="1420" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">一种备份服务，使用干净事件主题，并将它们存储在<a class="ae kl" href="https://aws.amazon.com/s3/" rel="noopener ugc nofollow" target="_blank"> S3 </a>中。</li><li id="28ac" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">一种数据库服务，使用clean events主题并将它们插入到Postgres集群中。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6a354831b12e3010d065b60008fb6c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*QqdIbmKmCsTV2-vJaE7GbA.png"/></div></figure><p id="a58c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这意味着我需要加载到BigQuery的事件要么在S3，要么在卡夫卡，等待被写入备份</p><h1 id="2dbf" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">内在的敌人</h1><p id="915f" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">我遇到的第一个障碍是<strong class="jp ir"> BigQuery不允许在您正在加载的JSON的字段名中使用标点符号</strong>。</p><p id="4084" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二个问题是，它要求将要加载的数据已经根据目标表进行了<strong class="jp ir">预分类</strong>。</p><p id="eb76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">backups &amp; Kafka主题包含11种相互混合的不同事件类型。</p><blockquote class="nj"><p id="09e5" class="nk nl iq bd nm nn no np nq nr ns kk dk translated"><strong class="ak">这意味着我们在卡夫卡和S3中的每一行JSON都需要在加载到BigQuery之前进行转换… </strong></p></blockquote><figure class="nu nv nw nx ny kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nt"><img src="../Images/83efc9a1da391f2b9ada72e85087a42a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oGcO9IF8vCKl1mz0tiw9YQ.png"/></div></div></figure><h1 id="b890" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">明天是昨天</h1><p id="6eae" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">还记得备份服务吗？它从Kafka主题中读取X个事件，保存它已经消耗的偏移量，然后gzip将其全部打包并发送到S3。</p><p id="a80b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当前设置是每个主题分区100k个事件，该主题有32个分区，这意味着在clean Kafka主题中可能有大约300万个事件等待备份。</p><p id="9ada" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了从Kafka获取事件，我想我应该停止我们的备份服务，以便有一个稳定的偏移量来开始使用，并通过我刚才用于事件备份文件的同一个JSON解析器传递回数据</p><h1 id="815c" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated"><strong class="ak">学习曲线</strong></h1><p id="aede" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">下载一些gzip文件并对内容做一些事情听起来并不坏，所以我去了</p><p id="5e1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们当时有大约37k个文件，总计大约800GB的大约10年前的gzipped JSON，这些文件都需要转换。</p><p id="66c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">几年前，当我们搬到Citus时，以前的一位同事也做过类似的事情。当时使用的软件需要运行50份拷贝，几乎需要一整个星期才能完成。</p><p id="e9d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我想在合理的时间内做到这一点，这样我就不必为结果等待几天，“Bash + AWS cli + jq”是不可能的。它需要快速且可扩展，以便一次处理多个文件</p><p id="6ef1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的备份文件是以卡夫卡为中心的，而不是围绕事件形式设计的。</p><p id="188f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Kafka主题有32个分区，每个分区都有一个包含多达11种不同事件类型的文件，每个文件中的事件时间戳平均在4-6天之间。</p><blockquote class="nz oa ob"><p id="bfb0" class="jn jo oc jp b jq jr js jt ju jv jw jx od jz ka kb oe kd ke kf of kh ki kj kk ij bi translated">000.00000000000000000000–00000000000000999999.jsonl.gz【000.00000000000001000000–00000000000001999999.jsonl.gz】T2<br/>000.00000000000002000000–00000000000002999999.jsonl.gz<br/>000.00000000000003000000–00000000000003999999.jsonl.gz<br/>000.00000000000004000000–00000000000004999999.jsonl.gz<br/>…<br/>031.00000000000566000000–00000000000566099999.jsonl.gz</p></blockquote><p id="3327" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我开始在<a class="ae kl" href="https://golang.org/" rel="noopener ugc nofollow" target="_blank"> Go </a>中编写一个工具，运行下载备份文件并并行处理它们的工人，然后根据事件类型和备份文件来源将输出分类到不同的文件夹和文件中。</p><p id="1402" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">历史上，备份文件以1M的间隔写入，这意味着最初几年每个文件可能有数周的数据，并行打开32个文件试图将输出写入不同的文件被证明是太多了</p><p id="e7ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">IO访问会激增，一切都会停止，因为在运行许多工作线程时，它会同时写入数千个文件</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8d7718cdd130964adc2f92571da926d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*3H4aXIsADnhZL5MPA_WxAw.jpeg"/></div></figure><p id="a7da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我发现我需要同时处理文件，同时将磁盘写入序列化到更少的更大的文件中，这在底层存储上应该更容易，让我可以更好地控制资源</p><p id="f880" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我决定采用下面的文件格式(hive启发的命名)。</p><p id="96de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">/data/type = % d/event-YYYY-MM-DD . jsonl . gz</p><p id="d10b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这将按每天和事件类型生成一个文件。</p><p id="82a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我创建了一个我称之为<a class="ae kl" href="https://github.com/roffe/multiwriter" rel="noopener ugc nofollow" target="_blank"> Multiwriter </a>的Go包来帮助我做这件事。</p><p id="defc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你输入多写者文件名和一个io。您希望持久存储到磁盘的阅读器。</p><p id="4829" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，Multiwriter使用LRU缓存来保存最新写入文件的文件句柄。它还使用信号量通道来防止同时进行超过X次写入。</p><p id="e196" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我还改变了文件的输入顺序，不再从S3中读取按数字排序的文件，而是根据它们属于哪个分区将它们分类成片，并在分区间“水平地”消耗文件。</p><blockquote class="nz oa ob"><p id="bd98" class="jn jo oc jp b jq jr js jt ju jv jw jx od jz ka kb oe kd ke kf of kh ki kj kk ij bi translated">000.00000000000000000000–00000000000000999999.jsonl.gz<br/>001.00000000000000000000–00000000000000999999.jsonl.gz<br/>002.00000000000000000000–00000000000000999999.jsonl.gz<br/>…<br/>031.00000000000000000000–00000000000000999999.jsonl.gz<br/>然后在分区0上重新开始并增加偏移量<br/>000.00000000000001000000–00000000000001999999.jsonl.gz<br/>001.00000000000001000000–00000000000001999999.jsonl.gz<br/>002.00000000000001000000–00000000000001999999.jsonl.gz<br/>…<br/>031.00000000000001000000–00000000000001999999.jsonl.gz<br/>…</p></blockquote><p id="f816" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有了32个worker，理论上我可以在Kafka流被写入时重放它，同时我也可以减少打开的文件，因为与“垂直”遍历整个文件列表相比，它将解析的文件包含基本相同的日期</p><p id="7fd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">偶数处理器中的内部数据流如下所示</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/45cc6c1998a0b462a602b378b3ba2d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*UiIXvXXYtKC10Qa6vZiI5Q.png"/></div></figure><h1 id="04c8" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">有人来照看我</h1><p id="e395" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">在解析器的第一次试运行期间，花了将近6个小时来解析所有的备份，记住JSON解码和编码是非常昂贵的，并且标准JSON包严重依赖于反射。</p><p id="2b51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我已经看到了承诺比stock快得多的包，并决定尝试一下json迭代器，因为它声称替换率100%。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/9ea23bdcde2aa45bb34f3c00a2f1b76d.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*m1Ubr8g5At-ghYFEK4zsYA.png"/></div></figure><p id="d020" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它还有一个“最快”的设置，不对输出中的字段名进行排序，这样可以节省更多的cpu周期</p><h1 id="9c17" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">荣誉日</h1><p id="3f34" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">在AWS的36核机器上运行备份解析器。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oj"><img src="../Images/b281740d61690df3b167f6708d12ebff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gpxwOpvikIXD3EvM177MGw.png"/></div></div><figcaption class="ok ol gj gh gi om on bd b be z dk translated">飞吧，小苍蝇</figcaption></figure><p id="2b3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦所有文件都解析完毕，下一个问题就出现了。上传这些文件需要“很长时间”,因为将文件上传到S3的CLI客户端一次只能上传一个文件。</p><p id="751c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我在我的解析器中编写了一个函数，它将产生上传工人，并使用<a class="ae kl" href="https://github.com/Ticketmaster/aws-sdk-go" rel="noopener ugc nofollow" target="_blank"> AWS SDK Go </a>一次上传15个文件，同时将传输时间减少到仅仅半小时。</p><p id="f9db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从这里开始，只需要在BigQuery中创建传输作业，并观察它的运行和等待</p><p id="b0a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总之，<strong class="jp ir"> 18 315 948 132 </strong>事件在大约4小时内被解析，重写&amp;上传。</p><p id="e4d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">去掉上传的半个小时意味着该工具每分钟转换了大约8720万个事件</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c6fde5931f951b9bea707914fc64d43e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2X5Eebw9O50SA9BB1lTCIg.jpeg"/></div></figure><p id="074f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望你喜欢阅读我的JSON小冒险。如果您有任何问题，请随时评论！:)</p><p id="2125" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Tjek正在寻找另一位敬业的DevOps工程师。如果你热爱<a class="ae kl" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank"> Cloud </a>、<a class="ae kl" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a>、Linux &amp;、<a class="ae kl" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker </a>并希望在弹性工作环境中工作，请点击此处<a class="ae kl" href="https://tjek.breezy.hr/p/b27dedd16b4501-devops-engineer-with-a-passion-for-problem-solving" rel="noopener ugc nofollow" target="_blank">查看职位</a></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi op"><img src="../Images/b07fedc84100959d2535ffec4db00ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*NnqPIq_rerd2CuN2rbzVTg.png"/></div></figure><h1 id="b83b" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">笔记</h1><p id="c54d" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">我说的JSON行是指<a class="ae kl" href="http://jsonlines.org/" rel="noopener ugc nofollow" target="_blank"> JSONL </a>。<br/>也叫换行符分隔的JSON (NDJSON)</p><p id="4c1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">示例:</p><pre class="kn ko kp kq gt oq or os ot aw ou bi"><span id="e51e" class="ov kz iq or b gy ow ox l oy oz">{"name": "Gilbert", "wins": [["straight", "7♣"], ["one pair", "10♥"]]}<br/>{"name": "Alexa", "wins": [["two pair", "4♠"], ["two pair", "9♠"]]}<br/>{"name": "May", "wins": []}<br/>{"name": "Deloise", "wins": [["three of a kind", "5♣"]]}</span></pre></div></div>    
</body>
</html>