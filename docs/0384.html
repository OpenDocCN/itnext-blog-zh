<html>
<head>
<title>Reinforcement Learning with Q tables</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Q表进行强化学习</h1>
<blockquote>原文：<a href="https://itnext.io/reinforcement-learning-with-q-tables-5f11168862c8?source=collection_archive---------0-----------------------#2018-03-02">https://itnext.io/reinforcement-learning-with-q-tables-5f11168862c8?source=collection_archive---------0-----------------------#2018-03-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/6694258426e33359e15dc2150764f947.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*QVsnwatDVz8wcqJUsLJejw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">强化学习——主体的行动和环境的回应</figcaption></figure><h1 id="1fd6" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">什么是强化学习</h1><p id="68cb" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">强化学习是机器学习中处理延迟奖励的一个领域。<br/> <strong class="ky ir">这是什么意思？</strong>嗯，简单，我举个例子解释一下。对于这一点，我假设你已经听说过(如果你知道就更好了)神经网络，甚至回归或分类的基本知识都可以。让我们举一个分类问题的例子，你已经得到了一大块狗的图像，你必须设计一个系统，它能够通过判断图像是否是狗的来区分图像。任何有一点机器学习知识的人都会建议你使用卷积神经网络，并用提供的图像进行训练，是的，它会起作用。但是怎么做呢？好吧，不用深入细节(也许稍后会有一篇关于这个的文章？！)你先在样本图像上训练神经网络。在训练时，神经网络学习狗的图像特有的小特征和模式。在训练期间，你知道预期的输出，这是一个狗的图像，所以每当网络预测错误，我们纠正它。在某种程度上，我们知道对提供的图像的奖励，如果预测是正确的，我们给予积极的奖励，如果预测是错误的，奖励是消极的，并采取纠正措施来学习和适应。所以我们知道直接的回报。</p><p id="c2ea" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">但是如果我们不知道眼前的回报呢？</strong>在这里，强化学习进入了画面。</p><p id="ef67" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">为了解释这一点，让我们创建一个游戏。游戏很简单，一排有10块瓷砖。所有的瓷砖都不一样，有些有我们不想去的洞，而有些有啤酒，我们肯定想去。当游戏开始时，你可以在任何牌上产卵，并且可以向左或向右走。游戏将继续下去，除非我们赢了或者游戏结束，让我们称这样的每一次迭代为一集。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi lz"><img src="../Images/7ee94ef35bef58fe70bc3431dd266c15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZRhGiCE6oOjvyCDj2nsPA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">去喝啤酒！！</figcaption></figure><p id="0442" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">所以，如果你在第0块瓷砖上产卵，或者以某种方式移动到第0块瓷砖，游戏就结束了，但是如果我们移动到第6块瓷砖，我们就赢了。</p><p id="e8f4" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">让我们以一个简单的插曲为例。好吧，让我们假设我们在瓷砖2上产卵。现在假设我没有给你看游戏地图，你只有向左或向右的选择，你会走哪条路？啊你不能说除非你尝试它。假设你一直向左走，直到你到了第0张牌，那张有洞的牌，你输了。这不是我们想要发生的，所以让我们给我们从2到1到0的左转动作分配一个负奖励。在下一集，你可能会再次出现在瓷砖2，这次你继续向右，直到你到达瓷砖6。啤酒来了，让我们给行动分配积极的奖励。</p><p id="77ab" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">我们学到了什么？对于我们所走的每一步，直到我们打了洞或喝了啤酒，我们都不知道会有什么回报。延迟奖励伙计们。没有人告诉你正确的方向，每一步之后都没有回报，暗示它是正确还是错误的方向。现在我们甚至很难掌握正确动作的感觉，如果我们想让计算机学习呢？强化学习拯救世界。</strong></p><h1 id="c672" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">马尔可夫决策过程</h1><p id="6e70" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这个马尔可夫过程是什么，为什么我们需要学习它？我也是这么想的，而且要明确的是，我们不需要深究，只要一个基本的直觉就可以了。</p><p id="3616" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">因此，马尔可夫决策过程被用于在结果部分是随机的，部分在决策者控制下的情况下对决策进行建模。简而言之，所有的瓷砖，左和右的行动，我们讨论的消极和积极的奖励可以用马尔可夫过程建模。</p><p id="5688" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">马尔可夫决策过程包括，</p><ol class=""><li id="bdb9" class="mi mj iq ky b kz lu ld lv lh mk ll ml lp mm lt mn mo mp mq bi translated"><strong class="ky ir">状态</strong>:是一组状态。我们例子中的瓷砖。所以我们的游戏中有10个州。</li><li id="dd0e" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt mn mo mp mq bi translated"><strong class="ky ir">动作(A) </strong>:状态<code class="fe mw mx my mz b">s</code>下可用的一组动作。从我们的游戏中向左&amp;向右。</li><li id="d356" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt mn mo mp mq bi translated"><strong class="ky ir">转移概率</strong> <code class="fe mw mx my mz b">P(s'|s, a)</code>:如果我们在<code class="fe mw mx my mz b">t</code>时刻在<code class="fe mw mx my mz b">s</code>状态采取动作<code class="fe mw mx my mz b">a</code>，那么在<code class="fe mw mx my mz b">t+1</code>时刻转移到<code class="fe mw mx my mz b">s'</code>状态的概率。我们在这前面被排序，从瓦片3到瓦片2的一个左边，没有问题被问。</li><li id="df15" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt mn mo mp mq bi translated"><strong class="ky ir">奖励</strong> <code class="fe mw mx my mz b">R(s'|s, a)</code>:这是我们通过采取行动<code class="fe mw mx my mz b">a</code>从状态<code class="fe mw mx my mz b">s</code>过渡到状态<code class="fe mw mx my mz b">s'</code>时得到的奖励。</li><li id="6c3d" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt mn mo mp mq bi translated"><strong class="ky ir">贴现(Y) </strong>:是贴现因子，代表未来和现在奖励的差异。</li></ol><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/33c6688da50ae2ec1a77aac730db34db.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*AekKnqbj6Er2AEVqP3541g.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">由于某种动作而引起的状态变化。就这么简单！</figcaption></figure><p id="3866" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">因此，马尔可夫过程可以理解为状态<code class="fe mw mx my mz b">S</code>的集合，其中一些动作<code class="fe mw mx my mz b">A</code>可能来自具有某种概率<code class="fe mw mx my mz b">P</code>的每个状态。每一个这样的行动都会导致一些奖励。如果概率和回报未知，问题就在于强化学习。在这里，我们将使用Q学习或者更好地使用它的最基本实现Q表来解决一个简单的问题。</p><h1 id="47ba" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">q学习</h1><p id="a964" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">现在考虑到以上所有学到的理论，我们想要建立一个代理来像人类一样遍历我们的啤酒和洞游戏(寻找更好的名字)。为此，我们应该有一个政策，告诉我们什么时候做什么。就当是游戏的揭秘图吧。政策越好，我们赢得比赛的机会就越大，因此得名Q(质量)学习。我们政策的质量将随着培训而提高，并将继续提高。为了学习，我们将使用贝尔曼方程，它是这样的:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ef19c6a65d0f4a8aa37a4fc3e03c2bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*PjruvnkIjxI3OHVdPjueMQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk translated">未来报酬折现的贝尔曼方程</figcaption></figure><p id="ea57" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在哪里，</p><ul class=""><li id="bb2d" class="mi mj iq ky b kz lu ld lv lh mk ll ml lp mm lt nc mo mp mq bi translated"><code class="fe mw mx my mz b">Q(s,a)</code>当前的行动方针<code class="fe mw mx my mz b">a</code>是从<code class="fe mw mx my mz b">s</code>状态</li><li id="8a6e" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt nc mo mp mq bi translated"><code class="fe mw mx my mz b">r</code>是对行动的奖励</li><li id="590b" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt nc mo mp mq bi translated"><code class="fe mw mx my mz b">max(Q(s',a'))</code>定义了未来的最大奖励。假设我们在状态<code class="fe mw mx my mz b">s</code>采取动作<code class="fe mw mx my mz b">a</code>到达状态<code class="fe mw mx my mz b">s'</code>。从这里我们可能有多个动作，每个动作对应一些奖励。计算奖励的最大值。</li><li id="e325" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt nc mo mp mq bi translated"><code class="fe mw mx my mz b">Y</code>是贴现因子。现在，价值从0到1变化，如果价值接近0，立即奖励优先，如果价值接近1，未来奖励的重要性增加，直到1，它被认为等于立即奖励。</li></ul><p id="90f5" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在这里，我们试图把延迟的奖励变成直接的奖励。对于我们从一个州采取的每一个行动，我们更新我们的政策表，让我们称之为Q表，以包括积极或消极的奖励。假设我们在牌4中，我们将右转，使牌5成为下一个状态，牌4的直接奖励将包括牌5中所有可能行动的最大奖励的某个因子(由折扣确定)。如果你查看游戏地图，从方块5向右会导致方块6，这是我们游戏的最终目标，所以方块4的正确动作也会被分配一些积极的奖励。</p><h1 id="53f6" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">密码</h1><p id="7a70" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">好了，现在有太多的理论，让代码。</p><p id="5add" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">让我将状态、行动和奖励定义为矩阵。一种方法是用行表示所有状态，用列表示动作，因此我们有10个状态和2个动作，我们将定义一个10x2的矩阵。为了简单起见，我没有使用任何库，只是用python中的列表对它们进行编码。</p><pre class="ma mb mc md gt nd mz ne nf aw ng bi"><span id="22ab" class="nh jz iq mz b gy ni nj l nk nl">environment_matrix = [[None, 0],<br/>                  [-100, 0],<br/>                  [0, 0],<br/>                  [0, 0],<br/>                  [0, 0],<br/>                  [0, 100],<br/>                  [0, 0],<br/>                  [100, 0],<br/>                  [0, 0],<br/>                  [0, None]]</span></pre><p id="fdb7" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">如你所见，从方块5向右走和从方块7向左走有100的高回报，因为它通向方块6。同样，从牌1向左导致洞，所以它有一个负奖励。由于没有-1或第10个牌，所以牌0和9具有左右奖励<code class="fe mw mx my mz b">None</code>。</p><p id="7c41" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在是我们的魔法Q表的时间了，它将随着代理在每一集的学习而更新。</p><pre class="ma mb mc md gt nd mz ne nf aw ng bi"><span id="b15e" class="nh jz iq mz b gy ni nj l nk nl">q_matrix = [[0, 0],<br/>      [0, 0],<br/>      [0, 0],<br/>      [0, 0],<br/>      [0, 0],<br/>      [0, 0],<br/>      [0, 0],<br/>      [0, 0],<br/>      [0, 0],<br/>      [0, 0]]</span></pre><p id="c877" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">首先，让我们把所有的赋值为零。</p><p id="35b8" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">定义一些有助于游戏遍历的函数。</p><pre class="ma mb mc md gt nd mz ne nf aw ng bi"><span id="3293" class="nh jz iq mz b gy ni nj l nk nl">win_loss_states = [0,6]</span><span id="6787" class="nh jz iq mz b gy nm nj l nk nl">def getAllPossibleNextAction(cur_pos):<br/>    step_matrix = [x != None for x in environment_matrix[cur_pos]]<br/>    action = []<br/>    if(step_matrix[0]):<br/>        action.append(0)    <br/>    if(step_matrix[1]):<br/>        action.append(1)<br/>    return(action)</span><span id="bae6" class="nh jz iq mz b gy nm nj l nk nl">def isGoalStateReached(cur_pos):<br/>    return (cur_pos in [6])</span><span id="3617" class="nh jz iq mz b gy nm nj l nk nl">def getNextState(cur_pos, action):<br/>    if (action == 0):<br/>        return cur_pos - 1<br/>    else:<br/>        return cur_pos + 1</span><span id="7a6b" class="nh jz iq mz b gy nm nj l nk nl">def isGameOver(cur_pos):<br/>    return cur_pos in win_loss_states</span></pre><p id="3c7b" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">让我们一个一个来看，</p><ul class=""><li id="94ed" class="mi mj iq ky b kz lu ld lv lh mk ll ml lp mm lt nc mo mp mq bi translated">传递你的当前状态，它会返回所有可能的动作。注意，对于图块0，只有右动作，对于只有左动作的图块9也是如此</li><li id="1ef3" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt nc mo mp mq bi translated"><code class="fe mw mx my mz b">isGoalStateReached</code>如果当前图块是6，它将返回<code class="fe mw mx my mz b">True</code></li><li id="c4cd" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt nc mo mp mq bi translated"><code class="fe mw mx my mz b">getNextState</code>传递当前状态和动作，返回下一个状态</li><li id="47c2" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt nc mo mp mq bi translated"><code class="fe mw mx my mz b">isGameOver</code>如果状态为0或6，游戏结束，返回<code class="fe mw mx my mz b">True</code></li></ul><p id="809c" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在是训练部分，</p><pre class="ma mb mc md gt nd mz ne nf aw ng bi"><span id="c8b6" class="nh jz iq mz b gy ni nj l nk nl">discount = 0.9<br/>learning_rate = 0.1</span><span id="4437" class="nh jz iq mz b gy nm nj l nk nl">for _ in range(1000):<br/>    # get starting place<br/>    cur_pos = random.choice([0,1,2,3,4,5,6,7,8,9])<br/>    # while goal state is not reached<br/>    while(not isGameOver(cur_pos)):<br/>        # get all possible next states from cur_step<br/>        possible_actions = getAllPossibleNextAction(cur_pos)<br/>        # select any one action randomly<br/>        action = random.choice(possible_actions)<br/>        # find the next state corresponding to the action selected<br/>        next_state = getNextState(cur_pos, action)<br/>        # update the q_matrix<br/>        q_matrix[cur_pos][action] = q_matrix[cur_pos][action] + learning_rate * (environment_matrix[cur_pos][action] + <br/>            discount * max(q_matrix[next_state]) - q_matrix[cur_pos][action])<br/>        # go to next state<br/>        cur_pos = next_state<br/>    # print status<br/>    print("Episode ", _ , " done")</span><span id="c964" class="nh jz iq mz b gy nm nj l nk nl">print(q_matrix)<br/>print("Training done...")</span></pre><p id="a732" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">让我澄清一下，</p><ul class=""><li id="0706" class="mi mj iq ky b kz lu ld lv lh mk ll ml lp mm lt nc mo mp mq bi translated">首先，我们定义了折现因子和学习率</li><li id="8ca7" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt nc mo mp mq bi translated">我们要训练1000集</li><li id="91f1" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt nc mo mp mq bi translated">产卵是完全随机的，它可能是任何瓷砖</li><li id="aef2" class="mi mj iq ky b kz mr ld ms lh mt ll mu lp mv lt nc mo mp mq bi translated">虽然这一集还没有结束，我们继续采取随机行动，并更新Q表</li></ul><p id="0cd9" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">1000集之后，Q表有些什么样子，</p><pre class="ma mb mc md gt nd mz ne nf aw ng bi"><span id="6d3b" class="nh jz iq mz b gy ni nj l nk nl">[[0, 0], [-99.99999999730835, 65.60999997057485], [59.04899994359059, 72.8999999993016], [65.60999999858613, 80.99999999978154], <br/>[72.89999999929572, 89.99999999991468], [80.99999999863587, 99.99999999997391], [0, 0], [99.9999999999985, 80.99999999994624], <br/>[89.99999999999515, 72.89999999997386], [80.99999999999046, 0]]</span></pre><p id="a578" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">让我们稍微美化一下，开始吧，</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/787613cc01ed67948d93c3e827cc858c.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*RhO9Ulh5nF_zc2pcgBHzAg.png"/></div></figure><p id="0949" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在我们的政策是什么，如果你发现自己处于任何状态，选择具有较高值的动作(这里是较暗的绿色阴影),你将获得啤酒。不错吧！</p><h1 id="93a4" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">下一步</h1><p id="fc4c" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如果我们用有多个洞的2D板来增加游戏的复杂性，那会是一件有趣的事情。</p><p id="c084" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">注</strong>:此注来自我个人<a class="ae no" href="http://mohitmayank.com" rel="noopener ugc nofollow" target="_blank">网站</a>。</p></div></div>    
</body>
</html>