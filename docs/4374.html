<html>
<head>
<title>Five highlights on the Spark 3.0.0 Release</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark 3.0.0版本的五大亮点</h1>
<blockquote>原文：<a href="https://itnext.io/five-highlights-on-the-spark-3-0-release-ab8775804e4b?source=collection_archive---------1-----------------------#2020-06-19">https://itnext.io/five-highlights-on-the-spark-3-0-release-ab8775804e4b?source=collection_archive---------1-----------------------#2020-06-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="54d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Spark 3.0.0于昨天(18/Jun/2020)正式发布，它是世界上最流行的数据处理引擎(<a class="ae km" href="https://flink.apache.org/" rel="noopener ugc nofollow" target="_blank"> <em class="kl">【弗林克】</em> </a> <em class="kl">，你也很棒，但Spark更出名一点</em>)的重大变化(<em class="kl">)。</em></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kn"><img src="../Images/46406198ec5c40133b3293df33c087d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nh0EEU87snaVf0LnR3Q_WA.png"/></div></div></figure><p id="9cb9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，提醒一下，火花到底是什么？它是一个开源、容错、高速的内存数据处理引擎。它支持批处理和流处理以及一些很酷的分析。ML部分在Spark 2中用得不多，但现在我们可能会在Spark 3中有新的变化！</p><p id="8190" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了Databricks服务和Spark运行的内部设置之外，这将影响许多工作负载。Cloudera发行版、在Spark上构建产品的公司和每个公共云提供商(<em class="kl">他们都有Spark托管服务</em>)将对此做出反应，我急切地等待着看他们会构建什么！(<a class="ae km" href="https://databricks.com/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html" rel="noopener ugc nofollow" target="_blank"> <em class="kl"> Databricks已经推出了Spark 3.0！</em> </a>)。</p><p id="0dec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回到几年前，Spark 2带来了许多基本的新东西(<em class="kl">catalyst optimizer是最值得注意的一个，在我看来</em>)真正将Spark推向了企业世界。与不同输入源的集成、Hive metastore连接、结构化流等等将Spark变成了今天的样子:<strong class="jp ir">在数据湖之上构建数据管道</strong>和工作负载最常用的引擎。所以，我们开门见山吧，蒂亚戈？你认为惊天动地的发布中有哪些必不可少的东西？</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="kz la l"/></div></figure><h1 id="6b91" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">1.SQL ANSI和兼容性</h1><p id="d230" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">如果说Excel统治世界，SQL统治数据的黑社会。它很容易学习，很直观，你可以用它做很多事情(在<em class="kl">和</em> <a class="ae km" href="https://www.reddit.com/r/programminghorror/comments/2wr19u/whats_the_worst_abuse_of_sql_youve_ever_seen/" rel="noopener ugc nofollow" target="_blank"> <em class="kl">这个Reddit帖子</em> </a>)上看到一些不错的SQL滥用)。然而，我们总是不得不学习一些在不同系统中编写SQL的技巧和诀窍，因为它们有细微的区别。SQL ANSI为它建立了一个标准，Spark在这方面有些落后——我在过滤聚合表达式时总是出错，但现在已经解决了！</p><blockquote class="me mf mg"><p id="3c65" class="jn jo kl jp b jq jr js jt ju jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj kk ij bi translated">当然，我们还没有完全实现，因为Spark 3.0还没有完全实现ANSI/SQL，但是你可以在这里查看这项工作的状态<a class="ae km" href="https://issues.apache.org/jira/browse/SPARK-30374" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><p id="9fdc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是否意味着如果我升级我的Spark版本，我的旧Spark-SQL代码将停止工作？嗯，升级主要版本总是一个问题，但ANSI SQL部分可以切换为一个配置，以保留一些关键字，只需添加:</p><pre class="ko kp kq kr gt mk ml mm mn aw mo bi"><span id="f56d" class="mp lc iq ml b gy mq mr l ms mt">spark.sql.ansi.enabled = True</span></pre><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="mu la l"/></div></figure><h1 id="e256" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.土著普罗米修斯</h1><p id="2273" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">在运行生产工作负载时，对工作的可观察性不是一件好事，而是必须的。尽管我们可以从Spark jobs获得大量指标，但我认为几乎每个人都构建了一些定制的网关，将他们的指标转发到其他地方，以监控和警告您的作业。尤其是JVM内存和垃圾收集信息，对吧？:-)</p><p id="a86f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如今监控和报警几乎是<a class="ae km" href="https://prometheus.io/" rel="noopener ugc nofollow" target="_blank">普罗米修斯</a>的同义词。这个由<a class="ae km" href="https://www.cncf.io/" rel="noopener ugc nofollow" target="_blank"> CNCF </a>开发的令人惊叹的开源项目非常棒，有很多贡献者，进展迅速，并且易于建立。在我们参与的大多数Spark 2.x项目中，我们会构建一个Prometheus exporter来导出Graphite并对其进行清理，这样我们就可以获得度量信息，并为执行者提供警报。具体怎么做？</p><h2 id="6e80" class="mp lc iq bd ld mv mw dn lh mx my dp ll jy mz na lp kc nb nc lt kg nd ne lx nf bi translated">2.1火花2普罗米修斯监控</h2><p id="27ad" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">㈠将数据导出到Graphite。<br/>我们将在容器中部署导出器，并通过适当的自动扩展与集群并行运行。用于本地运行的简单docker-compose文件及其配置文件如下所示(<em class="kl">配置文件是指标的正确标记，因此我们可以使用Prometheus </em>查询它们)。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ng la l"/></div></figure><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ng la l"/></div></figure><p id="d74f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(II)当运行您的spark应用程序时，您的<code class="fe nh ni nj ml b">$SPARK_HOME/conf/</code>文件夹中必须有一个适当的<code class="fe nh ni nj ml b">metrics.properties</code>文件，它从(I)指向作为接收器的石墨导出器。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ng la l"/></div></figure><p id="075e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如你所见，并不复杂，但很麻烦。特别是当你开始有数百个新的Spark应用程序时，有些人可能会忘记这一点，而你可能会意识到当应用程序没有运行几个小时时。当然，您可以将这些步骤添加到您的CD管道中，但是您可以想象这种自动化需要做多少工作。</p><h2 id="5795" class="mp lc iq bd ld mv mw dn lh mx my dp ll jy mz na lp kc nb nc lt kg nd ne lx nf bi translated">火花3.0上的2.1？简单多了…</h2><p id="4c37" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">除了通过<code class="fe nh ni nj ml b">/applications/[app-id]/executors</code>上的REST API公开执行器指标，我们现在将有一个原生的<a class="ae km" href="https://issues.apache.org/jira/browse/SPARK-29429" rel="noopener ugc nofollow" target="_blank">普罗米修斯端点</a>T3】，在这里您只需要切换配置<code class="fe nh ni nj ml b">spark.ui.prometheus.enabled</code>。</p><p id="82a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那个。是。它。一个开关… :-D</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nk la l"/></div></figure><h1 id="9cc2" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">3.语言版本变化，Hadoop和Kubernetes</h1><p id="d521" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">Spark有4个SDK:Python、Scala、Java和R (the。网一非官方)。Python是最常用的，这就是为什么Spark 3特别关注Pandas集成、Python上的UDF优化等等。而且，正如预期的那样，在如此重大的版本变化中，一些东西将被添加，而另一些则被弃用。这些就是变化:</p><ul class=""><li id="371f" class="nl nm iq jp b jq jr ju jv jy nn kc no kg np kk nq nr ns nt bi translated">Python 2支持被否决了，我非常高兴！</li><li id="1018" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">早于3.4的r版本也不推荐使用。</li><li id="b2ed" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">Java 11支持。</li><li id="6a5a" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">Hadoop 3支持，移除Hadoop 2.6支持。</li><li id="f308" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">从现在开始Scala 2.12这与<code class="fe nh ni nj ml b">DataStreamWritter.foreachBatch</code>产生了歧义，所以要小心。</li><li id="aa30" class="nl nm iq jp b jq nu ju nv jy nw kc nx kg ny kk nq nr ns nt bi translated">Kubernetes正在成为运行Spark应用程序的可靠方式。随着新的shuffle和Kerberos对k8s的支持，我们将开始看到企业这样做，而不用担心“成为第一”。</li></ul><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nz la l"/></div></figure><h1 id="186e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">4.动态分区剪枝</h1><p id="ffcb" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">这里看一下原票<a class="ae km" href="https://issues.apache.org/jira/browse/SPARK-11150" rel="noopener ugc nofollow" target="_blank">这里</a>查一下上次PR <a class="ae km" href="https://github.com/apache/spark/pull/25600/files" rel="noopener ugc nofollow" target="_blank">。如果我能用两段话来解释，我会的，但是我不能。因此，我建议您参考我们去年在阿姆斯特丹举行的Spark + AI峰会上的精彩演示。向</a><a class="ae km" href="https://twitter.com/bogdanghit?lang=en" rel="noopener ugc nofollow" target="_blank"> @BogdanGhit </a>和<a class="ae km" href="https://databricks.com/speaker/juliusz-sompolski" rel="noopener ugc nofollow" target="_blank"> Juliusz Sompolski </a>致敬。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="oa la l"/></div></figure><p id="b588" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，为什么这很重要，很神奇呢？查询小维度的星型模式表不是很有执行性，我们必须在这样做之前改变数据结构。现在，嘭！现成的快速查询对于高级分析(也称为TPC-DS数据类型)至关重要。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ob la l"/></div></figure><h1 id="2969" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">5.自适应查询执行</h1><p id="adf5" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">Spark SQL在最近几年越来越多地被使用，很多人致力于SQL查询优化器，所以我们有最好的查询执行计划。当您用自己选择的语言为Spark编写一个SQL查询时，Spark会接受这个查询，并把它翻译成一种易于理解的形式(逻辑计划)。在这个阶段，它通过应用一组基于规则的转换(常量合并、过滤器下推、列修剪等)来优化逻辑计划。下一步是物理规划阶段，在这个阶段，它会生成一个可执行的计划(物理计划),告知如何在集群中分配计算。</p><p id="dfb5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于这种理解，您可以想象为运行查询选择一个更好的计划是多么重要。这包括收集数据统计和基于成本的优化。比如我应该通过<code class="fe nh ni nj ml b">sort merge</code>加入还是通过<code class="fe nh ni nj ml b">broadcast hash</code>加入？如果我正在进行多路连接，连接的顺序是什么？如果您向优化器提供过时的统计信息，您将得到一个非最优的计划，并且执行查询的效果很差。因此，现在我们可以根据查询执行期间收集的统计数据来重新优化和调整查询计划！魔法，对吧？</p><p id="a9ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想了解这方面的基本变化，类似于动态修剪，我必须就此写一整篇文章。所以我可以告诉你在这里看一看原始的史诗<a class="ae km" href="https://issues.apache.org/jira/browse/SPARK-9850" rel="noopener ugc nofollow" target="_blank">，在这里</a>看一看“动态重用子查询”PR <a class="ae km" href="https://github.com/apache/spark/pull/25471/files" rel="noopener ugc nofollow" target="_blank">上的精彩</a><a class="ae km" href="https://twitter.com/maryannxue" rel="noopener ugc nofollow" target="_blank"> @maryannxue </a>代码，并在Spark峰会上通过<a class="ae km" href="https://databricks.com/speaker/carson-wang" rel="noopener ugc nofollow" target="_blank"> Carson Wang </a>的这个稍微老一点的会议(2018)。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="oa la l"/></div></figure><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="ng la l"/></div></figure><h1 id="672e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="a28d" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">Spark通过这一新版本继续改变数据世界。它留下了一个相关的信息，不管你喜不喜欢:</p><blockquote class="me mf mg"><p id="61ac" class="jn jo kl jp b jq jr js jt ju jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj kk ij bi translated">Python、高级分析、数据仓库中的SQL、数据流和深度学习是现在的焦点。</p></blockquote><p id="31f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你想为Spark做贡献吗？复习一些公关？帮助文档？我们都需要你，所以看看问题页面<a class="ae km" href="https://issues.apache.org/jira/projects/SPARK/issues/SPARK-28367?filter=allopenissues" rel="noopener ugc nofollow" target="_blank">这里</a>并投稿吧！</p></div></div>    
</body>
</html>