<html>
<head>
<title>Reinforcement Learning with Raw Actions and Observations in PySC2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySC2中基于原始动作和观察的强化学习</h1>
<blockquote>原文：<a href="https://itnext.io/reinforcement-learning-with-raw-actions-and-observations-in-pysc2-af0b6fd8391f?source=collection_archive---------1-----------------------#2019-09-15">https://itnext.io/reinforcement-learning-with-raw-actions-and-observations-in-pysc2-af0b6fd8391f?source=collection_archive---------1-----------------------#2019-09-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/471dcd63d4df340f4322b3ad523a1bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpUAu8_Q0HitaIBcjbrF6w.png"/></div></div></figure><div class=""/><figure class="gl gn jy jz ka is"><div class="bz fp l di"><div class="kb kc l"/></div></figure><p id="b0b3" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我之前的教程中，我介绍了PySC2的一个新特性，叫做原始观察和原始动作。现在我们可以利用这些知识，尝试用强化学习来教我们的机器人如何玩游戏。</p><p id="f099" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将强化学习应用于完整的游戏极其复杂，需要大量的时间和计算能力，正如DeepMind通过<a class="ae lb" href="https://medium.com/@skjb/the-evolution-of-alphastar-cefff389b9d5" rel="noopener"> AlphaStar </a>向我们展示的那样。我们可以通过显著降低游戏的复杂性来让事情变得更简单。</p><p id="6e34" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">降低复杂性的第一步是限制机器人可以做的事情，所以我们将创建一个人族机器人，它可以建立一个单一的补给站，一个单一的兵营，都在固定的位置，可以训练海军陆战队，并可以攻击预定的位置。</p><p id="3636" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">降低复杂性的第二步是，我们将让机器人与具有相同限制的对手竞争。我们这样做是因为使用游戏中的AI对手会导致更多的单位需要追踪，而且有些东西，比如隐形单位，是我们无法处理的。</p><p id="0914" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第三，对手将简单地随机执行动作，这将给我们一个很好的比较，以确保我们的机器人正在学习，而不仅仅是运气好。</p><p id="001f" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们将消除战争的迷雾，这样就没有隐藏的信息，机器人的世界更可预测，更容易做出反应。</p><p id="1aea" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们开始吧。</p><h1 id="32bb" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">1.创建基本代理</h1><p id="e300" class="pw-post-body-paragraph kd ke jb kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">由于我们的两个机器人基本上以相同的方式运行，我们将创建一个通用的代理类，其中包含学习代理和随机代理都将使用的代码。</p><p id="9545" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们需要导入几个库:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="044c" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们创建代理类:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="a47a" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们定义代理可以执行的操作:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="772b" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我们不允许代理重建他们的指挥中心。你当然可以这么做，但是为了简单起见，我选择不做，因为一般来说，一旦代理失去了指挥中心，他们很快就会被击败。</p><p id="1362" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将需要一些实用方法，首先是一些从观察中收集单位的方法:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="6e45" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还将添加一个方法来计算单位列表和指定点之间的距离:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="fd0c" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些方法将被我们的行动方法所使用。</p><h1 id="ad73" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">2.创建操作方法</h1><p id="e88d" class="pw-post-body-paragraph kd ke jb kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我们需要为之前创建的列表中定义的每个动作添加一个方法。这些方法中的每一个都将接收来自每一步的观察，以便它可以独立地行动。稍后您将会看到，这大大简化了我们代理的结构。</p><p id="58c2" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从简单的“无操作”动作开始，它什么也不做:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="1593" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将创建一个方法，将闲置的SCV发送回一个矿块:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="2ccf" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如你所看到的，有很多矿物补丁单位类型！我们在这里做的是找到最接近所选SCV的矿块。首先，我们使用我们的<code class="fe mk ml mm mn b">get_distances</code>方法计算从每个矿块到SCV的距离，然后我们使用<code class="fe mk ml mm mn b">np.argmin</code>找到最近的距离。</p><p id="61b4" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe mk ml mm mn b">Harvest_Gather_unit</code>原始动作非常简洁，它接受一个工人的单元标签和一个资源(矿物或天然气)的单元标签。因此，不存在常规操作中可能出现的误点击风险。</p><p id="5a40" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们创建一个建立补给站的方法:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="dee6" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类似于我们之前对SCV和矿点所做的，我们找到离建造地点最近的SCV，然后指示它在那里建造。当一个SCV行进到建造地点时，代理可能会多次调用这个方法，所以我们不想随机选择scv，否则会从矿线中取出多个scv。</p><p id="431f" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与常规动作不同，如果动作无法执行，raw动作不会崩溃，但我喜欢执行自己的检查，以便错误通知不会出现在游戏中。我只是需要保持东西整洁。</p><p id="76ff" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的补给站位置是我们将要使用的Simple64地图的位置，并且是我选择的看起来和每个基地位置平衡的位置。<code class="fe mk ml mm mn b">self.base_top_left</code>值将在后面解释。</p><p id="9a0a" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们创建建造兵营的方法:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="ee43" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这与上面的补给站方法非常相似。</p><p id="3135" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们有了兵营，我们就可以训练海军陆战队:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="1205" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">原始观察的一个很酷的特性是，我们可以通过使用<code class="fe mk ml mm mn b">barracks.order_length</code>来查看有多少海军陆战队队员在军营中排队，我们知道对于一个标准的军营来说，这个数字被限制为5。</p><p id="bcde" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，攻击方法:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="e791" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法有一些技巧。首先我们找到离攻击地点最远的陆战队员，这和我们之前用scv做的类似，除了我们用<code class="fe mk ml mm mn b">np.argmax</code>找到最远的距离。</p><p id="6361" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们选择了一个陆战队员，我们实际上是在预定的攻击坐标周围随机选择了一个位置。我们这样做是为了确保我们的单位完全探索敌人的基地位置并摧毁所有建筑。</p><h1 id="9dd8" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">3.创建随机代理</h1><p id="7c4f" class="pw-post-body-paragraph kd ke jb kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我们几乎准备好测试我们的代理了，但是首先我们应该创建一个随机执行动作的代理。首先，向基本代理添加一个方法，该方法将在每个游戏开始时确定基本位置:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="b4c7" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然我们的基本代理相当复杂，但我们的随机代理实际上非常简单:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="53e2" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在这里从预定义的列表中随机选择一个动作，然后使用Python的<code class="fe mk ml mm mn b">getattr</code>将动作名转换为方法调用，并将观察结果作为参数传入。</p><p id="21d1" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们添加运行代理的代码:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="3625" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以看到我们已经使用<code class="fe mk ml mm mn b">disable_fog</code>禁用了战争迷雾，我们还选择了一个相当大的48步乘数，这使得游戏运行得更快，并且不会真正影响这些简单代理的结果。</p><p id="1998" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还为1000个游戏运行代理，这对于聪明的代理学习如何以大约95%的成功率获胜来说是绰绰有余的。</p><p id="5939" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行你的代码，你应该看到两个机器人互相争斗。</p><p id="513a" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这很好，但我们真的希望其中一个机器人学会如何获胜，对吗？</p><h1 id="5e33" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">4.创建Q表</h1><p id="4a12" class="pw-post-body-paragraph kd ke jb kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">强化学习的一个最简单的形式是Q表，它本质上是一个游戏所有状态的电子表格，以及每个状态中每个动作的好坏。机器人根据它是赢还是输来更新每个动作的值，随着时间的推移，它为各种场景建立了一个相当好的策略。</p><p id="a5a9" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我修改了一个版本的<a class="ae lb" href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/2_Q_Learning_maze/RL_brain.py" rel="noopener ugc nofollow" target="_blank"> Morvan Zhou的代码</a>。</p><p id="595f" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先创建<code class="fe mk ml mm mn b">QLearningTable</code>类:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="99f2" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将添加方法来选择机器人应该执行的操作:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="fb15" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的<code class="fe mk ml mm mn b">e_greedy</code>参数决定了机器人选择随机动作而不是最佳动作的频率。值为0.9意味着它在90%的情况下会选择最佳操作，在10%的情况下会选择随机操作。</p><p id="50c2" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了选择最佳动作，它首先检索当前状态下每个动作的值，然后选择值最高的动作。如果多个动作具有相同的最高值，它将随机选择其中一个状态。</p><p id="0e69" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们需要添加允许机器人学习的方法:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="b0bb" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是Q表的神奇之处。参数<code class="fe mk ml mm mn b">s</code>是指<em class="mo">之前的</em>状态，<code class="fe mk ml mm mn b">a</code>是在该状态下执行的动作，<code class="fe mk ml mm mn b">r</code>是采取该动作后收到的奖励，<code class="fe mk ml mm mn b">s_</code>是机器人采取该动作后所处的状态。</p><p id="3083" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先在<code class="fe mk ml mm mn b">q_predict</code>中，我们得到当我们第一次进入状态时，为采取行动所给的值。假设这个值是0.1。</p><p id="86de" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们确定当前状态下所有行为的最大可能值，用衰减率(0.9)将其贴现，然后加上我们收到的奖励。例如，如果当前状态的最大动作值是0.5，而我们收到的奖励是0，那么<code class="fe mk ml mm mn b">q_target</code>将是0.45，因为我们用0.9的衰减率乘以0.5，然后加上奖励0。</p><p id="18a4" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们取新值和先前值之间的差(例如，0.45 - 0.1 = 0.35)，并将其乘以学习率(例如，0.35 *0.1 = 0.035)。然后，我们将它添加到先前的动作值(例如，0.1 + 0.035 = 0.135)，并将其存储回Q表中。</p><p id="e837" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有这一切的结果是，根据我们最终所处的状态，行动会增加或减少一点，这将使我们在未来再次进入之前的状态时，或多或少地被选择。</p><p id="7188" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可能已经注意到这两种方法都使用了我们还没有添加的另一种方法，所以让我们这样做:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="d640" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个方法所做的就是检查状态是否已经在Q表中，如果没有，它将为所有可能的动作添加一个值0。</p><p id="8be3" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你的头受伤了吗？抱歉。好吧，让我们回到我们的代理人。</p><h1 id="6cac" class="lc ld jb bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">5.创建智能代理</h1><p id="4c16" class="pw-post-body-paragraph kd ke jb kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">首先创建类:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="ed1e" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们希望在创建代理时创建Q表的一个实例。</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="3686" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在游戏的每一步，我们需要选择一个动作来执行。为了选择行动，我们需要知道当前的状态。让我们创建一个方法来跟踪游戏状态的简化版本:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="e749" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这看起来像很多代码，但实际上我们只是跟踪我们的单位和敌人的单位。因为我们禁用了战争迷雾，并且原始单位允许我们看到地图上的所有单位，我们可以100%准确地看到所有的敌人单位。</p><p id="d160" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们不再追踪我们的矿物数量，而是将不同的单位简化为一个简单的布尔值。这大大降低了代理的粒度，而没有真正损失任何价值。</p><p id="7cb0" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在在我们的步骤中，我们可以使用状态来选择一个动作:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="5f68" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此时，我们的代理将随机选择一个动作，但它从不学习，所以让我们添加最后一部分:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="0666" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们已经看到了一个状态并执行了一个动作，我们现在已经进入了一个新的状态并收到了一个奖励，所以我们可以将这个输入到我们的Q表中。</p><p id="24b6" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们不重置<code class="fe mk ml mm mn b">previous_state</code>和<code class="fe mk ml mm mn b">previous_action</code>，它可能会在每个游戏开始时错误地教导我们的代理，所以让我们在开始新游戏时重置这些值:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="9cee" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，将我们的第一个代理更改为智能代理:</p><figure class="mf mg mh mi gt is"><div class="bz fp l di"><div class="mj kc l"/></div></figure><p id="dedd" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！如果你运行代码，你会看到最终智能代理开始赢得几乎每一场比赛。</p><p id="ce81" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇教程的完整代码可以在<a class="ae lb" href="https://github.com/skjb/pysc2-tutorial/blob/master/Reinforcement%20Learning%20Terran%20Bot/learning_agent.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="2967" class="pw-post-body-paragraph kd ke jb kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这个教程，请在<a class="ae lb" href="https://www.patreon.com/skjb" rel="noopener ugc nofollow" target="_blank"> Patreon </a>上支持我。也请和我一起上<a class="ae lb" href="https://discord.gg/zXHU4wM" rel="noopener ugc nofollow" target="_blank">不和谐</a>，或者关注我上<a class="ae lb" href="https://www.twitch.tv/skjb" rel="noopener ugc nofollow" target="_blank"> Twitch </a>、<a class="ae lb" href="https://medium.com/@skjb" rel="noopener"> Medium </a>、<a class="ae lb" href="https://github.com/skjb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>、<a class="ae lb" href="https://twitter.com/theskjb" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae lb" href="https://www.youtube.com/channel/UCZcEvhpV4_6llcrWrWQ2wsg" rel="noopener ugc nofollow" target="_blank"> YouTube </a>。</p></div></div>    
</body>
</html>