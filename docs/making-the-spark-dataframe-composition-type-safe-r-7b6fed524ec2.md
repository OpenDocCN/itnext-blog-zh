# 使火花数据帧合成类型安全(r)

> 原文：<https://itnext.io/making-the-spark-dataframe-composition-type-safe-r-7b6fed524ec2?source=collection_archive---------5----------------------->

![](img/b6893553dbbb41a289aaa03679bb8985.png)

米卡·鲍梅斯特在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

在 Spark 1.6 中，引入了类型化数据集 API。这是一个非常强大的特性，支持高阶函数(map、flatMap 等)。)和类型安全转换，其正确性在编译时得到验证。类型化 API 并非没有缺点，其中一些缺点是:

*   Spark 配备了一系列 SQL 函数，这通常会让生活变得更加轻松。例如， *coalesce(a，b，c)* 看起来比*r =>r . a or else(r . b)干净得多。getOrElse(r.c)* 。尽管后者显然是类型安全的，而前者却不是。
*   在许多情况下，使用非类型化 API 对于数据转换来说更方便。考虑一个使用从现有属性派生的附加属性来扩展输入的用例。这可以通过使用 [*和*](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html#withColumn-java.lang.String-org.apache.spark.sql.Column-) 列的方法轻松实现。来自类型化数据集 API 的高阶函数将需要创建一个新类型(原始类型的超集)，这又会导致在计算新属性的同时复制所有未改变的属性。这个过程很快就变得繁琐而难以维护。另外，每次输入的模式改变时，代码中的多个地方都需要更新。
*   尽管数据集和数据帧之间的性能差距不断缩小，但 Spark 仍然无法反思前者使用的 lambdas，Catalyst optimizer 最终会为后者提供更好的性能。关于这个话题有石崎和树博士的综合讲座([链接](https://databricks.com/session/demystifying-dataframe-and-dataset))。
*   一旦我们通过连接将数据集组合在一起，类型将不可避免地丢失。人们总是可以求助于使用 [*joinWith*](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html#joinWith-org.apache.spark.sql.Dataset-org.apache.spark.sql.Column-) 方法，该方法返回一个类型化元组的数据集，但事实证明，该输出很少能在下游使用。通常需要做额外的工作来使输出平坦，以便可以存储在表中。身边还带着 *_1* 、 *_2* 等。引用使得代码更难阅读和理解。以这种方式连接两个以上的数据集会使问题变得更糟。

由于上述原因，我经常看到非类型化的 DataFrame API 比它的强类型对应 API 更受青睐。

同时，如果我们能在编译时确保数据帧转换和组合的正确性，那就太好了。这就是为什么本文的目的是回答一个问题:**我们是否可以利用 Scala 类型系统的能力，为非类型化 DataFrame API 引入某种级别的类型安全，同时保留这个 API 提供的其他好处？**

# 假设和要求

在 OLAP 世界中，大多数(但不是全部)日常遇到的用例涉及一个单一的事实表(或其他来源),伴随着多个维度表。常见的管道实现包括以下操作的任意组合:

*   从不同的来源(蜂巢/冰山表、拼花文件等)创建一个或多个数据集(事实和维度)。)
*   事实数据集(进一步称为“事实”)通过与一个或多个维度数据集的组合(连接)来扩展维度属性。
*   使用一些谓词过滤事实。
*   用从现有属性计算出的新属性来扩展事实。
*   事实在一个或多个维度上聚合(分组依据)。
*   生成的数据集被投影以匹配输出的模式，并存储在 Hive/Iceberg 表或其他位置。

本文主要关注这个列表中的前两个操作:数据集创建和组合。

考虑到上述假设，我们现在可以概述以下要求:

1.  我们需要一种方法，在创建时用相应的源“注释”每个数据帧实例。在缺乏强类型 API 的情况下，我们需要某种类型的“标签”,它将指示给定的 DataFrame 实例代表哪种数据集。
2.  假设满足了第一个要求，我们应该能够组合(通过连接)带注释的数据帧实例。理想情况下，我们希望保留这个组合的血统，并在类型级别上反映出来。

事实证明，Scala 可以满足这两个要求。

# 遇见未注释的数据框架

让我们首先关注第一个需求，看看如何用类型“tag”来丰富每个 DataFrame 实例，该类型“tag”指示给定实例的来源。为了实现这一点，我们引入了一个名为 *AnnotatedDataFrame* 的新实体:

定义中的类型参数 **D** ( **d** ataset)就是我们之前说的那个“标签”。

现在我们需要建立一种惯用的方法来产生带注释的数据帧实例。为此，我们引入一个名为 *DataSource* 的类型类:

有了这两个定义，我们现在可以尝试定义一些数据集。

让我们想象一下，我们对从一些任意设备收集的测量值进行分析。假设我们用以下两个数据集来建模我们的数据:

*   **设备模型** —维度数据集。每个记录包含每个已知设备型号的详细信息(型号名称、硬件规格等。).该数据集中的主键是*设备型号 id* 。
*   **设备测量** —事实数据集。该数据集中的每条记录代表从设备收集的单个测量值，包括 *device_model_id* 属性，该属性是用于引用 **DeviceModel** 数据集的外键。

现在我们可以将上述定义转换成类型标签，并实现各自的*数据源*实例。

为了使例子更简单，我们省略了自定义源参数，只在第二个类型参数中提供了*单元*给*数据源*类型类。

DeviceMeasurement 标记和源实例的定义看起来非常相似:

现在很容易在我们的 Spark 应用程序中实例化 2 个数据集:

请注意，我们将创建 *AnnotatedDataFrame* 实例的过程与实际使用它的逻辑分离得多么好。

在这一点上，我们不再处理裸露的数据帧。我们现在可以通过查看每个实例的类型来推断其来源。不幸的是，这种方法的实用性和适用性非常有限。缺失的部分可以在我们的第二个需求中找到:通过连接组成带注释的数据集。

# 类型安全连接

因为我们将数据集建模为类型参数，所以我们需要另一个类型类来描述如何连接任意两个数据集。让我们从一个简单的定义开始:

马上弹出的第一个问题:在 *join* 函数的返回类型中，应该用什么类型参数来代替一个问号？

让我们重温一下我们之前所做的假设，并对其进行一些扩展:

*   我们正在处理连接到一个或多个维度数据集的单个事实数据集。
*   当连接时，我们希望事实数据集在左边，而维度数据集在右边。
*   有时，维度数据集也可以位于左侧。这方面的一个例子是从多个规范化的源组成一个维度数据集。

给定上面的假设，我们得出结论，我们的连接的组成应该是面向左的，这导致选择 **L** 作为输出类型的类型参数。

一旦实现了相应的 *Join* 实例，我们会在代码中看到类似这样的内容:

不错，但还是不够好。产生的类型丢失了所有关于带有来自**设备模型**的属性的**设备测量**数据集的扩展的信息。在编译器看来*enrichedDeviceMeasurement*和 *deviceMeasurement* 是等价的(类型方面)，尽管在这一点上它们是完全不同的实体。

有没有一种方法可以跟踪哪些数据集被加入到我们的事实数据集，并在类型级别存储这些信息？原来是有的！

# 修订的注释数据框架

解决方案就在问题本身之中:我们只需要维护一个连接数据集的异构列表，并将其存储为一种类型。

幸运的是，类型安全异构列表已经在 [Shapeless](https://github.com/milessabin/shapeless) 中实现了——一个 Scala 的通用编程库。我们需要的类型叫做 *HList* ，它本质上是一个*列表*，其中每个元素的类型在编译时都是静态已知的。后一个属性使得 *HList* 更类似于 Scala 的 tuple。例如:

关于 *HList* 有趣的事情是，它的项目同时存在于两个平面上:作为值( *1，" Hello "，Foo("World")* )，也作为类型( *Int，String，Foo* )。

在我们的例子中，我们并不关心 HList 的值，但是我们可以从它的类型属性中获益匪浅。

让我们先介绍一个 *JoinList* 的概念:

下一步是用一个额外的类型参数 **J** 来扩展 *AnnotatedDataFrame* 定义，该参数携带连接数据集的列表:

最后，我们需要相应地更新*数据源*定义:

注意，当我们最初从源代码中读取时，连接列表是空的(因为我们还没有执行任何连接)，这就是为什么 *AnnotatedDataFrame* 的第二个类型参数被设置为 *HNil* ，这表示一个空列表。

唯一剩下的步骤是更新 *Join* 类型类的定义。

# 加入沿袭跟踪

为了确保保留连接数据集的血统，我们需要执行以下类型级别的操作:

1.  连接左右数据集的连接列表。
2.  将右侧数据集的类型标记添加到上一步获得的列表中。

让我们试着在代码中反映这一点:

新定义的第一个不同之处是, *Join* 类型类扩展了两个额外的类型参数——**LJ**和 **RJ** ,它们分别代表左右数据集的连接列表。

另一个有趣的细节是在 *join* 的定义中对 *Prepend* 类型类的隐式引用。 *Prepend* —是由 *Shapeless* 库提供的类型类，它将第一个类型参数中提供的 *HList* 类型前置到第二个类型参数中指定的类型。串联产生的类型存储在路径相关类型 *Out* 中。整个操作只涉及类型，并且发生在编译时。

如果我们查看返回类型的第二个类型参数，我们会发现它反映了我们在本节开始时概述的两个操作。我们将类型 **R** 前置到 **P.Out** ，正如我们现在所知，这又是 **LJ** 和 **RJ** 串联的结果。

此外，我们可以用一个 *join* 语法来扩展 *AnnotatedDataFrame* :

现在我们只需要把所有的拼图拼在一起。

# 把所有的碎片放在一起

为了演示沿袭跟踪的强大功能，我们应该首先介绍一个 *Join* 类型类的实现，该类描述了如何连接 **DeviceMeasurement** 和 **DeviceModel** 数据集。我们可以引入相应的类型类实例作为**设备模型**伴随对象的一部分:

注意**设备测量**数据集在左侧位置，而**设备模型**数据集在右侧。

现在，如果我们连接两个数据集:

我们看到连接操作已经反映在 *AnnotatedDataFrame* 类型的第二个类型参数中！

连接操作也是可组合的。换句话说，如果我们选择用一个额外的维度数据集 **Country** 来扩展我们的数据模型，那么组合将如下所示:

另一个很棒的特性是，我们现在可以从丰富的类型类[集合](https://github.com/milessabin/shapeless/blob/master/core/src/main/scala/shapeless/ops/hlists.scala)中受益，它与 *HList* 一起出现在 *Shapeless* 库中。

例如，如果我们需要确保来自 **DeviceModel** 数据集的属性是我们业务逻辑的输入的一部分，我们可以这样实现:

同样，指定的约束将在编译时强制执行。

# 结论

为了让非类型化的 DataFrame API 更加类型安全，我们创建了一个固执己见的框架，它涵盖了几乎每个 ETL 管道都有的几个关键步骤:

*   数据集创建。
*   通过连接进行数据集合成。

我们在这两个操作中都实现了一定程度的类型安全。现在，数据集是使用规则创建和组成的，其正确性在编译时强制执行。这进一步允许开发人员对业务逻辑的某些部分的输入进行约束，这些约束也将在编译时实施。这反过来会导致更安全的实现和更少的错误。

这个框架适用于任何用例吗？答案是否定的。该框架对数据集的组成模式(事实、维度等)做出了某些假设。)，这在很多情况下可能不成立。

提议的框架可以补充现有的管道实现。它还可以与类型化数据集 API 共存，并在后者能力不足的地方作为一种增强(例如连接)。

本文中提出的想法可以很容易地扩展到典型 ETL 管道的其他步骤，比如从一个数据集到另一个数据集的转换以及投影。

最后是包含完整实现、示例等内容的 gist:[https://gist . github . com/izeigerman/d1fe 83519767 de 6514 CFD 02384075457](https://gist.github.com/izeigerman/d1fe83519767de6514cfd02384075457)