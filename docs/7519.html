<html>
<head>
<title>Does Spark-Kafka Writer maintain ordering semantics between Spark partitions?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark-Kafka Writer是否维护Spark分区之间的排序语义？</h1>
<blockquote>原文：<a href="https://itnext.io/does-spark-kafka-writer-maintain-ordering-semantics-between-spark-partitions-c832117aa111?source=collection_archive---------2-----------------------#2022-10-19">https://itnext.io/does-spark-kafka-writer-maintain-ordering-semantics-between-spark-partitions-c832117aa111?source=collection_archive---------2-----------------------#2022-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="80e3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我已经深入源代码进行了检查</h2></div><h1 id="eb2a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">你能保证订购吗？</h1><p id="7b85" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">你有一个很酷的Spark工作，你最终想要<a class="ae lt" href="https://spark.apache.org/docs/3.2.1/structured-streaming-kafka-integration.html#writing-data-to-kafka" rel="noopener ugc nofollow" target="_blank">将你的结果</a> <code class="fe lu lv lw lx b"><a class="ae lt" href="https://spark.apache.org/docs/3.2.1/structured-streaming-kafka-integration.html#writing-data-to-kafka" rel="noopener ugc nofollow" target="_blank">DataFrame</a></code> <a class="ae lt" href="https://spark.apache.org/docs/3.2.1/structured-streaming-kafka-integration.html#writing-data-to-kafka" rel="noopener ugc nofollow" target="_blank">发送到一个卡夫卡主题</a>。</p><p id="20eb" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">但是，您需要保证表顺序在目标主题中得到维护。换句话说，对于下面的代码:</p><pre class="md me mf mg gt mh lx mi bn mj mk bi"><span id="e35d" class="ml kg iq lx b be mm mn l mo mp">df<br/>  .writeStream<br/>  .format("kafka")<br/>  .options(kafkaConfig.kafkaConnectionConfig ++<br/>   Map("topic" -&gt; targetKafkaTopic)<br/>  )<br/>  .start()</span></pre><p id="16a6" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">或者以批处理方式:</p><pre class="md me mf mg gt mh lx mi bn mj mk bi"><span id="1791" class="ml kg iq lx b be mm mn l mo mp">df<br/>  .write<br/>  .format("kafka")<br/>  .options(kafkaConfig.kafkaConnectionConfig ++<br/>   Map("topic" -&gt; targetKafkaTopic)<br/>  )<br/>  .save()</span></pre><p id="e565" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">… <strong class="kz ir">您期望主题中的记录与Spark </strong> <code class="fe lu lv lw lx b"><strong class="kz ir">DataFrame</strong></code> <strong class="kz ir">中的记录具有相同的顺序。</strong></p><blockquote class="mq"><p id="1540" class="mr ms iq bd mt mu mv mw mx my mz ls dk translated">你能保证吗？</p></blockquote><p id="aa68" class="pw-post-body-paragraph kx ky iq kz b la na jr lc ld nb ju lf lg nc li lj lk nd lm ln lo ne lq lr ls ij bi translated">要回答这个问题，我们需要把问题分解成两部分:</p><ol class=""><li id="f4e3" class="nf ng iq kz b la ly ld lz lg nh lk ni lo nj ls nk nl nm nn bi translated"><strong class="kz ir">单一生产商订购保证</strong></li><li id="01d6" class="nf ng iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">分布式系统增加了复杂性</strong></li></ol><p id="df49" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">我将首先描述单个非分布式生产者在“普通”Kafka中可用的订购保证和配置选项，然后我将讨论从Apache Spark这样的分布式系统进行生产所增加的复杂性。最后，我将描述如何解决这个问题。</p><h2 id="d514" class="nt kg iq bd kh nu nv dn kl nw nx dp kp lg ny nz kr lk oa ob kt lo oc od kv oe bi translated"><strong class="ak">单一生产商订购保证</strong></h2><p id="e68f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">流语义允许我们在“最多一次”、“至少一次”和“恰好一次”之间选择传递保证(这个<a class="ae lt" href="https://kafka.apache.org/33/documentation.html#semantics" rel="noopener ugc nofollow" target="_blank"> Kafka文档部分</a>是关于这个问题的必读内容)。</p><p id="5cde" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">你可能知道，卡夫卡保证了分区内部的顺序<a class="ae lt" href="https://developer.confluent.io/learn-kafka/apache-kafka/partitions/#:~:text=Kafka%20Partitioning&amp;text=Partitioning%20takes%20the%20single%20topic,many%20nodes%20in%20the%20cluster." rel="noopener ugc nofollow" target="_blank">，而不是分区之间的顺序。</a></p><p id="9149" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">某些Kafka配置可能会导致重复的消息，或传递顺序的改变——这是“确保”语义“至少一次”的一种折衷。</p><p id="a6f9" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">这是Kafka通过使用以下设置内置解决的(在非分布式生产者设置中):</p><pre class="md me mf mg gt mh lx mi bn mj mk bi"><span id="1027" class="ml kg iq lx b be mm mn l mo mp">val idempotentProducerConfig: Map[String, String] = Map(<br/>  "kafka.enable.idempotence" -&gt; true.toString,<br/>  "kafka.max.in.flight.requests.per.connection" -&gt; 5.toString,<br/>  "kafka.acks" -&gt; "all",<br/>)</span></pre><p id="8d10" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">注意，<code class="fe lu lv lw lx b">max.in.flight.requests.per.connection = 5</code>是幂等生产者的<a class="ae lt" href="https://kafka.apache.org/33/documentation.html#producerconfigs_enable.idempotence" rel="noopener ugc nofollow" target="_blank">最大可能配置</a>，这是这种设置的一个折衷例子。此外，请注意，为了保证排序，我们可能没有使用幂等配置，但是<code class="fe lu lv lw lx b">max.in.flight.requests.per.connection=1</code>会获得相同的结果。然而，看着这个Kafka KIP ，感觉幂等开销将低于从更高的运行中请求(在这种情况下最多5个)获得的性能。</p><h2 id="51bd" class="nt kg iq bd kh nu nv dn kl nw nx dp kp lg ny nz kr lk oa ob kt lo oc od kv oe bi translated"><strong class="ak">分布式系统增加了复杂性</strong></h2><p id="ead6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Spark是一个分布式系统。因此，Spark集群中的数据分布在工作人员之间。</p><p id="593a" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated"><strong class="kz ir">此外，数据被分区(注意“spark”分区，而不是Kafka分区)分割，每个工作者持有分区的子集。</strong></p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi of"><img src="../Images/37f1a75553181c5951c674bea0c8488e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nUEkOM3wzXILc66k"/></div></div></figure><blockquote class="mq"><p id="a776" class="mr ms iq bd mt mu on oo op oq or ls dk translated"><strong class="ak">假设数据是分布式的，我们如何确保Spark workers按照期望的顺序同步生成Kafka主题？</strong></p></blockquote><p id="b9bd" class="pw-post-body-paragraph kx ky iq kz b la na jr lc ld nb ju lf lg nc li lj lk nd lm ln lo ne lq lr ls ij bi translated"><strong class="kz ir">我们无法期待。但是我们可以强制执行:</strong></p><h2 id="58bb" class="nt kg iq bd kh nu nv dn kl nw nx dp kp lg ny nz kr lk oa ob kt lo oc od kv oe bi translated">强制执行命令</h2><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi os"><img src="../Images/abd345f07184920819d551713e474f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ACjxhsaOpOjlpLwq"/></div></div></figure><p id="9ec0" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">这里的<code class="fe lu lv lw lx b">enforceOrdering</code>方法是:</p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi os"><img src="../Images/6b4e0a5245160d96474c5693aa214dbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1f3vJM9p8TOG3x17"/></div></div></figure><p id="8844" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">我们在这里做了什么？让我们再细分一下:</p><ol class=""><li id="918d" class="nf ng iq kz b la ly ld lz lg nh lk ni lo nj ls nk nl nm nn bi translated">我们设置标准的Kafka生产者配置来保证分区中的顺序。</li><li id="fc6d" class="nf ng iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated">我们在Spark分区和Kafka分区之间建立了1:1的关系，确保Spark workers的本地Kafka生产者将拥有所有可用的数据，避免不一致。</li><li id="fd82" class="nf ng iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated">我们在spark分区内进行分类，因此Spark workers的本地Kafka生产商将为其Kafka分区生产它。</li></ol><figure class="md me mf mg gt og gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/025a64e47e0a84bfcd1b5d416ac1cb37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/0*NEo0OJABEthYd-GR"/></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated">*生产者是在worker上创建和缓存的，所以Spark分区数可以大于worker的本地生产者</figcaption></figure><h1 id="e3d0" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">长篇大论——火花——卡夫卡V2作家源代码分析</h1><p id="8cd2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在让我们走一条长路，探索Spark-Kafka连接器源代码，以便检查我上面的假设。</p><p id="81d4" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated"><strong class="kz ir"> 1。Spark-Kafka writer在Spark分区上迭代，所以Kafka partitioner是在spark分区上调用的，而不是在整个表上:</strong></p><p id="1a49" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">因此，当<a class="ae lt" href="https://spark.apache.org/docs/3.2.1/structured-streaming-kafka-integration.html#writing-data-to-kafka" rel="noopener ugc nofollow" target="_blank">我们发布从spark到Kafka的主题</a>时，<code class="fe lu lv lw lx b">org.apache.spark.sql.kafka010.KafkaWriter#write</code>方法被称为:</p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi oz"><img src="../Images/f2ce6c6c368a2cf7cabeb5b4f5a94140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AX0gzqDP4_fs2-Wk"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated"><code class="fe lu lv lw lx b">org.apache.spark.sql.kafka010.KafkaWriter#write</code></figcaption></figure><p id="f389" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated"><strong class="kz ir">我们可以清楚地看到(第70-74行), Spark-Kafka作者在Spark分区上进行迭代，其方式是在Spark分区上调用</strong> <code class="fe lu lv lw lx b"><strong class="kz ir">KafkaWriteTask#execute</strong></code> <strong class="kz ir">，而不是在整个表中调用。</strong></p><h2 id="eae6" class="nt kg iq bd kh nu nv dn kl nw nx dp kp lg ny nz kr lk oa ob kt lo oc od kv oe bi translated">2.每个分区使用一个缓存的本地Kafka生成器:</h2><p id="07b8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">被调用的<code class="fe lu lv lw lx b">KafkaWriteTask#execute</code>使用一个内部生产者，而这个内部生产者又是一个缓存生产者，可能在同一个工作者的任务之间共享。</p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi pa"><img src="../Images/dc22cea193ac8385c14a0dddff9a1c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4ed91n5mzgYUif6M"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated"><code class="fe lu lv lw lx b">org.apache.spark.sql.kafka010.</code>KafkaWriteTask #执行</figcaption></figure><h2 id="732a" class="nt kg iq bd kh nu nv dn kl nw nx dp kp lg ny nz kr lk oa ob kt lo oc od kv oe bi translated">3.然后它为每一行构建一个Kafka ProducerRecord:</h2><p id="ece1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><code class="fe lu lv lw lx b">org.apache.spark.sql.kafka010.KafkaRowWriter#sendRow</code>从Spark <code class="fe lu lv lw lx b">InternalRow</code>构建Kafka <code class="fe lu lv lw lx b">ProducerRecord</code>，并使用<a class="ae lt" href="https://kafka.apache.org/33/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html" rel="noopener ugc nofollow" target="_blank">标准Java生成器API </a>生产它。</p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi pb"><img src="../Images/ac203d7e0db411bc0a979b8d3784402b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9pBgCo-08Ujs35Wg"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated"><code class="fe lu lv lw lx b">org.apache.spark.sql.kafka010.KafkaRowWriter#sendRow</code></figcaption></figure><p id="dc3c" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">关注93行很有意思。它首先将<code class="fe lu lv lw lx b">InternalRow</code>投射到实际上是<code class="fe lu lv lw lx b">UnsafeRow</code>的<code class="fe lu lv lw lx b">UnsafeProjection</code>上，也就是说，它以相关的顺序投射<a class="ae lt" href="https://spark.apache.org/docs/3.2.1/structured-streaming-kafka-integration.html#writing-data-to-kafka" rel="noopener ugc nofollow" target="_blank">相关信息</a>以用于建立记录。这就是为什么我们可以在上面的代码片段中看到通过按序号获取行片段来构建记录(例如，第94–96行)。</p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi pc"><img src="../Images/6fa691d8ea7ebec4ec13e95e09247550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*E2RRCpIlutLsTN8x"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated"><code class="fe lu lv lw lx b">org.apache.spark.sql.kafka010.KafkaRowWriter</code></figcaption></figure><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi pd"><img src="../Images/3bebb847e40c8c4f42eab09143fdf4c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IZTHx_b2KYjX81pC"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated"><code class="fe lu lv lw lx b">org.apache.spark.sql.kafka010.KafkaRowWriter#createProjection</code></figcaption></figure><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi pd"><img src="../Images/8436b0eb9a4b29bb7444a7c93d8b898a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*n7NzBL59-8DloVQM"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated">org . Apache . spark . SQL . catalyst . expressions . unsafe projection</figcaption></figure><h2 id="e2f8" class="nt kg iq bd kh nu nv dn kl nw nx dp kp lg ny nz kr lk oa ob kt lo oc od kv oe bi translated">4.同样，<a class="ae lt" href="https://spark.apache.org/docs/3.2.1/structured-streaming-kafka-integration.html#writing-data-to-kafka" rel="noopener ugc nofollow" target="_blank">如文档</a>所示，我们可以强制一个分区设置一个分区列，或者让它自动解析它:</h2><p id="68b1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">从<code class="fe lu lv lw lx b">sendRow</code>方法:</p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi pe"><img src="../Images/67b7dafa82f526fc8a9028352f67890c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K0ha5bdQ4b-5TQdU"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated"><code class="fe lu lv lw lx b">org.apache.spark.sql.kafka010.KafkaRowWriter#sendRow</code></figcaption></figure><p id="eb16" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">如果我们定义一个分区列，它将用于构建<code class="fe lu lv lw lx b">ProducerRecord</code>，否则，它将为空。</p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi pf"><img src="../Images/8b94f53e92bbd3e66b60bbf8b0fb3ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cj5qSU2fTqOiQRuO"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated"><code class="fe lu lv lw lx b">org.apache.spark.sql.kafka010.KafkaRowWriter#sendRow</code></figcaption></figure><p id="0cf3" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">在这种情况下，Null意味着我们将让Kafka生成器Java API为我们解析分区，并返回到Kafka <code class="fe lu lv lw lx b">DefaultPartitioner</code>。</p><p id="4ef8" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">来自<code class="fe lu lv lw lx b">org.apache.kafka.clients.producer.KafkaProducer#partition</code>:</p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi pg"><img src="../Images/67a9a198e3f10e2be03491477d8f3093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KBv5uPsOYNEAtbrk"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk translated"><code class="fe lu lv lw lx b">org.apache.kafka.clients.producer.KafkaProducer#partition</code></figcaption></figure><p id="76b2" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">并且，根据<a class="ae lt" href="https://spark.apache.org/docs/3.2.1/structured-streaming-kafka-integration.html#writing-data-to-kafka" rel="noopener ugc nofollow" target="_blank"> Spark文档</a>我们可以通过提供不同的分区器来更改默认分区器:</p><blockquote class="ph pi pj"><p id="86d4" class="kx ky ot kz b la ly jr lc ld lz ju lf pk ma li lj pl mb lm ln pm mc lq lr ls ij bi translated">通过设置kafka.partitioner.class选项，可以在Spark中指定Kafka partitioner。如果不存在，将使用Kafka默认分区器。</p></blockquote><p id="bbfc" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">但是，如果我们为KafkaProducer提供一个定制的分区器，这个分区器不同于Spark用来对其<code class="fe lu lv lw lx b">DataFrame</code>进行重新分区的分区器，我们可能还需要改变Spark对数据帧进行重新分区的方式，因为我们希望Spark分区和Kafka分区之间是1:1的关系。</p><h2 id="c1af" class="nt kg iq bd kh nu nv dn kl nw nx dp kp lg ny nz kr lk oa ob kt lo oc od kv oe bi translated"><strong class="ak">所以我们可以看到，我们不能天真地期待订购… </strong></h2><ol class=""><li id="06c3" class="nf ng iq kz b la lb ld le lg pn lk po lo pp ls nk nl nm nn bi translated">每个spark分区都是独立的，可以有自己的Kafka本地制作商</li><li id="17bc" class="nf ng iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated">默认的分区器将使用<a class="ae lt" href="https://stackoverflow.com/questions/72704353/does-spark-structured-streaming-producer-using-the-kafka-default-partitioner-bet?noredirect=1#comment128426751_72704353" rel="noopener ugc nofollow" target="_blank"> "Murmur2 hash for the keys(所以相同的keys hash)应该是相等的，并将被写入同一个主题分区"</a></li><li id="71fd" class="nf ng iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated">然而，如果相同的键不在同一个spark分区中，它们可能会被不同的生产者发送，即，在Spark分区之间没有顺序保持，只在Spark分区内部</li></ol><p id="1765" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated"><strong class="kz ir">因此，如果我们关心整个数据帧的排序，并且我们不确定Spark-partition-key是否与Kafka-partition-key相同，我们需要在将它们发布到我们的Kafka主题之前实施它。</strong></p><figure class="md me mf mg gt og gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi pq"><img src="../Images/fa2d77d2f4924fdd097002b6ce5f1626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CFg81t-zrXEl-LpS"/></div></div></figure><p id="b69d" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated"><strong class="kz ir">快乐管道！；)</strong></p></div><div class="ab cl pr ps hu pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="ij ik il im in"><p id="570d" class="pw-post-body-paragraph kx ky iq kz b la ly jr lc ld lz ju lf lg ma li lj lk mb lm ln lo mc lq lr ls ij bi translated">脚注:</p><ol class=""><li id="9bba" class="nf ng iq kz b la ly ld lz lg nh lk ni lo nj ls nk nl nm nn bi translated">Spark版本免责声明:Apache Spark版本3.2.1上的源代码分析</li><li id="0ad3" class="nf ng iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated">如果你想更深入地了解可能的不同配置及其对交付语义的影响，请看这里的<a class="ae lt" href="https://kafka.apache.org/33/documentation.html#producerconfigs_retries" rel="noopener ugc nofollow" target="_blank"/>、这里的<a class="ae lt" href="https://kafka.apache.org/33/documentation.html#producerconfigs_max.in.flight.requests.per.connection" rel="noopener ugc nofollow" target="_blank"/>和这里的<a class="ae lt" href="https://stackoverflow.com/questions/49982786/ordering-guarantees-when-using-idempotent-kafka-producer" rel="noopener ugc nofollow" target="_blank"/>。此外，值得注意的是，Kafka版本之间的默认配置会发生变化(例如，参见<a class="ae lt" href="https://kafka.apache.org/33/documentation.html#upgrade_311_notable" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae lt" href="https://kafka.apache.org/33/documentation.html#upgrade_320_notable" rel="noopener ugc nofollow" target="_blank">这个</a>)，这表明<a class="ae lt" href="https://medium.com/zencity-engineering/mongo-spark-connector-deep-dive-part-ii-json-serialization-4b126d2f210b" rel="noopener">应用显式配置</a>是多么重要。</li></ol></div></div>    
</body>
</html>