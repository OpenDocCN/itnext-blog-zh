<html>
<head>
<title>Change Data Capture with Azure, PostgreSQL, and Kafka</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Azure、PostgreSQL和Kafka改变数据捕获</h1>
<blockquote>原文：<a href="https://itnext.io/change-data-capture-with-azure-postgresql-and-kafka-4598dbf0b57a?source=collection_archive---------8-----------------------#2020-11-02">https://itnext.io/change-data-capture-with-azure-postgresql-and-kafka-4598dbf0b57a?source=collection_archive---------8-----------------------#2020-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8af6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何使用Kafka Connect平台，使用变更数据捕获将数据库修改从PostgreSQL传输到<a class="ae kf" href="https://docs.microsoft.com/en-us/azure/data-explorer/" rel="noopener ugc nofollow" target="_blank"> Azure Data Explorer </a> (Kusto)。</h2></div><h1 id="39f7" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">介绍</h1><p id="8074" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">变更数据捕获(CDC)可用于跟踪数据库表中的行级变更，以响应创建、更新和删除操作。这是一种强大的技术，但是只有当有办法利用这些事件并使它们对其他服务可用时才有用。</p><p id="8622" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">使用Apache Kafka，可以将传统的批处理ETL过程转换为实时的流模式。你可以自己动手(DIY ),使用自己选择的客户端SDK编写优秀的老Kafka生产者/消费者。但是，当您拥有Kafka Connect及其即用型连接器套件时，为什么还要这样做呢？</p><p id="90bf" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">一旦你选择了Kafka Connect，你有几个选择。一个是JDBC连接器，它主要通过轮询目标数据库表来获取信息。有一种基于变更数据捕获的更好的方法(尽管有点复杂)。进入<a class="ae kf" href="https://debezium.io/" rel="noopener ugc nofollow" target="_blank"> Debezium </a>，这是一个分布式平台，构建在不同数据库中可用的变更数据捕获特性之上。它提供了一组<a class="ae kf" href="https://debezium.io/documentation/reference/1.2/connectors/index.html" rel="noopener ugc nofollow" target="_blank"> Kafka Connect连接器</a>，这些连接器接入数据库表中的行级更改，并将它们转换成发送给Apache Kafka的事件流。一旦变更日志事件在Kafka中，它们将对所有下游应用程序可用。</p><p id="fbf3" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">这是这篇文章中呈现的用例的高级概述。出于演示目的，对其进行了简化。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="7f19" class="kg kh iq bd ki kj mg kl km kn mh kp kq jw mi jx ks jz mj ka ku kc mk kd kw kx bi translated">概观</h1><p id="d1e2" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">与<code class="fe ml mm mn mo b">Orders</code>相关的数据存储在PostgreSQL数据库中，包含订单ID、客户ID、城市、交易金额等信息。时间等。这些数据被PostgreSQL的<a class="ae kf" href="https://debezium.io/documentation/reference/1.2/connectors/postgresql.html" rel="noopener ugc nofollow" target="_blank"> Debezium连接器</a>拾取，并发送给Kafka主题。一旦数据在Kafka中，另一个(sink)连接器将它们发送到Azure Data Explorer，允许进一步的查询和分析。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mp"><img src="../Images/6f32162d718586d3a8d934d0cf9f9100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g02WfOJU0wOsX0Oz.jpg"/></div></div></figure><p id="b91d" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">端到端解决方案中使用的各个组件如下:</p><h2 id="5b28" class="nb kh iq bd ki nc nd dn km ne nf dp kq lh ng nh ks ll ni nj ku lp nk nl kw nm bi translated">来源和目的地</h2><p id="cc8f" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">数据管道可能相当复杂！这篇博文提供了一个简单的例子，其中PostgreSQL数据库将被用作数据源，大数据分析引擎充当最终目的地(接收器)。这两个组件都运行在Azure中:<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank">Azure Database for PostgreSQL</a>(<em class="nn">Source</em>)是一个基于开源<a class="ae kf" href="https://www.postgresql.org/" rel="noopener ugc nofollow" target="_blank"> Postgres </a>数据库引擎的关系数据库服务，而<a class="ae kf" href="https://docs.microsoft.com/azure/data-explorer/?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank">Azure Data Explorer</a>(<em class="nn">Sink</em>)是一个快速且可扩展的数据探索服务，它让您可以收集、存储和分析来自任何不同来源的大量数据，如网站、应用程序、物联网设备等等。</p><blockquote class="no np nq"><p id="6a83" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">虽然在这篇博客中使用了Azure PostgreSQL DB，但是这些说明应该适用于任何Postgres数据库。因此，如果您愿意，请随意使用替代选项！</em></p><p id="267c" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">与这篇博文相关的代码和配置可以在这个</em> <a class="ae kf" href="https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> GitHub资源库</em> </a>中找到</p></blockquote><h2 id="0c6b" class="nb kh iq bd ki nc nd dn km ne nf dp kq lh ng nh ks ll ni nj ku lp nk nl kw nm bi translated">卡夫卡和卡夫卡连接</h2><p id="1cde" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">Apache Kafka和Kafka Connect一起充当流数据管道的可伸缩平台——这里的关键组件是源和接收器连接器。</p><p id="0a80" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">用于PostgreSQL的Debezium连接器捕获插入、更新和删除数据库内容的行级更改，并提交给PostgreSQL数据库，生成数据更改事件记录，并将它们传输到Kafka主题。在幕后，它使用Postgres输出插件的组合(例如，<code class="fe ml mm mn mo b">wal2json</code>、<code class="fe ml mm mn mo b">pgoutput</code>等)。)并且(Java)连接器本身使用<a class="ae kf" href="https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html" rel="noopener ugc nofollow" target="_blank"> PostgreSQL的流复制协议</a>和<a class="ae kf" href="https://github.com/pgjdbc/pgjdbc" rel="noopener ugc nofollow" target="_blank"> JDBC驱动程序</a>读取由输出插件产生的变化。</p><p id="844c" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><a class="ae kf" href="https://github.com/Azure/kafka-sink-azure-kusto" rel="noopener ugc nofollow" target="_blank">Azure Data Explorer sink connector</a>从配置的Kafka主题中提取数据，对其进行批处理并将其发送到Azure Data Explorer，在Azure Data Explorer中它们被排队接收并最终写入Azure Data Explorer中的一个表。连接器利用了用于Azure Data Explorer 的<a class="ae kf" href="https://github.com/Azure/azure-kusto-java" rel="noopener ugc nofollow" target="_blank"> Java SDK。</a></p><p id="4cc1" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">大多数组件(除了Azure Data Explorer和Azure PostgreSQL DB)都作为Docker容器(使用Docker Compose)运行——Kafka(和Zookeeper)、Kafka Connect workers和数据生成器应用程序。话虽如此，只要所有组件都配置为根据需要相互访问和通信，这些指令将适用于任何Kafka集群和Kafka Connect workers。例如，你可以在Azure HD Insight上拥有Kafka集群，在Azure Marketplace上拥有融合云。</p><blockquote class="no np nq"><p id="9633" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">如果您对这些场景感兴趣，请查看这些</em> <a class="ae kf" href="https://github.com/Azure/azure-kusto-labs/tree/master/kafka-integration" rel="noopener ugc nofollow" target="_blank"> <em class="iq">动手实验</em> </a> <em class="iq"/></p></blockquote><p id="d0bd" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">下面是组件及其服务定义的分类——你可以参考GitHub repo 中的完整<code class="fe ml mm mn mo b">docker-compose</code>文件<a class="ae kf" href="https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo/blob/master/docker-compose.yaml" rel="noopener ugc nofollow" target="_blank"/></p><h2 id="d16a" class="nb kh iq bd ki nc nd dn km ne nf dp kq lh ng nh ks ll ni nj ku lp nk nl kw nm bi translated">Docker编写服务</h2><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="57cd" class="nb kh iq mo b gy ny nz l oa ob">zookeeper:<br/>    image: debezium/zookeeper:1.2<br/>    ports:<br/>      - 2181:2181<br/>  kafka:<br/>    image: debezium/kafka:1.2<br/>    ports:<br/>      - 9092:9092<br/>    links:<br/>      - zookeeper<br/>    depends_on:<br/>      - zookeeper<br/>    environment:<br/>      - ZOOKEEPER_CONNECT=zookeeper:2181<br/>      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092</span></pre><p id="c022" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Kafka和Zookeeper使用debezium的图像运行——它们可以工作，非常适合快速反馈循环、演示等迭代开发。</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="48da" class="nb kh iq mo b gy ny nz l oa ob">dataexplorer-connector:<br/>    build:<br/>      context: ./connector<br/>      args:<br/>        KUSTO_KAFKA_SINK_VERSION: 1.0.1<br/>    ports:<br/>      - 8080:8083<br/>    links:<br/>      - kafka<br/>    depends_on:<br/>      - kafka<br/>    environment:<br/>      - BOOTSTRAP_SERVERS=kafka:9092<br/>      - GROUP_ID=adx<br/>      - CONFIG_STORAGE_TOPIC=adx_connect_configs<br/>      - OFFSET_STORAGE_TOPIC=adx_connect_offsets<br/>      - STATUS_STORAGE_TOPIC=adx_connect_statuses<br/>  postgres-connector:<br/>    image: debezium/connect:1.2<br/>    ports:<br/>      - 9090:8083<br/>    links:<br/>      - kafka<br/>    depends_on:<br/>      - kafka<br/>    environment:<br/>      - BOOTSTRAP_SERVERS=kafka:9092<br/>      - GROUP_ID=pg<br/>      - CONFIG_STORAGE_TOPIC=pg_connect_configs<br/>      - OFFSET_STORAGE_TOPIC=pg_connect_offsets<br/>      - STATUS_STORAGE_TOPIC=pg_connect_statuses</span></pre><p id="72fc" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Kafka Connect源和接收器连接器作为单独的容器运行，只是为了让您更容易理解和推理它们——也可以在单个容器中运行这两个连接器。</p><p id="30d1" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">注意，虽然PostgreSQL连接器内置在<a class="ae kf" href="https://hub.docker.com/r/debezium/connect" rel="noopener ugc nofollow" target="_blank"> debezium/connect </a>映像中，但是Azure Data Explorer连接器是使用自定义映像设置的。<a class="ae kf" href="https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo/blob/master/connector/Dockerfile" rel="noopener ugc nofollow" target="_blank"> Dockerfile </a>相当紧凑；</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="6833" class="nb kh iq mo b gy ny nz l oa ob">FROM debezium/connect:1.2<br/>WORKDIR $KAFKA_HOME/connect<br/>ARG KUSTO_KAFKA_SINK_VERSION<br/>RUN curl -L -O <a class="ae kf" href="https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v$KUSTO_KAFKA_SINK_VERSION/kafka-sink-azure-kusto-$KUSTO_KAFKA_SINK_VERSION-jar-with-dependencies.jar" rel="noopener ugc nofollow" target="_blank">https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v$KUSTO_KAFKA_SINK_VERSION/kafka-sink-azure-kusto-$KUSTO_KAFKA_SINK_VERSION-jar-with-dependencies.jar</a></span></pre><p id="d90c" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">最后，<code class="fe ml mm mn mo b">orders-gen</code>服务只是<a class="ae kf" href="https://golang.org/" rel="noopener ugc nofollow" target="_blank"> Go </a>应用程序将随机订单数据植入PostgreSQL。您可以参考<a class="ae kf" href="https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo/blob/master/orders-generator/Dockerfile" rel="noopener ugc nofollow" target="_blank">GitHub repo</a>中的Dockerfile</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="e859" class="nb kh iq mo b gy ny nz l oa ob">orders-gen:<br/>    build:<br/>      context: ./orders-generator<br/>    environment:<br/>      - PG_HOST=&lt;postgres host&gt;<br/>      - PG_USER=&lt;postgres username&gt;<br/>      - PG_PASSWORD=&lt;postgres password&gt;<br/>      - PG_DB=&lt;postgres db name&gt;</span></pre><p id="323b" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">希望到现在为止，您已经对架构和相关组件有了合理的理解。在深入实际问题之前，你需要注意几件事情。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="f69d" class="kg kh iq bd ki kj mg kl km kn mh kp kq jw mi jx ks jz mj ka ku kc mk kd kw kx bi translated">先决条件</h1><ul class=""><li id="029f" class="oc od iq la b lb lc le lf lh oe ll of lp og lt oh oi oj ok bi translated">你需要一个<a class="ae kf" href="https://docs.microsoft.com/azure/?product=featured&amp;WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank">微软Azure账户</a>。别担心，如果你还没有的话，你可以免费得到<a class="ae kf" href="https://azure.microsoft.com/free/?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"/>！</li><li id="9bff" class="oc od iq la b lb ol le om lh on ll oo lp op lt oh oi oj ok bi translated">安装<a class="ae kf" href="https://docs.microsoft.com/cli/azure/install-azure-cli?view=azure-cli-latest&amp;WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Azure CLI </a></li><li id="9d3c" class="oc od iq la b lb ol le om lh on ll oo lp op lt oh oi oj ok bi translated">安装<a class="ae kf" href="https://docs.docker.com/get-docker/" rel="noopener ugc nofollow" target="_blank">对接器</a>和<a class="ae kf" href="https://docs.docker.com/compose/install" rel="noopener ugc nofollow" target="_blank">对接器组合</a></li></ul><p id="7e51" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">最后，克隆这个GitHub repo:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="c767" class="nb kh iq mo b gy ny nz l oa ob">git clone https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo<br/>cd kafka-adx-postgres-cdc-demo</span></pre><p id="10ea" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">首先，让我们确保您已经设置并配置了Azure Data Explorer和PostgreSQL数据库。</p><h1 id="7857" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">设置和配置Azure数据浏览器</h1><ol class=""><li id="8f86" class="oc od iq la b lb lc le lf lh oe ll of lp og lt oq oi oj ok bi translated">创建一个Azure Data Explorer集群和一个数据库— <a class="ae kf" href="https://docs.microsoft.com/azure/data-explorer/create-cluster-database-portal?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank">这个快速入门</a>将指导你完成这个过程。</li><li id="a303" class="oc od iq la b lb ol le om lh on ll oo lp op lt oq oi oj ok bi translated">使用下面的<a class="ae kf" href="https://docs.microsoft.com/azure/data-explorer/kql-quick-reference?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> KQL </a>查询创建一个表(<code class="fe ml mm mn mo b">Orders</code>和映射(<code class="fe ml mm mn mo b">OrdersEventMapping</code>):</li></ol><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="9a74" class="nb kh iq mo b gy ny nz l oa ob">.create table Orders (orderid: string, custid: string, city: string, amount: int, purchase_time: datetime)</span><span id="ef21" class="nb kh iq mo b gy or nz l oa ob">.create table Orders ingestion json mapping 'OrdersEventMapping' '[{"column":"orderid","Properties":{"path":"$.orderid"}},{"column":"custid","Properties":{"path":"$.custid"}},{"column":"city","Properties":{"path":"$.city"}},{"column":"amount","Properties":{"path":"$.amount"}},{"column":"purchase_time","Properties":{"path":"$.purchase_time"}}]'</span></pre><p id="4a68" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在摄取过程中，Azure Data Explorer试图通过在小的入口数据块等待摄取时将它们批处理在一起来优化吞吐量——可以使用<a class="ae kf" href="https://docs.microsoft.com/azure/data-explorer/kusto/management/batchingpolicy?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank">摄取批处理策略</a>来微调这一过程。或者，出于本演示的目的，您可以按如下方式更新策略:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="b18b" class="nb kh iq mo b gy ny nz l oa ob">.alter table Orders policy ingestionbatching @'{"MaximumBatchingTimeSpan":"00:00:30", "MaximumNumberOfItems": 500, "MaximumRawDataSizeMB": 1024}'</span><span id="2a1d" class="nb kh iq mo b gy or nz l oa ob">.show table &lt;enter database name&gt;.Orders policy ingestionbatching</span></pre><blockquote class="no np nq"><p id="d310" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">详见</em> <a class="ae kf" href="https://docs.microsoft.com/azure/data-explorer/kusto/management/batching-policy?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> <em class="iq">摄取批处理策略命令参考</em> </a> <em class="iq"/></p></blockquote><p id="4c3a" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">3.创建服务主体，以便连接器进行身份验证并连接到Azure Data Explorer服务。如果你想用Azure门户来做这件事，请参考<a class="ae kf" href="https://docs.microsoft.com/azure/active-directory/develop/howto-create-service-principal-portal?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> How to:使用门户创建一个Azure AD应用和服务主体，可以访问资源</a>。以下示例使用了Azure CLI<a class="ae kf" href="https://docs.microsoft.com/cli/azure/ad/sp?view=azure-cli-latest&amp;WT.mc_id=medium-blog-abhishgu#az_ad_sp_create_for_rbac" rel="noopener ugc nofollow" target="_blank">az ad sp create-for-RBAC</a>命令。例如，创建一个名为<code class="fe ml mm mn mo b">adx-sp</code>的服务主体:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="7883" class="nb kh iq mo b gy ny nz l oa ob">az ad sp create-for-rbac -n "adx-sp"</span></pre><p id="449f" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">您将得到一个JSON响应:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="6fcf" class="nb kh iq mo b gy ny nz l oa ob">{<br/>  "appId": "fe7280c7-5705-4789-b17f-71a472340429",<br/>  "displayName": "kusto-sp",<br/>  "name": "http://kusto-sp",<br/>  "password": "29c719dd-f2b3-46de-b71c-4004fb6116ee",<br/>  "tenant": "42f988bf-86f1-42af-91ab-2d7cd011db42"<br/>}</span></pre><blockquote class="no np nq"><p id="263b" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">请记下</em> <code class="fe ml mm mn mo b"><em class="iq">appId</em></code> <em class="iq">、</em> <code class="fe ml mm mn mo b"><em class="iq">password</em></code> <em class="iq">和</em> <code class="fe ml mm mn mo b"><em class="iq">tenant</em></code> <em class="iq">，因为您将在后续步骤中使用它们</em></p></blockquote><p id="80a5" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">4.向数据库添加权限</p><p id="9518" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">为您刚刚创建的服务主体提供适当的角色。要分配<code class="fe ml mm mn mo b">admin</code>角色，<a class="ae kf" href="https://docs.microsoft.com/azure/data-explorer/manage-database-permissions?WT.mc_id=medium-blog-abhishgu#manage-permissions-in-the-azure-portal" rel="noopener ugc nofollow" target="_blank">请遵循本指南</a>来使用Azure门户或在您的数据浏览器集群中使用以下命令</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="4053" class="nb kh iq mo b gy ny nz l oa ob">.add database &lt;enter database name&gt; admins  ('aadapp=&lt;enter service principal appId&gt;;&lt;enter service principal tenant&gt;') 'AAD App'</span></pre><h1 id="4907" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">设置和配置Azure PostgreSQL数据库</h1><p id="3754" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">你可以使用各种选项在Azure上设置PostgreSQL，包括<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/quickstart-create-server-database-portal?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Azure门户</a>、<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/quickstart-create-server-database-azure-cli?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Azure CLI </a>、<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/quickstart-create-postgresql-server-database-using-azure-powershell?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Azure PowerShell </a>、<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/quickstart-create-postgresql-server-database-using-arm-template?tabs=azure-portal&amp;WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> ARM模板</a>。一旦你这样做了，你就可以用你最喜欢的编程语言，如<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/connect-java?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Java </a>、<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/connect-csharp?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank">轻松连接到数据库。NET </a>、<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/connect-nodejs?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Node.js </a>、<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/connect-python?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Python </a>、<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/connect-go?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Go </a>等。</p><blockquote class="no np nq"><p id="7db1" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">虽然上述参考资料针对单服务器部署模式，但请注意，</em> <a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/overview?WT.mc_id=medium-blog-abhishgu#azure-database-for-postgresql---hyperscale-citus" rel="noopener ugc nofollow" target="_blank"> <em class="iq">超大规模(Citus)是另一种部署模式，您可以将</em> </a> <em class="iq">用于“接近或已经超过100 GB数据的工作负载”</em></p></blockquote><p id="54af" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">请确保准备好以下PostgreSQL相关信息，因为您将需要它们来配置后续部分中的Debezium连接器—数据库主机名(和端口)、用户名、密码</p><p id="376c" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">为了让端到端解决方案按预期工作，我们需要:</p><ul class=""><li id="583b" class="oc od iq la b lb lu le lv lh os ll ot lp ou lt oh oi oj ok bi translated">确保可以从本地Kafka Connect workers(容器)访问Azure中的PostgreSQL实例</li><li id="d419" class="oc od iq la b lb ol le om lh on ll oo lp op lt oh oi oj ok bi translated">确保适当的PostrgeSQL复制设置(“逻辑”)</li><li id="8bc4" class="oc od iq la b lb ol le om lh on ll oo lp op lt oh oi oj ok bi translated">创建<code class="fe ml mm mn mo b">Orders</code>表，您将使用它来测试变更数据捕获特性</li></ul><p id="01e9" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">如果你使用Azure DB for PostgreSQL，使用<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/howto-manage-firewall-using-cli?WT.mc_id=medium-blog-abhishgu#create-firewall-rule" rel="noopener ugc nofollow" target="_blank">az postgres server firewall-rule create</a>命令创建一个防火墙规则，将你的主机列入白名单。因为我们在Docker中本地运行Kafka Connect，所以只需导航到Azure门户(<strong class="la ir">我的PostrgreSQL实例的连接安全性</strong>部分)并选择<strong class="la ir">添加当前客户端IP地址</strong>以确保您的本地IP被添加到防火墙规则中:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/8493601b8f7f4cf9037be65a280b9193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9z-Wa61waqFYbyrm.png"/></div></div></figure><p id="a7a1" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">要更改Azure DB for PostgreSQL的复制模式，可以使用<a class="ae kf" href="https://docs.microsoft.com/cli/azure/postgres/server/configuration?view=azure-cli-latest&amp;WT.mc_id=medium-blog-abhishgu#az-postgres-server-configuration-set" rel="noopener ugc nofollow" target="_blank"> az postgres服务器配置</a>命令:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="6f69" class="nb kh iq mo b gy ny nz l oa ob">az postgres server configuration set --resource-group &lt;name of resource group&gt; --server-name &lt;name of server&gt; --name azure.replication_support --value logical</span></pre><p id="a62a" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">..或者使用Azure门户中PostgreSQL实例的<strong class="la ir">复制</strong>菜单:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/8dcaef3e291e27456e89ca6a16947cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pxjLoduqZLDz6e3N.png"/></div></div></figure><p id="521a" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">更新配置后，您需要使用CLI(<a class="ae kf" href="https://docs.microsoft.com/cli/azure/postgres/server?view=azure-cli-latest&amp;WT.mc_id=medium-blog-abhishgu#az-postgres-server-restart" rel="noopener ugc nofollow" target="_blank">az postgres server restart</a>)或门户重新启动服务器。</p><p id="45bd" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">一旦数据库启动并运行，就创建表。我在这个例子中使用了CLI，但也可以随意使用其他工具。例如，通过SSL连接到Azure上的PostgreSQL数据库(系统会提示您输入密码):</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="af71" class="nb kh iq mo b gy ny nz l oa ob">psql -h &lt;POSTGRESQL_INSTANCE_NAME&gt;.postgres.database.azure.com -p 5432 -U &lt;POSTGRES_USER_NAME&gt; -W -d &lt;POSTGRES_DB_NAME&gt; --set=sslmode=require</span><span id="ba23" class="nb kh iq mo b gy or nz l oa ob">//example<br/>psql -h my-pgsql.postgres.database.azure.com -p 5432 -U foo@my-pgsql -W -d postgres --set=sslmode=require</span></pre><p id="aa0f" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">使用下面的SQL创建表:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="0386" class="nb kh iq mo b gy ny nz l oa ob">CREATE SCHEMA retail;</span><span id="b668" class="nb kh iq mo b gy or nz l oa ob">CREATE TABLE retail.orders_info (<br/>    orderid SERIAL NOT NULL PRIMARY KEY,<br/>    custid INTEGER NOT NULL,<br/>    amount INTEGER NOT NULL,<br/>    city VARCHAR(255) NOT NULL,<br/>    purchase_time VARCHAR(20) NOT NULL<br/>);</span></pre><blockquote class="no np nq"><p id="5167" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq"/><code class="fe ml mm mn mo b"><em class="iq">purchase_time</em></code><em class="iq">捕获执行购买的时间，但是它使用</em> <code class="fe ml mm mn mo b"><em class="iq">VARCHAR</em></code> <em class="iq">而不是</em> <code class="fe ml mm mn mo b"><em class="iq">TIMESTAMP</em></code> <em class="iq">类型(理想情况下)来降低整体复杂性。这是因为Debezium Postgres连接器对待时间戳数据类型的方式。)</em></p></blockquote><p id="583e" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在接下来的几节中，您将设置源(PostgreSQL)、接收器(Azure Data Explorer)连接器，并验证端到端管道。</p><h1 id="b247" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">启动码头集装箱</h1><p id="0a32" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">由于Docker Compose，启动我们的本地环境非常容易——我们所需要的只是一个命令:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="36ac" class="nb kh iq mo b gy ny nz l oa ob">docker-compose --project-name adx-kafka-cdc up --build</span></pre><p id="c3f4" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">这将与Kafka、Zookeeper和Kafka Connect workers一起构建(并启动)订单生成器应用程序容器。</p><blockquote class="no np nq"><p id="49f2" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated">下载和启动容器可能需要一段时间:这只是一个一次性的过程。</p></blockquote><p id="68d8" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">要确认是否所有容器都已启动:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="dd15" class="nb kh iq mo b gy ny nz l oa ob">docker-compose -p adx-kafka-cdc ps<br/></span><span id="1659" class="nb kh iq mo b gy or nz l oa ob">//output</span><span id="7e0c" class="nb kh iq mo b gy or nz l oa ob">                 Name                              Command             State                      Ports                   <br/>--------------------------------------------------------------------------------------------------------------------------<br/>adx-kafka-cdc_dataexplorer-connector_1   /docker-entrypoint.sh start   Up      0.0.0.0:8080-&gt;8083/tcp, 8778/tcp, 9092/tcp,<br/>                                                                               9779/tcp                                   <br/>adx-kafka-cdc_kafka_1                    /docker-entrypoint.sh start   Up      8778/tcp, 0.0.0.0:9092-&gt;9092/tcp, 9779/tcp <br/>adx-kafka-cdc_orders-gen_1               /orders-gen                   Up                                                 <br/>adx-kafka-cdc_postgres-connector_1       /docker-entrypoint.sh start   Up      0.0.0.0:9090-&gt;8083/tcp, 8778/tcp, 9092/tcp,<br/>                                                                               9779/tcp                                   <br/>adx-kafka-cdc_zookeeper_1                /docker-entrypoint.sh start   Up      0.0.0.0:2181-&gt;2181/tcp, 2888/tcp, 3888/tcp,<br/>                                                                               8778/tcp, 9779/tcp</span></pre><p id="781f" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">订单生成器应用程序将开始向PostgreSQL中的<code class="fe ml mm mn mo b">orders_info</code>表插入随机订单事件。此时，您还可以进行快速的完整性检查，以确认订单信息被持久化——在下面的示例中，我使用了<a class="ae kf" href="https://www.postgresql.org/docs/13/app-psql.html" rel="noopener ugc nofollow" target="_blank"> psql </a>:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="23c0" class="nb kh iq mo b gy ny nz l oa ob">psql -h &lt;POSTGRESQL_INSTANCE_NAME&gt;.postgres.database.azure.com -p 5432 -U &lt;POSTGRES_USER_NAME&gt; -W -d &lt;POSTGRES_DB_NAME&gt; --set=sslmode=require</span><span id="84db" class="nb kh iq mo b gy or nz l oa ob">select * from retail.orders_info order by orderid desc limit 5;</span></pre><p id="d855" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">这将为您提供最近的五个订单:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="3d97" class="nb kh iq mo b gy ny nz l oa ob">orderid | custid | amount |   city    |    purchase_time    <br/>---------+--------+--------+-----------+---------------------<br/>      10 |     77 |    140 | Seattle   | 2020-10-09 07:10:49<br/>      9  |    541 |    186 | Cleveland | 2020-10-09 07:10:46<br/>      8  |    533 |    116 | Cleveland | 2020-10-09 07:10:42<br/>      7  |    225 |    147 | Chicago   | 2020-10-09 07:10:39<br/>      6  |    819 |    184 | Austin    | 2020-10-09 07:10:36<br/>(5 rows)</span></pre><p id="84ee" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">为了将<code class="fe ml mm mn mo b">orders</code>数据传输到Kafka，我们需要配置并启动Debezium PostgreSQL源连接器的一个实例。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="70bc" class="kg kh iq bd ki kj mg kl km kn mh kp kq jw mi jx ks jz mj ka ku kc mk kd kw kx bi translated">Debezium PostgreSQL源连接器设置</h1><p id="74eb" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">将下面的JSON内容复制到一个文件中(可以命名为<code class="fe ml mm mn mo b">pg-source-config.json</code>)。请确保使用与您的PostgreSQL实例对应的值更新以下属性:<code class="fe ml mm mn mo b">database.hostname</code>、<code class="fe ml mm mn mo b">database.user</code>、<code class="fe ml mm mn mo b">database.password</code>。</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="aae9" class="nb kh iq mo b gy ny nz l oa ob">{<br/>    "name": "pg-orders-source",<br/>    "config": {<br/>        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",<br/>        "database.hostname": "&lt;enter database name&gt;.postgres.database.azure.com",<br/>        "database.port": "5432",<br/>        "database.user": "&lt;enter admin username&gt;@&lt;enter database name&gt;",<br/>        "database.password": "&lt;enter admin password&gt;",<br/>        "database.dbname": "postgres",<br/>        "database.server.name": "myserver",<br/>        "plugin.name": "wal2json",<br/>        "table.whitelist": "retail.orders_info",<br/>        "value.converter": "org.apache.kafka.connect.json.JsonConverter"<br/>    }<br/>}</span></pre><p id="9265" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在编写时，Debezium支持以下插件:<code class="fe ml mm mn mo b">decoderbufs</code>、<code class="fe ml mm mn mo b">wal2json</code>、<code class="fe ml mm mn mo b">wal2json_rds</code>、<code class="fe ml mm mn mo b">wal2json_streaming</code>、<code class="fe ml mm mn mo b">wal2json_rds_streaming</code>和<code class="fe ml mm mn mo b">pgoutput</code>。我在这个例子中使用了<code class="fe ml mm mn mo b">wal2json</code>，Azure也支持<a class="ae kf" href="https://docs.microsoft.com/azure/postgresql/concepts-logical?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="631d" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">要启动连接器，只需使用Kafka Connect REST端点来提交配置。</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="3118" class="nb kh iq mo b gy ny nz l oa ob">curl -X POST -H "Content-Type: application/json" --data @pg-source-config.json http://localhost:9090/connectors</span><span id="e133" class="nb kh iq mo b gy or nz l oa ob"># to confirm<br/>curl <a class="ae kf" href="http://localhost:9090/connectors/pg-orders-source" rel="noopener ugc nofollow" target="_blank">http://localhost:9090/connectors/pg-orders-source</a></span></pre><blockquote class="no np nq"><p id="bb56" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">注意，REST端点的端口是</em><code class="fe ml mm mn mo b"><em class="iq">9090</em></code><em class="iq">——这是在</em> <code class="fe ml mm mn mo b"><em class="iq">docker-compose.yaml</em></code>中定义的每个服务端口映射</p></blockquote><p id="cebc" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">让我们来看看Kafka主题，看看由源连接器产生的变更数据捕获事件。</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="e36b" class="nb kh iq mo b gy ny nz l oa ob">docker exec -it adx-kafka-cdc_kafka_1 bash</span></pre><p id="78a8" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">你将被丢进一个壳里(在容器里)。执行以下命令以使用Kafka中的变更数据事件:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="fa12" class="nb kh iq mo b gy ny nz l oa ob">cd bin &amp;&amp; ./kafka-console-consumer.sh --topic myserver.retail.orders_info --bootstrap-server kafka:9092 --from-beginning</span></pre><blockquote class="no np nq"><p id="3aff" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">注意，主题名</em> <code class="fe ml mm mn mo b"><em class="iq">myserver.retail.orders_info</em></code> <em class="iq">是Debezium连接器</em>使用的约定的结果</p></blockquote><p id="e34a" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">主题中的每个事件都对应一个特定的顺序。它是JSON格式的，如下图所示。请注意，有效载荷还包含整个<code class="fe ml mm mn mo b">schema</code>，为简洁起见，已将其移除。</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="5254" class="nb kh iq mo b gy ny nz l oa ob">{<br/>    "schema": {....},<br/>    "payload": {<br/>        "before": null,<br/>        "after": {<br/>            "orderid": 51,<br/>            "custid": 306,<br/>            "amount": 183,<br/>            "city": "Austin",<br/>            "purchase_time":"2020-10-09 07:23:10"<br/>        },<br/>        "source": {<br/>            "version": "1.2.1.Final",<br/>            "connector": "postgresql",<br/>            "name": "myserver",<br/>            "ts_ms": 1602057392691,<br/>            "snapshot": "false",<br/>            "db": "postgres",<br/>            "schema": "retail",<br/>            "table": "orders_info",<br/>            "txId": 653,<br/>            "lsn": 34220200,<br/>            "xmin": null<br/>        },<br/>        "op": "c",<br/>        "ts_ms": 1602057392818,<br/>        "transaction": null<br/>    }<br/>}</span></pre><p id="1184" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">到目前为止，我们已经有了前半部分的管道。让我们开始第二部分吧！</p><h1 id="3c30" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">Azure Data Explorer接收器连接器安装程序</h1><p id="f5f2" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">将下面的JSON内容复制到一个文件中(可以命名为<code class="fe ml mm mn mo b">adx-sink-config.json</code>)。根据您的Azure Data Explorer设置替换以下属性的值- <code class="fe ml mm mn mo b">aad.auth.authority</code>、<code class="fe ml mm mn mo b">aad.auth.appid</code>、<code class="fe ml mm mn mo b">aad.auth.appkey</code>、<code class="fe ml mm mn mo b">kusto.tables.topics.mapping</code>(数据库名称)和<code class="fe ml mm mn mo b">kusto.url</code></p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="07c7" class="nb kh iq mo b gy ny nz l oa ob">{<br/>    "name": "adx-orders-sink",<br/>    "config": {<br/>        "connector.class": "com.microsoft.azure.kusto.kafka.connect.sink.KustoSinkConnector",<br/>        "flush.size.bytes": 10000,<br/>        "flush.interval.ms": 30000,<br/>        "tasks.max": 2,<br/>        "topics": "myserver.retail.orders_info",<br/>        "kusto.tables.topics.mapping": "[{'topic': 'myserver.retail.orders_info','db': '&lt;enter database name&gt;', 'table': 'Orders','format': 'json', 'mapping':'OrdersEventMapping'}]",<br/>        "aad.auth.authority": "&lt;enter tenant ID from service principal info&gt;",<br/>        "kusto.url": "https://ingest-&lt;enter cluster name&gt;.&lt;enter region&gt;.kusto.windows.net",<br/>        "aad.auth.appid": "&lt;enter app ID from service principal info&gt;",<br/>        "aad.auth.appkey": "&lt;enter password from service principal info&gt;",<br/>        "key.converter": "org.apache.kafka.connect.storage.StringConverter",<br/>        "transforms": "unwrap",<br/>        "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState"<br/>    }<br/>}</span></pre><p id="d3f0" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">注意，这里使用了Kafka Connect <a class="ae kf" href="https://kafka.apache.org/documentation/#connect_transforms" rel="noopener ugc nofollow" target="_blank">单消息转换</a>(SMT)——这是Debezium提供的<code class="fe ml mm mn mo b">ExtractNewRecordState</code>转换。您可以在文档中详细阅读<a class="ae kf" href="https://debezium.io/documentation/reference/1.2/configuration/event-flattening.html" rel="noopener ugc nofollow" target="_blank"/></p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="79cf" class="nb kh iq mo b gy ny nz l oa ob">"transforms": "unwrap",<br/>"transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState"</span></pre><p id="109b" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">它从JSON有效负载中删除了<code class="fe ml mm mn mo b">schema</code>和其他部分，并只保留所需的部分。在这种情况下，我们从<code class="fe ml mm mn mo b">after</code>属性(在有效载荷中)中寻找订单信息。例如</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="b7cf" class="nb kh iq mo b gy ny nz l oa ob">{<br/>    "orderid": 51,<br/>    "custid": 306,<br/>    "amount": 183,<br/>    "city": "Austin",<br/>    "purchase_time":"2020-10-09 07:23:10"<br/>}</span></pre><p id="9849" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">当然，您可以对此进行不同的建模(在源连接器本身中应用转换)，但是这种方法有几个好处:</p><ol class=""><li id="e385" class="oc od iq la b lb lu le lv lh os ll ot lp ou lt oq oi oj ok bi translated">仅将相关数据发送到Azure Data Explorer</li><li id="ac64" class="oc od iq la b lb ol le om lh on ll oo lp op lt oq oi oj ok bi translated">Kafka主题包含整个变更数据事件(以及模式),任何下游服务都可以利用该事件</li></ol><p id="5fff" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">要安装连接器，只需像以前一样使用Kafka Connect REST端点:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="f6c0" class="nb kh iq mo b gy ny nz l oa ob">curl -X POST -H "Content-Type: application/json" --data @adx-sink-config.json http://localhost:8080/connectors</span><span id="b6c9" class="nb kh iq mo b gy or nz l oa ob"># check status<br/>curl <a class="ae kf" href="http://localhost:8080/connectors/adx-orders-sink/status" rel="noopener ugc nofollow" target="_blank">http://localhost:8080/connectors/adx-orders-sink/status</a></span></pre><blockquote class="no np nq"><p id="5b6c" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">注意，REST端点的端口是</em><code class="fe ml mm mn mo b"><em class="iq">8080</em></code><em class="iq">——这是在</em> <code class="fe ml mm mn mo b"><em class="iq">docker-compose.yaml</em></code>中定义的每个服务端口映射</p></blockquote><p id="f78f" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">连接器应该投入运行，向Azure Data Explorer进行身份验证，并开始批处理接收过程。</p><blockquote class="no np nq"><p id="6ba8" class="ky kz nn la b lb lu jr ld le lv ju lg nr lw lj lk ns lx ln lo nt ly lr ls lt ij bi translated"><em class="iq">注意</em><code class="fe ml mm mn mo b"><em class="iq">flush.size.bytes</em></code><em class="iq"/><code class="fe ml mm mn mo b"><em class="iq">flush.interval.ms</em></code><em class="iq">用于调节配料过程。请参考</em> <a class="ae kf" href="https://github.com/Azure/kafka-sink-azure-kusto/blob/master/README.md#5-sink-properties" rel="noopener ugc nofollow" target="_blank"> <em class="iq">连接器文档</em> </a> <em class="iq">了解各个属性的详细信息。</em></p></blockquote><p id="7f89" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">由于连接器的刷新配置和Azure Data Explorer中的<code class="fe ml mm mn mo b">Orders</code>表的批处理策略相当激进(出于演示目的)，您应该会看到数据快速流入Data Explorer。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="b399" class="kg kh iq bd ki kj mg kl km kn mh kp kq jw mi jx ks jz mj ka ku kc mk kd kw kx bi translated">查询Azure数据浏览器</h1><p id="8cf8" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">您可以在数据资源管理器中查询Orders表来分割数据。下面是一些简单的查询。</p><p id="7207" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">获取纽约市订单的详细信息；</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="bc76" class="nb kh iq mo b gy ny nz l oa ob">Orders<br/>| where city == 'New York'</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/79656e67f5e33e0e798657e6200869d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WtQus-sObRR9j7lr.png"/></div></div></figure><p id="644a" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">只获取来自纽约市的订单的购买金额和时间，并按金额排序</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="baa5" class="nb kh iq mo b gy ny nz l oa ob">Orders<br/>| where city == 'New York'<br/>| project amount, purchase_time<br/>| sort by amount</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ow"><img src="../Images/5f59379531c7363074faa951df287d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YkCgq4YIS8engcr8.png"/></div></div></figure><p id="a83b" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">找出每个城市的平均销售额，并用柱形图表示出来:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="e6c3" class="nb kh iq mo b gy ny nz l oa ob">Orders<br/>| summarize avg_sales = avg(amount) by city <br/>| render columnchart</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/77c513ffc11ecc99d7a752023a8a47f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vFILfOtROdR4qfrX.png"/></div></div></figure><p id="d6a4" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">每个城市的总购买量，以饼图表示:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="ca45" class="nb kh iq mo b gy ny nz l oa ob">Orders <br/>| summarize total = sum(amount) by city <br/>| sort by total<br/>| render piechart</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/9fe1afb8e9b853dfc5466e3062fe18ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zZEiV3oPDpAtMzMw.png"/></div></div></figure><p id="5bf9" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">每个城市的订单数量，以折线图表示:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="802c" class="nb kh iq mo b gy ny nz l oa ob">Orders<br/>| summarize orders = count() by city<br/>| sort by orders<br/>| render linechart</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/e79489461c060fa1b53d3b0e81ea9e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*la9GeTXw_zzCERbu.png"/></div></div></figure><p id="6823" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">购买量在一天内如何变化？</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="af53" class="nb kh iq mo b gy ny nz l oa ob">Orders<br/>| extend hour = floor(purchase_time % 1d , 10m)<br/>| summarize event_count=count() by hour<br/>| sort by hour asc<br/>| render timechart</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/bd2559569db163b375fc0af40d63b6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*R8ekLpZ5PcD_NyTL.png"/></div></div></figure><p id="f5d0" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在不同的城市，一天之内它是如何变化的？</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="1fd7" class="nb kh iq mo b gy ny nz l oa ob">Orders<br/>| extend hour= floor( purchase_time % 1d , 10m)<br/>| where city in ("New Delhi", "Seattle", "New York", "Austin", "Chicago", "Cleveland")<br/>| summarize event_count=count() by hour, city<br/>| render columnchart</span></pre><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/69c61b5678dfd84fe6ce822eb9b66bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SrVkmUQIF8sDUPTO.png"/></div></div></figure><h2 id="cf7b" class="nb kh iq bd ki nc nd dn km ne nf dp kq lh ng nh ks ll ni nj ku lp nk nl kw nm bi translated">Azure数据浏览器仪表板</h2><p id="6985" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">了解如何<a class="ae kf" href="https://docs.microsoft.com/en-us/azure/data-explorer/azure-data-explorer-dashboards" rel="noopener ugc nofollow" target="_blank">使用Azure Data Explorer仪表盘可视化数据</a></p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/41f14ec1f0ae1628eacbb4fc71129dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TKf78nJHtdyGXQoJ.png"/></div></div></figure><h1 id="17c2" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">打扫</h1><p id="c4c8" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">要停止容器，您可以:</p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="2dc2" class="nb kh iq mo b gy ny nz l oa ob">docker-compose -p adx-kafka-cdc down -v</span></pre><p id="ac51" class="pw-post-body-paragraph ky kz iq la b lb lu jr ld le lv ju lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">要删除Azure Data Explorer集群/数据库，请使用<a class="ae kf" href="https://docs.microsoft.com/cli/azure/kusto/cluster?view=azure-cli-latest&amp;WT.mc_id=medium-blog-abhishgu#az-kusto-cluster-delete" rel="noopener ugc nofollow" target="_blank"> az集群删除</a>或<a class="ae kf" href="https://docs.microsoft.com/cli/azure/kusto/database?view=azure-cli-latest&amp;WT.mc_id=medium-blog-abhishgu#az-kusto-database-delete" rel="noopener ugc nofollow" target="_blank"> az kusto数据库删除</a>。对于PostgreSQL，只需使用<a class="ae kf" href="https://docs.microsoft.com/cli/azure/postgres/server?view=azure-cli-latest&amp;WT.mc_id=medium-blog-abhishgu#az_postgres_server_delete" rel="noopener ugc nofollow" target="_blank"> az postgres服务器删除</a></p><pre class="mq mr ms mt gt nu mo nv nw aw nx bi"><span id="198b" class="nb kh iq mo b gy ny nz l oa ob">az postgres server delete -g &lt;resource group name&gt; -n &lt;server name&gt;<br/>az kusto cluster delete -n &lt;cluster name&gt; -g &lt;resource group name&gt;<br/>az kusto database delete -n &lt;database name&gt; --cluster-name &lt;cluster name&gt; -g &lt;resource group name&gt;</span></pre></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><h1 id="9c84" class="kg kh iq bd ki kj mg kl km kn mh kp kq jw mi jx ks jz mj ka ku kc mk kd kw kx bi translated">结论</h1><p id="e5db" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">Kafka Connect帮助您构建可扩展的数据管道，而无需编写定制的管道代码。您主要需要设置、配置和操作连接器。请记住，Kafka Connect worker实例只是JVM进程，根据您的规模和需求，您可以选择使用<a class="ae kf" href="https://docs.microsoft.com/azure/aks/?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Azure Kubernetes服务</a>来操作它们。因为Kafka Connect实例是无状态的实体，所以您在集群工作负载的拓扑和规模方面有很大的自由度！</p><h2 id="ba94" class="nb kh iq bd ki nc nd dn km ne nf dp kq lh ng nh ks ll ni nj ku lp nk nl kw nm bi translated">额外资源</h2><p id="4007" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如果你想进一步探索，我推荐</p><ul class=""><li id="d23f" class="oc od iq la b lb lu le lv lh os ll ot lp ou lt oh oi oj ok bi translated">探索<a class="ae kf" href="https://debezium.io/documentation/reference/1.2/connectors/index.html" rel="noopener ugc nofollow" target="_blank"> Debezium连接器</a></li><li id="663b" class="oc od iq la b lb ol le om lh on ll oo lp op lt oh oi oj ok bi translated">Azure Data Explorer <a class="ae kf" href="https://docs.microsoft.com/en-us/azure/data-explorer/ingest-data-overview" rel="noopener ugc nofollow" target="_blank">摄取概述</a></li><li id="13b1" class="oc od iq la b lb ol le om lh on ll oo lp op lt oh oi oj ok bi translated">探索使用Kusto查询语言能做些什么</li><li id="f212" class="oc od iq la b lb ol le om lh on ll oo lp op lt oh oi oj ok bi translated"><a class="ae kf" href="https://github.com/Azure/kafka-sink-azure-kusto/blob/master/README.md#3-features-supported" rel="noopener ugc nofollow" target="_blank">数据浏览器连接器特性</a></li><li id="f45f" class="oc od iq la b lb ol le om lh on ll oo lp op lt oh oi oj ok bi translated">什么是<a class="ae kf" href="https://docs.microsoft.com/azure/event-hubs/event-hubs-about?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank">蔚蓝事件中心</a>？</li><li id="6d35" class="oc od iq la b lb ol le om lh on ll oo lp op lt oh oi oj ok bi translated">在Azure Event Hubs上使用带有<a class="ae kf" href="https://docs.microsoft.com/azure/event-hubs/event-hubs-kafka-connect-debezium?WT.mc_id=medium-blog-abhishgu" rel="noopener ugc nofollow" target="_blank"> Kafka Connect支持的变更数据捕获(预览)</a></li></ul></div></div>    
</body>
</html>